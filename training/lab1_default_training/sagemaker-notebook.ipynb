{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1: Finetuning HuggingFace models with Amazon SageMaker\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the first Lab and our end-to-end multi-class Text-Classification example. In this Lab, we will use the Hugging Faces `transformers` and `datasets` library together with Amazon SageMaker to fine-tune a pre-trained transformer on text classification. In particular, the pre-trained model will be fine-tuned using the `amazon_us_reviews` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we install the required libraries from Hugging Face and AWS. Additionally, we make sure we have a compatible PyTorch version installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.7/site-packages (2.49.1)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Using cached sagemaker-2.56.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (19.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.17.3)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.18.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.5.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.21.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.2 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker>=2.48.0) (1.21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker>=2.48.0) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker>=2.48.0) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.2->boto3>=1.16.32->sagemaker>=2.48.0) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.2->boto3>=1.16.32->sagemaker>=2.48.0) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.48.0) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.48.0) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.0)\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.49.1\n",
      "    Uninstalling sagemaker-2.49.1:\n",
      "      Successfully uninstalled sagemaker-2.49.1\n",
      "Successfully installed sagemaker-2.56.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "As dataset are we going to use the [amazon_us_reviews](https://huggingface.co/datasets/amazon_us_reviews).\n",
    "\n",
    "the dataset contains the customer review text with accompanying metadata, consisting of three major components:\n",
    "\n",
    "1. A collection of reviews written in the Amazon.com marketplace and associated metadata from 1995 until 2015. This is intended to facilitate study into the properties (and the evolution) of customer reviews potentially including how people evaluate and express their experiences with respect to products at scale. (130M+ customer reviews)\n",
    "2. A collection of reviews about products in multiple languages from different Amazon marketplaces, intended to facilitate analysis of customers’ perception of the same products and wider consumer preferences across languages and countries. (200K+ customer reviews in 5 countries)\n",
    "3. A collection of reviews that have been identified as non-compliant with respect to Amazon policies. This is intended to provide a reference dataset for research on detecting promotional or biased reviews. (several thousand customer reviews). This part of the dataset is distributed separately and is available upon request – please contact the email address below if you are interested in obtaining this dataset.  \n",
    "\n",
    "_https://s3.amazonaws.com/amazon-reviews-pds/readme.html_\n",
    "\n",
    "The Lab already includes a downsampled train dataset `data/amazon_us_reviews_apparel_v1_00_train.json` and downsampled test dataset `data/amazon_us_reviews_apparel_v1_00_test.json`. The train dataset contains 29750 rows and the test dataset 5250. \n",
    "For the dataset files the `Apparel_v1_00` split was used. \n",
    "\n",
    "In Addition to the dataset files the repository also contains the script of how these datasets has been generated. You can find the script at `data/create_dataset.py`. You can use this to change for example the size or category split of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\n",
      "\n",
      "\u001b[34massert\u001b[39;49;00m \u001b[36mfloat\u001b[39;49;00m(datasets.__version__[:\u001b[34m3\u001b[39;49;00m]) >= \u001b[34m1.8\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m`datasets 1.8.0` or higher need to be installed to generate dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33mThis script is creating a sample dataset for the workshop using the `datasets` library and the \"amazon_us_reviews\" \u001b[39;49;00m\n",
      "\u001b[33mdataset. You can configure which dataset split should be used, by default it is the apparel split. You can also\u001b[39;49;00m\n",
      "\u001b[33mconfigure the size of the dataset, which is generated. The script creates 2 json files one for training and one for\u001b[39;49;00m\n",
      "\u001b[33mtesting, which need to be uploaded to s3 for the workshop.\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# configuration\u001b[39;49;00m\n",
      "dataset_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mamazon_us_reviews\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "dataset_split = \u001b[33m\"\u001b[39;49;00m\u001b[33mApparel_v1_00\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "train_dataset_length = \u001b[34m35_000\u001b[39;49;00m\n",
      "test_split_size = \u001b[34m0.15\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# load dataset using datasets library, using the Apperal split.\u001b[39;49;00m\n",
      "\u001b[37m# full information can be found here: https://huggingface.co/datasets/amazon_us_reviews\u001b[39;49;00m\n",
      "dataset = load_dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mamazon_us_reviews\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mApparel_v1_00\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# since there is only a \"tran\" split assign it as dataset\u001b[39;49;00m\n",
      "dataset = dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal dataset contains: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(dataset)} rows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# remove unnecessary columns from dataset\u001b[39;49;00m\n",
      "remove_columns = [\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "]\n",
      "dataset = dataset.remove_columns(remove_columns)\n",
      "\n",
      "\u001b[37m# rename columns to match schema\u001b[39;49;00m\n",
      "dataset = dataset.rename_column(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "dataset = dataset.rename_column(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mThe dataset features are now \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlist(dataset.features.keys())}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# shuffle dataset and select x samples\u001b[39;49;00m\n",
      "sampled_dataset = dataset.shuffle().select(\u001b[36mrange\u001b[39;49;00m(train_dataset_length))\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msampled dataset contains: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(sampled_dataset)} rows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# split sampled dataset into test and train split\u001b[39;49;00m\n",
      "processed_dataset_dict = sampled_dataset.train_test_split(test_size=test_split_size)\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain dataset contains: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(processed_dataset_dict[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m])} rows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtest dataset contains: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(processed_dataset_dict[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m])} rows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[37m# save datasets as json for uploading to s3\u001b[39;49;00m\n",
      "processed_dataset_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to_json(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{dataset_name}\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mdataset_split.lower()}_train.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "processed_dataset_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to_json(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{dataset_name}\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mdataset_split.lower()}_test.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../data/create_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "Upload the `dataset` files to the default bucket in Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset uploaded to: s3://sagemaker-us-east-1-558105141721/lab1/data/amazon_us_reviews_apparel_v1_00_train.json\n",
      "test dataset uploaded to: s3://sagemaker-us-east-1-558105141721/lab1/data/amazon_us_reviews_apparel_v1_00_test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "local_train_dataset = \"amazon_us_reviews_apparel_v1_00_train.json\"\n",
    "local_test_dataset = \"amazon_us_reviews_apparel_v1_00_test.json\"\n",
    "\n",
    "# s3 uris for datasets\n",
    "remote_train_dataset = f\"s3://{sess.default_bucket()}/lab1/data\"\n",
    "remote_test_dataset = f\"s3://{sess.default_bucket()}/lab1/data\"\n",
    "\n",
    "# upload datasets\n",
    "S3Uploader.upload(os.path.join(\"../data\",local_train_dataset),remote_train_dataset)\n",
    "S3Uploader.upload(os.path.join(\"../data\",local_test_dataset),remote_test_dataset)\n",
    "\n",
    "print(f\"train dataset uploaded to: {remote_train_dataset}/{local_train_dataset}\")\n",
    "print(f\"test dataset uploaded to: {remote_test_dataset}/{local_test_dataset}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create our sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles all end-to-end Amazon SageMaker training and deployment tasks. In the Estimator we define, which fine-tuning script (`entry_point`) should be used, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing the required ec2 instances for us, providing the fine-tuning script `train.py` and downloading the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. When starting the training SageMaer executes the following command:\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The CLI arguments you see are passed in as `hyperparameters`, when creating the `HuggingFace` estimator.\n",
    "\n",
    "Sagemaker is also providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script that performs fine tuning is located here: `training/scripts/train.py`. Navigate to the source code location and open the `train.py` file. You can also go through it's contents by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_file\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mamazon_us_reviews_apparel_v1_00_train.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_file\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mamazon_us_reviews_apparel_v1_00_test.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    raw_train_dataset = load_dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, data_files=os.path.join(args.training_dir, args.train_file))[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    raw_test_dataset = load_dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, data_files=os.path.join(args.test_dir, args.test_file))[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# load tokenizer\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)\n",
      "\n",
      "    \u001b[37m# preprocess function, tokenizes text\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpreprocess_function\u001b[39;49;00m(examples):\n",
      "        \u001b[34mreturn\u001b[39;49;00m tokenizer(examples[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], padding=\u001b[34mTrue\u001b[39;49;00m, max_length=\u001b[34mTrue\u001b[39;49;00m, truncation=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# preprocess dataset\u001b[39;49;00m\n",
      "    train_dataset = raw_train_dataset.map(\n",
      "        preprocess_function,\n",
      "        batched=\u001b[34mTrue\u001b[39;49;00m,\n",
      "    )\n",
      "    test_dataset = raw_test_dataset.map(\n",
      "        preprocess_function,\n",
      "        batched=\u001b[34mTrue\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# define labels\u001b[39;49;00m\n",
      "    num_labels = \u001b[36mlen\u001b[39;49;00m(train_dataset.unique(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "\n",
      "    \u001b[37m# print size\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(train_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(test_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        labels = pred.label_ids\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m\"\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        acc = accuracy_score(labels, preds)\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: acc, \u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: f1, \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: precision, \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: recall}\n",
      "\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.model_id, num_labels=num_labels)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.output_data_dir}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        load_best_model_at_end=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        metric_for_best_model=\u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{key}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{value}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 3,                          # number of training epochs\n",
    "                 'train_batch_size': 16,               # batch size for training\n",
    "                 'eval_batch_size': 32,                # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                # learning rate used during training\n",
    "                 'model_id':'distilbert-base-uncased', # pre-trained model\n",
    "                 'train_file': local_train_dataset,    # training dataset\n",
    "                 'test_file': local_test_dataset,      # test dataset\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of supported models: https://huggingface.co/models?library=pytorch,transformers&sort=downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `metric_definition` dictionary that contains regex-based definitions that will be used to parse the job logs and extract metrics. You can read more about parsing the cloudwatch logs [here](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'eval_loss',               'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy',           'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1',                 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision',          'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-workshop-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = '../scripts',      # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6',             # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "    metric_definitions   = metric_definitions # the metrics regex definitions to extract logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-30 13:26:58 Starting - Starting the training job...\n",
      "2021-08-30 13:27:21 Starting - Launching requested ML instancesProfilerReport-1630330018: InProgress\n",
      "...\n",
      "2021-08-30 13:27:55 Starting - Preparing the instances for training............\n",
      "2021-08-30 13:29:58 Downloading - Downloading input data\n",
      "2021-08-30 13:29:58 Training - Downloading the training image..................\n",
      "2021-08-30 13:32:51 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:51,834 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:51,858 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:51,865 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:52,253 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 16,\n",
      "        \"train_file\": \"amazon_us_reviews_apparel_v1_00_train.json\",\n",
      "        \"model_id\": \"distilbert-base-uncased\",\n",
      "        \"test_file\": \"amazon_us_reviews_apparel_v1_00_test.json\",\n",
      "        \"epochs\": 3,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"eval_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"eval_batch_size\":32,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"test_file\":\"amazon_us_reviews_apparel_v1_00_test.json\",\"train_batch_size\":16,\"train_file\":\"amazon_us_reviews_apparel_v1_00_train.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"eval_batch_size\":32,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"test_file\":\"amazon_us_reviews_apparel_v1_00_test.json\",\"train_batch_size\":16,\"train_file\":\"amazon_us_reviews_apparel_v1_00_train.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--eval_batch_size\",\"32\",\"--learning_rate\",\"3e-05\",\"--model_id\",\"distilbert-base-uncased\",\"--test_file\",\"amazon_us_reviews_apparel_v1_00_test.json\",\"--train_batch_size\",\"16\",\"--train_file\",\"amazon_us_reviews_apparel_v1_00_train.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=amazon_us_reviews_apparel_v1_00_train.json\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=amazon_us_reviews_apparel_v1_00_test.json\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 3 --eval_batch_size 32 --learning_rate 3e-05 --model_id distilbert-base-uncased --test_file amazon_us_reviews_apparel_v1_00_test.json --train_batch_size 16 --train_file amazon_us_reviews_apparel_v1_00_train.json\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,006 - datasets.builder - WARNING - Using custom data configuration default-5f1863f34eaff146\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-5f1863f34eaff146/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5f1863f34eaff146/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,105 - datasets.builder - WARNING - Using custom data configuration default-44388daae09ade12\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-44388daae09ade12/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-44388daae09ade12/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,150 - filelock - INFO - Lock 140309344959736 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,175 - filelock - INFO - Lock 140309344959736 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,197 - filelock - INFO - Lock 140309344959736 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,226 - filelock - INFO - Lock 140309344959736 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,250 - filelock - INFO - Lock 140309121264664 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,286 - filelock - INFO - Lock 140309121264664 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,353 - filelock - INFO - Lock 140309121264664 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:32:57,375 - filelock - INFO - Lock 140309121264664 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:33:06,293 - __main__ - INFO -  loaded train_dataset length is: 29750\u001b[0m\n",
      "\u001b[34m2021-08-30 13:33:06,293 - __main__ - INFO -  loaded test_dataset length is: 5250\u001b[0m\n",
      "\u001b[34m2021-08-30 13:33:06,340 - filelock - INFO - Lock 140309121262704 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-08-30 13:33:10,964 - filelock - INFO - Lock 140309121262704 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\n",
      "2021-08-30 13:33:23 Uploading - Uploading generated training model\u001b[34m[2021-08-30 13:33:16.554 algo-1:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.706 algo-1:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.707 algo-1:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.708 algo-1:25 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.710 algo-1:25 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.710 algo-1:25 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.899 algo-1:25 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.899 algo-1:25 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.899 algo-1:25 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.900 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.901 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.902 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.903 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.904 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.905 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.906 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.907 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.908 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.909 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:classifier.weight count_params:3840\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:591] name:classifier.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.910 algo-1:25 INFO hook.py:593] Total Trainable Params: 66957317\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.911 algo-1:25 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-08-30 13:33:16.913 algo-1:25 INFO hook.py:488] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 649kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 40.4MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 33.7kB/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/30 [00:00<?, ?ba/s]#015  3%|▎         | 1/30 [00:00<00:15,  1.89ba/s]#015  7%|▋         | 2/30 [00:00<00:11,  2.41ba/s]#015 10%|█         | 3/30 [00:00<00:09,  2.81ba/s]#015 13%|█▎        | 4/30 [00:01<00:07,  3.35ba/s]#015 17%|█▋        | 5/30 [00:01<00:06,  3.84ba/s]#015 20%|██        | 6/30 [00:01<00:05,  4.52ba/s]#015 23%|██▎       | 7/30 [00:01<00:04,  4.64ba/s]#015 27%|██▋       | 8/30 [00:01<00:04,  5.30ba/s]#015 33%|███▎      | 10/30 [00:01<00:03,  5.91ba/s]#015 37%|███▋      | 11/30 [00:02<00:03,  5.48ba/s]#015 40%|████      | 12/30 [00:02<00:03,  5.38ba/s]#015 43%|████▎     | 13/30 [00:02<00:02,  5.79ba/s]#015 47%|████▋     | 14/30 [00:02<00:02,  5.69ba/s]#015 50%|█████     | 15/30 [00:03<00:03,  3.94ba/s]#015 53%|█████▎    | 16/30 [00:03<00:03,  3.79ba/s]#015 57%|█████▋    | 17/30 [00:03<00:02,  4.34ba/s]#015 60%|██████    | 18/30 [00:03<00:02,  4.52ba/s]#015 63%|██████▎   | 19/30 [00:03<00:02,  5.07ba/s]#015 67%|██████▋   | 20/30 [00:04<00:01,  5.33ba/s]#015 70%|███████   | 21/30 [00:04<00:01,  5.75ba/s]#015 73%|███████▎  | 22/30 [00:04<00:01,  5.71ba/s]#015 77%|███████▋  | 23/30 [00:06<00:04,  1.47ba/s]#015 80%|████████  | 24/30 [00:06<00:03,  1.91ba/s]#015 83%|████████▎ | 25/30 [00:06<00:02,  1.87ba/s]#015 87%|████████▋ | 26/30 [00:07<00:01,  2.38ba/s]#015 90%|█████████ | 27/30 [00:07<00:01,  2.65ba/s]#015 93%|█████████▎| 28/30 [00:07<00:00,  3.17ba/s]#015 97%|█████████▋| 29/30 [00:07<00:00,  3.42ba/s]#015100%|██████████| 30/30 [00:07<00:00,  3.81ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/6 [00:00<?, ?ba/s]#015 17%|█▋        | 1/6 [00:00<00:00,  5.86ba/s]#015 33%|███▎      | 2/6 [00:00<00:00,  5.93ba/s]#015 50%|█████     | 3/6 [00:00<00:00,  5.50ba/s]#015 67%|██████▋   | 4/6 [00:00<00:00,  5.08ba/s]#015 83%|████████▎ | 5/6 [00:00<00:00,  5.37ba/s]#015100%|██████████| 6/6 [00:00<00:00,  6.14ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.65M/268M [00:00<00:05, 46.5MB/s]#015Downloading:   3%|▎         | 9.08M/268M [00:00<00:05, 45.8MB/s]#015Downloading:   5%|▌         | 13.5M/268M [00:00<00:05, 45.4MB/s]#015Downloading:   7%|▋         | 19.3M/268M [00:00<00:05, 48.5MB/s]#015Downloading:   9%|▉         | 25.0M/268M [00:00<00:04, 50.8MB/s]#015Downloading:  12%|█▏        | 30.9M/268M [00:00<00:04, 52.9MB/s]#015Downloading:  13%|█▎        | 35.7M/268M [00:00<00:04, 49.1MB/s]#015Downloading:  15%|█▌        | 40.8M/268M [00:00<00:04, 49.6MB/s]#015Downloading:  17%|█▋        | 46.4M/268M [00:00<00:04, 51.4MB/s]#015Downloading:  19%|█▉        | 51.4M/268M [00:01<00:04, 50.3MB/s]#015Downloading:  21%|██▏       | 57.4M/268M [00:01<00:03, 52.8MB/s]#015Downloading:  24%|██▎       | 63.3M/268M [00:01<00:03, 54.6MB/s]#015Downloading:  26%|██▌       | 69.4M/268M [00:01<00:03, 56.4MB/s]#015Downloading:  28%|██▊       | 75.2M/268M [00:01<00:03, 57.0MB/s]#015Downloading:  30%|███       | 81.4M/268M [00:01<00:03, 58.4MB/s]#015Downloading:  33%|███▎      | 87.6M/268M [00:01<00:03, 59.4MB/s]#015Downloading:  35%|███▍      | 93.7M/268M [00:01<00:02, 59.7MB/s]#015Downloading:  37%|███▋      | 99.7M/268M [00:01<00:02, 60.0MB/s]#015Downloading:  39%|███▉      | 106M/268M [00:01<00:02, 60.2MB/s] #015Downloading:  42%|████▏     | 112M/268M [00:02<00:02, 60.7MB/s]#015Downloading:  44%|████▍     | 118M/268M [00:02<00:02, 61.1MB/s]#015Downloading:  46%|████▋     | 124M/268M [00:02<00:02, 60.8MB/s]#015Downloading:  49%|████▊     | 130M/268M [00:02<00:02, 61.0MB/s]#015Downloading:  51%|█████     | 137M/268M [00:02<00:02, 61.3MB/s]#015Downloading:  53%|█████▎    | 143M/268M [00:02<00:02, 61.6MB/s]#015Downloading:  56%|█████▌    | 149M/268M [00:02<00:01, 61.8MB/s]#015Downloading:  58%|█████▊    | 155M/268M [00:02<00:01, 61.8MB/s]#015Downloading:  60%|██████    | 162M/268M [00:02<00:01, 61.9MB/s]#015Downloading:  63%|██████▎   | 168M/268M [00:02<00:01, 62.1MB/s]#015Downloading:  65%|██████▍   | 174M/268M [00:03<00:01, 62.1MB/s]#015Downloading:  67%|██████▋   | 180M/268M [00:03<00:01, 57.5MB/s]#015Downloading:  69%|██████▉   | 186M/268M [00:03<00:01, 57.6MB/s]#015Downloading:  72%|███████▏  | 192M/268M [00:03<00:01, 58.8MB/s]#015Downloading:  74%|███████▍  | 198M/268M [00:03<00:01, 59.7MB/s]#015Downloading:  76%|███████▋  | 205M/268M [00:03<00:01, 60.3MB/s]#015Downloading:  79%|███████▊  | 211M/268M [00:03<00:00, 60.6MB/s]#015Downloading:  81%|████████  | 217M/268M [00:03<00:00, 61.1MB/s]#015Downloading:  83%|████████▎ | 223M/268M [00:03<00:00, 61.6MB/s]#015Downloading:  86%|████████▌ | 229M/268M [00:03<00:00, 61.2MB/s]#015Downloading:  88%|████████▊ | 236M/268M [00:04<00:00, 61.0MB/s]#015Downloading:  90%|█████████ | 242M/268M [00:04<00:00, 61.5MB/s]#015Downloading:  93%|█████████▎| 248M/268M [00:04<00:00, 61.0MB/s]#015Downloading:  95%|█████████▍| 254M/268M [00:04<00:00, 61.4MB/s]#015Downloading:  97%|█████████▋| 260M/268M [00:04<00:00, 61.7MB/s]#015Downloading:  99%|█████████▉| 267M/268M [00:04<00:00, 59.8MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5580 [00:00<?, ?it/s]/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 107, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1272, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1734, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1766, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 624, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 487, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 307, in forward\n",
      "    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 254, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 177, in forward\n",
      "    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5580 [00:01<?, ?it/s]\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-30 13:33:18,325 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 train.py --epochs 3 --eval_batch_size 32 --learning_rate 3e-05 --model_id distilbert-base-uncased --test_file amazon_us_reviews_apparel_v1_00_test.json --train_batch_size 16 --train_file amazon_us_reviews_apparel_v1_00_train.json\"\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 483/483 [00:00<00:00, 649kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 466k/466k [00:00<00:00, 40.4MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 28.0/28.0 [00:00<00:00, 33.7kB/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/30 [00:00<?, ?ba/s]#015  3%|â         | 1/30 [00:00<00:15,  1.89ba/s]#015  7%|â         | 2/30 [00:00<00:11,  2.41ba/s]#015 10%|â         | 3/30 [00:00<00:09,  2.81ba/s]#015 13%|ââ        | 4/30 [00:01<00:07,  3.35ba/s]#015 17%|ââ        | 5/30 [00:01<00:06,  3.84ba/s]#015 20%|ââ        | 6/30 [00:01<00:05,  4.52ba/s]#015 23%|âââ       | 7/30 [00:01<00:04,  4.64ba/s]#015 27%|âââ       | 8/30 [00:01<00:04,  5.30ba/s]#015 33%|ââââ      | 10/30 [00:01<00:03,  5.91ba/s]#015 37%|ââââ      | 11/30 [00:02<00:03,  5.48ba/s]#015 40%|ââââ      | 12/30 [00:02<00:03,  5.38ba/s]#015 43%|âââââ     | 13/30 [00:02<00:02,  5.79ba/s]#015 47%|âââââ     | 14/30 [00:02<00:02,  5.69ba/s]#015 50%|âââââ     | 15/30 [00:03<00:03,  3.94ba/s]#015 53%|ââââââ    | 16/30 [00:03<00:03,  3.79ba/s]#015 57%|ââââââ    | 17/30 [00:03<00:02,  4.34ba/s]#015 60%|ââââââ    | 18/30 [00:03<00:02,  4.52ba/s]#015 63%|âââââââ   | 19/30 [00:03<00:02,  5.07ba/s]#015 67%|âââââââ   | 20/30 [00:04<00:01,  5.33ba/s]#015 70%|âââââââ   | 21/30 [00:04<00:01,  5.75ba/s]#015 73%|ââââââââ  | 22/30 [00:04<00:01,  5.71ba/s]#015 77%|ââââââââ  | 23/30 [00:06<00:04,  1.47ba/s]#015 80%|ââââââââ  | 24/30 [00:06<00:03,  1.91ba/s]#015 83%|âââââââââ | 25/30 [00:06<00:02,  1.87ba/s]#015 87%|âââââââââ | 26/30 [00:07<00:01,  2.38ba/s]#015 90%|âââââââââ | 27/30 [00:07<00:01,  2.65ba/s]#015 93%|ââââââââââ| 28/30 [00:07<00:00,  3.17ba/s]#015 97%|ââââââââââ| 29/30 [00:07<00:00,  3.42ba/s]#015100%|ââââââââââ| 30/30 [00:07<00:00,  3.81ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/6 [00:00<?, ?ba/s]#015 17%|ââ        | 1/6 [00:00<00:00,  5.86ba/s]#015 33%|ââââ      | 2/6 [00:00<00:00,  5.93ba/s]#015 50%|âââââ     | 3/6 [00:00<00:00,  5.50ba/s]#015 67%|âââââââ   | 4/6 [00:00<00:00,  5.08ba/s]#015 83%|âââââââââ | 5/6 [00:00<00:00,  5.37ba/s]#015100%|ââââââââââ| 6/6 [00:00<00:00,  6.14ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|â         | 4.65M/268M [00:00<00:05, 46.5MB/s]#015Downloading:   3%|â         | 9.08M/268M [00:00<00:05, 45.8MB/s]#015Downloading:   5%|â         | 13.5M/268M [00:00<00:05, 45.4MB/s]#015Downloading:   7%|â         | 19.3M/268M [00:00<00:05, 48.5MB/s]#015Downloading:   9%|â         | 25.0M/268M [00:00<00:04, 50.8MB/s]#015Downloading:  12%|ââ        | 30.9M/268M [00:00<00:04, 52.9MB/s]#015Downloading:  13%|ââ        | 35.7M/268M [00:00<00:04, 49.1MB/s]#015Downloading:  15%|ââ        | 40.8M/268M [00:00<00:04, 49.6MB/s]#015Downloading:  17%|ââ        | 46.4M/268M [00:00<00:04, 51.4MB/s]#015Downloading:  19%|ââ        | 51.4M/268M [00:01<00:04, 50.3MB/s]#015Downloading:  21%|âââ       | 57.4M/268M [00:01<00:03, 52.8MB/s]#015Downloading:  24%|âââ       | 63.3M/268M [00:01<00:03, 54.6MB/s]#015Downloading:  26%|âââ       | 69.4M/268M [00:01<00:03, 56.4MB/s]#015Downloading:  28%|âââ       | 75.2M/268M [00:01<00:03, 57.0MB/s]#015Downloading:  30%|âââ       | 81.4M/268M [00:01<00:03, 58.4MB/s]#015Downloading:  33%|ââââ      | 87.6M/268M [00:01<00:03, 59.4MB/s]#015Downloading:  35%|ââââ      | 93.7M/268M [00:01<00:02, 59.7MB/s]#015Downloading:  37%|ââââ      | 99.7M/268M [00:01<00:02, 60.0MB/s]#015Downloading:  39%|ââââ      | 106M/268M [00:01<00:02, 60.2MB/s] #015Downloading:  42%|âââââ     | 112M/268M [00:02<00:02, 60.7MB/s]#015Downloading:  44%|âââââ     | 118M/268M [00:02<00:02, 61.1MB/s]#015Downloading:  46%|âââââ     | 124M/268M [00:02<00:02, 60.8MB/s]#015Downloading:  49%|âââââ     | 130M/268M [00:02<00:02, 61.0MB/s]#015Downloading:  51%|âââââ     | 137M/268M [00:02<00:02, 61.3MB/s]#015Downloading:  53%|ââââââ    | 143M/268M [00:02<00:02, 61.6MB/s]#015Downloading:  56%|ââââââ    | 149M/268M [00:02<00:01, 61.8MB/s]#015Downloading:  58%|ââââââ    | 155M/268M [00:02<00:01, 61.8MB/s]#015Downloading:  60%|ââââââ    | 162M/268M [00:02<00:01, 61.9MB/s]#015Downloading:  63%|âââââââ   | 168M/268M [00:02<00:01, 62.1MB/s]#015Downloading:  65%|âââââââ   | 174M/268M [00:03<00:01, 62.1MB/s]#015Downloading:  67%|âââââââ   | 180M/268M [00:03<00:01, 57.5MB/s]#015Downloading:  69%|âââââââ   | 186M/268M [00:03<00:01, 57.6MB/s]#015Downloading:  72%|ââââââââ  | 192M/268M [00:03<00:01, 58.8MB/s]#015Downloading:  74%|ââââââââ  | 198M/268M [00:03<00:01, 59.7MB/s]#015Downloading:  76%|ââââââââ  | 205M/268M [00:03<00:01, 60.3MB/s]#015Downloading:  79%|ââââââââ  | 211M/268M [00:03<00:00, 60.6MB/s]#015Downloading:  81%|ââââââââ  | 217M/268M [00:03<00:00, 61.1MB/s]#015Downloading:  83%|âââââââââ | 223M/268M [00:03<00:00, 61.6MB/s]#015Downloading:  86%|âââââââââ | 229M/268M [00:03<00:00, 61.2MB/s]#015Downloading:  88%|âââââââââ | 236M/268M [00:04<00:00, 61.0MB/s]#015Downloading:  90%|âââââââââ | 242M/268M [00:04<00:00, 61.5MB/s]#015Downloading:  93%|ââââââââââ| 248M/268M [00:04<00:00, 61.0MB/s]#015Downloading:  95%|ââââââââââ| 254M/268M [00:04<00:00, 61.4MB/s]#015Downloading:  97%|ââââââââââ| 260M/268M [00:04<00:00, 61.7MB/s]#015Downloading:  99%|ââââââââââ| 267M/268M [00:04<00:00, 59.8MB/s]#015Downloading: 100%|ââââââââââ| 268M/268M [00:04<00:00, 58.4MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5580 [00:00<?, ?it/s]/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34m/codebuild/output/src740861454/src/aten/src/ATen/native/cuda/Indexing.cu:658: indexSelectLargeIndex: block: [50,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 107, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1272, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1734, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1766, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 624, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 487, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 307, in forward\n",
      "    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 254, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 177, in forward\n",
      "    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5580 [00:01<?, ?it/s]\u001b[0m\n",
      "\n",
      "2021-08-30 13:34:23 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 train.py --epochs 3 --eval_batch_size 32 --learning_rate 3e-05 --model_id distilbert-base-uncased --test_file amazon_us_reviews_apparel_v1_00_test.json --train_batch_size 16 --train_file amazon_us_reviews_apparel_v1_00_train.json\"\n\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r\rDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 483/483 [00:00<00:00, 649kB/s]\n\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 37.2MB/s]\n\rDownloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 466k/466k [00:00<00:00, 40.4MB/s]\n\rDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 28.0/28.0 [00:00<00:00, 33.7kB/s]\n\r  0%|          | 0/30 [00:00<?, ?ba/s]\r ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7a4d3426c635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3676\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3677\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3678\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3234\u001b[0m                 ),\n\u001b[1;32m   3235\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m             )\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-workshop-2021-08-30-13-26-5-2021-08-30-13-26-57-988: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 train.py --epochs 3 --eval_batch_size 32 --learning_rate 3e-05 --model_id distilbert-base-uncased --test_file amazon_us_reviews_apparel_v1_00_test.json --train_batch_size 16 --train_file amazon_us_reviews_apparel_v1_00_train.json\"\n\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r\rDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 483/483 [00:00<00:00, 649kB/s]\n\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 37.2MB/s]\n\rDownloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 466k/466k [00:00<00:00, 40.4MB/s]\n\rDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 28.0/28.0 [00:00<00:00, 33.7kB/s]\n\r  0%|          | 0/30 [00:00<?, ?ba/s]\r "
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "training_data = {\n",
    "    'train': remote_train_dataset,\n",
    "    'test': remote_test_dataset\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(training_data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training jobname: huggingface-workshop-2021-08-26-17-53-4-2021-08-26-17-53-43-765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_loss</td>\n",
       "      <td>0.206314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_loss</td>\n",
       "      <td>0.235827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_loss</td>\n",
       "      <td>0.280385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_f1</td>\n",
       "      <td>0.921273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_f1</td>\n",
       "      <td>0.932751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_f1</td>\n",
       "      <td>0.935031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_precision</td>\n",
       "      <td>0.890375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_precision</td>\n",
       "      <td>0.929065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_precision</td>\n",
       "      <td>0.934195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_recall</td>\n",
       "      <td>0.954392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_recall</td>\n",
       "      <td>0.936467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_recall</td>\n",
       "      <td>0.935869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_runtime</td>\n",
       "      <td>50.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_runtime</td>\n",
       "      <td>50.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_runtime</td>\n",
       "      <td>50.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval_samples_per_second</td>\n",
       "      <td>199.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>540.0</td>\n",
       "      <td>eval_samples_per_second</td>\n",
       "      <td>199.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1140.0</td>\n",
       "      <td>eval_samples_per_second</td>\n",
       "      <td>199.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>300.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>420.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>540.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>720.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>900.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>1.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>960.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1080.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1260.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>2.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>2.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>epoch</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp              metric_name       value\n",
       "0         0.0                eval_loss    0.206314\n",
       "1       540.0                eval_loss    0.235827\n",
       "2      1140.0                eval_loss    0.280385\n",
       "3         0.0            eval_accuracy    0.918100\n",
       "4       540.0            eval_accuracy    0.932200\n",
       "5      1140.0            eval_accuracy    0.934700\n",
       "6         0.0                  eval_f1    0.921273\n",
       "7       540.0                  eval_f1    0.932751\n",
       "8      1140.0                  eval_f1    0.935031\n",
       "9         0.0           eval_precision    0.890375\n",
       "10      540.0           eval_precision    0.929065\n",
       "11     1140.0           eval_precision    0.934195\n",
       "12        0.0              eval_recall    0.954392\n",
       "13      540.0              eval_recall    0.936467\n",
       "14     1140.0              eval_recall    0.935869\n",
       "15        0.0             eval_runtime   50.174500\n",
       "16      540.0             eval_runtime   50.095400\n",
       "17     1140.0             eval_runtime   50.141100\n",
       "18        0.0  eval_samples_per_second  199.304000\n",
       "19      540.0  eval_samples_per_second  199.619000\n",
       "20     1140.0  eval_samples_per_second  199.437000\n",
       "21        0.0                    epoch    0.320000\n",
       "22      120.0                    epoch    0.640000\n",
       "23      300.0                    epoch    0.960000\n",
       "24      420.0                    epoch    1.000000\n",
       "25      540.0                    epoch    1.280000\n",
       "26      720.0                    epoch    1.600000\n",
       "27      900.0                    epoch    1.920000\n",
       "28      960.0                    epoch    2.000000\n",
       "29     1080.0                    epoch    2.240000\n",
       "30     1260.0                    epoch    2.560000\n",
       "31     1440.0                    epoch    2.880000\n",
       "32     1560.0                    epoch    3.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
