{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "### Distributed Question Answering with `transformers` scripts +  `Trainer` and `squad` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions) \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end `distributed` Question-Answering example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for question-answering on multiple-gpus. In particular, the pre-trained model will be fine-tuned using the `squad` dataset. The demo will use the new `smdistributed` library to run training on multiple gpus as training scripting we are going to use one of the `transformers` [example scripts from the repository](https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_qa.py).\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"awscli\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.6/site-packages (2.59.0)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Using cached sagemaker-2.59.1.tar.gz (440 kB)\n",
      "Requirement already satisfied: transformers==4.6.1 in /opt/conda/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied: datasets[s3]==1.6.2 in /opt/conda/lib/python3.6/site-packages (1.6.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.45)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (2021.8.28)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1) (4.8.1)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.11.1)\n",
      "Requirement already satisfied: botocore==1.19.52 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.19.52)\n",
      "Requirement already satisfied: boto3==1.16.43 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.16.43)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.4.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.25.11)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (21.2.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.17.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.59.1-py2.py3-none-any.whl size=616387 sha256=d8654ba4e8b03a82520e82f2ee80d6817d2497bcdfdd9655fe24286a097ce302\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/3a/94/f540e7aa9bb359dc39648180191bbda715170f88a617e462f6\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.59.0\n",
      "    Uninstalling sagemaker-2.59.0:\n",
      "      Successfully uninstalled sagemaker-2.59.0\n",
      "Successfully installed sagemaker-2.59.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upgrade ipywidgets for `datasets` library and restart kernel, only needed when prerpocessing is done in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::029438132834:role/service-role/AmazonSageMaker-ExecutionRole-20210215T132835\n",
      "sagemaker bucket: sagemaker-us-east-1-029438132834\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6.0',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n",
    "\n",
    "In this example we are going to use the capability to download/use a fine-tuning script from a `git`- repository. We are using the `run_qa.py` from the `transformers` example scripts. You can find the code [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path': 'bert-large-uncased-whole-word-masking',\n",
    "    'dataset_name':'squad',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 2,\n",
    "    'max_seq_length': 384,\n",
    "    'max_steps': 100,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2\n",
    "volume_size=200\n",
    "\n",
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "     {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./examples/pytorch/question-answering',\n",
    "                                    git_config=git_config,\n",
    "                                    metric_definitions=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-05 07:50:20 Starting - Starting the training job...\n",
      "2021-09-05 07:50:28 Starting - Launching requested ML instancesProfilerReport-1630828214: InProgress\n",
      "............\n",
      "2021-09-05 07:52:50 Starting - Preparing the instances for training.........\n",
      "2021-09-05 07:54:11 Downloading - Downloading input data...\n",
      "2021-09-05 07:54:51 Training - Downloading the training image.............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-09-05 07:56:56,395 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-09-05 07:56:56,474 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:01,644 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:01,724 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:04,747 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:04,747 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:05,111 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.4.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.4.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\n",
      "2021-09-05 07:57:12 Training - Training image download completed. Training in progress.\u001b[35mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,805 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,805 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,806 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,806 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:08,808 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:08,808 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,113 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:09,810 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:09,811 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,427 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:10,812 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:10,812 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.4.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.4.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:11,814 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:11,814 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,070 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,070 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,073 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,074 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,074 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,822 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,894 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,082 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,153 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,155 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,237 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"max_steps\": 100,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"squad\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"do_eval\": true,\n",
      "        \"doc_stride\": 128,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"max_seq_length\": 384,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"model_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-09-05-07-50-14-818\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-09-05-07-50-14-818\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"squad\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--max_steps\",\"100\",\"--model_name_or_path\",\"bert-large-uncased-whole-word-masking\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=squad\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-large-uncased-whole-word-masking\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.6 -m mpi4py run_qa.py --dataset_name squad --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path bert-large-uncased-whole-word-masking --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO comm 0x555d1552df40 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO comm 0x564f04a8b110 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO comm 0x560286410140 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO comm 0x564825e6e300 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO comm 0x557385d4d010 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO comm 0x5637e61c3ed0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO comm 0x55db42bbb240 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO comm 0x55672dd96570 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO comm 0x5641c1b43780 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO comm 0x5594c388b580 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO comm 0x56496b013720 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO comm 0x55e52d082260 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO comm 0x560c5b985d70 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO comm 0x55f04cbd0cd0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO comm 0x55f2cb99e360 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO comm 0x561d02b2e540 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO comm 0x5637e8e94900 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO comm 0x55db4588bd70 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO comm 0x55f04f8a1800 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO comm 0x5641c4814500 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO comm 0x55f2ce66ee90 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO comm 0x560c5e656af0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO comm 0x561d057ff2c0 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO comm 0x555d181fecc0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO comm 0x5602890e0ec0 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO comm 0x557388a1dd90 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO comm 0x56496dce4250 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO comm 0x556730a66fa0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO comm 0x564828b3f080 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO comm 0x5594c655c300 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO comm 0x564f0775be90 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO comm 0x55e52fd52fe0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep05_07-57-19_algo-2, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep05_07-57-18_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,192 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,193 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,217 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,217 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,239 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmputh1vp4b\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,264 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,264 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,274 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,274 >> creating metadata file for /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,288 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,307 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvce9yq53\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,356 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,356 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,530 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:19,509 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:19,509 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:23,793 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:23,793 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:398] 2021-09-05 07:59:21,316 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:415] 2021-09-05 07:59:21,316 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:784 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:398] 2021-09-05 07:59:27,951 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:415] 2021-09-05 07:59:27,951 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,580 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,580 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,581 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,581 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,581 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,582 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,617 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,617 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,618 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,618 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,618 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,618 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.813 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.813 algo-1:59 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.813 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.813 algo-1:61 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.813 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.813 algo-1:63 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.831 algo-1:65 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.836 algo-1:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:28.870 algo-2:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:28.870 algo-2:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:28.870 algo-2:72 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:28.873 algo-2:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:28.877 algo-2:74 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:28.877 algo-2:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:28.886 algo-2:70 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:28.887 algo-2:73 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.939 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.939 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.939 algo-1:59 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.939 algo-1:57 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.939 algo-1:63 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.939 algo-1:61 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.939 algo-1:65 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.939 algo-1:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.939 algo-1:55 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.939 algo-1:61 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.940 algo-1:59 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.940 algo-1:63 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.940 algo-1:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.940 algo-1:66 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.940 algo-1:55 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.940 algo-1:61 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.940 algo-1:65 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.940 algo-1:59 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.940 algo-1:63 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.940 algo-1:57 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.940 algo-1:65 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.941 algo-1:55 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.941 algo-1:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.941 algo-1:61 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.941 algo-1:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.941 algo-1:55 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.941 algo-1:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.941 algo-1:61 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.941 algo-1:59 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.941 algo-1:59 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.942 algo-1:67 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.942 algo-1:63 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.942 algo-1:65 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.942 algo-1:63 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.942 algo-1:57 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.942 algo-1:65 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.942 algo-1:57 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.942 algo-1:67 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.943 algo-1:67 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.943 algo-1:67 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.012 algo-2:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.012 algo-2:72 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.012 algo-2:73 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.012 algo-2:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.012 algo-2:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.012 algo-2:74 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.012 algo-2:72 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.012 algo-2:62 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.013 algo-2:64 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.013 algo-2:68 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.013 algo-2:74 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.013 algo-2:73 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.013 algo-2:74 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.013 algo-2:62 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.013 algo-2:64 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.013 algo-2:68 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.013 algo-2:72 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.013 algo-2:73 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.014 algo-2:62 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.014 algo-2:72 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.014 algo-2:62 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.014 algo-2:74 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.014 algo-2:72 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.014 algo-2:64 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.014 algo-2:74 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.014 algo-2:68 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.014 algo-2:64 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.015 algo-2:68 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.014 algo-2:73 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.015 algo-2:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.015 algo-2:73 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.015 algo-2:70 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.015 algo-2:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.015 algo-2:70 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.016 algo-2:66 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.016 algo-2:70 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.017 algo-2:70 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.018 algo-2:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.018 algo-2:70 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.018 algo-2:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.135 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.151 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.162 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.162 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.163 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.166 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.166 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.180 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.180 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.183 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.183 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.186 algo-1:63 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.190 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.190 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.195 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.195 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.209 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.209 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.212 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.212 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.213 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.213 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.217 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.217 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.221 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.239 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.273 algo-2:62 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.275 algo-2:72 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.286 algo-2:73 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.286 algo-2:73 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.291 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.291 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.297 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.302 algo-1:67 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.302 algo-1:67 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.359 algo-2:68 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.359 algo-2:68 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:772 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:776 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,565 >> \u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,693 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'train_runtime': 161.1116, 'train_samples_per_second': 0.621, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{'train_runtime': 160.9471, 'train_samples_per_second': 0.621, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 08:02:10 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:516] 2021-09-05 08:02:10,635 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2115] 2021-09-05 08:02:10,639 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2117] 2021-09-05 08:02:10,639 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2120] 2021-09-05 08:02:10,639 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1885] 2021-09-05 08:02:10,793 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:351] 2021-09-05 08:02:10,794 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:889] 2021-09-05 08:02:13,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1924] 2021-09-05 08:02:13,044 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1930] 2021-09-05 08:02:13,044 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:02:13,088 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   epoch                      =       0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_alloc_delta   =     -122MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_peaked_delta  =      121MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_alloc_delta   =     1275MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_alloc_delta  =     1424MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_alloc_delta  =     5113MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_peaked_delta =     4884MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_runtime              = 0:02:41.11\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples              =      88524\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples_per_second   =      0.621\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:02:13 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2021-09-05 08:02:13,091 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2021-09-05 08:02:13,094 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2021-09-05 08:02:13,094 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2021-09-05 08:02:13,094 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:02:37 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10784 features.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:03:05 - INFO - utils_qa -   Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:03:05 - INFO - utils_qa -   Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:03:12,263 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   epoch        =   0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   eval_samples =  10784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   exact_match  = 0.3217\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   f1           = 4.4442\u001b[0m\n",
      "\u001b[35m2021-09-05 08:03:19,443 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.74.39' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 5.03kB [00:00, 4.20MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,5]<stderr>:#015Downloading: 5.03kB [00:00, 2.88MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,6]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 5.03kB [00:00, 4.28MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading: 5.03kB [00:00, 4.05MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 5.03kB [00:00, 4.58MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,12]<stderr>:#015Downloading: 5.03kB [00:00, 3.15MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,1]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,1]<stderr>:#015Downloading: 2.19kB [00:00, 2.21MB/s]                 [1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading: 2.19kB [00:00, 1.75MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,10]<stderr>:#015Downloading: 5.03kB [00:00, 4.47MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,3]<stderr>:#015Downloading: 2.19kB [00:00, 2.12MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,9]<stderr>:#015Downloading: 2.19kB [00:00, 1.16MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 2.19kB [00:00, 2.21MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 2.19kB [00:00, 2.13MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,2]<stderr>:#015Downloading: 2.19kB [00:00, 1.74MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 2.19kB [00:00, 2.14MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,4]<stderr>:#015Downloading: 2.19kB [00:00, 2.06MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,0]<stderr>:#015Downloading: 2.19kB [00:00, 1.95MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,11]<stderr>:#015Downloading: 2.19kB [00:00, 2.24MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,14]<stderr>:#015Downloading: 2.19kB [00:00, 2.30MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s][1,13]<stderr>:#015Downloading:  58%|█████▊    | 4.73M/8.12M [00:00<00:00, 47.3MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 6.96M/8.12M [00:00<00:00, 69.6MB/s][1,13]<stderr>:#015Downloading: 9.07MB [00:00, 46.0MB/s]                            [1,7]<stderr>:#015Downloading: 13.9MB [00:00, 69.6MB/s]                            [1,13]<stderr>:#015Downloading: 14.8MB [00:00, 49.0MB/s][1,7]<stderr>:#015Downloading: 20.8MB [00:00, 69.4MB/s][1,13]<stderr>:#015Downloading: 22.0MB [00:00, 54.2MB/s][1,7]<stderr>:#015Downloading: 27.8MB [00:00, 69.6MB/s][1,7]<stderr>:#015Downloading: 30.3MB [00:00, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading: 29.4MB [00:00, 58.8MB/s][1,13]<stderr>:#015Downloading: 30.3MB [00:00, 58.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 4.85MB [00:00, 51.6MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading: 4.85MB [00:00, 78.8MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#0150 examples [00:00, ? examples/s][1,7]<stderr>:#0150 examples [00:00, ? examples/s][1,13]<stderr>:#0151 examples [00:00,  1.98 examples/s][1,7]<stderr>:#0151 examples [00:00,  1.96 examples/s][1,13]<stderr>:#0152064 examples [00:00,  2.83 examples/s][1,7]<stderr>:#0152046 examples [00:00,  2.80 examples/s][1,13]<stderr>:#0154158 examples [00:00,  4.04 examples/s][1,7]<stderr>:#0154113 examples [00:00,  4.00 examples/s][1,13]<stderr>:#0156258 examples [00:00,  5.78 examples/s][1,7]<stderr>:#0156214 examples [00:00,  5.72 examples/s][1,13]<stderr>:#0158378 examples [00:00,  8.25 examples/s][1,7]<stderr>:#0158313 examples [00:00,  8.17 examples/s][1,13]<stderr>:#0159975 examples [00:01, 11.78 examples/s][1,7]<stderr>:#0159898 examples [00:01, 11.66 examples/s][1,13]<stderr>:#01511341 examples [00:01, 16.83 examples/s][1,7]<stderr>:#01511249 examples [00:01, 16.65 examples/s][1,13]<stderr>:#01513467 examples [00:01, 24.03 examples/s][1,7]<stderr>:#01513376 examples [00:01, 23.78 examples/s][1,13]<stderr>:#01515614 examples [00:01, 34.31 examples/s][1,7]<stderr>:#01515480 examples [00:01, 33.96 examples/s][1,13]<stderr>:#01517760 examples [00:01, 48.99 examples/s][1,7]<stderr>:#01517608 examples [00:01, 48.48 examples/s][1,13]<stderr>:#01519917 examples [00:01, 69.91 examples/s][1,7]<stderr>:#01519734 examples [00:01, 69.19 examples/s][1,13]<stderr>:#01521866 examples [00:01, 99.71 examples/s][1,7]<stderr>:#01521662 examples [00:01, 98.68 examples/s][1,13]<stderr>:#01524028 examples [00:01, 142.16 examples/s][1,7]<stderr>:#01523803 examples [00:01, 140.70 examples/s][1,13]<stderr>:#01526183 examples [00:01, 202.51 examples/s][1,7]<stderr>:#01525933 examples [00:01, 200.43 examples/s][1,13]<stderr>:#01528343 examples [00:02, 288.15 examples/s][1,7]<stderr>:#01528062 examples [00:02, 285.18 examples/s][1,13]<stderr>:#01530418 examples [00:02, 408.73 examples/s][1,7]<stderr>:#01530111 examples [00:02, 404.55 examples/s][1,13]<stderr>:#01532590 examples [00:02, 579.22 examples/s][1,7]<stderr>:#01532269 examples [00:02, 573.32 examples/s][1,13]<stderr>:#01534770 examples [00:02, 818.14 examples/s][1,7]<stderr>:#01534425 examples [00:02, 809.80 examples/s][1,13]<stderr>:#01536926 examples [00:02, 1150.07 examples/s][1,7]<stderr>:#01536570 examples [00:02, 1138.44 examples/s][1,13]<stderr>:#01539078 examples [00:02, 1606.16 examples/s][1,7]<stderr>:#01538710 examples [00:02, 1590.07 examples/s][1,13]<stderr>:#01541194 examples [00:02, 2132.18 examples/s][1,7]<stderr>:#01540811 examples [00:02, 2106.36 examples/s][1,13]<stderr>:#01543368 examples [00:02, 2923.07 examples/s][1,7]<stderr>:#01542942 examples [00:02, 2886.78 examples/s][1,13]<stderr>:#01545533 examples [00:02, 3947.35 examples/s][1,7]<stderr>:#01545091 examples [00:02, 3899.44 examples/s][1,13]<stderr>:#01547673 examples [00:03, 5225.93 examples/s][1,7]<stderr>:#01547229 examples [00:03, 5166.63 examples/s][1,13]<stderr>:#01549825 examples [00:03, 6761.76 examples/s][1,7]<stderr>:#01549363 examples [00:03, 6686.79 examples/s][1,13]<stderr>:#01551883 examples [00:03, 8280.88 examples/s][1,7]<stderr>:#01551403 examples [00:03, 8203.52 examples/s][1,13]<stderr>:#01554049 examples [00:03, 10164.36 examples/s][1,7]<stderr>:#01553542 examples [00:03, 10064.46 examples/s][1,13]<stderr>:#01556224 examples [00:03, 12097.23 examples/s][1,7]<stderr>:#01555691 examples [00:03, 11973.66 examples/s][1,13]<stderr>:#01558349 examples [00:03, 13891.97 examples/s][1,7]<stderr>:#01557828 examples [00:03, 13792.11 examples/s][1,13]<stderr>:#01560435 examples [00:03, 14794.61 examples/s][1,7]<stderr>:#01559929 examples [00:03, 15375.34 examples/s][1,13]<stderr>:#01562600 examples [00:03, 16346.25 examples/s][1,7]<stderr>:#01562012 examples [00:03, 16046.69 examples/s][1,13]<stderr>:#01564769 examples [00:03, 17649.27 examples/s][1,7]<stderr>:#01564144 examples [00:03, 17331.26 examples/s][1,13]<stderr>:#01566943 examples [00:04, 18703.33 examples/s][1,7]<stderr>:#01566294 examples [00:04, 18400.24 examples/s][1,13]<stderr>:#01569049 examples [00:04, 14776.56 examples/s][1,7]<stderr>:#01568370 examples [00:04, 14200.61 examples/s][1,13]<stderr>:#01570821 examples [00:04, 15512.45 examples/s][1,7]<stderr>:#01570097 examples [00:04, 14830.32 examples/s][1,13]<stderr>:#01572972 examples [00:04, 16928.11 examples/s][1,7]<stderr>:#01572232 examples [00:04, 16324.58 examples/s][1,13]<stderr>:#01575142 examples [00:04, 18121.85 examples/s][1,7]<stderr>:#01574380 examples [00:04, 17590.64 examples/s][1,13]<stderr>:#01577321 examples [00:04, 19083.70 examples/s][1,7]<stderr>:#01576532 examples [00:04, 18608.40 examples/s][1,13]<stderr>:#01579490 examples [00:04, 19796.01 examples/s][1,7]<stderr>:#01578684 examples [00:04, 19394.99 examples/s][1,13]<stderr>:#01581563 examples [00:04, 19149.35 examples/s][1,7]<stderr>:#01580726 examples [00:04, 18780.61 examples/s][1,13]<stderr>:#01583742 examples [00:04, 19869.76 examples/s][1,7]<stderr>:#01582878 examples [00:04, 19525.79 examples/s][1,13]<stderr>:#01585923 examples [00:05, 20414.12 examples/s][1,7]<stderr>:#01585028 examples [00:05, 20077.61 examples/s][1,13]<stderr>:#015                                           #015[1,13]<stderr>:#0150 examples [00:00, ? examples/s][1,7]<stderr>:#01587180 examples [00:05, 20488.84 examples/s][1,7]<stderr>:#015                                           #015[1,13]<stderr>:#015910 examples [00:00, 9097.38 examples/s][1,7]<stderr>:#0150 examples [00:00, ? examples/s][1,13]<stderr>:#0152524 examples [00:00, 10467.38 examples/s][1,7]<stderr>:#015887 examples [00:00, 8865.36 examples/s][1,13]<stderr>:#0153881 examples [00:00, 11237.43 examples/s][1,7]<stderr>:#0152678 examples [00:00, 10448.23 examples/s][1,7]<stderr>:#0154385 examples [00:00, 11822.87 examples/s][1,13]<stderr>:#0154995 examples [00:00, 10201.57 examples/s][1,7]<stderr>:#0155404 examples [00:00, 10663.20 examples/s][1,13]<stderr>:#0156785 examples [00:00, 11712.43 examples/s][1,7]<stderr>:#0157168 examples [00:00, 12098.17 examples/s][1,13]<stderr>:#0158573 examples [00:00, 13063.78 examples/s][1,7]<stderr>:#0158954 examples [00:00, 13393.62 examples/s][1,13]<stderr>:#01510000 examples [00:00, 13278.44 examples/s][1,13]<stderr>:#015                                           #015[1,7]<stderr>:#01510337 examples [00:00, 13335.11 examples/s][1,7]<stderr>:#015                                           #015[1,13]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 349kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,192 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,193 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,217 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,217 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,239 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmputh1vp4b\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 364kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s][1,0]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,264 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,264 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 21.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,274 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,274 >> creating metadata file for /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,288 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m2021-09-05 08:03:19,438 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,307 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvce9yq53\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 24.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,356 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,356 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 44.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 39.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 22.2kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,530 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015Downloading:   0%|          | 0.00/1.35G [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.35G [00:00<?, ?B/s][1,12]<stderr>:#015Downloading:   0%|          | 4.19M/1.35G [00:00<00:31, 41.9MB/s][1,7]<stderr>:#015Downloading:   0%|          | 4.12M/1.35G [00:00<00:32, 41.2MB/s][1,12]<stderr>:#015Downloading:   1%|          | 8.38M/1.35G [00:00<00:33, 39.8MB/s][1,7]<stderr>:#015Downloading:   1%|          | 8.40M/1.35G [00:00<00:32, 41.7MB/s][1,12]<stderr>:#015Downloading:   1%|          | 11.4M/1.35G [00:00<00:36, 36.4MB/s][1,7]<stderr>:#015Downloading:   1%|          | 11.8M/1.35G [00:00<00:34, 38.8MB/s][1,12]<stderr>:#015Downloading:   1%|          | 13.9M/1.35G [00:00<00:41, 32.0MB/s][1,7]<stderr>:#015Downloading:   1%|          | 14.3M/1.35G [00:00<00:39, 33.6MB/s][1,12]<stderr>:#015Downloading:   1%|          | 16.8M/1.35G [00:00<00:51, 25.7MB/s][1,7]<stderr>:#015Downloading:   1%|▏         | 16.8M/1.35G [00:00<00:52, 25.3MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 22.0M/1.35G [00:00<00:43, 30.4MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 21.8M/1.35G [00:00<00:44, 29.7MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 25.2M/1.35G [00:00<00:45, 29.1MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 25.2M/1.35G [00:00<00:45, 29.3MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 30.3M/1.35G [00:00<00:39, 33.5MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 30.1M/1.35G [00:00<00:39, 33.4MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 33.7M/1.35G [00:01<00:54, 24.2MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 34.0M/1.35G [00:01<00:54, 24.1MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 38.8M/1.35G [00:01<00:45, 28.8MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 39.6M/1.35G [00:01<00:44, 29.1MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 42.4M/1.35G [00:01<00:52, 24.9MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 43.4M/1.35G [00:01<00:52, 25.0MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 45.5M/1.35G [00:01<00:50, 25.6MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 46.6M/1.35G [00:01<00:53, 24.2MB/s][1,7]<stderr>:#015Downloading:   4%|▎         | 48.6M/1.35G [00:01<01:08, 18.9MB/s][1,12]<stderr>:#015Downloading:   4%|▎         | 49.5M/1.35G [00:01<01:08, 18.9MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 51.0M/1.35G [00:01<01:08, 18.8MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 51.9M/1.35G [00:01<01:08, 18.9MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 54.2M/1.35G [00:02<01:00, 21.4MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 54.8M/1.35G [00:02<01:07, 19.0MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 56.8M/1.35G [00:02<01:03, 20.3MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 57.0M/1.35G [00:02<01:15, 17.1MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 58.9M/1.35G [00:02<01:22, 15.7MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 59.1M/1.35G [00:02<01:22, 15.5MB/s][1,12]<stderr>:#015Downloading:   5%|▍         | 63.8M/1.35G [00:02<01:05, 19.7MB/s][1,7]<stderr>:#015Downloading:   5%|▍         | 63.9M/1.35G [00:02<01:05, 19.5MB/s][1,12]<stderr>:#015Downloading:   5%|▌         | 68.6M/1.35G [00:02<00:53, 23.9MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 68.7M/1.35G [00:02<00:53, 23.7MB/s][1,12]<stderr>:#015Downloading:   5%|▌         | 73.7M/1.35G [00:02<00:45, 28.2MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 73.7M/1.35G [00:02<00:45, 28.2MB/s][1,12]<stderr>:#015Downloading:   6%|▌         | 77.5M/1.35G [00:02<00:59, 21.3MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 77.6M/1.35G [00:02<01:00, 20.9MB/s][1,12]<stderr>:#015Downloading:   6%|▌         | 83.8M/1.35G [00:03<00:47, 26.6MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 83.3M/1.35G [00:03<00:48, 25.8MB/s][1,7]<stderr>:#015Downloading:   6%|▋         | 87.2M/1.35G [00:03<00:43, 28.7MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 87.8M/1.35G [00:03<00:44, 28.0MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 92.4M/1.35G [00:03<00:37, 33.1MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 92.6M/1.35G [00:03<00:39, 32.0MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 97.6M/1.35G [00:03<00:33, 37.1MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 97.5M/1.35G [00:03<00:34, 35.7MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 104M/1.35G [00:03<00:29, 41.9MB/s] [1,12]<stderr>:#015Downloading:   8%|▊         | 103M/1.35G [00:03<00:31, 39.9MB/s] [1,7]<stderr>:#015Downloading:   8%|▊         | 110M/1.35G [00:03<00:26, 46.0MB/s][1,12]<stderr>:#015Downloading:   8%|▊         | 108M/1.35G [00:03<00:28, 43.2MB/s][1,7]<stderr>:#015Downloading:   9%|▊         | 116M/1.35G [00:03<00:24, 49.4MB/s][1,12]<stderr>:#015Downloading:   8%|▊         | 114M/1.35G [00:03<00:26, 46.2MB/s][1,12]<stderr>:#015Downloading:   9%|▉         | 119M/1.35G [00:03<00:26, 46.3MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 121M/1.35G [00:03<00:27, 44.3MB/s][1,12]<stderr>:#015Downloading:   9%|▉         | 125M/1.35G [00:03<00:24, 50.0MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 126M/1.35G [00:03<00:25, 46.9MB/s][1,12]<stderr>:#015Downloading:  10%|▉         | 130M/1.35G [00:04<00:25, 48.6MB/s][1,7]<stderr>:#015Downloading:  10%|▉         | 132M/1.35G [00:04<00:25, 47.1MB/s][1,12]<stderr>:#015Downloading:  10%|█         | 135M/1.35G [00:04<00:24, 49.2MB/s][1,7]<stderr>:#015Downloading:  10%|█         | 137M/1.35G [00:04<00:24, 48.7MB/s][1,12]<stderr>:#015Downloading:  10%|█         | 141M/1.35G [00:04<00:23, 51.3MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 143M/1.35G [00:04<00:23, 51.6MB/s][1,12]<stderr>:#015Downloading:  11%|█         | 147M/1.35G [00:04<00:23, 52.1MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 149M/1.35G [00:04<00:22, 53.6MB/s][1,12]<stderr>:#015Downloading:  11%|█▏        | 152M/1.35G [00:04<00:22, 52.8MB/s][1,7]<stderr>:#015Downloading:  11%|█▏        | 155M/1.35G [00:04<00:21, 55.0MB/s][1,12]<stderr>:#015Downloading:  12%|█▏        | 158M/1.35G [00:04<00:21, 54.9MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 160M/1.35G [00:04<00:23, 49.9MB/s][1,12]<stderr>:#015Downloading:  12%|█▏        | 164M/1.35G [00:04<00:23, 50.8MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 166M/1.35G [00:04<00:22, 51.3MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 169M/1.35G [00:04<00:30, 38.6MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 171M/1.35G [00:04<00:30, 38.5MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 175M/1.35G [00:04<00:27, 42.9MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 177M/1.35G [00:04<00:27, 42.5MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 180M/1.35G [00:05<00:26, 44.6MB/s][1,7]<stderr>:#015Downloading:  14%|█▎        | 182M/1.35G [00:05<00:25, 45.6MB/s][1,12]<stderr>:#015Downloading:  14%|█▎        | 184M/1.35G [00:05<00:28, 40.5MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 187M/1.35G [00:05<00:32, 35.7MB/s][1,12]<stderr>:#015Downloading:  14%|█▍        | 189M/1.35G [00:05<00:30, 38.0MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 193M/1.35G [00:05<00:28, 40.5MB/s][1,12]<stderr>:#015Downloading:  14%|█▍        | 193M/1.35G [00:05<00:34, 33.7MB/s][1,12]<stderr>:#015Downloading:  15%|█▍        | 197M/1.35G [00:05<00:40, 28.6MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 198M/1.35G [00:05<00:39, 28.9MB/s][1,12]<stderr>:#015Downloading:  15%|█▍        | 201M/1.35G [00:05<00:36, 31.5MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 201M/1.35G [00:05<00:38, 29.4MB/s][1,12]<stderr>:#015Downloading:  15%|█▌        | 205M/1.35G [00:05<00:34, 33.3MB/s][1,7]<stderr>:#015Downloading:  15%|█▌        | 207M/1.35G [00:05<00:33, 33.7MB/s][1,12]<stderr>:#015Downloading:  16%|█▌        | 210M/1.35G [00:06<00:31, 35.7MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 211M/1.35G [00:06<00:32, 34.8MB/s][1,12]<stderr>:#015Downloading:  16%|█▌        | 215M/1.35G [00:06<00:28, 39.2MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 216M/1.35G [00:06<00:28, 39.4MB/s][1,12]<stderr>:#015Downloading:  16%|█▋        | 219M/1.35G [00:06<00:40, 28.1MB/s][1,7]<stderr>:#015Downloading:  16%|█▋        | 221M/1.35G [00:06<00:38, 29.0MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 225M/1.35G [00:06<00:33, 33.3MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 226M/1.35G [00:06<00:33, 33.7MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 230M/1.35G [00:06<00:29, 37.5MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 231M/1.35G [00:06<00:29, 37.7MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 235M/1.35G [00:06<00:29, 38.0MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 236M/1.35G [00:06<00:28, 39.1MB/s][1,12]<stderr>:#015Downloading:  18%|█▊        | 240M/1.35G [00:06<00:27, 40.8MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 242M/1.35G [00:06<00:25, 43.5MB/s][1,12]<stderr>:#015Downloading:  18%|█▊        | 245M/1.35G [00:06<00:25, 43.6MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 248M/1.35G [00:06<00:23, 47.3MB/s][1,12]<stderr>:#015Downloading:  19%|█▊        | 251M/1.35G [00:06<00:22, 47.9MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 253M/1.35G [00:07<00:26, 40.6MB/s][1,12]<stderr>:#015Downloading:  19%|█▉        | 256M/1.35G [00:07<00:25, 43.4MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 259M/1.35G [00:07<00:24, 44.1MB/s][1,12]<stderr>:#015Downloading:  19%|█▉        | 262M/1.35G [00:07<00:23, 45.7MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 263M/1.35G [00:07<00:24, 44.5MB/s][1,12]<stderr>:#015Downloading:  20%|█▉        | 267M/1.35G [00:07<00:22, 47.3MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 268M/1.35G [00:07<00:23, 46.0MB/s][1,12]<stderr>:#015Downloading:  20%|██        | 272M/1.35G [00:07<00:22, 48.7MB/s][1,7]<stderr>:#015Downloading:  20%|██        | 274M/1.35G [00:07<00:22, 47.8MB/s][1,12]<stderr>:#015Downloading:  21%|██        | 278M/1.35G [00:07<00:20, 50.9MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 279M/1.35G [00:07<00:22, 48.0MB/s][1,12]<stderr>:#015Downloading:  21%|██        | 284M/1.35G [00:07<00:19, 53.4MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 284M/1.35G [00:07<00:20, 50.8MB/s][1,12]<stderr>:#015Downloading:  21%|██▏       | 289M/1.35G [00:07<00:21, 48.7MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 290M/1.35G [00:07<00:20, 51.2MB/s][1,12]<stderr>:#015Downloading:  22%|██▏       | 294M/1.35G [00:08<00:34, 30.4MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 295M/1.35G [00:08<00:33, 31.4MB/s][1,12]<stderr>:#015Downloading:  22%|██▏       | 300M/1.35G [00:08<00:29, 35.2MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 300M/1.35G [00:08<00:29, 35.7MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 304M/1.35G [00:08<00:31, 33.0MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 305M/1.35G [00:08<00:30, 34.2MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 310M/1.35G [00:08<00:27, 37.3MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 310M/1.35G [00:08<00:28, 36.1MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 315M/1.35G [00:08<00:25, 39.9MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 315M/1.35G [00:08<00:26, 39.5MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 320M/1.35G [00:08<00:24, 42.4MB/s][1,12]<stderr>:#015Downloading:  24%|██▍       | 321M/1.35G [00:08<00:23, 42.7MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 326M/1.35G [00:08<00:21, 46.6MB/s][1,12]<stderr>:#015Downloading:  24%|██▍       | 326M/1.35G [00:08<00:26, 39.0MB/s][1,12]<stderr>:#015Downloading:  25%|██▍       | 331M/1.35G [00:09<00:31, 32.1MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 331M/1.35G [00:09<00:31, 32.1MB/s][1,12]<stderr>:#015Downloading:  25%|██▍       | 334M/1.35G [00:09<00:34, 28.9MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 335M/1.35G [00:09<00:39, 25.7MB/s][1,12]<stderr>:#015Downloading:  25%|██▌       | 337M/1.35G [00:09<00:43, 23.3MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 339M/1.35G [00:09<00:43, 23.1MB/s][1,12]<stderr>:#015Downloading:  25%|██▌       | 341M/1.35G [00:09<00:39, 25.5MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 343M/1.35G [00:09<00:37, 26.8MB/s][1,12]<stderr>:#015Downloading:  26%|██▌       | 344M/1.35G [00:09<00:39, 25.2MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 346M/1.35G [00:09<00:37, 26.4MB/s][1,12]<stderr>:#015Downloading:  26%|██▌       | 348M/1.35G [00:09<00:34, 28.8MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 352M/1.35G [00:09<00:31, 31.2MB/s][1,12]<stderr>:#015Downloading:  26%|██▋       | 354M/1.35G [00:09<00:29, 34.1MB/s][1,7]<stderr>:#015Downloading:  26%|██▋       | 356M/1.35G [00:09<00:28, 34.5MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 358M/1.35G [00:09<00:27, 36.2MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 361M/1.35G [00:10<00:30, 31.8MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 363M/1.35G [00:10<00:30, 31.9MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 364M/1.35G [00:10<00:31, 31.4MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 366M/1.35G [00:10<00:31, 31.5MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 369M/1.35G [00:10<00:30, 31.8MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 370M/1.35G [00:10<00:32, 30.2MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 374M/1.35G [00:10<00:27, 35.6MB/s][1,12]<stderr>:#015Downloading:  28%|██▊       | 375M/1.35G [00:10<00:27, 35.1MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 378M/1.35G [00:10<00:29, 33.3MB/s][1,12]<stderr>:#015Downloading:  28%|██▊       | 379M/1.35G [00:10<00:30, 32.0MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 383M/1.35G [00:10<00:25, 37.8MB/s][1,12]<stderr>:#015Downloading:  29%|██▊       | 385M/1.35G [00:10<00:25, 37.4MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 387M/1.35G [00:10<00:25, 38.2MB/s][1,12]<stderr>:#015Downloading:  29%|██▉       | 390M/1.35G [00:10<00:25, 37.7MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 393M/1.35G [00:10<00:23, 41.2MB/s][1,12]<stderr>:#015Downloading:  29%|██▉       | 395M/1.35G [00:10<00:22, 41.4MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 397M/1.35G [00:10<00:21, 43.5MB/s][1,12]<stderr>:#015Downloading:  30%|██▉       | 400M/1.35G [00:10<00:21, 44.4MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 402M/1.35G [00:11<00:20, 45.1MB/s][1,12]<stderr>:#015Downloading:  30%|███       | 406M/1.35G [00:11<00:20, 46.4MB/s][1,7]<stderr>:#015Downloading:  30%|███       | 407M/1.35G [00:11<00:20, 46.3MB/s][1,12]<stderr>:#015Downloading:  31%|███       | 411M/1.35G [00:11<00:19, 48.1MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 412M/1.35G [00:11<00:19, 47.4MB/s][1,12]<stderr>:#015Downloading:  31%|███       | 416M/1.35G [00:11<00:18, 49.4MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 418M/1.35G [00:11<00:18, 49.2MB/s][1,12]<stderr>:#015Downloading:  31%|███▏      | 421M/1.35G [00:11<00:18, 50.3MB/s][1,7]<stderr>:#015Downloading:  31%|███▏      | 423M/1.35G [00:11<00:18, 51.0MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 427M/1.35G [00:11<00:18, 50.8MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 429M/1.35G [00:11<00:17, 52.3MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 432M/1.35G [00:11<00:17, 51.4MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 435M/1.35G [00:11<00:16, 53.6MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 437M/1.35G [00:11<00:17, 51.7MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 440M/1.35G [00:11<00:16, 54.9MB/s][1,12]<stderr>:#015Downloading:  33%|███▎      | 442M/1.35G [00:11<00:17, 52.0MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 446M/1.35G [00:11<00:16, 55.7MB/s][1,12]<stderr>:#015Downloading:  33%|███▎      | 448M/1.35G [00:11<00:17, 52.3MB/s][1,7]<stderr>:#015Downloading:  34%|███▎      | 452M/1.35G [00:11<00:15, 57.5MB/s][1,12]<stderr>:#015Downloading:  34%|███▎      | 454M/1.35G [00:11<00:16, 54.2MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 458M/1.35G [00:12<00:16, 54.3MB/s][1,12]<stderr>:#015Downloading:  34%|███▍      | 460M/1.35G [00:12<00:15, 55.7MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 464M/1.35G [00:12<00:23, 37.8MB/s][1,12]<stderr>:#015Downloading:  35%|███▍     \n",
      " | 465M/1.35G [00:12<00:23, 38.1MB/s][1,7]<stderr>:#015Downloading:  35%|███▍      | 469M/1.35G [00:12<00:21, 41.6MB/s][1,12]<stderr>:#015Downloading:  35%|███▌      | 471M/1.35G [00:12<00:20, 42.9MB/s][1,7]<stderr>:#015Downloading:  35%|███▌      | 475M/1.35G [00:12<00:19, 45.0MB/s][1,12]<stderr>:#015Downloading:  35%|███▌      | 476M/1.35G [00:12<00:19, 45.2MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 480M/1.35G [00:12<00:19, 44.4MB/s][1,12]<stderr>:#015Downloading:  36%|███▌      | 481M/1.35G [00:12<00:19, 44.7MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 485M/1.35G [00:12<00:19, 44.9MB/s][1,12]<stderr>:#015Downloading:  36%|███▌      | 486M/1.35G [00:12<00:20, 42.9MB/s][1,7]<stderr>:#015Downloading:  36%|███▋      | 490M/1.35G [00:12<00:22, 38.6MB/s][1,12]<stderr>:#015Downloading:  36%|███▋      | 491M/1.35G [00:12<00:21, 39.3MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 495M/1.35G [00:12<00:20, 41.7MB/s][1,12]<stderr>:#015Downloading:  37%|███▋      | 496M/1.35G [00:13<00:19, 43.0MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 500M/1.35G [00:13<00:18, 44.8MB/s][1,12]<stderr>:#015Downloading:  37%|███▋      | 502M/1.35G [00:13<00:17, 46.9MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 505M/1.35G [00:13<00:18, 45.7MB/s][1,12]<stderr>:#015Downloading:  38%|███▊      | 507M/1.35G [00:13<00:17, 47.5MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 511M/1.35G [00:13<00:16, 49.1MB/s][1,12]<stderr>:#015Downloading:  38%|███▊      | 512M/1.35G [00:13<00:17, 47.8MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 517M/1.35G [00:13<00:16, 51.5MB/s][1,12]<stderr>:#015Downloading:  39%|███▊      | 518M/1.35G [00:13<00:16, 50.4MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 523M/1.35G [00:13<00:15, 53.5MB/s][1,12]<stderr>:#015Downloading:  39%|███▉      | 523M/1.35G [00:13<00:16, 50.6MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 529M/1.35G [00:13<00:14, 55.8MB/s][1,12]<stderr>:#015Downloading:  39%|███▉      | 529M/1.35G [00:13<00:15, 51.8MB/s][1,7]<stderr>:#015Downloading:  40%|███▉      | 535M/1.35G [00:13<00:14, 56.9MB/s][1,12]<stderr>:#015Downloading:  40%|███▉      | 534M/1.35G [00:13<00:15, 52.7MB/s][1,7]<stderr>:#015Downloading:  40%|████      | 541M/1.35G [00:13<00:14, 56.7MB/s][1,12]<stderr>:#015Downloading:  40%|████      | 540M/1.35G [00:13<00:15, 52.1MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 546M/1.35G [00:13<00:14, 57.0MB/s][1,12]<stderr>:#015Downloading:  41%|████      | 545M/1.35G [00:13<00:15, 52.0MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 552M/1.35G [00:13<00:13, 57.6MB/s][1,12]<stderr>:#015Downloading:  41%|████      | 551M/1.35G [00:14<00:14, 53.6MB/s][1,12]<stderr>:#015Downloading:  41%|████▏     | 556M/1.35G [00:14<00:14, 53.1MB/s][1,7]<stderr>:#015Downloading:  41%|████▏     | 558M/1.35G [00:14<00:16, 49.0MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 561M/1.35G [00:14<00:20, 37.9MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 563M/1.35G [00:14<00:24, 31.8MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 566M/1.35G [00:14<00:21, 36.1MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 569M/1.35G [00:14<00:21, 36.5MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 570M/1.35G [00:14<00:21, 36.5MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 573M/1.35G [00:14<00:21, 36.0MB/s][1,12]<stderr>:#015Downloading:  43%|████▎     | 575M/1.35G [00:14<00:19, 39.6MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 579M/1.35G [00:14<00:19, 40.0MB/s][1,12]<stderr>:#015Downloading:  43%|████▎     | 581M/1.35G [00:14<00:17, 42.6MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 584M/1.35G [00:14<00:18, 41.3MB/s][1,12]<stderr>:#015Downloading:  44%|████▎     | 585M/1.35G [00:14<00:17, 44.2MB/s][1,7]<stderr>:#015Downloading:  44%|████▎     | 588M/1.35G [00:15<00:22, 33.3MB/s][1,12]<stderr>:#015Downloading:  44%|████▍     | 590M/1.35G [00:15<00:22, 33.8MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 593M/1.35G [00:15<00:20, 36.7MB/s][1,12]<stderr>:#015Downloading:  44%|████▍     | 595M/1.35G [00:15<00:19, 38.0MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 598M/1.35G [00:15<00:18, 39.6MB/s][1,12]<stderr>:#015Downloading:  45%|████▍     | 601M/1.35G [00:15<00:17, 41.8MB/s][1,7]<stderr>:#015Downloading:  45%|████▍     | 603M/1.35G [00:15<00:17, 43.3MB/s][1,12]<stderr>:#015Downloading:  45%|████▌     | 606M/1.35G [00:15<00:16, 44.2MB/s][1,7]<stderr>:#015Downloading:  45%|████▌     | 609M/1.35G [00:15<00:15, 46.8MB/s][1,12]<stderr>:#015Downloading:  45%|████▌     | 612M/1.35G [00:15<00:15, 47.8MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 615M/1.35G [00:15<00:15, 48.5MB/s][1,12]<stderr>:#015Downloading:  46%|████▌     | 617M/1.35G [00:15<00:15, 47.8MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 620M/1.35G [00:15<00:14, 49.8MB/s][1,12]<stderr>:#015Downloading:  46%|████▌     | 622M/1.35G [00:15<00:19, 37.0MB/s][1,7]<stderr>:#015Downloading:  46%|████▋     | 625M/1.35G [00:15<00:18, 38.4MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 628M/1.35G [00:15<00:17, 41.9MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 629M/1.35G [00:15<00:18, 39.6MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 633M/1.35G [00:16<00:17, 41.2MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 635M/1.35G [00:16<00:16, 43.1MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 638M/1.35G [00:16<00:17, 40.8MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 640M/1.35G [00:16<00:17, 40.6MB/s][1,12]<stderr>:#015Downloading:  48%|████▊     | 643M/1.35G [00:16<00:16, 43.6MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 646M/1.35G [00:16<00:15, 45.0MB/s][1,12]<stderr>:#015Downloading:  48%|████▊     | 648M/1.35G [00:16<00:15, 45.7MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 651M/1.35G [00:16<00:15, 46.2MB/s][1,12]<stderr>:#015Downloading:  49%|████▊     | 654M/1.35G [00:16<00:14, 49.1MB/s][1,7]<stderr>:#015Downloading:  49%|████▊     | 655M/1.35G [00:16<00:15, 45.3MB/s][1,12]<stderr>:#015Downloading:  49%|████▉     | 659M/1.35G [00:16<00:14, 47.6MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 661M/1.35G [00:16<00:17, 39.7MB/s][1,12]<stderr>:#015Downloading:  49%|████▉     | 664M/1.35G [00:16<00:20, 33.6MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 665M/1.35G [00:16<00:19, 35.4MB/s][1,12]<stderr>:#015Downloading:  50%|████▉     | 669M/1.35G [00:16<00:17, 38.2MB/s][1,7]<stderr>:#015Downloading:  50%|████▉     | 671M/1.35G [00:16<00:17, 39.6MB/s][1,12]<stderr>:#015Downloading:  50%|█████     | 674M/1.35G [00:17<00:17, 38.7MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 675M/1.35G [00:17<00:16, 40.7MB/s][1,12]<stderr>:#015Downloading:  51%|█████     | 679M/1.35G [00:17<00:15, 42.1MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 681M/1.35G [00:17<00:14, 44.5MB/s][1,12]<stderr>:#015Downloading:  51%|█████     | 685M/1.35G [00:17<00:14, 45.1MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 686M/1.35G [00:17<00:14, 45.9MB/s][1,12]<stderr>:#015Downloading:  51%|█████▏    | 690M/1.35G [00:17<00:13, 47.7MB/s][1,12]<stderr>:#015Downloading:  52%|█████▏    | 695M/1.35G [00:17<00:13, 48.7MB/s][1,7]<stderr>:#015Downloading:  51%|█████▏    | 691M/1.35G [00:17<00:17, 37.6MB/s][1,12]<stderr>:#015Downloading:  52%|█████▏    | 701M/1.35G [00:17<00:12, 51.4MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 696M/1.35G [00:17<00:15, 41.5MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 707M/1.35G [00:17<00:12, 51.5MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 701M/1.35G [00:17<00:14, 43.0MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 712M/1.35G [00:17<00:11, 53.7MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 706M/1.35G [00:17<00:14, 44.9MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 711M/1.35G [00:17<00:13, 46.1MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 718M/1.35G [00:17<00:13, 47.8MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 716M/1.35G [00:17<00:13, 47.1MB/s][1,12]<stderr>:#015Downloading:  54%|█████▍    | 724M/1.35G [00:18<00:12, 49.5MB/s][1,7]<stderr>:#015Downloading:  54%|█████▎    | 721M/1.35G [00:18<00:13, 47.8MB/s][1,12]<stderr>:#015Downloading:  54%|█████▍    | 729M/1.35G [00:18<00:12, 51.2MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 726M/1.35G [00:18<00:12, 48.6MB/s][1,12]<stderr>:#015Downloading:  55%|█████▍    | 735M/1.35G [00:18<00:11, 52.6MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 731M/1.35G [00:18<00:12, 48.6MB/s][1,12]<stderr>:#015Downloading:  55%|█████▌    | 740M/1.35G [00:18<00:12, 47.0MB/s][1,7]<stderr>:#015Downloading:  55%|█████▍    | 736M/1.35G [00:18<00:12, 50.3MB/s][1,12]<stderr>:#015Downloading:  55%|█████▌    | 746M/1.35G [00:18<00:11, 50.6MB/s][1,7]<stderr>:#015Downloading:  55%|█████▌    | 742M/1.35G [00:18<00:11, 52.2MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 747M/1.35G [00:18<00:11, 50.2MB/s][1,12]<stderr>:#015Downloading:  56%|█████▌    | 752M/1.35G [00:18<00:15, 38.5MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 752M/1.35G [00:18<00:12, 48.9MB/s][1,12]<stderr>:#015Downloading:  56%|█████▋    | 757M/1.35G [00:18<00:14, 41.8MB/s][1,7]<stderr>:#015Downloading:  56%|█████▋    | 757M/1.35G [00:18<00:11, 49.2MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 763M/1.35G [00:18<00:12, 46.1MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 763M/1.35G [00:18<00:11, 50.2MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 768M/1.35G [00:18<00:12, 47.2MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 768M/1.35G [00:18<00:11, 50.9MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 773M/1.35G [00:19<00:12, 45.4MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 773M/1.35G [00:19<00:11, 51.1MB/s][1,12]<stderr>:#015Downloading:  58%|█████▊    | 779M/1.35G [00:19<00:11, 49.3MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 779M/1.35G [00:19<00:10, 53.2MB/s][1,12]<stderr>:#015Downloading:  58%|█████▊    | 784M/1.35G [00:19<00:12, 45.0MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 784M/1.35G [00:19<00:11, 47.8MB/s][1,12]<stderr>:#015Downloading:  59%|█████▊    | 789M/1.35G [00:19<00:15, 35.8MB/s][1,7]<stderr>:#015Downloading:  59%|█████▊    | 789M/1.35G [00:19<00:14, 37.5MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 794M/1.35G [00:19<00:14, 38.9MB/s][1,12]<stderr>:#015Downloading:  59%|█████▉    | 794M/1.35G [00:19<00:14, 37.6MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 798M/1.35G [00:19<00:13, 39.1MB/s][1,12]<stderr>:#015Downloading:  59%|█████▉    | 798M/1.35G [00:19<00:14, 37.7MB/s][1,7]<stderr>:#015Downloading:  60%|█████▉    | 803M/1.35G [00:19<00:12, 41.8MB/s][1,12]<stderr>:#015Downloading:  60%|█████▉    | 803M/1.35G [00:19<00:13, 41.3MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 807M/1.35G [00:19<00:15, 35.2MB/s][1,12]<stderr>:#015Downloading:  60%|██████    | 807M/1.35G [00:20<00:15, 35.0MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 811M/1.35G [00:20<00:16, 32.2MB/s][1,12]<stderr>:#015Downloading:  60%|██████    | 811M/1.35G [00:20<00:16, 32.1MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 815M/1.35G [00:20<00:15, 34.4MB/s][1,12]<stderr>:#015Downloading:  61%|██████    | 815M/1.35G [00:20<00:15, 33.9MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 821M/1.35G [00:20<00:13, 38.9MB/s][1,12]<stderr>:#015Downloading:  61%|██████    | 821M/1.35G [00:20<00:13, 39.0MB/s][1,7]<stderr>:#015Downloading:  61%|██████▏   | 825M/1.35G [00:20<00:12, 40.5MB/s][1,12]<stderr>:#015Downloading:  61%|██████▏   | 826M/1.35G [00:20<00:12, 41.6MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 830M/1.35G [00:20<00:11, 43.3MB/s][1,12]<stderr>:#015Downloading:  62%|██████▏   | 831M/1.35G [00:20<00:11, 44.4MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 836M/1.35G [00:20<00:10, 46.4MB/s][1,12]<stderr>:#015Downloading:  62%|██████▏   | 837M/1.35G [00:20<00:10, 48.1MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 841M/1.35G [00:20<00:12, 40.7MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 843M/1.35G [00:20<00:12, 40.4MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 847M/1.35G [00:20<00:11, 45.1MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 847M/1.35G [00:20<00:11, 42.7MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 852M/1.35G [00:20<00:11, 44.5MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 853M/1.35G [00:21<00:10, 45.1MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 858M/1.35G [00:21<00:10, 47.8MB/s][1,12]<stderr>:#015Downloading:  64%|██████▍   | 858M/1.35G [00:21<00:10, 46.9MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 863M/1.35G [00:21<00:09, 48.9MB/s][1,12]<stderr>:#015Downloading:  64%|██████▍   | 863M/1.35G [00:21<00:10, 44.8MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 868M/1.35G [00:21<00:12, 38.6MB/s][1,12]<stderr>:#015Downloading:  65%|██████▍   | 868M/1.35G [00:21<00:12, 38.9MB/s][1,12]<stderr>:#015Downloading:  65%|██████▍   | 872M/1.35G [00:21<00:16, 28.6MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 872M/1.35G [00:21<00:16, 28.9MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 875M/1.35G [00:21<00:20, 23.4MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 876M/1.35G [00:21<00:19, 23.8MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 878M/1.35G [00:22<00:21, 21.6MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 879M/1.35G [00:21<00:21, 22.1MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 881M/1.35G [00:22<00:20, 22.1MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 881M/1.35G [00:22<00:22, 20.4MB/s][1,12]<stderr>:#015Downloading:  66%|██████▌   | 883M/1.35G [00:22<00:20, 23.0MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 886M/1.35G [00:22<00:18, 24.5MB/s][1,12]<stderr>:#015Downloading:  66%|██████▌   | 889M/1.35G [00:22<00:16, 27.8MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 891M/1.35G [00:22<00:15, 28.7MB/s][1,12]<stderr>:#015Downloading:  66%|██████▋   | 894M/1.35G [00:22<00:13, 32.5MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 897M/1.35G [00:22<00:13, 33.8MB/s][1,12]<stderr>:#015Downloading:  67%|██████▋   | 899M/1.35G [00:22<00:12, 35.6MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 903M/1.35G [00:22<00:11, 39.0MB/s][1,12]<stderr>:#015Downloading:  67%|██████▋   | 904M/1.35G [00:22<00:12, 34.8MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 907M/1.35G [00:22<00:16, 25.9MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 908M/1.35G [00:22<00:16, 26.6MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 913M/1.35G [00:22<00:13, 30.9MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 914M/1.35G [00:23<00:13, 31.7MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 918M/1.35G [00:23<00:12, 34.5MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 919M/1.35G [00:23<00:11, 35.7MB/s][1,7]<stderr>:#015Downloading:  69%|██████▊   | 923M/1.35G [00:23<00:11, 37.9MB/s][1,12]<stderr>:#015Downloading:  69%|██████▊   | 924M/1.35G [00:23<00:10, 39.0MB/s][1,7]<stderr>:#015Download\u001b[0m\n",
      "\u001b[34ming:  69%|██████▉   | 928M/1.35G [00:23<00:10, 41.3MB/s][1,12]<stderr>:#015Downloading:  69%|██████▉   | 929M/1.35G [00:23<00:09, 42.0MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 933M/1.35G [00:23<00:09, 44.5MB/s][1,12]<stderr>:#015Downloading:  69%|██████▉   | 934M/1.35G [00:23<00:09, 44.3MB/s][1,7]<stderr>:#015Downloading:  70%|██████▉   | 939M/1.35G [00:23<00:08, 47.1MB/s][1,12]<stderr>:#015Downloading:  70%|██████▉   | 939M/1.35G [00:23<00:08, 46.3MB/s][1,7]<stderr>:#015Downloading:  70%|███████   | 945M/1.35G [00:23<00:07, 50.1MB/s][1,12]<stderr>:#015Downloading:  70%|███████   | 945M/1.35G [00:23<00:08, 49.9MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 950M/1.35G [00:23<00:09, 43.8MB/s][1,12]<stderr>:#015Downloading:  71%|███████   | 951M/1.35G [00:23<00:08, 44.1MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 955M/1.35G [00:23<00:09, 40.2MB/s][1,12]<stderr>:#015Downloading:  71%|███████   | 955M/1.35G [00:23<00:09, 39.0MB/s][1,12]<stderr>:#015Downloading:  71%|███████▏  | 960M/1.35G [00:24<00:10, 35.4MB/s][1,7]<stderr>:#015Downloading:  71%|███████▏  | 960M/1.35G [00:24<00:10, 36.0MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 965M/1.35G [00:24<00:09, 38.5MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 965M/1.35G [00:24<00:10, 37.8MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 969M/1.35G [00:24<00:09, 37.9MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 969M/1.35G [00:24<00:09, 37.8MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 974M/1.35G [00:24<00:08, 41.5MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 974M/1.35G [00:24<00:09, 40.9MB/s][1,12]<stderr>:#015Downloading:  73%|███████▎  | 980M/1.35G [00:24<00:08, 45.2MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 979M/1.35G [00:24<00:08, 43.5MB/s][1,12]<stderr>:#015Downloading:  73%|███████▎  | 985M/1.35G [00:24<00:07, 46.4MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 984M/1.35G [00:24<00:07, 46.0MB/s][1,12]<stderr>:#015Downloading:  74%|███████▎  | 990M/1.35G [00:24<00:07, 49.2MB/s][1,7]<stderr>:#015Downloading:  74%|███████▎  | 990M/1.35G [00:24<00:07, 48.9MB/s][1,12]<stderr>:#015Downloading:  74%|███████▍  | 996M/1.35G [00:24<00:06, 51.5MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 996M/1.35G [00:24<00:06, 51.6MB/s][1,12]<stderr>:#015Downloading:  74%|███████▍  | 1.00G/1.35G [00:24<00:06, 51.0MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 1.00G/1.35G [00:24<00:06, 52.9MB/s][1,12]<stderr>:#015Downloading:  75%|███████▍  | 1.01G/1.35G [00:24<00:06, 51.9MB/s][1,7]<stderr>:#015Downloading:  75%|███████▍  | 1.01G/1.35G [00:24<00:06, 54.2MB/s][1,12]<stderr>:#015Downloading:  75%|███████▌  | 1.01G/1.35G [00:25<00:06, 54.4MB/s][1,7]<stderr>:#015Downloading:  75%|███████▌  | 1.01G/1.35G [00:25<00:05, 56.2MB/s][1,12]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 39.6MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 40.1MB/s][1,12]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 40.1MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:09, 33.8MB/s][1,12]<stderr>:#015Downloading:  76%|███████▋  | 1.03G/1.35G [00:25<00:09, 33.7MB/s][1,7]<stderr>:#015Downloading:  76%|███████▋  | 1.03G/1.35G [00:25<00:08, 35.8MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.03G/1.35G [00:25<00:09, 34.3MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.03G/1.35G [00:25<00:08, 35.7MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 38.4MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:07, 39.3MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 36.1MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 36.7MB/s][1,12]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 41.0MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 41.5MB/s][1,12]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 38.7MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 39.2MB/s][1,12]<stderr>:#015Downloading:  79%|███████▊  | 1.06G/1.35G [00:26<00:08, 35.4MB/s][1,7]<stderr>:#015Downloading:  79%|███████▊  | 1.06G/1.35G [00:26<00:08, 36.1MB/s][1,12]<stderr>:#015Downloading:  79%|███████▉  | 1.06G/1.35G [00:26<00:08, 33.0MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.06G/1.35G [00:26<00:08, 33.4MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.07G/1.35G [00:26<00:08, 33.1MB/s][1,12]<stderr>:#015Downloading:  79%|███████▉  | 1.07G/1.35G [00:26<00:08, 33.0MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:07, 35.9MB/s][1,12]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:07, 36.7MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:06, 39.2MB/s][1,12]<stderr>:#015Downloading:  80%|███████▉  | 1.08G/1.35G [00:26<00:06, 40.4MB/s][1,12]<stderr>:#015Downloading:  80%|████████  | 1.08G/1.35G [00:26<00:06, 43.8MB/s][1,7]<stderr>:#015Downloading:  80%|████████  | 1.08G/1.35G [00:26<00:06, 42.0MB/s][1,12]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 46.1MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.08G/1.35G [00:27<00:05, 43.8MB/s][1,12]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 47.7MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 45.4MB/s][1,7]<stderr>:#015Downloading:  81%|████████▏ | 1.09G/1.35G [00:27<00:05, 46.6MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 48.7MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 47.5MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:04, 50.0MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 48.1MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.11G/1.35G [00:27<00:04, 51.5MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.11G/1.35G [00:27<00:04, 48.5MB/s][1,12]<stderr>:#015Downloading:  83%|████████▎ | 1.11G/1.35G [00:27<00:04, 53.8MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 50.8MB/s][1,12]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 45.7MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 49.9MB/s][1,12]<stderr>:#015Downloading:  84%|████████▎ | 1.12G/1.35G [00:27<00:04, 47.8MB/s][1,7]<stderr>:#015Downloading:  84%|████████▎ | 1.13G/1.35G [00:27<00:04, 50.1MB/s][1,12]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:27<00:04, 49.5MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:27<00:04, 52.1MB/s][1,12]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:28<00:04, 46.6MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.14G/1.35G [00:28<00:04, 50.7MB/s][1,12]<stderr>:#015Downloading:  85%|████████▍ | 1.14G/1.35G [00:28<00:04, 49.4MB/s][1,7]<stderr>:#015Downloading:  85%|████████▍ | 1.14G/1.35G [00:28<00:03, 52.9MB/s][1,12]<stderr>:#015Downloading:  85%|████████▌ | 1.15G/1.35G [00:28<00:03, 52.1MB/s][1,7]<stderr>:#015Downloading:  85%|████████▌ | 1.15G/1.35G [00:28<00:04, 40.5MB/s][1,12]<stderr>:#015Downloading:  86%|████████▌ | 1.15G/1.35G [00:28<00:05, 33.7MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.15G/1.35G [00:28<00:05, 35.7MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:05, 31.6MB/s][1,12]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:06, 30.8MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:08, 22.0MB/s][1,12]<stderr>:#015Downloading:  86%|████████▋ | 1.16G/1.35G [00:28<00:08, 22.3MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.16G/1.35G [00:29<00:06, 26.6MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:06, 27.0MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:05, 31.7MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:06, 28.8MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:05, 29.4MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.18G/1.35G [00:29<00:05, 31.4MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:05, 32.9MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:04, 34.6MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:05, 32.2MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:04, 33.4MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.19G/1.35G [00:29<00:04, 32.0MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.19G/1.35G [00:29<00:05, 29.1MB/s][1,12]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:29<00:07, 21.1MB/s][1,7]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:29<00:07, 20.9MB/s][1,12]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:30<00:07, 20.7MB/s][1,7]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:30<00:07, 20.4MB/s][1,12]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.5MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.3MB/s][1,12]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.3MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.5MB/s][1,12]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 27.1MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 27.1MB/s][1,12]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 26.0MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 25.1MB/s][1,12]<stderr>:#015Downloading:  90%|█████████ | 1.21G/1.35G [00:30<00:05, 26.1MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.21G/1.35G [00:30<00:04, 27.7MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.22G/1.35G [00:30<00:04, 28.2MB/s][1,12]<stderr>:#015Downloading:  90%|█████████ | 1.22G/1.35G [00:30<00:04, 28.2MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:30<00:03, 32.2MB/s][1,12]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:30<00:03, 32.5MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:31<00:04, 28.0MB/s][1,12]<stderr>:#015Downloading:  91%|█████████ | 1.23G/1.35G [00:31<00:04, 25.6MB/s][1,7]<stderr>:#015Downloading:  91%|█████████▏| 1.23G/1.35G [00:31<00:04, 28.1MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.23G/1.35G [00:31<00:03, 31.0MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.23G/1.35G [00:31<00:04, 24.0MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:04, 25.1MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 28.9MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 30.4MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 33.9MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 34.1MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 37.7MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 35.0MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 36.7MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 38.1MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▎| 1.26G/1.35G [00:31<00:02, 41.2MB/s][1,12]<stderr>:#015Downloading:  94%|█████████▎| 1.26G/1.35G [00:31<00:02, 41.4MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.26G/1.35G [00:32<00:01, 42.1MB/s][1,12]<stderr>:#015Downloading:  94%|█████████▍| 1.27G/1.35G [00:32<00:01, 45.5MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.27G/1.35G [00:32<00:01, 43.8MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▍| 1.27G/1.35G [00:32<00:01, 47.0MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▍| 1.27G/1.35G [00:32<00:01, 46.8MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▍| 1.28G/1.35G [00:32<00:01, 44.2MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 45.3MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 44.8MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 41.4MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 39.0MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 42.4MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 42.8MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 45.3MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▋| 1.30G/1.35G [00:32<00:01, 46.3MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:00, 47.8MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:01, 39.5MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:01, 37.7MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 31.9MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 33.6MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 31.7MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.31G/1.35G [00:33<00:01, 31.5MB/s][1,12]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 36.7MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 34.9MB/s][1,12]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 40.9MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 37.5MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▊| 1.33G/1.35G [00:33<00:00, 31.3MB/s][1,12]<stderr>:#015Downloading:  99%|█████████▊| 1.33G/1.35G [00:33<00:00, 25.6MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.33G/1.35G [00:33<00:00, 27.4MB/s][1,12]<stderr>:#015Downloading:  99%|████████�\u001b[0m\n",
      "\u001b[34m��▉| 1.33G/1.35G [00:33<00:00, 30.3MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.33G/1.35G [00:33<00:00, 30.7MB/s][1,12]<stderr>:#015Downloading:  99%|█████████▉| 1.34G/1.35G [00:34<00:00, 25.6MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.34G/1.35G [00:34<00:00, 25.7MB/s][1,12]<stderr>:#015Downloading: 100%|█████████▉| 1.34G/1.35G [00:34<00:00, 30.3MB/s][1,7]<stderr>:#015Downloading: 100%|█████████▉| 1.34G/1.35G [00:34<00:00, 30.4MB/s][1,12]<stderr>:#015Downloading: 100%|██████████| 1.35G/1.35G [00:34<00:00, 39.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading: 100%|██████████| 1.35G/1.35G [00:34<00:00, 39.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,0]<stderr>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:19,509 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:19,509 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,2]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,5]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,6]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,1]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,3]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,4]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,7]<stderr>:#015  1%|          | 1/88 [00:00<00:54,  1.60ba/s][1,1]<stderr>:#015  1%|          | 1/88 [00:00<01:01,  1.42ba/s][1,0]<stderr>:#015  1%|          | 1/88 [00:00<01:04,  1.35ba/s][1,2]<stderr>:#015  1%|          | 1/88 [00:00<01:10,  1.23ba/s][1,6]<stderr>:#015  1%|          | 1/88 [00:00<01:11,  1.21ba/s][1,5]<stderr>:#015  1%|          | 1/88 [00:00<01:15,  1.15ba/s][1,3]<stderr>:#015  1%|          | 1/88 [00:00<01:09,  1.25ba/s][1,7]<stderr>:#015  2%|▏         | 2/88 [00:00<00:46,  1.84ba/s][1,4]<stderr>:#015  1%|          | 1/88 [00:00<01:08,  1.27ba/s][1,1]<stderr>:#015  2%|▏         | 2/88 [00:01<00:54,  1.58ba/s][1,2]<stderr>:#015  2%|▏         | 2/88 [00:01<01:00,  1.42ba/s][1,0]<stderr>:#015  2%|▏         | 2/88 [00:01<00:58,  1.47ba/s][1,6]<stderr>:#015  2%|▏         | 2/88 [00:01<01:01,  1.40ba/s][1,5]<stderr>:#015  2%|▏         | 2/88 [00:01<01:04,  1.34ba/s][1,3]<stderr>:#015  2%|▏         | 2/88 [00:01<01:02,  1.38ba/s][1,4]<stderr>:#015  2%|▏         | 2/88 [00:01<00:59,  1.45ba/s][1,7]<stderr>:#015  3%|▎         | 3/88 [00:01<00:45,  1.88ba/s][1,1]<stderr>:#015  3%|▎         | 3/88 [00:01<00:49,  1.71ba/s][1,2]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.60ba/s][1,6]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.60ba/s][1,0]<stderr>:#015  3%|▎         | 3/88 [00:01<00:51,  1.64ba/s][1,5]<stderr>:#015  3%|▎         | 3/88 [00:01<00:54,  1.55ba/s][1,3]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.59ba/s][1,4]<stderr>:#015  3%|▎         | 3/88 [00:01<00:51,  1.66ba/s][1,7]<stderr>:#015  5%|▍         | 4/88 [00:01<00:40,  2.06ba/s][1,1]<stderr>:#015  5%|▍         | 4/88 [00:02<00:46,  1.82ba/s][1,7]<stderr>:#015  6%|▌         | 5/88 [00:02<00:37,  2.24ba/s][1,6]<stderr>:#015  5%|▍         | 4/88 [00:02<00:49,  1.71ba/s][1,5]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.68ba/s][1,0]<stderr>:#015  5%|▍         | 4/88 [00:02<00:48,  1.72ba/s][1,2]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.65ba/s][1,4]<stderr>:#015  5%|▍         | 4/88 [00:02<00:47,  1.77ba/s][1,3]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.67ba/s][1,1]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.85ba/s][1,5]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.81ba/s][1,2]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.82ba/s][1,0]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.85ba/s][1,4]<stderr>:#015  6%|▌         | 5/88 [00:02<00:42,  1.96ba/s][1,6]<stderr>:#015  6%|▌         | 5/88 [00:02<00:46,  1.80ba/s][1,3]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.86ba/s][1,7]<stderr>:#015  7%|▋         | 6/88 [00:02<00:40,  2.03ba/s][1,2]<stderr>:#015  7%|▋         | 6/88 [00:03<00:42,  1.94ba/s][1,0]<stderr>:#015  7%|▋         | 6/88 [00:03<00:41,  1.97ba/s][1,6]<stderr>:#015  7%|▋         | 6/88 [00:03<00:42,  1.92ba/s][1,4]<stderr>:#015  7%|▋         | 6/88 [00:02<00:40,  2.05ba/s][1,3]<stderr>:#015  7%|▋         | 6/88 [00:03<00:40,  2.02ba/s][1,1]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.89ba/s][1,7]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.18ba/s][1,5]<stderr>:#015  7%|▋         | 6/88 [00:03<00:46,  1.78ba/s][1,0]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.07ba/s][1,2]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.04ba/s][1,6]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.04ba/s][1,4]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.13ba/s][1,7]<stderr>:#015  9%|▉         | 8/88 [00:03<00:35,  2.26ba/s][1,5]<stderr>:#015  8%|▊         | 7/88 [00:03<00:40,  2.01ba/s][1,1]<stderr>:#015  8%|▊         | 7/88 [00:03<00:42,  1.92ba/s][1,3]<stderr>:#015  8%|▊         | 7/88 [00:03<00:41,  1.96ba/s][1,0]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.08ba/s][1,7]<stderr>:#015 10%|█         | 9/88 [00:04<00:35,  2.25ba/s][1,4]<stderr>:#015  9%|▉         | 8/88 [00:03<00:38,  2.08ba/s][1,6]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.98ba/s][1,2]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.97ba/s][1,1]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  2.00ba/s][1,3]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.05ba/s][1,5]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.99ba/s][1,15]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:23,793 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:23,793 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,11]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,8]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,10]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,12]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,11]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,9]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,13]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.08ba/s][1,13]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,2]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.08ba/s][1,1]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.09ba/s][1,3]<stderr>:#015 10%|█         | 9/88 [00:04<00:36,  2.16ba/s][1,4]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.08ba/s][1,5]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.08ba/s][1,7]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:37,  2.09ba/s][1,6]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.93ba/s][1,0]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.13ba/s][1,15]<stderr>:#015  1%|          | 1/88 [00:00<00:58,  1.49ba/s][1,4]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.17ba/s][1,12]<stderr>:#015  1%|          | 1/88 [00:00<00:58,  1.48ba/s][1,1]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.14ba/s][1,2]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:37,  2.09ba/s][1,5]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.13ba/s][1,7]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:35,  2.16ba/s][1,3]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.14ba/s][1,13]<stderr>:#015  1%|          | 1/88 [00:00<00:46,  1.87ba/s][1,6]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.06ba/s][1,10]<stderr>:#015  1%|          | 1/88 [00:00<01:11,  1.22ba/s][1,8]<stderr>:#015  1%|          | 1/88 [00:00<01:17,  1.12ba/s][1,11]<stderr>:#015  1%|          | 1/88 [00:00<01:16,  1.14ba/s][1,14]<stderr>:#015  1%|          | 1/88 [00:00<01:24,  1.03ba/s][1,9]<stderr>:#015  1%|          | 1/88 [00:00<01:23,  1.05ba/s][1,12]<stderr>:#015  2%|▏         | 2/88 [00:01<00:51,  1.66ba/s][1,0]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:36,  2.10ba/s][1,15]<stderr>:#015  2%|▏         | 2/88 [00:01<00:53,  1.61ba/s][1,7]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.18ba/s][1,13]<stderr>:#015  2%|▏         | 2/88 [00:00<00:43,  1.97ba/s][1,10]<stderr>:#015  2%|▏         | 2/88 [00:01<00:59,  1.45ba/s][1,5]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.06ba/s][1,4]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s][1,3]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.07ba/s][1,2]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  2.00ba/s][1,6]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.05ba/s][1,1]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  2.00ba/s][1,11]<stderr>:#015  2%|▏         | 2/88 [00:01<01:03,  1.35ba/s][1,8]<stderr>:#015  2%|▏         | 2/88 [00:01<01:05,  1.32ba/s][1,14]<stderr>:#015  2%|▏         | 2/88 [00:01<01:09,  1.24ba/s][1,12]<stderr>:#015  3%|▎         | 3/88 [00:01<00:45,  1.85ba/s][1,0]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.19ba/s][1,9]<stderr>:#015  2%|▏         | 2/88 [00:01<01:10,  1.21ba/s][1,7]<stderr>:#015 15%|█▍        | 13/88 [00:05<00:34,  2.19ba/s][1,10]<stderr>:#015  3%|▎         | 3/88 [00:01<00:52,  1.63ba/s][1,5]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.16ba/s][1,4]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.14ba/s][1,3]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.17ba/s][1,15]<stderr>:#015  3%|▎         | 3/88 [00:01<00:49,  1.72ba/s][1,2]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.12ba/s][1,6]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.13ba/s][1,1]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:36,  2.09ba/s][1,11]<stderr>:#015  3%|▎         | 3/88 [00:01<00:55,  1.53ba/s][1,8]<stderr>:#015  3%|▎         | 3/88 [00:01<00:56,  1.51ba/s][1,14]<stderr>:#015  3%|▎         | 3/88 [00:01<00:58,  1.46ba/s][1,13]<stderr>:#015  3%|▎         | 3/88 [00:01<00:48,  1.74ba/s][1,12]<stderr>:#015  5%|▍         | 4/88 [00:01<00:42,  1.96ba/s][1,9]<stderr>:#015  3%|▎         | 3/88 [00:01<00:58,  1.45ba/s][1,7]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:31,  2.32ba/s][1,0]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.22ba/s][1,5]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.26ba/s][1,2]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.22ba/s][1,6]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.21ba/s][1,3]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.19ba/s][1,4]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.15ba/s][1,1]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.14ba/s][1,11]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.65ba/s][1,15]<stderr>:#015  5%|▍         | 4/88 [00:02<00:49,  1.70ba/s][1,8]<stderr>:#015  5%|▍         | 4/88 [00:02<00:51,  1.64ba/s][1,10]<stderr>:#015  5%|▍         | 4/88 [00:02<00:52,  1.61ba/s][1,12]<stderr>:#015  6%|▌         | 5/88 [00:02<00:38,  2.16ba/s][1,13]<stderr>:#015  5%|▍         | 4/88 [00:02<00:43,  1.93ba/s][1,14]<stderr>:#015  5%|▍         | 4/88 [00:02<00:53,  1.56ba/s][1,7]<stderr>:#015 17%|█▋        | 15/88 [00:06<00:30,  2.37ba/s][1,0]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.29ba/s][1,5]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:31,  2.33ba/s][1,2]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.28ba/s][1,9]<stderr>:#015  5%|▍         | 4/88 [00:02<00:53,  1.58ba/s][1,1]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.27ba/s][1,4]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:33,  2.24ba/s][1,6]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.26ba/s][1,3]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:34,  2.16ba/s][1,10]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.76ba/s][1,12]<stderr>:#015  7%|▋         | 6/88 [00:02<00:37,  2.21ba/s][1,8]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.76ba/s][1,14]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.74ba/s][1,7]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:29,  2.43ba/s][1,11]<stderr>:#015  6%|▌         | 5/88 [00:02<00:48,  1.72ba/s][1,15]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.74ba/s][1,9]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.84ba/s][1,13]<stderr>:#015  6%|▌         | 5/88 [00:02<00:42,  1.95ba/s][1,5]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.17ba/s][1,0]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.08ba/s][1,2]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.16ba/s][1,3]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.20ba/s][1,6]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.18ba/s][1,4]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:34,  2.14ba/s][1,1]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:34,  2.13ba/s][1,10]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.87ba/s][1,7]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:29,  2.43ba/s][1,14]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.85ba/s][1,12]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.16ba/s][1,11]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.86ba/s][1,8]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.84ba/s][1,15]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.89ba/s][1,9]<stderr>:#015  7%|▋         | 6/88 [00:03<00:41,  1.97ba/s][1,5]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.25ba/s][1,6]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.29ba/s][1,3]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.30ba/s][1,1]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.29ba/s][1,2]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.22ba/s][1,13]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.82ba/s][1,0]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:34,  2.07ba/s][1,7]<stderr>:#015 20%|██        | 18/88 [00:07<00:26,  2.67ba/s][1,4]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.15ba/s][1,10]<stderr>:#015  8%|▊         | 7/88 [00:03<00:41,  1.93ba/s][1,15]<stderr>:#015  8%|▊         | 7/88 [00:03<00:40,  1.99ba/s][1,9]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.03ba/s][1,14]<stderr>:#015  8%|▊         | 7/88 [00:03<00:43,  1.84ba/s][1,11]<stderr>:#015  8%|▊         | 7/88 [00:03<00:43,  1.85ba/s][1,13]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.07ba/s][1,8]<stderr>:#015  8%|▊         | 7/88 [00:03<00:44,  1.82ba/s][1,1]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:30,  2.34ba/s][1,12]<stderr>:#015  9%|▉         | 8/88 [00:03<00:39,  2.01ba/s][1,6]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:30,  2.33ba/s][1,5]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.27ba/s][1,0]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.22ba/s][1,4]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:30,  2.30ba/s][1,7]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:25,  2.66ba/s][1,2]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:32,  2.17ba/s][1,3]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:33,  2.14ba/s][1,9]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.09ba/s][1,15]<stderr>:#015  9%|▉         | 8/88 [00:04<00:39,  2.01ba/s][1,10]<stderr>:#015  9%|▉         | 8/88 [00:04<00:41,  1.92ba/s][1,13]<stderr>:#015  9%|▉         | 8/88 [00:03<00:37,  2.14ba/s][1,12]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.09ba/s][1,14]<stderr>:#015  9%|▉         | 8/88 [00:04<00:42,  1.89ba/s][1,5]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.20ba/s][1,0]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,6]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,7]<stderr>:#015 23%|██▎       | 20/88 [00:08<00:27,  2.45ba/s][1,11]<stderr>:#015  9%|▉         | 8/88 [00:04<00:44,  1.80ba/s][1,1]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.15ba/s][1,8]<stderr>:#015  9%|▉         | 8/88 [00:04<00:44,  1.79ba/s][1,2]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.16ba/s][1,3]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.20ba/s][1,4]<stderr>:#015 20%|██        | 18/88 [00:08<00:33,  2.10ba/s][1,9]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.07ba/s][1,10]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  1.98ba/s][1,14]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  2.03ba/s][1,13]<stderr>:#015 10%|█         | 9/88 [00:04<00:36,  2.16ba/s][1,15]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  1.98ba/s][1,12]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.12ba/s][1,5]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.23ba/s][1,6]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.26ba/s][1,11]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.97ba/s][1,7]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:27,  2.44ba/s][1,0]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.21ba/s][1,1]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.20ba/s][1,8]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.95ba/s][1,4]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:31,  2.22ba/s][1,3]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.14ba/s][1,2]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:33,  2.09ba/s][1,9]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:34,  2.23ba/s][1,14]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.09ba/s][1,15]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.07ba/s][1,11]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:36,  2.15ba/s][1,5]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.31ba/s][1,6]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.34ba/s][1,8]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:36,  2.14ba/s][1,0]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.31ba/s][1,10]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:39,  1.96ba/s][1,7]<stderr>:#015 25%|██▌       | 22/88 [00:09<00:27,  2.40ba/s][1,4]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.33ba/s][1,12]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.04ba/s][1,13]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:38,  2.02ba/s][1,3]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.28ba/s][1,2]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.26ba/s][1,1]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.16ba/s][1,9]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:35,  2.16ba/s][1,6]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.36ba/s][1,12]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.21ba/s][1,5]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.29ba/s][1,13]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:34,  2.21ba/s][1,0]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.29ba/s][1,7]<stderr>:#015 26%|██▌       | 23/88 [00:09<00:26,  2.42ba/s][1,4]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.35ba/s][1,3]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.38ba/s][1,14]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s][1,1]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.30ba/s][1,2]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.31ba/s][1,15]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,11]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,8]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,10]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:41,  1.85ba/s][1,9]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.20ba/s][1,6]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:28,  2.31ba/s][1,12]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.15ba/s][1,14]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.08ba/s][1,5]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:30,  2.18ba/s][1,4]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.27ba/s][1,15]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.07ba/s][1,7]<stderr>:#015 27%|██▋       | 24/88 [00:10<00:28,  2.28ba/s][1,3]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.23ba/s][1,11]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.09ba/s][1,2]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.22ba/s][1,0]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.12ba/s][1,13]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:36,  2.06ba/s][1,1]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:30,  2.18ba/s][1,8]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.07ba/s][1,10]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:37,  2.03ba/s][1,9]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.20ba/s][1,6]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.31ba/s][1,5]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.30ba/s][1,7]<stderr>:#015 28%|██▊       | 25/88 [00:10<00:26,  2.37ba/s][1,4]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:27,  2.33ba/s][1,1]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.32ba/s][1,0]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.25ba/s][1,14]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.12ba/s][1,12]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:34,  2.12ba/s][1,3]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.25ba/s][1,15]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:36,  2.08ba/s][1,8]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.14ba/s][1,2]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.22ba/s][1,11]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:36,  2.08ba/s][1,10]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:37,  2.01ba/s][1,13]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:39,  1.90ba/s][1,9]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.28ba/s][1,7]<stderr>:#015 30%|██▉       | 26/88 [00:11<00:25,  2.40ba/s][1,6]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.27ba/s][1,5]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.20ba/s][1,1]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.28ba/s][1,3]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:27,  2.29ba/s][1,14]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:34,  2.12ba/s][1,8]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:33,  2.22ba/s][1,15]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:35,  2.11ba/s][1,11]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:34,  2.18ba/s][1,10]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:33,  2.20ba/s][1,4]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.13ba/s][1,2]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.18ba/s][1,0]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.10ba/s][1,13]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:35,  2.08ba/s][1,12]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:38,  1.88ba/s][1,9]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,5]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.20ba/s][1,3]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:27,  2.29ba/s][1,6]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:29,  2.14ba/s][1,7]<stderr>:#015 31%|███       | 27/88 [00:11<00:28,  2.16ba/s][1,1]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.22ba/s][1,13]<stderr>:#015 17%\u001b[0m\n",
      "\u001b[34m|█▋        | 15/88 [00:07<00:32,  2.25ba/s][1,15]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.08ba/s][1,14]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.05ba/s][1,12]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.16ba/s][1,4]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:29,  2.12ba/s][1,0]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.09ba/s][1,11]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,10]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,8]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.03ba/s][1,2]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.06ba/s][1,9]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.20ba/s][1,7]<stderr>:#015 32%|███▏      | 28/88 [00:12<00:26,  2.22ba/s][1,5]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.17ba/s][1,13]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.27ba/s][1,3]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.16ba/s][1,4]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.19ba/s][1,1]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.15ba/s][1,6]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.05ba/s][1,0]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.20ba/s][1,11]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.19ba/s][1,15]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.13ba/s][1,10]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.21ba/s][1,8]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.18ba/s][1,14]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:34,  2.07ba/s][1,12]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:33,  2.15ba/s][1,2]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.17ba/s][1,9]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:30,  2.34ba/s][1,15]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.22ba/s][1,5]<stderr>:#015 31%|███       | 27/88 [00:12<00:28,  2.14ba/s][1,1]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.21ba/s][1,11]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.25ba/s][1,10]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.27ba/s][1,8]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.25ba/s][1,14]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:32,  2.20ba/s][1,3]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.18ba/s][1,4]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.21ba/s][1,2]<stderr>:#015 31%|███       | 27/88 [00:12<00:26,  2.29ba/s][1,0]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.22ba/s][1,6]<stderr>:#015 31%|███       | 27/88 [00:12<00:29,  2.09ba/s][1,7]<stderr>:#015 33%|███▎      | 29/88 [00:12<00:28,  2.09ba/s][1,13]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:34,  2.08ba/s][1,12]<stderr>:#015 20%|██        | 18/88 [00:08<00:35,  1.95ba/s][1,9]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.21ba/s][1,5]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.18ba/s][1,13]<stderr>:#015 20%|██        | 18/88 [00:08<00:30,  2.29ba/s][1,1]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.19ba/s][1,3]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.19ba/s][1,0]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.21ba/s][1,15]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.17ba/s][1,4]<stderr>:#015 32%|███▏      | 28/88 [00:12<00:27,  2.19ba/s][1,14]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.16ba/s][1,2]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:26,  2.23ba/s][1,10]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,6]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.11ba/s][1,7]<stderr>:#015 34%|███▍      | 30/88 [00:13<00:27,  2.09ba/s][1,8]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.13ba/s][1,11]<stderr>:#015 20%|██        | 18/88 [00:08<00:33,  2.10ba/s][1,12]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:31,  2.20ba/s][1,9]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.12ba/s][1,13]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.29ba/s][1,14]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.21ba/s][1,10]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.23ba/s][1,11]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.22ba/s][1,8]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.19ba/s][1,12]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.27ba/s][1,15]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.12ba/s][1,5]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:28,  2.06ba/s][1,7]<stderr>:#015 35%|███▌      | 31/88 [00:13<00:28,  2.03ba/s][1,1]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.00ba/s][1,3]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  1.99ba/s][1,4]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  1.98ba/s][1,2]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.01ba/s][1,0]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.96ba/s][1,6]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:31,  1.85ba/s][1,9]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:32,  2.11ba/s][1,14]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.26ba/s][1,10]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.25ba/s][1,12]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.32ba/s][1,8]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.18ba/s][1,11]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.18ba/s][1,7]<stderr>:#015 36%|███▋      | 32/88 [00:14<00:26,  2.08ba/s][1,15]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:33,  2.02ba/s][1,13]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:33,  2.03ba/s][1,1]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.04ba/s][1,3]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.02ba/s][1,0]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.05ba/s][1,4]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.03ba/s][1,5]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.94ba/s][1,6]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.02ba/s][1,9]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:30,  2.17ba/s][1,2]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:31,  1.85ba/s][1,14]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:29,  2.29ba/s][1,8]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:29,  2.25ba/s][1,10]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:30,  2.18ba/s][1,13]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:30,  2.17ba/s][1,15]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:31,  2.15ba/s][1,7]<stderr>:#015 38%|███▊      | 33/88 [00:14<00:26,  2.11ba/s][1,11]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:32,  2.07ba/s][1,12]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.08ba/s][1,1]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,0]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,3]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.02ba/s][1,4]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.03ba/s][1,6]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.01ba/s][1,2]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,9]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.11ba/s][1,14]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.26ba/s][1,5]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:33,  1.73ba/s][1,13]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.26ba/s][1,12]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.23ba/s][1,8]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.06ba/s][1,10]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.02ba/s][1,11]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.05ba/s][1,15]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.04ba/s][1,7]<stderr>:#015 39%|███▊      | 34/88 [00:15<00:26,  2.05ba/s][1,1]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.95ba/s][1,9]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.19ba/s][1,14]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:27,  2.33ba/s][1,4]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.94ba/s][1,0]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.89ba/s][1,6]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.89ba/s][1,2]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.90ba/s][1,13]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.24ba/s][1,15]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:29,  2.17ba/s][1,5]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:32,  1.74ba/s][1,7]<stderr>:#015 40%|███▉      | 35/88 [00:15<00:24,  2.16ba/s][1,11]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:30,  2.14ba/s][1,10]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:32,  2.02ba/s][1,12]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.05ba/s][1,3]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:34,  1.63ba/s][1,8]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:32,  1.97ba/s][1,1]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:26,  2.06ba/s][1,14]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:27,  2.36ba/s][1,9]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.14ba/s][1,4]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.93ba/s][1,0]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.92ba/s][1,6]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.92ba/s][1,15]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.22ba/s][1,5]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:29,  1.89ba/s][1,7]<stderr>:#015 41%|████      | 36/88 [00:16<00:23,  2.17ba/s][1,2]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:30,  1.81ba/s][1,10]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.08ba/s][1,8]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.08ba/s][1,13]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.00ba/s][1,3]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:31,  1.76ba/s][1,11]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.04ba/s][1,1]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:25,  2.14ba/s][1,12]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:32,  1.96ba/s][1,14]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.18ba/s][1,4]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.98ba/s][1,9]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.06ba/s][1,6]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.98ba/s][1,5]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.99ba/s][1,0]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:29,  1.86ba/s][1,15]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:28,  2.19ba/s][1,13]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.00ba/s][1,8]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:30,  2.05ba/s][1,7]<stderr>:#015 42%|████▏     | 37/88 [00:16<00:25,  2.01ba/s][1,10]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.01ba/s][1,1]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:25,  2.11ba/s][1,11]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.03ba/s][1,12]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.05ba/s][1,3]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:30,  1.80ba/s][1,2]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:30,  1.77ba/s][1,14]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.28ba/s][1,9]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.24ba/s][1,4]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:24,  2.13ba/s][1,6]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.01ba/s][1,15]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.25ba/s][1,5]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.04ba/s][1,0]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:28,  1.84ba/s][1,11]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:29,  2.08ba/s][1,8]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.06ba/s][1,9]<stderr>:#015 31%|███       | 27/88 [00:12<00:26,  2.29ba/s][1,10]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.03ba/s][1,14]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.19ba/s][1,13]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:32,  1.91ba/s][1,7]<stderr>:#015 43%|████▎     | 38/88 [00:17<00:25,  1.93ba/s][1,3]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:29,  1.80ba/s][1,15]<stderr>:#015 31%|███       | 27/88 [00:12<00:25,  2.40ba/s][1,2]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:30,  1.73ba/s][1,12]<stderr>:#015 31%|███       | 27/88 [00:12<00:32,  1.86ba/s][1,4]<stderr>:#015 41%|████      | 36/88 [00:17<00:26,  1.93ba/s][1,6]<stderr>:#015 41%|████      | 36/88 [00:17<00:26,  1.98ba/s][1,1]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.79ba/s][1,0]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,9]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:26,  2.22ba/s][1,15]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:24,  2.50ba/s][1,13]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.02ba/s][1,14]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.22ba/s][1,10]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.02ba/s][1,8]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.01ba/s][1,5]<stderr>:#015 41%|████      | 36/88 [00:17<00:29,  1.78ba/s][1,7]<stderr>:#015 44%|████▍     | 39/88 [00:17<00:24,  2.01ba/s][1,11]<stderr>:#015 31%|███       | 27/88 [00:13<00:31,  1.95ba/s][1,12]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:29,  2.04ba/s][1,3]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.84ba/s][1,1]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:26,  1.92ba/s][1,4]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.96ba/s][1,6]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,2]<stderr>:#015 41%|████      | 36/88 [00:17<00:31,  1.63ba/s][1,0]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,11]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.12ba/s][1,10]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.08ba/s][1,8]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:29,  2.07ba/s][1,7]<stderr>:#015 45%|████▌     | 40/88 [00:18<00:23,  2.06ba/s][1,5]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:27,  1.85ba/s][1,15]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:26,  2.23ba/s][1,3]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.93ba/s][1,13]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:30,  1.94ba/s][1,9]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.00ba/s][1,14]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.96ba/s][1,12]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.92ba/s][1,4]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.96ba/s][1,1]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:26,  1.90ba/s][1,6]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.98ba/s][1,0]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.08ba/s][1,2]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:30,  1.68ba/s][1,5]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.02ba/s][1,8]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:29,  1.99ba/s][1,3]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.01ba/s][1,12]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.12ba/s][1,10]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:30,  1.97ba/s][1,14]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.12ba/s][1,9]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.08ba/s][1,13]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:29,  1.97ba/s][1,11]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:30,  1.93ba/s][1,15]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.13ba/s][1,7]<stderr>:#015 47%|████▋     | 41/88 [00:18<00:24,  1.93ba/s][1,2]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:26,  1.86ba/s][1,6]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:25,  1.91ba/s][1,4]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:26,  1.86ba/s][1,0]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:24,  1.98ba/s][1,5]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:24,  1.98ba/s][1,3]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:23,  2.07ba/s][1,1]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:28,  1.72ba/s][1,7]<stderr>:#015 48%|████▊     \u001b[0m\n",
      "\u001b[34m| 42/88 [00:19<00:22,  2.05ba/s][1,10]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.98ba/s][1,11]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.01ba/s][1,9]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.06ba/s][1,15]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:26,  2.14ba/s][1,13]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.01ba/s][1,8]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.95ba/s][1,14]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.02ba/s][1,12]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  1.98ba/s][1,6]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.00ba/s][1,2]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:26,  1.83ba/s][1,4]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.86ba/s][1,1]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.87ba/s][1,7]<stderr>:#015 49%|████▉     | 43/88 [00:19<00:20,  2.18ba/s][1,3]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:22,  2.14ba/s][1,5]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.03ba/s][1,11]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:27,  2.05ba/s][1,10]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:28,  2.02ba/s][1,0]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:26,  1.83ba/s][1,8]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:28,  2.00ba/s][1,14]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.91ba/s][1,13]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:30,  1.84ba/s][1,9]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:30,  1.86ba/s][1,6]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.07ba/s][1,15]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:30,  1.85ba/s][1,4]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  1.97ba/s][1,2]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.91ba/s][1,12]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:32,  1.72ba/s][1,5]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.05ba/s][1,3]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.12ba/s][1,1]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:24,  1.92ba/s][1,7]<stderr>:#015 50%|█████     | 44/88 [00:19<00:20,  2.14ba/s][1,0]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  1.99ba/s][1,8]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.93ba/s][1,10]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.92ba/s][1,9]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.97ba/s][1,14]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.99ba/s][1,15]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.99ba/s][1,13]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.88ba/s][1,6]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:21,  2.09ba/s][1,11]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:31,  1.78ba/s][1,12]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.90ba/s][1,4]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,5]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.08ba/s][1,0]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.06ba/s][1,1]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:23,  1.98ba/s][1,2]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:25,  1.88ba/s][1,3]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.05ba/s][1,7]<stderr>:#015 51%|█████     | 45/88 [00:20<00:22,  1.91ba/s][1,8]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.96ba/s][1,10]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.93ba/s][1,9]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.02ba/s][1,14]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.02ba/s][1,15]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.03ba/s][1,12]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.05ba/s][1,11]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:29,  1.89ba/s][1,6]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.09ba/s][1,13]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:29,  1.87ba/s][1,4]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.05ba/s][1,2]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.05ba/s][1,3]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.08ba/s][1,5]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  1.99ba/s][1,7]<stderr>:#015 52%|█████▏    | 46/88 [00:20<00:20,  2.09ba/s][1,0]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:23,  1.90ba/s][1,8]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.00ba/s][1,1]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:25,  1.75ba/s][1,10]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.95ba/s][1,12]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:25,  2.12ba/s][1,13]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.06ba/s][1,15]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.02ba/s][1,6]<stderr>:#015 50%|█████     | 44/88 [00:21<00:20,  2.12ba/s][1,9]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:27,  1.94ba/s][1,11]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:28,  1.92ba/s][1,14]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:27,  1.90ba/s][1,4]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.09ba/s][1,2]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  2.03ba/s][1,3]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.04ba/s][1,5]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.98ba/s][1,0]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.99ba/s][1,7]<stderr>:#015 53%|█████▎    | 47/88 [00:21<00:20,  2.05ba/s][1,8]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.99ba/s][1,10]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.99ba/s][1,1]<stderr>:#015 50%|█████     | 44/88 [00:21<00:24,  1.81ba/s][1,6]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.14ba/s][1,11]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.97ba/s][1,13]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:27,  1.90ba/s][1,2]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,15]<stderr>:#015 41%|████      | 36/88 [00:17<00:27,  1.88ba/s][1,9]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,4]<stderr>:#015 51%|█████     | 45/88 [00:21<00:21,  1.99ba/s][1,12]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,14]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.81ba/s][1,0]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.05ba/s][1,5]<stderr>:#015 51%|█████     | 45/88 [00:21<00:21,  2.01ba/s][1,3]<stderr>:#015 51%|█████     | 45/88 [00:21<00:22,  1.92ba/s][1,1]<stderr>:#015 51%|█████     | 45/88 [00:22<00:22,  1.93ba/s][1,7]<stderr>:#015 55%|█████▍    | 48/88 [00:22<00:21,  1.90ba/s][1,8]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,2]<stderr>:#015 51%|█████     | 45/88 [00:22<00:20,  2.13ba/s][1,15]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,9]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.97ba/s][1,10]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,14]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.98ba/s][1,13]<stderr>:#015 41%|████      | 36/88 [00:17<00:27,  1.92ba/s][1,6]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.93ba/s][1,11]<stderr>:#015 41%|████      | 36/88 [00:18<00:27,  1.88ba/s][1,12]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.90ba/s][1,4]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:23,  1.76ba/s][1,0]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.88ba/s][1,7]<stderr>:#015 56%|█████▌    | 49/88 [00:22<00:19,  1.95ba/s][1,5]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.86ba/s][1,3]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.87ba/s][1,1]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.91ba/s][1,6]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:19,  2.14ba/s][1,8]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.92ba/s][1,15]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.03ba/s][1,9]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.01ba/s][1,13]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:25,  2.02ba/s][1,11]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:25,  2.00ba/s][1,14]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.04ba/s][1,12]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.07ba/s][1,10]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:27,  1.86ba/s][1,2]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:20,  2.03ba/s][1,4]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:22,  1.85ba/s][1,5]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.94ba/s][1,7]<stderr>:#015 57%|█████▋    | 50/88 [00:23<00:19,  2.00ba/s][1,0]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.92ba/s][1,3]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:21,  1.95ba/s][1,6]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:18,  2.16ba/s][1,2]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:18,  2.19ba/s][1,8]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.99ba/s][1,11]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.06ba/s][1,10]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.02ba/s][1,1]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.85ba/s][1,13]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.94ba/s][1,12]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.96ba/s][1,14]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.92ba/s][1,9]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:27,  1.81ba/s][1,15]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:27,  1.81ba/s][1,4]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.95ba/s][1,7]<stderr>:#015 58%|█████▊    | 51/88 [00:23<00:17,  2.06ba/s][1,5]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.00ba/s][1,3]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.03ba/s][1,0]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.98ba/s][1,1]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.04ba/s][1,6]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.14ba/s][1,2]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.09ba/s][1,14]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.08ba/s][1,12]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.06ba/s][1,10]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.95ba/s][1,8]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.90ba/s][1,13]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:24,  1.97ba/s][1,15]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:24,  2.00ba/s][1,11]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.90ba/s][1,9]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:26,  1.84ba/s][1,4]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.02ba/s][1,5]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.04ba/s][1,3]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.04ba/s][1,0]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.01ba/s][1,1]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.11ba/s][1,2]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:17,  2.24ba/s][1,7]<stderr>:#015 59%|█████▉    | 52/88 [00:24<00:18,  1.91ba/s][1,12]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.11ba/s][1,6]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.92ba/s][1,13]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.02ba/s][1,11]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:24,  1.98ba/s][1,15]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  2.01ba/s][1,9]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.05ba/s][1,8]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.90ba/s][1,10]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.92ba/s][1,14]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:24,  1.92ba/s][1,7]<stderr>:#015 60%|██████    | 53/88 [00:24<00:16,  2.09ba/s][1,4]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,3]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,6]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:17,  2.15ba/s][1,0]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.90ba/s][1,1]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.96ba/s][1,5]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.88ba/s][1,2]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:18,  2.08ba/s][1,12]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:21,  2.09ba/s][1,11]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:22,  2.06ba/s][1,13]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:22,  2.05ba/s][1,10]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:23,  1.98ba/s][1,15]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.01ba/s][1,9]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.04ba/s][1,8]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:24,  1.90ba/s][1,14]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:25,  1.83ba/s][1,4]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:18,  2.04ba/s][1,7]<stderr>:#015 61%|██████▏   | 54/88 [00:24<00:16,  2.11ba/s][1,3]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:18,  2.01ba/s][1,6]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.10ba/s][1,2]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:17,  2.12ba/s][1,5]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.94ba/s][1,1]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.99ba/s][1,11]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.04ba/s][1,12]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  2.01ba/s][1,10]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,0]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:20,  1.80ba/s][1,8]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,9]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  2.01ba/s][1,13]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:24,  1.88ba/s][1,15]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:23,  1.91ba/s][1,4]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.05ba/s][1,14]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:24,  1.86ba/s][1,3]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.05ba/s][1,1]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.08ba/s][1,2]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:16,  2.14ba/s][1,5]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  2.00ba/s][1,0]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.01ba/s][1,7]<stderr>:#015 62%|██████▎   | 55/88 [00:25<00:17,  1.90ba/s][1,12]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,11]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  1.99ba/s][1,9]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,13]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  2.03ba/s][1,6]<stderr>:#015 60%|██████    | 53/88 [00:25<00:19,  1.84ba/s][1,15]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.06ba/s][1,14]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.04ba/s][1,8]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:24,  1.81ba/s][1,7]<stderr>:#015 64%|██████▎   | 56/88 [00:25<00:15,  2.09ba/s][1,10]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:26,  1.73ba/s][1,4]<stderr>:#015 60%|██████    | 53/88 [00:25<00:18,  1.87ba/s][1,2]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  2.00ba/s][1,6]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.06ba/s][1,5]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.93ba/s][1,1]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.92ba/s][1,0]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.93ba/s][1,12]<stderr>:#015 51%|███\u001b[0m\n",
      "\u001b[34m██     | 45/88 [00:21<00:21,  2.04ba/s][1,3]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.83ba/s][1,9]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.08ba/s][1,13]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.07ba/s][1,11]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.99ba/s][1,14]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.08ba/s][1,8]<stderr>:#015 50%|█████     | 44/88 [00:22<00:22,  1.99ba/s][1,15]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  1.97ba/s][1,4]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.02ba/s][1,10]<stderr>:#015 50%|█████     | 44/88 [00:22<00:24,  1.80ba/s][1,7]<stderr>:#015 65%|██████▍   | 57/88 [00:26<00:15,  1.97ba/s][1,1]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,0]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,2]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,6]<stderr>:#015 62%|██████▎   | 55/88 [00:26<00:16,  2.00ba/s][1,5]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.89ba/s][1,3]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.87ba/s][1,12]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.95ba/s][1,8]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  2.03ba/s][1,11]<stderr>:#015 51%|█████     | 45/88 [00:22<00:22,  1.92ba/s][1,9]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.93ba/s][1,14]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:20,  2.00ba/s][1,10]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  2.02ba/s][1,15]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.87ba/s][1,13]<stderr>:#015 51%|█████     | 45/88 [00:22<00:23,  1.79ba/s][1,4]<stderr>:#015 62%|██████▎   | 55/88 [00:26<00:16,  1.97ba/s][1,7]<stderr>:#015 66%|██████▌   | 58/88 [00:27<00:15,  1.95ba/s][1,6]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,2]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.04ba/s][1,1]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,3]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.01ba/s][1,5]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,0]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.03ba/s][1,12]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:20,  1.97ba/s][1,14]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:20,  2.02ba/s][1,9]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:20,  1.95ba/s][1,13]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.98ba/s][1,15]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:20,  1.96ba/s][1,11]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.82ba/s][1,4]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.05ba/s][1,8]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.80ba/s][1,3]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.12ba/s][1,7]<stderr>:#015 67%|██████▋   | 59/88 [00:27<00:14,  2.02ba/s][1,5]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,0]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.10ba/s][1,10]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.78ba/s][1,1]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.97ba/s][1,2]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.97ba/s][1,12]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.04ba/s][1,9]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.97ba/s][1,6]<stderr>:#015 65%|██████▍   | 57/88 [00:27<00:17,  1.78ba/s][1,14]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.99ba/s][1,15]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.98ba/s][1,8]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.94ba/s][1,13]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.85ba/s][1,7]<stderr>:#015 68%|██████▊   | 60/88 [00:27<00:13,  2.07ba/s][1,11]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:23,  1.77ba/s][1,4]<stderr>:#015 65%|██████▍   | 57/88 [00:27<00:16,  1.90ba/s][1,12]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.14ba/s][1,10]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.82ba/s][1,0]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:15,  1.97ba/s][1,2]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:15,  1.98ba/s][1,3]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.85ba/s][1,1]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.84ba/s][1,5]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:17,  1.82ba/s][1,6]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.92ba/s][1,15]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.03ba/s][1,9]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:20,  1.92ba/s][1,13]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.97ba/s][1,14]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:20,  1.92ba/s][1,8]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:20,  1.95ba/s][1,7]<stderr>:#015 69%|██████▉   | 61/88 [00:28<00:12,  2.15ba/s][1,4]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.96ba/s][1,2]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.09ba/s][1,10]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:20,  1.92ba/s][1,11]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:22,  1.79ba/s][1,0]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.02ba/s][1,3]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.97ba/s][1,6]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:14,  1.98ba/s][1,12]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.93ba/s][1,5]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.87ba/s][1,1]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.85ba/s][1,15]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.99ba/s][1,8]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.01ba/s][1,14]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.86ba/s][1,11]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:20,  1.94ba/s][1,10]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.02ba/s][1,9]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.81ba/s][1,4]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:14,  2.01ba/s][1,2]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:13,  2.10ba/s][1,7]<stderr>:#015 70%|███████   | 62/88 [00:29<00:13,  1.95ba/s][1,3]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:13,  2.10ba/s][1,13]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:22,  1.73ba/s][1,0]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.90ba/s][1,5]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  1.95ba/s][1,12]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:19,  1.90ba/s][1,1]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.88ba/s][1,6]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.84ba/s][1,14]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.97ba/s][1,9]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.94ba/s][1,8]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:19,  1.93ba/s][1,15]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.89ba/s][1,7]<stderr>:#015 72%|███████▏  | 63/88 [00:29<00:12,  1.99ba/s][1,13]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,2]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:13,  2.00ba/s][1,4]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.91ba/s][1,3]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.98ba/s][1,11]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:21,  1.81ba/s][1,10]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:20,  1.85ba/s][1,0]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.89ba/s][1,12]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.00ba/s][1,5]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.90ba/s][1,6]<stderr>:#015 69%|██████▉   | 61/88 [00:29<00:13,  1.97ba/s][1,1]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.85ba/s][1,7]<stderr>:#015 73%|███████▎  | 64/88 [00:29<00:11,  2.11ba/s][1,15]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.95ba/s][1,8]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.97ba/s][1,9]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.93ba/s][1,14]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.90ba/s][1,13]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s][1,10]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  2.00ba/s][1,11]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s][1,2]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.00ba/s][1,3]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.01ba/s][1,4]<stderr>:#015 69%|██████▉   | 61/88 [00:29<00:14,  1.91ba/s][1,0]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.96ba/s][1,5]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.91ba/s][1,1]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.93ba/s][1,8]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.05ba/s][1,12]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.84ba/s][1,6]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.87ba/s][1,15]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  1.95ba/s][1,11]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.07ba/s][1,10]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.07ba/s][1,7]<stderr>:#015 74%|███████▍  | 65/88 [00:30<00:11,  1.96ba/s][1,4]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.00ba/s][1,2]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.02ba/s][1,3]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.03ba/s][1,14]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.77ba/s][1,13]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:19,  1.82ba/s][1,0]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.00ba/s][1,9]<stderr>:#015 60%|██████    | 53/88 [00:26<00:20,  1.74ba/s][1,5]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.96ba/s][1,1]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.93ba/s][1,12]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.89ba/s][1,15]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.02ba/s][1,6]<stderr>:#015 72%|███████▏  | 63/88 [00:30<00:13,  1.88ba/s][1,7]<stderr>:#015 75%|███████▌  | 66/88 [00:30<00:10,  2.05ba/s][1,4]<stderr>:#015 72%|███████▏  | 63/88 [00:30<00:12,  2.03ba/s][1,8]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.82ba/s][1,9]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.91ba/s][1,11]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.92ba/s][1,13]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  1.96ba/s][1,2]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.00ba/s][1,0]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:11,  2.10ba/s][1,10]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.87ba/s][1,14]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.84ba/s][1,3]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  1.93ba/s][1,5]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.04ba/s][1,1]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  1.99ba/s][1,12]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.92ba/s][1,15]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  1.95ba/s][1,7]<stderr>:#015 76%|███████▌  | 67/88 [00:31<00:10,  2.03ba/s][1,6]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.86ba/s][1,11]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:16,  2.01ba/s][1,13]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:16,  2.01ba/s][1,9]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  1.96ba/s][1,8]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:18,  1.88ba/s][1,10]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:17,  1.99ba/s][1,0]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:11,  2.03ba/s][1,2]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.92ba/s][1,5]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:11,  2.08ba/s][1,4]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.85ba/s][1,14]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:18,  1.82ba/s][1,3]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.90ba/s][1,12]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.99ba/s][1,1]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.96ba/s][1,15]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,6]<stderr>:#015 74%|███████▍  | 65/88 [00:31<00:11,  1.98ba/s][1,7]<stderr>:#015 77%|███████▋  | 68/88 [00:31<00:09,  2.06ba/s][1,9]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.01ba/s][1,11]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,13]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,8]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.91ba/s][1,0]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  2.07ba/s][1,2]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.93ba/s][1,3]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.98ba/s][1,10]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.89ba/s][1,4]<stderr>:#015 74%|███████▍  | 65/88 [00:31<00:12,  1.92ba/s][1,14]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:17,  1.79ba/s][1,5]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:12,  1.88ba/s][1,1]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.98ba/s][1,6]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.04ba/s][1,0]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:09,  2.20ba/s][1,15]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.90ba/s][1,8]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.01ba/s][1,11]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.01ba/s][1,10]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.07ba/s][1,7]<stderr>:#015 78%|███████▊  | 69/88 [00:32<00:10,  1.89ba/s][1,12]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:18,  1.71ba/s][1,3]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.05ba/s][1,2]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.99ba/s][1,4]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.98ba/s][1,13]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:17,  1.87ba/s][1,9]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,5]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.88ba/s][1,1]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.96ba/s][1,6]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:10,  1.95ba/s][1,0]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:09,  2.19ba/s][1,7]<stderr>:#015 80%|███████▉  | 70/88 [00:32<00:08,  2.09ba/s][1,14]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:18,  1.70ba/s][1,15]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.87ba/s][1,9]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.02ba/s][1,12]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.84ba/s][1,4]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:10,  1.93ba/s][1,8]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:17,  1.78ba/s][1,10]<stderr>:#015 6\u001b[0m\n",
      "\u001b[34m5%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,13]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,2]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.86ba/s][1,3]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.84ba/s][1,11]<stderr>:#015 65%|██████▍   | 57/88 [00:29<00:18,  1.69ba/s][1,0]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:08,  2.26ba/s][1,6]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.05ba/s][1,7]<stderr>:#015 81%|████████  | 71/88 [00:33<00:07,  2.14ba/s][1,5]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.78ba/s][1,14]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:17,  1.73ba/s][1,9]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  2.03ba/s][1,15]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.89ba/s][1,12]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.86ba/s][1,1]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:12,  1.72ba/s][1,4]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.01ba/s][1,2]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.01ba/s][1,3]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  2.00ba/s][1,10]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:15,  1.92ba/s][1,13]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:15,  1.92ba/s][1,8]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:16,  1.83ba/s][1,0]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:08,  2.21ba/s][1,11]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:17,  1.76ba/s][1,6]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:09,  1.95ba/s][1,5]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  1.86ba/s][1,14]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.82ba/s][1,7]<stderr>:#015 82%|████████▏ | 72/88 [00:33<00:08,  1.93ba/s][1,9]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.94ba/s][1,1]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:11,  1.78ba/s][1,10]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  2.03ba/s][1,3]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.07ba/s][1,2]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.02ba/s][1,4]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:09,  2.00ba/s][1,12]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.80ba/s][1,0]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:07,  2.29ba/s][1,8]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.87ba/s][1,15]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:16,  1.73ba/s][1,11]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.91ba/s][1,13]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.85ba/s][1,6]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.93ba/s][1,5]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.90ba/s][1,7]<stderr>:#015 83%|████████▎ | 73/88 [00:34<00:07,  1.90ba/s][1,14]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:15,  1.77ba/s][1,2]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.08ba/s][1,9]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.98ba/s][1,1]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.86ba/s][1,4]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.02ba/s][1,3]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.04ba/s][1,15]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.88ba/s][1,13]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:13,  2.01ba/s][1,10]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:14,  1.90ba/s][1,0]<stderr>:#015 81%|████████  | 71/88 [00:34<00:07,  2.16ba/s][1,12]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:15,  1.70ba/s][1,8]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:15,  1.79ba/s][1,11]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:16,  1.74ba/s][1,5]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.91ba/s][1,7]<stderr>:#015 84%|████████▍ | 74/88 [00:34<00:07,  2.00ba/s][1,6]<stderr>:#015 81%|████████  | 71/88 [00:34<00:09,  1.87ba/s][1,9]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.05ba/s][1,14]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.84ba/s][1,1]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.93ba/s][1,0]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:06,  2.33ba/s][1,4]<stderr>:#015 81%|████████  | 71/88 [00:34<00:08,  1.98ba/s][1,3]<stderr>:#015 81%|████████  | 71/88 [00:35<00:08,  1.99ba/s][1,13]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.00ba/s][1,15]<stderr>:#015 70%|███████   | 62/88 [00:30<00:14,  1.85ba/s][1,2]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.84ba/s][1,12]<stderr>:#015 70%|███████   | 62/88 [00:30<00:14,  1.83ba/s][1,10]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.84ba/s][1,8]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.87ba/s][1,11]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.89ba/s][1,9]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:11,  2.09ba/s][1,6]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.92ba/s][1,3]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:07,  2.16ba/s][1,4]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:07,  2.14ba/s][1,7]<stderr>:#015 85%|████████▌ | 75/88 [00:35<00:06,  1.93ba/s][1,14]<stderr>:#015 70%|███████   | 62/88 [00:31<00:13,  1.92ba/s][1,5]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.79ba/s][1,13]<stderr>:#015 70%|███████   | 62/88 [00:31<00:12,  2.06ba/s][1,0]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.05ba/s][1,2]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.96ba/s][1,15]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.90ba/s][1,12]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.87ba/s][1,10]<stderr>:#015 70%|███████   | 62/88 [00:31<00:13,  1.87ba/s][1,1]<stderr>:#015 81%|████████  | 71/88 [00:35<00:10,  1.66ba/s][1,8]<stderr>:#015 70%|███████   | 62/88 [00:31<00:14,  1.82ba/s][1,11]<stderr>:#015 70%|███████   | 62/88 [00:31<00:14,  1.85ba/s][1,14]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.05ba/s][1,4]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.13ba/s][1,5]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.96ba/s][1,3]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.12ba/s][1,9]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.94ba/s][1,2]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,6]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:08,  1.75ba/s][1,7]<stderr>:#015 86%|████████▋ | 76/88 [00:36<00:06,  1.77ba/s][1,15]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.90ba/s][1,1]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.84ba/s][1,0]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.94ba/s][1,13]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.88ba/s][1,10]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.91ba/s][1,8]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.87ba/s][1,12]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.75ba/s][1,5]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,11]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.84ba/s][1,14]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:12,  1.97ba/s][1,9]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  2.07ba/s][1,3]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:06,  2.05ba/s][1,4]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:06,  2.03ba/s][1,1]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,0]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.08ba/s][1,7]<stderr>:#015 88%|████████▊ | 77/88 [00:36<00:05,  1.84ba/s][1,2]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.91ba/s][1,6]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:08,  1.74ba/s][1,13]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:12,  1.91ba/s][1,15]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:12,  1.87ba/s][1,10]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.81ba/s][1,9]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.10ba/s][1,8]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.82ba/s][1,4]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.10ba/s][1,14]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.97ba/s][1,12]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:13,  1.76ba/s][1,5]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.95ba/s][1,3]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.01ba/s][1,0]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.20ba/s][1,11]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.79ba/s][1,6]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.92ba/s][1,2]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  2.03ba/s][1,7]<stderr>:#015 89%|████████▊ | 78/88 [00:37<00:05,  1.87ba/s][1,13]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.94ba/s][1,15]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.93ba/s][1,1]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.80ba/s][1,10]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:12,  1.87ba/s][1,4]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.07ba/s][1,11]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:11,  1.94ba/s][1,5]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  2.00ba/s][1,14]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.96ba/s][1,12]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:12,  1.81ba/s][1,8]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:13,  1.76ba/s][1,0]<stderr>:#015 88%|████████▊ | 77/88 [00:37<00:05,  2.10ba/s][1,2]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.12ba/s][1,3]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:06,  1.94ba/s][1,6]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.00ba/s][1,9]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.79ba/s][1,15]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:10,  1.93ba/s][1,1]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.87ba/s][1,10]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.97ba/s][1,7]<stderr>:#015 90%|████████▉ | 79/88 [00:37<00:05,  1.74ba/s][1,13]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.84ba/s][1,5]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.08ba/s][1,4]<stderr>:#015 88%|████████▊ | 77/88 [00:37<00:05,  2.05ba/s][1,11]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.93ba/s][1,2]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.12ba/s][1,9]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  1.99ba/s][1,3]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.99ba/s][1,8]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:12,  1.83ba/s][1,6]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.99ba/s][1,14]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.83ba/s][1,15]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.10ba/s][1,12]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:12,  1.68ba/s][1,0]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.91ba/s][1,1]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.91ba/s][1,13]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.90ba/s][1,5]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.12ba/s][1,7]<stderr>:#015 91%|█████████ | 80/88 [00:38<00:04,  1.77ba/s][1,10]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.88ba/s][1,4]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.97ba/s][1,11]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.90ba/s][1,9]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.96ba/s][1,14]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,15]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.11ba/s][1,2]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.99ba/s][1,12]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:11,  1.82ba/s][1,0]<stderr>:#015 90%|████████▉ | 79/88 [00:38<00:04,  2.01ba/s][1,8]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.76ba/s][1,6]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.90ba/s][1,1]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.07ba/s][1,3]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.84ba/s][1,13]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,7]<stderr>:#015 92%|█████████▏| 81/88 [00:38<00:03,  1.87ba/s][1,10]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.96ba/s][1,5]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.98ba/s][1,4]<stderr>:#015 90%|████████▉ | 79/88 [00:38<00:04,  2.03ba/s][1,2]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  2.10ba/s][1,11]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,9]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.98ba/s][1,14]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.94ba/s][1,15]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.09ba/s][1,8]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.88ba/s][1,12]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.87ba/s][1,0]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.99ba/s][1,6]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.93ba/s][1,7]<stderr>:#015 93%|█████████▎| 82/88 [00:39<00:02,  2.05ba/s][1,3]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:05,  1.79ba/s][1,4]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.11ba/s][1,1]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.82ba/s][1,10]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.96ba/s][1,5]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  2.02ba/s][1,13]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.87ba/s][1,2]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.21ba/s][1,11]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.99ba/s][1,14]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.94ba/s][1,8]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.91ba/s][1,12]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.86ba/s][1,9]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.85ba/s][1,6]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.91ba/s][1,3]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.90ba/s][1,15]<stderr>:#015 81%|████████  | 71/88 [00:35<00:08,  1.90ba/s][1,7]<stderr>:#015 94%|█████████▍| 83/88 [00:39<00:02,  2.01ba/s][1,0]<stderr>:#015 92%|█████████▏| 81/88 [00:39<00:03,  1.85ba/s][1,1]<stderr>:#015 90%|██�\u001b[0m\n",
      "\u001b[34m��█████▉ | 79/88 [00:39<00:04,  1.89ba/s][1,5]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.03ba/s][1,10]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.97ba/s][1,11]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:08,  2.03ba/s][1,13]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:10,  1.79ba/s][1,2]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  2.00ba/s][1,4]<stderr>:#015 92%|█████████▏| 81/88 [00:39<00:03,  1.88ba/s][1,9]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.88ba/s][1,7]<stderr>:#015 95%|█████████▌| 84/88 [00:40<00:01,  2.04ba/s][1,14]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.82ba/s][1,0]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.93ba/s][1,15]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.94ba/s][1,8]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.82ba/s][1,1]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.05ba/s][1,6]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.87ba/s][1,3]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.88ba/s][1,12]<stderr>:#015 81%|████████  | 71/88 [00:36<00:09,  1.78ba/s][1,10]<stderr>:#015 81%|████████  | 71/88 [00:36<00:08,  1.93ba/s][1,2]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:02,  2.07ba/s][1,4]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.94ba/s][1,5]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.85ba/s][1,11]<stderr>:#015 81%|████████  | 71/88 [00:36<00:08,  1.91ba/s][1,13]<stderr>:#015 81%|████████  | 71/88 [00:36<00:09,  1.80ba/s][1,7]<stderr>:#015 97%|█████████▋| 85/88 [00:40<00:01,  2.09ba/s][1,15]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.97ba/s][1,6]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.96ba/s][1,9]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.88ba/s][1,12]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.89ba/s][1,3]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.94ba/s][1,0]<stderr>:#015 94%|█████████▍| 83/88 [00:40<00:02,  1.92ba/s][1,1]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.94ba/s][1,14]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:09,  1.70ba/s][1,10]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.96ba/s][1,8]<stderr>:#015 81%|████████  | 71/88 [00:36<00:10,  1.67ba/s][1,5]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.95ba/s][1,2]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.02ba/s][1,11]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.97ba/s][1,13]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.88ba/s][1,4]<stderr>:#015 94%|█████████▍| 83/88 [00:40<00:02,  1.92ba/s][1,12]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.97ba/s][1,6]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.96ba/s][1,0]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.96ba/s][1,1]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:02,  2.06ba/s][1,9]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.87ba/s][1,3]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.92ba/s][1,7]<stderr>:#015 98%|█████████▊| 86/88 [00:41<00:01,  1.91ba/s][1,15]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.81ba/s][1,8]<stderr>:#015 82%|████████▏ | 72/88 [00:37<00:08,  1.82ba/s][1,5]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.07ba/s][1,14]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.76ba/s][1,11]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:07,  2.07ba/s][1,4]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:01,  2.01ba/s][1,2]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.95ba/s][1,10]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.75ba/s][1,13]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.82ba/s][1,9]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.99ba/s][1,1]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.10ba/s][1,12]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.92ba/s][1,3]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.99ba/s][1,6]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.95ba/s][1,7]<stderr>:#015 99%|█████████▉| 87/88 [00:41<00:00,  1.97ba/s][1,5]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:01,  2.08ba/s][1,15]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:07,  1.84ba/s][1,8]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.86ba/s][1,0]<stderr>:#015 97%|█████████▋| 85/88 [00:41<00:01,  1.81ba/s][1,4]<stderr>:#015 97%|█████████▋| 85/88 [00:41<00:01,  2.01ba/s][1,14]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:08,  1.70ba/s][1,11]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.91ba/s][1,7]<stderr>:#015100%|██████████| 88/88 [00:42<00:00,  2.18ba/s][1,7]<stderr>:#015100%|██████████| 88/88 [00:42<00:00,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.88ba/s][1,13]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.87ba/s][1,7]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,1]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:01,  2.13ba/s][1,10]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:08,  1.74ba/s][1,12]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.98ba/s][1,9]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:06,  1.92ba/s][1,6]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.93ba/s][1,3]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.93ba/s][1,0]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:01,  1.96ba/s][1,15]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.92ba/s][1,4]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.09ba/s][1,5]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.92ba/s][1,8]<stderr>:#015 84%|████████▍ | 74/88 [00:38<00:07,  1.78ba/s][1,11]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.95ba/s][1,13]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.97ba/s][1,14]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:07,  1.74ba/s][1,2]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:01,  1.89ba/s][1,9]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.05ba/s][1,12]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:05,  2.04ba/s][1,10]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.86ba/s][1,6]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.02ba/s][1,1]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  2.03ba/s][1,3]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.05ba/s][1,0]<stderr>:#015 99%|█████████▉| 87/88 [00:42<00:00,  2.08ba/s][1,4]<stderr>:#015 99%|█████████▉| 87/88 [00:42<00:00,  2.27ba/s][1,15]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.97ba/s][1,5]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.05ba/s][1,8]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.90ba/s][1,2]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.03ba/s][1,11]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.97ba/s][1,13]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.99ba/s][1,6]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.14ba/s][1,3]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.16ba/s][1,0]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.17ba/s][1,0]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.94ba/s][1,14]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.77ba/s][1,4]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.39ba/s][1,4]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.05ba/s][1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:00,  2.10ba/s][1,12]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.03ba/s][1,0]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,4]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,5]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.24ba/s][1,9]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.91ba/s][1,2]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.21ba/s][1,2]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,15]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.84ba/s][1,8]<stderr>:#015 86%|████████▋ | 76/88 [00:39<00:06,  1.90ba/s][1,3]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.28ba/s][1,3]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  2.02ba/s][1,6]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.20ba/s][1,6]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,14]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.93ba/s][1,10]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  2.05ba/s][1,6]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,1]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.17ba/s][1,5]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.41ba/s][1,5]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s][1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.89ba/s][1,5]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,9]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.88ba/s][1,12]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.84ba/s][1,1]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.30ba/s][1,1]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.89ba/s][1,1]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,8]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.90ba/s][1,13]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:04,  2.01ba/s][1,14]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.87ba/s][1,11]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.89ba/s][1,7]<stderr>:#015  9%|▉         | 1/11 [00:02<00:20,  2.02s/ba][1,10]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.89ba/s][1,12]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.91ba/s][1,15]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.97ba/s][1,9]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.82ba/s][1,13]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  2.02ba/s][1,11]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.95ba/s][1,10]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  2.00ba/s][1,8]<stderr>:#015 89%|████████▊ | 78/88 [00:40<00:05,  1.80ba/s][1,12]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.97ba/s][1,14]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:05,  1.74ba/s][1,0]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.79s/ba][1,15]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.89ba/s][1,4]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.80s/ba][1,9]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.78ba/s][1,11]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.03ba/s][1,10]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.08ba/s][1,8]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.90ba/s][1,2]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.71s/ba][1,3]<stderr>:#015  9%|▉         | 1/11 [00:01<00:16,  1.66s/ba][1,14]<stderr>:#015 91%|█████████ | 80/88 [00:41<00:04,  1.84ba/s][1,5]<stderr>:#015  9%|▉         | 1/11 [00:01<00:16,  1.61s/ba][1,6]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.74s/ba][1,13]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.74ba/s][1,12]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.83ba/s][1,15]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.94ba/s][1,9]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.86ba/s][1,8]<stderr>:#015 91%|█████████ | 80/88 [00:41<00:04,  1.93ba/s][1,7]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:16,  1.85s/ba][1,10]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.94ba/s][1,11]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.89ba/s][1,14]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.84ba/s][1,13]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.83ba/s][1,1]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.87s/ba][1,12]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.89ba/s][1,15]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.93ba/s][1,9]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.88ba/s][1,8]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.85ba/s][1,11]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.89ba/s][1,10]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.84ba/s][1,14]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.82ba/s][1,12]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.94ba/s][1,13]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.77ba/s][1,4]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.70s/ba][1,9]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.95ba/s][1,15]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.96ba/s][1,0]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,2]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.62s/ba][1,3]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.57s/ba][1,5]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:13,  1.54s/ba][1,8]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.87ba/s][1,10]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.91ba/s][1,11]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.84ba/s][1,6]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.67s/ba][1,12]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:01,  2.01ba/s][1,14]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.86ba/s][1,13]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.77ba/s][1,7]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.72s/ba][1,9]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.84ba/s][1,15]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.76ba/s][1,10]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.97ba/s][1,11]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.88ba/s][1,14]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.99ba/s][1,8]<stderr>:#015 94%|█████████▍| 83/88 [00:43<00:02,  1.78ba/s][1,1]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.78s/ba][1,13]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.81ba/s][1,12]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.73ba/s][1,15]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.88ba/s][1,9]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.83ba/s][1,8]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.96ba/s][1,14]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.96ba/s][1,10]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.84ba/s][1,11]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.84ba/s][1,13]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.97ba/s][1,9]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  1.92ba/s][1,15]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  1.90ba/s][1,12]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.76ba/s][1,4]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.72s/ba][1,3]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:12,  1.60s/ba][1,0]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.72s/ba][1,2]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.65s/ba][1,11]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.94ba/s][1,8]<stderr>:#015 97%|█████████▋| 85/88 [00:44<00:01,  1.88ba/s][1,13]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:00,  2.10ba/s][1,5]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:12,  1.57s/ba][1,10]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.88ba/s][1,14]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.86ba/s][1,9]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.11ba/s][1,9]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.99ba/s][1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,6]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.70s/ba][1,15]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.93ba/s][1,15]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.70s/ba][1,15]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,12]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.77ba/s][1,8]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.98ba/s][1,14]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  2.04ba/s][1,13]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  2.00ba/s][1,11]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.85ba/s][1,10]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.84ba/s][1,13]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.39ba/s][1,13]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,12]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.91ba/s][1,12]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,12]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.21ba/s][1,14]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,14]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,14]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,11]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.06ba/s][1,11]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,11]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.97ba/s][1,1]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:14,  1.80s/ba][1,11]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,10]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.95ba/s][1,10]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.95ba/s][1,10]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,8]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  2.14ba/s][1,8]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,0]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,3]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.59s/ba][1,4]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.68s/ba][1,5]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:10,  1.57s/ba][1,2]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.64s/ba][1,6]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,9]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.83s/ba][1,15]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.81s/ba][1,7]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.78s/ba][1,13]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.74s/ba][1,12]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.72s/ba][1,1]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:12,  1.77s/ba][1,14]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.80s/ba][1,10]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.75s/ba][1,11]<stderr>:#015  9%|▉         | 1/11 [00:01<00:19,  1.99s/ba][1,8]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.71s/ba][1,0]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,9]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,3]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.72s/ba][1,4]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.82s/ba][1,15]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,2]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,5]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.75s/ba][1,6]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.78s/ba][1,13]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,12]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,14]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,7]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.81s/ba][1,10]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,11]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:16,  1.83s/ba][1,8]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.64s/ba][1,1]<stderr>:#015 45%|████▌     | 5/11 [00:09<00:11,  1.90s/ba][1,0]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,9]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.73s/ba][1,3]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,15]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.71s/ba][1,4]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.83s/ba][1,2]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.79s/ba][1,5]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,6]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,13]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.69s/ba][1,12]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,14]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.69s/ba][1,7]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.80s/ba][1,10]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,11]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:14,  1.79s/ba][1,8]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,1]<stderr>:#015 55%|█████▍    | 6/11 [00:11<00:09,  1.90s/ba][1,9]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.69s/ba][1,15]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.66s/ba][1,0]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.75s/ba][1,3]<stderr>:#015 64%|██████▎   | 7/11 [00:11<00:07,  1.77s/ba][1,4]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.83s/ba][1,2]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.80s/ba][1,5]<stderr>:#015 64%|██████▎   | 7/11 [00:11<00:07,  1.78s/ba][1,14]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,13]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,12]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.66s/ba][1,6]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.79s/ba][1,10]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,11]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.71s/ba][1,7]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.81s/ba][1,8]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,1]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.87s/ba][1,0]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.71s/ba][1,3]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,15]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,9]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.81s/ba][1,2]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.76s/ba][1,4]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.79s/ba][1,5]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.77s/ba][1,6]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.74s/ba][1,13]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.80s/ba][1,7]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,14]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,12]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,10]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.75s/ba][1,11]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:11,  1.85s/ba][1,8]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,0]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.69s/ba][1,1]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.83s/ba][1,3]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,2]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.73s/ba][1,4]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,9]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,15]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.78s/ba][1,6]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.71s/ba][1,5]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,7]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.76s/ba][1,13]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,14]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,12]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.81s/ba][1,10]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.77s/ba][1,11]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.86s/ba][1,8]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.79s/ba][1,0]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.69s/ba][1,1]<stderr>:#015 82%|████████▏ | 9/11 [00:16<00:03,  1.80s/ba][1,7]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.52s/ba][1,7]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.72s/ba][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 4.51kB [00:00, 4.06MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.76s/ba][1,15]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.77s/ba][1,9]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.79s/ba][1,4]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.78s/ba][1,5]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,6]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.73s/ba][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 3.31kB [00:00, 1.61MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.46s/ba][1,0]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.63s/ba]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.78s/ba][1,10]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.77s/ba][1,14]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.82s/ba][1,2]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,2]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.65s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.83s/ba][1,3]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.51s/ba][1,3]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:398] 2021-09-05 07:59:21,316 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:415] 2021-09-05 07:59:21,316 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,5]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.52s/ba][1,4]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.69s/ba]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,6]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.85s/ba][1,8]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.81s/ba][1,1]<stderr>:#015 91%|█████████ | 10/11 [00:18<00:01,  1.79s/ba][1,15]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,9]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.76s/ba][1,13]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,1]<stderr>:#015100%|██████████| 11/11 [00:19<00:00,  1.54s/ba][1,1]<stderr>:#015100%|██████████| 11/11 [00:19<00:00,  1.73s/ba]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.77s/ba][1,14]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.80s/ba][1,12]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.81s/ba][1,11]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.80s/ba][1,8]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.78s/ba][1,15]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.72s/ba][1,9]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.73s/ba][1,13]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.72s/ba][1,10]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.74s/ba][1,14]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,12]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.76s/ba][1,11]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,8]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,15]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.71s/ba][1,9]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.74s/ba][1,13]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.70s/ba][1,10]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.72s/ba][1,15]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.45s/ba][1,15]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,14]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.77s/ba][1,9]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,9]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba][1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 4.51kB [00:00, 2.08MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 3.31kB [00:00, 2.70MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,8]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.73s/ba][1,13]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.46s/ba][1,13]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.65s/ba]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,10]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,12]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.51s/ba][1,14]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.50s/ba][1,11]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.69s/ba]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.48s/ba][1,8]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:398] 2021-09-05 07:59:27,951 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:415] 2021-09-05 07:59:27,951 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,580 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,580 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,581 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,581 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,581 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,582 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s][1,8]<stderr>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,617 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,617 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,618 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,618 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,618 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,618 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 1/100 [00:03<06:31,  3.96s/it][1,8]<stderr>:#015  1%|          | 1/100 [00:03<06:28,  3.92s/it][1,8]<stderr>:#015  2%|▏         | 2/100 [00:05<05:21,  3.28s/it][1,0]<stderr>:#015  2%|▏         | 2/100 [00:05<05:24,  3.31s/it][1,8]<stderr>:#015  3%|▎         | 3/100 [00:07<04:21,  2.70s/it][1,0]<stderr>:#015  3%|▎         | 3/100 [00:07<04:24,  2.73s/it][1,8]<stderr>:#015  4%|▍         | 4/100 [00:08<03:56,  2.47s/it][1,0]<stderr>:#015  4%|▍         | 4/100 [00:09<03:57,  2.48s/it][1,0]<stderr>:#015  5%|▌         | 5/100 [00:10<03:29,  2.21s/it][1,8]<stderr>:#015  5%|▌         | 5/100 [00:10<03:30,  2.21s/it][1,8]<stderr>:#015  6%|▌         | 6/100 [00:11<03:05,  1.97s/it][1,0]<stderr>:#015  6%|▌         | 6/100 [00:12<03:06,  1.98s/it][1,8]<stderr>:#015  7%|▋         | 7/100 [00:13<02:51,  1.85s/it][1,0]<stderr>:#015  7%|▋         | 7/100 [00:13<02:53,  1.86s/it][1,0]<stderr>:#015  8%|▊         | 8/100 [00:15<02:40,  1.74s/it][1,8]<stderr>:#015  8%|▊         | 8/100 [00:15<02:42,  1.76s/it][1,0]<stderr>:#015  9%|▉         | 9/100 [00:16<02:40,  1.76s/it][1,8]<stderr>:#015  9%|▉         | 9/100 [00:16<02:41,  1.78s/it][1,8]<stderr>:#015 10%|█         | 10/100 [00:18<02:31,  1.68s/it][1,0]<stderr>:#015 10%|█         | 10/100 [00:18<02:32,  1.69s/it][1,8]<stderr>:#015 11%|█         | 11/100 [00:19<02:18,  1.56s/it][1,0]<stderr>:#015 11%|█         | 11/100 [00:19<02:20,  1.58s/it][1,0]<stderr>:#015 12%|█▏        | 12/100 [00:21<02:19,  1.58s/it][1,8]<stderr>:#015 12%|█▏        | 12/100 [00:21<02:20,  1.59s/it][1,8]<stderr>:#015 13%|█▎        | 13/100 [00:22<02:06,  1.46s/it][1,0]<stderr>:#015 13%|█▎        | 13/100 [00:22<02:10,  1.50s/it][1,8]<stderr>:#015 14%|█▍        | 14/100 [00:24<02:11,  1.53s/it][1,0]<stderr>:#015 14%|█▍        | 14/100 [00:24<02:10,  1.52s/it][1,8]<stderr>:#015 15%|█▌        | 15/100 [00:25<02:11,  1.55s/it][1,0]<stderr>:#015 15%|█▌        | 15/100 [00:25<02:12,  1.55s/it][1,0]<stderr>:#015 16%|█▌        | 16/100 [00:27<02:02,  1.45s/it][1,8]<stderr>:#015 16%|█▌        | 16/100 [00:27<02:08,  1.53s/it][1,8]<stderr>:#015 17%|█▋        | 17/100 [00:28<02:03,  1.49s/it][1,0]<stderr>:#015 17%|█▋        | 17/100 [00:28<02:08,  1.55s/it][1,0]<stderr>:#015 18%|█▊        | 18/100 [00:30<02:00,  1.47s/it][1,8]<stderr>:#015 18%|█▊        | 18/100 [00:30<02:01,  1.49s/it][1,0]<stderr>:#015 19%|█▉        | 19/100 [00:31<02:06,  1.56s/it][1,8]<stderr>:#015 19%|█▉        | 19/100 [00:31<02:07,  1.58s/it][1,0]<stderr>:#015 20%|██        | 20/100 [00:33<02:06,  1.59s/it][1,8]<stderr>:#015 20%|██        | 20/100 [00:33<02:06,  1.58s/it][1,0]<stderr>:#015 21%|██        | 21/100 [00:34<02:00,  1.53s/it][1,8]<stderr>:#015 21%|██        | 21/100 [00:34<02:01,  1.53s/it][1,8]<stderr>:#015 22%|██▏       | 22/100 [00:36<01:57,  1.51s/it][1,0]<stderr>:#015 22%|██▏       | 22/100 [00:36<02:02,  1.57s/it][1,8]<stderr>:#015 23%|██▎       | 23/100 [00:38<02:01,  1.58s/it][1,0]<stderr>:#015 23%|██▎       | 23/100 [00:38<02:00,  1.57s/it][1,8]<stderr>:#015 24%|██▍       | 24/100 [00:40<02:07,  1.68s/it][1,0]<stderr>:#015 24%|██▍       | 24/100 [00:40<02:08,  1.69s/it][1,8]<stderr>:#015 25%|██▌       | 25/100 [00:41<02:01,  1.62s/it][1,0]<stderr>:#015 25%|██▌       | 25/100 [00:41<02:00,  1.60s/it][1,8]<stderr>:#015 26%|██▌       | 26/100 [00:43<01:58,  1.61s/it][1,0]<stderr>:#015 26%|██▌       | 26/100 [00:43<01:58,  1.60s/it][1,8]<stderr>:#015 27%|██▋       | 27/100 [00:44<01:59,  1.64s/it][1,0]<stderr>:#015 27%|██▋       | 27/100 [00:44<02:00,  1.66s/it][1,0]<stderr>:#015 28%|██▊       | 28/100 [00:46<01:57,  1.63s/it][1,8]<stderr>:#015 28%|██▊       | 28/100 [00:46<01:58,  1.64s/it][1,8]<stderr>:#015 29%|██▉       | 29/100 [00:47<01:53,  1.59s/it][1,0]<stderr>:#015 29%|██▉       | 29/100 [00:48<01:54,  1.61s/it][1,8]<stderr>:#015 30%|███       | 30/100 [00:49<01:52,  1.61s/it][1,0]<stderr>:#015 30%|███       | 30/100 [00:49<01:53,  1.62s/it][1,8]<stderr>:#015 31%|███       | 31/100 [00:51<01:47,  1.56s/it][1,0]<stderr>:#015 31%|███       | 31/100 [00:51<01:47,  1.55s/it][1,8]<stderr>:#015 32%|███▏      | 32/100 [00:52<01:40,  1.48s/it][1,0]<stderr>:#015 32%|███▏      | 32/100 [00:52<01:39,  1.47s/it][1,8]<stderr>:#015 33%|███▎      | 33/100 [00:53<01:37,  1.46s/it][1,0]<stderr>:#015 33%|███▎      | 33/100 [00:53<01:36,  1.45s/it][1,8]<stderr>:#015 34%|███▍      | 34/100 [00:55<01:36,  1.46s/it][1,0]<stderr>:#015 34%|███▍      | 34/100 [00:55<01:36,  1.46s/it][1,8]<stderr>:#015 35%|███▌      | 35/100 [00:56<01:40,  1.55s/it][1,0]<stderr>:#015 35%|███▌      | 35/100 [00:57<01:41,  1.55s/it][1,8]<stderr>:#015 36%|███▌      | 36/100 [00:58<01:39,  1.55s/it][1,0]<stderr>:#015 36%|███▌      | 36/100 [00:58<01:39,  1.55s/it][1,8]<stderr>:#015 37%|███▋      | 37/100 [01:00<01:38,  1.57s/it][1,0]<stderr>:#015 37%|███▋      | 37/100 [01:00<01:38,  1.56s/it][1,8]<stderr>:#015 38%|███▊      | 38/100 [01:01<01:34,  1.53s/it][1,0]<stderr>:#015 38%|███▊      | 38/100 [01:01<01:36,  1.55s/it][1,8]<stderr>:#015 39%|███▉      | 39/100 [01:03<01:34,  1.55s/it][1,0]<stderr>:#015 39%|███▉      | 39/100 [01:03<01:33,  1.54s/it][1,0]<stderr>:#015 40%|████      | 40/100 [01:04<01:32,  1.54s/it][1,8]<stderr>:#015 40%|████      | 40/100 [01:04<01:34,  1.57s/it][1,8]<stderr>:#015 41%|████      | 41/100 [01:06<01:33,  1.59s/it][1,0]<stderr>:#015 41%|████      | 41/100 [01:06<01:34,  1.60s/it][1,0]<stderr>:#015 42%|████▏     | 42/100 [01:08<01:34,  1.63s/it][1,8]<stderr>:#015 42%|████▏     | 42/100 [01:08<01:34,  1.63s/it][1,8]<stderr>:#015 43%|████▎     | 43/100 [01:09<01:32,  1.63s/it][1,0]<stderr>:#015 43%|████▎     | 43/100 [01:09<01:32,  1.62s/it][1,0]<stderr>:#015 44%|████▍     | 44/100 [01:11<01:28,  1.58s/it][1,8]<stderr>:#015 44%|████▍     | 44/100 [01:11<01:31,  1.64s/it][1,8]<stderr>:#015 45%|████▌     | 45/100 [01:12<01:25,  1.55s/it][1,0]<stderr>:#015 45%|████▌     | 45/100 [01:12<01:28,  1.61s/it][1,0]<stderr>:#015 46%|████▌     | 46/100 [01:14<01:26,  1.59s/it][1,8]<stderr>:#015 46%|████▌     | 46/100 [01:14<01:29,  1.65s/it][1,8]<stderr>:#015 47%|████▋     | 47/100 [01:16<01:25,  1.61s/it][1,0]<stderr>:#015 47%|████▋     | 47/100 [01:16<01:27,  1.65s/it][1,8]<stderr>:#015 48%|████▊     | 48/100 [01:17<01:26,  1.66s/it][1,0]<stderr>:#015 48%|████▊     | 48/100 [01:18<01:27,  1.69s/it][1,8]<stderr>:#015 49%|████▉     | 49/100 [01:19<01:20,  1.58s/it][1,0]<stderr>:#015 49%|████▉     | 49/100 [01:19<01:21,  1.59s/it][1,0]<stderr>:#015 50%|█████     | 50/100 [01:20<01:19,  1.58s/it][1,8]<stderr>:#015 50%|█████     | 50/100 [01:20<01:19,  1.59s/it][1,8]<stderr>:#015 51%|█████     | 51/100 [01:22<01:16,  1.55s/it][1,0]<stderr>:#015 51%|█████     | 51/100 [01:22<01:16,  1.57s/it][1,8]<stderr>:#015 52%|█████▏    | 52/100 [01:23<01:13,  1.53s/it][1,0]<stderr>:#015 52%|█████▏    | 52/100 [01:23<01:13,  1.54s/it][1,0]<stderr>:#015 53%|█████▎    | 53/100 [01:25<01:09,  1.48s/it][1,8]<stderr>:#015 53%|█████▎    | 53/100 [01:25<01:10,  1.50s/it][1,8]<stderr>:#015 54%|█████▍    | 54/100 [01:27<01:11,  1.56s/it][1,0]<stderr>:#015 54%|█████▍    | 54/100 [01:27<01:12,  1.58s/it][1,0]<stderr>:#015 55%|█████▌    | 55/100 [01:28<01:08,  1.52s/it][1,8]<stderr>:#015 55%|█████▌    | 55/100 [01:28<01:10,  1.56s/it][1,8]<stderr>:#015 56%|█████▌    | 56/100 [01:29<01:05,  1.49s/it][1,0]<stderr>:#015 56%|█████▌    | 56/100 [01:29<01:06,  1.50s/it][1,8]<stderr>:#015 57%|█████▋    | 57/100 [01:31<01:05,  1.51s/it][1,0]<stderr>:#015 57%|█████▋    | 57/100 [01:31<01:06,  1.55s/it][1,8]<stderr>:#015 58%|█████▊    | 58/100 [01:33<01:04,  1.53s/it][1,0]<stderr>:#015 58%|█████▊    | 58/100 [01:33<01:04,  1.53s/it][1,0]<stderr>:#015 59%|█████▉    | 59/100 [01:34<01:04,  1.57s/it][1,8]<stderr>:#015 59%|█████▉    | 59/100 [01:34<01:05,  1.61s/it][1,8]<stderr>:#015 60%|██████    | 60/100 [01:36<01:04,  1.61s/it][1,0]<stderr>:#015 60%|██████    | 60/100 [01:36<01:04,  1.61s/it][1,0]<stderr>:#015 61%|██████    | 61/100 [01:38<01:05,  1.69s/it][1,8]<stderr>:#015 61%|██████    | 61/100 [01:38<01:06,  1.72s/it][1,0]<stderr>:#015 62%|██████▏   | 62/100 [01:39<01:02,  1.63s/it][1,8]<stderr>:#015 62%|██████▏   | 62/100 [01:39<01:02,  1.63s/it][1,0]<stderr>:#015 63%|██████▎   | 63/100 [01:41<01:00,  1.64s/it][1,8]<stderr>:#015 63%|██████▎   | 63/100 [01:41<01:01,  1.65s/it][1,8]<stderr>:#015 64%|██████▍   | 64/100 [01:43<01:01,  1.72s/it][1,0]<stderr>:#015 64%|██████▍   | 64/100 [01:43<01:02,  1.73s/it][1,8]<stderr>:#015 65%|██████▌   | 65/100 [01:45<00:59,  1.71s/it][1,0]<stderr>:#015 65%|██████▌   | 65/100 [01:45<01:00,  1.72s/it][1,8]<stderr>:#015 66%|██████▌   | 66/100 [01:46<00:57,  1.68s/it][1,0]<stderr>:#015 66%|██████▌   | 66/100 [01:46<00:57,  1.69s/it][1,8]<stderr>:#015 67%|██████▋   | 67/100 [01:48<00:53,  1.64s/it][1,0]<stderr>:#015 67%|██████▋   | 67/100 [01:48<00:54,  1.66s/it][1,0]<stderr>:#015 68%|██████▊   | 68/100 [01:50<00:54,  1.70s/it][1,8]<stderr>:#015 68%|██████▊   | 68/100 [01:50<00:55,  1.74s/it][1,0]<stderr>:#015 69%|██████▉   | 69/100 [01:51<00:53,  1.72s/it][1,8]<stderr>:#015 69%|██████▉   | 69/100 [01:51<00:53,  1.72s/it][1,8]<stderr>:#015 70%|███████   | 70/100 [01:53<00:48,  1.63s/it][1,0]<stderr>:#015 70%|███████   | 70/100 [01:53<00:49,  1.66s/it][1,0]<stderr>:#015 71%|███████   | 71/100 [01:55<00:47,  1.63s/it][1,8]<stderr>:#015 71%|███████   | 71/100 [01:55<00:49,  1.71s/it][1,8]<stderr>:#015 72%|███████▏  | 72/100 [01:56<00:47,  1.69s/it][1,0]<stderr>:#015 72%|███████▏  | 72/100 [01:56<00:47,  1.71s/it][1,8]<stderr>:#015 73%|███████▎  | 73/100 [01:58<00:45,  1.69s/it][1,0]<stderr>:#015 73%|███████▎  | 73/100 [01:58<00:47,  1.76s/it][1,0]<stderr>:#015 74%|███████▍  | 74/100 [02:00<00:43,  1.66s/it][1,8]<stderr>:#015 74%|███████▍  | 74/100 [02:00<00:43,  1.67s/it][1,8]<stderr>:#015 75%|███████▌  | 75/100 [02:01<00:41,  1.66s/it][1,0]<stderr>:#015 75%|███████▌  | 75/100 [02:01<00:41,  1.68s/it][1,8]<stderr>:#015 76%|███████▌  | 76/100 [02:03<00:39,  1.63s/it][1,0]<stderr>:#015 76%|███████▌  | 76/100 [02:03<00:39,  1.66s/it][1,8]<stderr>:#015 77%|███████▋  | 77/100 [02:04<00:37,  1.62s/it][1,0]<stderr>:#015 77%|███████▋  | 77/100 [02:05<00:37,  1.64s/it][1,0]<stderr>:#015 78%|███████▊  | 78/100 [02:06<00:34,  1.57s/it][1,8]<stderr>:#015 78%|███████▊  | 78/100 [02:06<00:35,  1.60s/it][1,8]<stderr>:#015 79%|███████▉  | 79/100 [02:08<00:33,  1.59s/it][1,0]<stderr>:#015 79%|███████▉  | 79/100 [02:08<00:33,  1.59s/it][1,0]<stderr>:#015 80%|████████  | 80/100 [02:09<00:31,  1.56s/it][1,8]<stderr>:#015 80%|████████  | 80/100 [02:09<00:31,  1.58s/it][1,0]<stderr>:#015 81%|████████  | 81/100 [02:11<00:29,  1.57s/it][1,8]<stderr>:#015 81%|████████  | 81/100 [02:11<00:30,  1.59s/it][1,8]<stderr>:#015 82%|████████▏ | 82/100 [02:12<00:28,  1.60s/it][1,0]<stderr>:#015 82%|████████▏ | 82/100 [02:12<00:28,  1.61s/it][1,0]<stderr>:#015 83%|████████▎ | 83/100 [02:14<00:27,  1.61s/it][1,8]<stderr>:#015 83%|████████▎ | 83/100 [02:14<00:27,  1.62s/it][1,0]<stderr>:#015 84%|████████▍ | 84/100 [02:16<00:25,  1.58s/it][1,8]<stderr>:#015 84%|████████▍ | 84/100 [02:16<00:25,  1.59s/it][1,8]<stderr>:#015 85%|████████▌ | 85/100 [02:17<00:23,  1.56s/it][1,0]<stderr>:#015 85%|████████▌ | 85/100 [02:17<00:23,  1.58s/it][1,8]<stderr>:#015 86%|████████▌ | 86/100 [02:19<00:22,  1.58s/it][1,0]<stderr>:#015 86%|████████▌ | 86/100 [02:19<00:22,  1.59s/it][1,8]<stderr>:#015 87%|████████▋ | 87/100 [02:20<00:20,  1.55s/it][1,0]<stderr>:#015 87%|████████▋ | 87/100 [02:20<00:20,  1.56s/it][1,0]<stderr>:#015 88%|████████▊ | 88/100 [02:22<00:19,  1.58s/it][1,8]<stderr>:#015 88%|████████▊ | 88/100 [02:22<00:19,  1.60s/it][1,0]<stderr>:#015 89%|████████▉ | 89/100 [02:23<00:16,  1.54s/it][1,8]<stderr>:#015 89%|████████▉ | 89/100 [02:23<00:17,  1.57s/it][1,8]<stderr>:#015 90%|█████████ | 90/100 [02:25<00:15,  1.56s/it][1,0]<stderr>:#015 90%|█████████ | 90/100 [02:25<00:15,  1.58s/it][1,8]<stderr>:#015 91%|█████████ | 91/100 [02:27<00:14,  1.59s/it][1,0]<stderr>:#015 91%|█████████ | 91/100 [02:27<00:14,  1.60s/it][1,8]<stderr>:#015 92%|█████████▏| 92/100 [02:28<00:12,  1.53s/it][1,0]<stderr>:#015 92%|█████████▏| 92/100 [02:28<00:12,  1.53s/it][1,8]<stderr>:#015 93%|█████████▎| 93/100 [02:30<00:10,  1.54s/it][1,0]<stderr>:#015 93%|█████████▎| 93/100 [02:30<00:10,  1.54s/it][1,8]<stderr>:#015 94%|█████████▍| 94/100 [02:31<00:09,  1.60s/it][1,0]<stderr>:#015 94%|█████████▍| 94/100 [02:31<00:09,  1.59s/it][1,0]<stderr>:#015 95%|█████████▌| 95/100 [02:33<00:07,  1.59s/it][1,8]<stderr>:#015 95%|█████████▌| 95/100 [02:33<00:07,  1.59s/it][1,0]<stderr>:#015 96%|█████████▌| 96/100 [02:34<00:06,  1.55s/it][1,8]<stderr>:#015 96%|█████████▌| 96/100 [02:34<00:06,  1.55s/it][1,0]<stderr>:#015 97%|█████████▋| 97/100 [02:36<00:04,  1.54s/it][1,8]<stderr>:#015 97%|█████████▋| 97/100 [02:36<00:04,  1.54s/it][1,8]<stderr>:#015 98%|█████████▊| 98/100 [02:37<00:03,  1.56s/it][1,0]<stderr>:#015 98%|█████████▊| 98/100 [02:37<00:03,  1.57s/it][1,0]<stderr>:#015 99%|█████████▉| 99/100 [02:39<00:01,  1.49s/it][1,8]<stderr>:#015 99%|█████████▉| 99/100 [02:39<00:01,  1.52s/it][1,8]<stderr>:#015100%|██████████| 100/100 [02:40<00:00,  1.54s/it][1,8]<stderr>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,565 >> \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.59s/it][1,0]<stderr>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,693 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015                                                 [1,0]<stderr>:#015[1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.59s/it][1,8]<stderr>:#015                                                 #015[1,8]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.54s/it][1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.62s/it][1,8]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.62s/it][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:516] 2021-09-05 08:02:10,635 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2115] 2021-09-05 08:02:10,639 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2117] 2021-09-05 08:02:10,639 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2120] 2021-09-05 08:02:10,639 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1885] 2021-09-05 08:02:10,793 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:351] 2021-09-05 08:02:10,794 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:889] 2021-09-05 08:02:13,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1924] 2021-09-05 08:02:13,044 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1930] 2021-09-05 08:02:13,044 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:02:13,088 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   epoch                      =       0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_alloc_delta   =     -122MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_peaked_delta  =      121MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_alloc_delta   =     1275MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_alloc_delta  =     1424MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_alloc_delta  =     5113MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_peaked_delta =     4884MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_runtime              = 0:02:41.11\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples              =      88524\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples_per_second   =      0.621\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2021-09-05 08:02:13,091 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2021-09-05 08:02:13,094 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2021-09-05 08:02:13,094 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2021-09-05 08:02:13,094 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s][1,8]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 2/169 [00:00<00:09, 18.30it/s][1,8]<stderr>:#015  1%|          | 2/169 [00:00<00:09, 18.29it/s][1,0]<stderr>:#015  2%|▏         | 4/169 [00:00<00:10, 16.32it/s][1,8]<stderr>:#015  2%|▏         | 4/169 [00:00<00:10, 16.31it/s][1,0]<stderr>:#015  4%|▎         | 6/169 [00:00<00:10, 15.10it/s][1,8]<stderr>:#015  4%|▎         | 6/169 [00:00<00:10, 15.09it/s][1,0]<stderr>:#015  5%|▍         | 8/169 [00:00<00:11, 14.41it/s][1,8]<stderr>:#015  5%|▍         | 8/169 [00:00<00:11, 14.41it/s][1,0]<stderr>:#015  6%|▌         | 10/169 [00:00<00:11, 13.95it/s][1,8]<stderr>:#015  6%|▌         | 10/169 [00:00<00:11, 13.95it/s][1,0]<stderr>:#015  7%|▋         | 12/169 [00:00<00:11, 13.17it/s][1,8]<stderr>:#015  7%|▋         | 12/169 [00:00<00:11, 13.17it/s][1,0]<stderr>:#015  8%|▊         | 14/169 [00:01<00:11, 13.02it/s][1,8]<stderr>:#015  8%|▊         | 14/169 [00:01<00:11, 13.02it/s][1,0]<stderr>:#015  9%|▉         | 16/169 [00:01<00:11, 13.04it/s][1,8]<stderr>:#015  9%|▉         | 16/169 [00:01<00:11, 13.04it/s][1,0]<stderr>:#015 11%|█         | 18/169 [00:01<00:11, 13.04it/s][1,8]<stderr>:#015 11%|█         | 18/169 [00:01<00:11, 13.04it/s][1,0]<stderr>:#015 12%|█▏        | 20/169 [00:01<00:11, 13.03it/s][1,8]<stderr>:#015 12%|█▏        | 20/169 [00:01<00:11, 13.03it/s][1,0]<stderr>:#015 13%|█▎        | 22/169 [00:01<00:11, 13.03it/s][1,8]<stderr>:#015 13%|█▎        | 22/169 [00:01<00:11, 13.03it/s][1,0]<stderr>:#015 14%|█▍        | 24/169 [00:01<00:11, 13.00it/s][1,8]<stderr>:#015 14%|█▍        | 24/169 [00:01<00:11, 13.00it/s][1,0]<stderr>:#015 15%|█▌        | 26/169 [00:01<00:10, 13.01it/s][1,8]<stderr>:#015 15%|█▌        | 26/169 [00:01<00:10, 13.01it/s][1,0]<stderr>:#015 17%|█▋        | 28/169 [00:02<00:10, 13.02it/s][1,8]<stderr>:#015 17%|█▋        | 28/169 [00:02<00:10, 13.02it/s][1,0]<stderr>:#015 18%|█▊        | 30/169 [00:02<00:10, 13.04it/s][1,8]<stderr>:#015 18%|█▊        | 30/169 [00:02<00:10, 13.04it/s][1,0]<stderr>:#015 19%|█▉        | 32/169 [00:02<00:10, 13.06it/s][1,8]<stderr>:#015 19%|█▉        | 32/169 [00:02<00:10, 13.06it/s][1,0]<stderr>:#015 20%|██        | 34/169 [00:02<00:10, 13.06it/s][1,8]<stderr>:#015 20%|██        | 34/169 [00:02<00:10, 13.06it/s][1,0]<stderr>:#015 21%|██▏       | 36/169 [00:02<00:10, 13.07it/s][1,8]<stderr>:#015 21%|██▏       | 36/169 [00:02<00:10, 13.07it/s][1,0]<stderr>:#015 22%|██▏       | 38/169 [00:02<00:10, 13.05it/s][1,8]<stderr>:#015 22%|██▏       | 38/169 [00:02<00:10, 13.05it/s][1,0]<stderr>:#015 24%|██▎       | 40/169 [00:03<00:09, 13.06it/s][1,8]<stderr>:#015 24%|██▎       | 40/169 [00:03<00:09, 13.06it/s][1,0]<stderr>:#015 25%|██▍       | 42/169 [00:03<00:09, 13.07it/s][1,8]<stderr>:#015 25%|██▍       | 42/169 [00:03<00:09, 13.07it/s][1,0]<stderr>:#015 26%|██▌       | 44/169 [00:03<00:09, 13.07it/s][1,8]<stderr>:#015 26%|██▌       | 44/169 [00:03<00:09, 13.07it/s][1,0]<stderr>:#015 27%|██▋       | 46/169 [00:03<00:09, 13.05it/s][1,8]<stderr>:#015 27%|██▋       | 46/169 [00:03<00:09, 13.04it/s][1,0]<stderr>:#015 28%|██▊       | 48/169 [00:03<00:09, 13.04it/s][1,8]<stderr>:#015 28%|██▊       | 48/169 [00:03<00:09, 13.04it/s][1,0]<stderr>:#015 30%|██▉       | 50/169 [00:03<00:09, 13.02it/s][1,8]<stderr>:#015 30%|██▉       | 50/169 [00:03<00:09, 13.02it/s][1,0]<stderr>:#015 31%|███       | 52/169 [00:03<00:08, 13.01it/s][1,8]<stderr>:#015 31%|███       | 52/169 [00:03<00:08, 13.02it/s][1,0]<stderr>:#015 32%|███▏      | 54/169 [00:04<00:08, 13.01it/s][1,8]<stderr>:#015 32%|███▏      | 54/169 [00:04<00:08, 13.01it/s][1,0]<stderr>:#015 33%|███▎      | 56/169 [00:04<00:08, 13.01it/s][1,8]<stderr>:#015 33%|███▎      | 56/169 [00:04<00:08, 13.01it/s][1,0]<stderr>:#015 34%|███▍      | 58/169 [00:04<00:08, 13.02it/s][1,8]<stderr>:#015 34%|███▍      | 58/169 [00:04<00:08, 13.02it/s][1,0]<stderr>:#015 36%|███▌      | 60/169 [00:04<00:08, 12.94it/s][1,8]<stderr>:#015 36%|███▌      | 60/169 [00:04<00:08, 12.94it/s][1,0]<stderr>:#015 37%|███▋      | 62/169 [00:04<00:08, 12.96it/s][1,8]<stderr>:#015 37%|███▋      | 62/169 [00:04<00:08, 12.96it/s][1,0]<stderr>:#015 38%|███▊      | 64/169 [00:04<00:08, 12.98it/s][1,8]<stderr>:#015 38%|███▊      | 64/169 [00:04<00:08, 12.98it/s][1,0]<stderr>:#015 39%|███▉      | 66/169 [00:05<00:07, 12.99it/s][1,8]<stderr>:#015 39%|███▉      | 66/169 [00:05<00:07, 12.99it/s][1,0]<stderr>:#015 40%|████      | 68/169 [00:05<00:07, 13.01it/s][1,8]<stderr>:#015 40%|████      | 68/169 [00:05<00:07, 13.01it/s][1,0]<stderr>:#015 41%|████▏     | 70/169 [00:05<00:07, 13.02it/s][1,8]<stderr>:#015 41%|████▏     | 70/169 [00:05<00:07, 13.02it/s][1,0]<stderr>:#015 43%|████▎     | 72/169 [00:05<00:08, 11.58it/s][1,8]<stderr>:#015 43%|████▎     | 72/169 [00:05<00:08, 11.58it/s][1,0]<stderr>:#015 44%|████▍     | 74/169 [00:05<00:09,  9.82it/s][1,8]<stderr>:#015 44%|████▍     | 74/169 [00:05<00:09,  9.82it/s][1,0]<stderr>:#015 45%|████▍     | 76/169 [00:06<00:08, 10.60it/s][1,8]<stderr>:#015 45%|████▍     | 76/169 [00:06<00:08, 10.60it/s][1,0]<stderr>:#015 46%|████▌     | 78/169 [00:06<00:08, 11.22it/s][1,8]<stderr>:#015 46%|████▌     | 78/169 [00:06<00:08, 11.22it/s][1,0]<stderr>:#015 47%|████▋     | 80/169 [00:06<00:07, 11.70it/s][1,8]<stderr>:#015 47%|████▋     | 80/169 [00:06<00:07, 11.70it/s][1,0]<stderr>:#015 49%|████▊     | 82/169 [00:06<00:07, 12.06it/s][1,8]<stderr>:#015 49%|████▊     | 82/169 [00:06<00:07, 12.06it/s][1,0]<stderr>:#015 50%|████▉     | 84/169 [00:06<00:06, 12.33it/s][1,8]<stderr>:#015 50%|████▉     | 84/169 [00:06<00:06, 12.33it/s][1,0]<stderr>:#015 51%|█████     | 86/169 [00:06<00:06, 12.51it/s][1,8]<stderr>:#015 51%|█████     | 86/169 [00:06<00:06, 12.51it/s][1,0]<stderr>:#015 52%|█████▏    | 88/169 [00:06<00:06, 12.65it/s][1,8]<stderr>:#015 52%|█████▏    | 88/169 [00:06<00:06, 12.65it/s][1,0]<stderr>:#015 53%|█████▎    | 90/169 [00:07<00:06, 12.70it/s][1,8]<stderr>:#015 53%|█████▎    | 90/169 [00:07<00:06, 12.70it/s][1,0]<stderr>:#015 54%|█████▍    | 92/169 [00:07<00:06, 12.75it/s][1,8]<stderr>:#015 54%|█████▍    | 92/169 [00:07<00:06, 12.75it/s][1,0]<stderr>:#015 56%|█████▌    | 94/169 [00:07<00:05, 12.81it/s][1,8]<stderr>:#015 56%|█████▌    | 94/169 [00:07<00:05, 12.81it/s][1,0]<stderr>:#015 57%|█████▋    | 96/169 [00:07<00:05, 12.86it/s][1,8]<stderr>:#015 57%|█████▋    | 96/169 [00:07<00:05, 12.86it/s][1,0]<stderr>:#015 58%|█████▊    | 98/169 [00:07<00:05, 12.87it/s][1,8]<stderr>:#015 58%|█████▊    | 98/169 [00:07<00:05, 12.88it/s][1,0]<stderr>:#015 59%|█████▉    | 100/169 [00:07<00:05, 12.83it/s][1,8]<stderr>:#015 59%|█████▉    | 100/169 [00:07<00:05, 12.83it/s][1,0]<stderr>:#015 60%|██████    | 102/169 [00:08<00:05, 12.78it/s][1,8]<stderr>:#015 60%|██████    | 102/169 [00:08<00:05, 12.78it/s][1,0]<stderr>:#015 62%|██████▏   | 104/169 [00:08<00:05, 12.82it/s][1,8]<stderr>:#015 62%|██████▏   | 104/169 [00:08<00:05, 12.82it/s][1,0]<stderr>:#015 63%|██████▎   | 106/169 [00:08<00:04, 12.83it/s][1,8]<stderr>:#015 63%|██████▎   | 106/169 [00:08<00:04, 12.83it/s][1,0]<stderr>:#015 64%|██████▍   | 108/169 [00:08<00:04, 12.81it/s][1,8]<stderr>:#015 64%|██████▍   | 108/169 [00:08<00:04, 12.81it/s][1,0]<stderr>:#015 65%|██████▌   | 110/169 [00:08<00:04, 12.82it/s][1,8]<stderr>:#015 65%|██████▌   | 110/169 [00:08<00:04, 12.81it/s][1,0]<stderr>:#015 66%|██████▋   | 112/169 [00:08<00:04, 12.85it/s][1,8]<stderr>:#015 66%|██████▋   | 112/169 [00:08<00:04, 12.85it/s][1,0]<stderr>:#015 67%|██████▋   | 114/169 [00:08<00:04, 12.85it/s][1,8]<stderr>:#015 67%|██████▋   | 114/169 [00:08<00:04, 12.86it/s][1,0]<stderr>:#015 69%|██████▊   | 116/169 [00:09<00:04, 12.89it/s][1,8]<stderr>:#015 69%|██████▊   | 116/169 [00:09<00:04, 12.89it/s][1,0]<stderr>:#015 70%|██████▉   | 118/169 [00:09<00:03, 12.85it/s][1,8]<stderr>:#015 70%|██████▉   | 118/169 [00:09<00:03, 12.85it/s][1,0]<stderr>:#015 71%|███████   | 120/169 [00:09<00:03, 12.87it/s][1,8]<stderr>:#015 71%|███████   | 120/169 [00:09<00:03, 12.87it/s][1,0]<stderr>:#015 72%|███████▏  | 122/169 [00:09<00:03, 12.89it/s][1,8]<stderr>:#015 72%|███████▏  | 122/169 [00:09<00:03, 12.89it/s][1,0]<stderr>:#015 73%|███████▎  | 124/169 [00:09<00:03, 12.80it/s][1,8]<stderr>:#015 73%|███████▎  | 124/169 [00:09<00:03, 12.80it/s][1,0]<stderr>:#015 75%|███████▍  | 126/169 [00:09<00:03, 12.81it/s][1,8]<stderr>:#015 75%|███████▍  | 126/169 [00:09<00:03, 12.81it/s][1,0]<stderr>:#015 76%|███████▌  | 128/169 [00:10<00:03, 12.78it/s][1,8]<stderr>:#015 76%|███████▌  | 128/169 [00:10<00:03, 12.79it/s][1,0]<stderr>:#015 77%|███████▋  | 130/169 [00:10<00:03, 12.84it/s][1,8]<stderr>:#015 77%|███████▋  | 130/169 [00:10<00:03, 12.84it/s][1,0]<stderr>:#015 78%|███████▊  | 132/169 [00:10<00:02, 12.87it/s][1,8]<stderr>:#015 78%|███████▊  | 132/169 [00:10<00:02, 12.87it/s][1,0]<stderr>:#015 79%|███████▉  | 134/169 [00:10<00:02, 12.90it/s][1,8]<stderr>:#015 79%|███████▉  | 134/169 [00:10<00:02, 12.90it/s][1,0]<stderr>:#015 80%|████████  | 136/169 [00:10<00:02, 12.89it/s][1,8]<stderr>:#015 80%|████████  | 136/169 [00:10<00:02, 12.89it/s][1,0]<stderr>:#015 82%|████████▏ | 138/169 [00:10<00:02, 12.91it/s][1,8]<stderr>:#015 82%|████████▏ | 138/169 [00:10<00:02, 12.91it/s][1,0]<stderr>:#015 83%|████████▎ | 140/169 [00:10<00:02, 12.91it/s][1,8]<stderr>:#015 83%|████████▎ | 140/169 [00:10<00:02, 12.91it/s][1,0]<stderr>:#015 84%|████████▍ | 142/169 [00:11<00:02, 12.93it/s][1,8]<stderr>:#015 84%|████████▍ | 142/169 [00:11<00:02, 12.93it/s][1,0]<stderr>:#015 85%|████████▌ | 144/169 [00:11<00:01, 12.94it/s][1,8]<stderr>:#015 85%|████████▌ | 144/169 [00:11<00:01, 12.94it/s][1,0]<stderr>:#015 86%|████████▋ | 146/169 [00:11<00:01, 12.93it/s][1,8]<stderr>:#015 86%|████████▋ | 146/169 [00:11<00:01, 12.93it/s][1,0]<stderr>:#015 88%|████████▊ | 148/169 [00:11<00:01, 12.93it/s][1,8]<stderr>:#015 88%|████████▊ | 148/169 [00:11<00:01, 12.93it/s][1,0]<stderr>:#015 89%|████████▉ | 150/169 [00:11<00:01, 11.36it/s][1,8]<stderr>:#015 89%|████████▉ | 150/169 [00:11<00:01, 11.36it/s][1,0]<stderr>:#015 90%|████████▉ | 152/169 [00:11<00:01, 11.80it/s][1,8]<stderr>:#015 90%|████████▉ | 152/169 [00:11<00:01, 11.80it/s][1,0]<stderr>:#015 91%|█████████ | 154/169 [00:12<00:01,  9.77it/s][1,8]<stderr>:#015 91%|█████████ | 154/169 [00:12<00:01,  9.77it/s][1,8]<stderr>:#015 92%|█████████▏| 156/169 [00:12<00:01, 10.56it/s][1,0]<stderr>:#015 92%|█████████▏| 156/169 [00:12<00:01, 10.56it/s][1,0]<stderr>:#015 93%|█████████▎| 158/169 [00:12<00:00, 11.19it/s][1,8]<stderr>:#015 93%|█████████▎| 158/169 [00:12<00:00, 11.19it/s][1,0]<stderr>:#015 95%|█████████▍| 160/169 [00:12<00:00, 11.65it/s][1,8]<stderr>:#015 95%|█████████▍| 160/169 [00:12<00:00, 11.65it/s][1,0]<stderr>:#015 96%|█████████▌| 162/169 [00:12<00:00, 12.03it/s][1,8]<stderr>:#015 96%|█████████▌| 162/169 [00:12<00:00, 12.03it/s][1,0]<stderr>:#015 97%|█████████▋| 164/169 [00:13<00:00, 12.30it/s][1,8]<stderr>:#015 97%|█████████▋| 164/169 [00:13<00:00, 12.30it/s][1,0]<stderr>:#015 98%|█████████▊| 166/169 [00:13<00:00, 12.50it/s][1,8]<stderr>:#015 98%|█████████▊| 166/169 [00:13<00:00, 12.50it/s][1,0]<stderr>:#015 99%|█████████▉| 168/169 [00:13<00:00, 12.65it/s][1,8]<stderr>:#015 99%|█████████▉| 168/169 [00:13<00:00, 12.65it/s][1,7]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,7]<stderr>:#015  0%|          | 41/10570 [00:00<00:25, 405.70it/s][1,2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 402.26it/s][1,7]<stderr>:#015  1%|          | 87/10570 [00:00<00:24, 420.49it/s][1,10]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,2]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 402.94it/s][1,4]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 416.71it/s][1,15]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,7]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 441.38it/s][1,10]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 397.04it/s][1,1]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 398.73it/s][1,2]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 417.46it/s][1,9]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,4]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 399.20it/s][1,12]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 437.66it/s][1,15]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 394.55it/s][1,7]<stderr>:#015  2%|▏         | 182/10570 [00:00<00:23, 441.25it/s][1,10]<stderr>:#015  1%|          | 86/10570 [00:00<00:25, 412.05it/s][1,1]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 412.18it/s][1,2]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 437.24it/s][1,9]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 397.42it/s][1,4]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 412.99it/s][1,12]<stderr>:#015  2%|▏         | 182/10570 [00:00<00:23, 438.19it/s][1,15]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 409.15it/s][1,7]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:25, 400.75it/s][1,14]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,10]<stderr>:#015  1%|▏         | 136/10570 [00:00<00:24, 432.73it/s][1,1]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 431.56it/s][1,2]<stderr>:#015  2%|▏         | 181/10570 [00:00<00:23, 437.17it/s][1,9]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 410.68it/s][1,5]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,4]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 433.59it/s][1,12]<stderr>:#015  2%|▏         | 220/10570 [00:00<00:24, 418.49it/s][1,15]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 430.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A[1,7]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:28, 367.16it/s][1,3]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,11]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,14]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 401.33it/s][1,10]<stderr>:#015  2%|▏         | 180/10570 [00:00<00:23, 433.75it/s][1,1]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 430.00it/s][1,2]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:24, 415.71it/s][1,9]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 431.21it/s][1,5]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 394.37it/s][1,4]<stderr>:#015  2%|▏         | 178/10570 [00:00<00:24, 432.13it/s][1,15]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 429.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 393.51it/s]#033[A[1,12]<stderr>:#015  2%|▏         | 256/10570 [00:00<00:28, 366.48it/s][1,3]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 387.59it/s][1,11]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 387.93it/s][1,14]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 415.20it/s][1,1]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:24, 416.21it/s][1,10]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:24, 415.45it/s][1,6]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,9]<stderr>:#015  2%|▏         | 178/10570 [00:00<00:24, 430.76it/s][1,5]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 408.22it/s][1,4]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:24, 415.30it/s][1,7]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 310.25it/s][1,2]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 366.53it/s][1,13]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,15]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:24, 415.22it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 406.67it/s]#033[A[1,11]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 401.54it/s][1,3]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 399.57it/s][1,14]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:24, 434.70it/s][1,12]<stderr>:#015  3%|▎         | 290/10570 [00:00<00:32, 320.16it/s][1,1]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 375.30it/s][1,6]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 384.12it/s][1,10]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 367.05it/s][1,9]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:25, 410.82it/s][1,5]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 429.42it/s][1,7]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 339.85it/s][1,13]<stderr>:#015  0%|          | 37/10570 [00:00<00:28, 369.49it/s][1,4]<stderr>:#015  2%|▏         | 253/10570 [00:00<00:27, 371.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 427.09it/s]#033[A[1,15]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 375.20it/s][1,2]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 316.02it/s][1,11]<stderr>:#015  1%|          | 132/10570 [00:00<00:24, 422.53it/s][1,3]<stderr>:#015  1%|          | 132/10570 [00:00<00:24, 420.94it/s][1,14]<stderr>:#015  2%|▏         | 181/10570 [00:00<00:23, 434.57it/s][1,12]<stderr>:#015  3%|▎         | 335/10570 [00:00<00:29, 349.94it/s][1,6]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 397.88it/s][1,5]<stderr>:#015  2%|▏         | 179/10570 [00:00<00:24, 430.12it/s][1,7]<stderr>:#015  4%|▎         | 377/10570 [00:00<00:27, 370.43it/s][1,9]<stderr>:#015  2%|▏         | 251/10570 [00:00<00:27, 372.65it/s][1,13]<stderr>:#015  1%|          | 79/10570 [00:00<00:27, 381.15it/s][1,10]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 315.69it/s][1,1]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:32, 312.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 426.41it/s]#033[A[1,2]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 345.85it/s][1,11]<stderr>:#015  2%|▏         | 174/10570 [00:00<00:24, 421.24it/s][1,3]<stderr>:#015  2%|▏         | 174/10570 [00:00<00:24, 420.61it/s][1,14]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:25, 413.93it/s][1,4]<stderr>:#015  3%|▎         | 287/10570 [00:00<00:32, 312.28it/s][1,12]<stderr>:#015  4%|▎         | 382/10570 [00:00<00:26, 378.03it/s][1,6]<stderr>:#015  1%|          | 131/10570 [00:00<00:24, 418.93it/s][1,15]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 310.73it/s][1,5]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:25, 412.84it/s][1,7]<stderr>:#015  4%|▍         | 417/10570 [00:01<00:26, 376.94it/s][1,13]<stderr>:#015  1%|          | 124/10570 [00:00<00:26, 399.00it/s][1,10]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 345.26it/s][1,1]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 340.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:25, 412.07it/s]#033[A[1,2]<stderr>:#015  4%|▎         | 380/10570 [00:00<00:27, 372.58it/s][1,11]<stderr>:#015  2%|▏         | 213/10570 [00:00<00:25, 408.91it/s][1,3]<stderr>:#015  2%|▏         | 213/10570 [00:00<00:25, 408.14it/s][1,9]<stderr>:#015  3%|▎         | 285/10570 [00:00<00:33, 306.86it/s][1,4]<stderr>:#015  3%|▎         | 331/10570 [00:00<00:29, 341.40it/s][1,12]<stderr>:#015  4%|▍         | 422/10570 [00:01<00:26, 383.27it/s][1,6]<stderr>:#015  2%|▏         | 173/10570 [00:00<00:24, 418.76it/s][1,14]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 366.78it/s][1,15]<stderr>:#015  3%|▎         | 329/10570 [00:00<00:30, 338.92it/s][1,7]<stderr>:#015  4%|▍         | 462/10570 [00:01<00:25, 395.39it/s][1,13]<stderr>:#015  2%|▏         | 165/10570 [00:00<00:25, 400.93it/s][1,5]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 372.88it/s][1,10]<stderr>:#015  4%|▎         | 381/10570 [00:00<00:27, 374.22it/s][1,1]<stderr>:#015  4%|▎         | 376/10570 [00:00<00:27, 368.98it/s][1,2]<stderr>:#015  4%|▍         | 419/10570 [00:01<00:26, 377.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 251/10570 [00:00<00:27, 374.17it/s]#033[A[1,9]<stderr>:#015  3%|▎         | 328/10570 [00:00<00:30, 335.30it/s][1,3]<stderr>:#015  2%|▏         | 248/10570 [00:00<00:27, 374.29it/s][1,11]<stderr>:#015  2%|▏         | 248/10570 [00:00<00:27, 373.69it/s][1,4]<stderr>:#015  4%|▎         | 378/10570 [00:00<00:27, 370.38it/s][1,12]<stderr>:#015  4%|▍         | 465/10570 [00:01<00:25, 395.81it/s][1,6]<stderr>:#015  2%|▏         | 212/10570 [00:00<00:25, 406.98it/s][1,15]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:27, 365.04it/s][1,7]<stderr>:#015  5%|▍         | 508/10570 [00:01<00:24, 411.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A[1,13]<stderr>:#015  2%|▏         | 204/10570 [00:00<00:26, 395.97it/s][1,14]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 316.69it/s][1,10]<stderr>:#015  4%|▍         | 420/10570 [00:01<00:26, 377.67it/s][1,1]<stderr>:#015  4%|▍         | 415/10570 [00:01<00:27, 374.39it/s][1,2]<stderr>:#015  4%|▍         | 463/10570 [00:01<00:25, 393.91it/s][1,9]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:28, 363.91it/s][1,5]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 309.86it/s][1,4]<stderr>:#015  4%|▍         | 417/10570 [00:01<00:27, 374.92it/s][1,12]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:24, 412.69it/s][1,6]<stderr>:#015  2%|▏         | 247/10570 [00:00<00:27, 372.17it/s][1,15]<stderr>:#015  4%|▍         | 414/10570 [00:01<00:27, 371.99it/s][1,7]<stderr>:#015  5%|▌         | 553/10570 [00:01<00:23, 421.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  3%|▎         | 285/10570 [00:00<00:33, 307.55it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 37/10570 [00:00<00:28, 367.44it/s]#033[A[1,13]<stderr>:#015  2%|▏         | 238/10570 [00:00<00:28, 364.92it/s][1,14]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 346.14it/s][1,1]<stderr>:#015  4%|▍         | 459/10570 [00:01<00:25, 391.11it/s][1,3]<stderr>:#015  3%|▎         | 282/10570 [00:00<00:34, 301.87it/s][1,10]<stderr>:#015  4%|▍         | 464/10570 [00:01<00:25, 392.24it/s][1,11]<stderr>:#015  3%|▎         | 282/10570 [00:00<00:34, 301.23it/s][1,2]<stderr>:#015  5%|▍         | 509/10570 [00:01<00:24, 409.94it/s][1,5]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 338.56it/s][1,9]<stderr>:#015  4%|▍         | 413/10570 [00:01<00:27, 365.87it/s][1,4]<stderr>:#015  4%|▍         | 461/10570 [00:01<00:25, 392.12it/s][1,12]<stderr>:#015  5%|▌         | 556/10570 [00:01<00:23, 420.53it/s][1,15]<stderr>:#015  4%|▍         | 458/10570 [00:01<00:26, 388.74it/s][1,7]<stderr>:#015  6%|▌         | 596/10570 [00:01<00:23, 420.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  3%|▎         | 328/10570 [00:00<00:30, 335.13it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  1%|          | 79/10570 [00:00<00:27, 380.26it/s]#033[A[1,14]<stderr>:#015  4%|▎         | 381/10570 [00:00<00:27, 374.75it/s][1,10]<stderr>:#015  5%|▍         | 509/10570 [00:01<00:24, 407.78it/s][1,1]<stderr>:#015  5%|▍         | 504/10570 [00:01<00:24, 405.21it/s][1,3]<stderr>:#015  3%|▎         | 324/10570 [00:00<00:31, 328.41it/s][1,11]<stderr>:#015  3%|▎         | 324/10570 [00:00<00:31, 327.71it/s][1,2]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:23, 418.92it/s][1,6]<stderr>:#015  3%|▎         | 281/10570 [00:00<00:34, 298.71it/s][1,5]<stderr>:#015  4%|▎         | 376/10570 [00:00<00:27, 367.19it/s][1,9]<stderr>:#015  4%|▍         | 456/10570 [00:01<00:26, 382.09it/s][1,4]<stderr>:#015  5%|▍         | 506/10570 [00:01<00:24, 406.98it/s][1,12]<stderr>:#015  6%|▌         | 599/10570 [00:01<00:23, 419.95it/s][1,13]<stderr>:#015  3%|▎         | 271/10570 [00:00<00:35, 289.55it/s][1,15]<stderr>:#015  5%|▍         | 502/10570 [00:01<00:25, 401.92it/s][1,7]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 421.28it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:28, 363.60it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  1%|          | 125/10570 [00:00<00:26, 399.43it/s]#033[A[1,14]<stderr>:#015  4%|▍         | 421/10570 [00:01<00:26, 380.05it/s][1,10]<stderr>:#015  5%|▌         | 552/10570 [00:01<00:24, 413.94it/s][1,1]<stderr>:#015  5%|▌         | 549/10570 [00:01<00:24, 416.97it/s][1,11]<stderr>:#015  3%|▎         | 369/10570 [00:00<00:28, 356.54it/s][1,3]<stderr>:#015  4%|▎         | 370/10570 [00:00<00:28, 357.53it/s][1,2]<stderr>:#015  6%|▌         | 597/10570 [00:01<00:23, 417.08it/s][1,6]<stderr>:#015  3%|▎         | 322/10570 [00:00<00:31, 324.25it/s][1,5]<stderr>:#015  4%|▍         | 415/10570 [00:01<00:27, 372.94it/s][1,9]<stderr>:#015  5%|▍         | 500/10570 [00:01<00:25, 395.75it/s][1,4]<stderr>:#015  5%|▌         | 551/10570 [00:01<00:23, 417.59it/s][1,12]<stderr>:#015  6%|▌         | 644/10570 [00:01<00:23, 426.04it/s][1,13]<stderr>:#015  3%|▎         | 307/10570 [00:00<00:33, 306.43it/s][1,15]<stderr>:#015  5%|▌         | 548/10570 [00:01<00:24, 415.38it/s][1,7]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 429.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▍         | 413/10570 [00:01<00:27, 370.01it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 166/10570 [00:00<00:25, 401.51it/s]#033[A[1,14]<stderr>:#015  4%|▍         | 465/10570 [00:01<00:25, 393.77it/s][1,10]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:24, 413.89it/s][1,11]<stderr>:#015  4%|▍         | 409/10570 [00:01<00:27, 366.65it/s][1,1]<stderr>:#015  6%|▌         | 592/10570 [00:01<00:23, 415.82it/s][1,3]<stderr>:#015  4%|▍         | 410/10570 [00:01<00:27, 366.43it/s][1,2]<stderr>:#015  6%|▌         | 642/10570 [00:01<00:23, 425.30it/s][1,6]<stderr>:#015  3%|▎         | 367/10570 [00:00<00:28, 353.82it/s][1,5]<stderr>:#015  4%|▍         | 459/10570 [00:01<00:25, 390.04it/s][1,9]<stderr>:#015  5%|▌         | 546/10570 [00:01<00:24, 411.66it/s][1,4]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:24, 415.26it/s][1,12]<stderr>:#015  7%|▋         | 690/10570 [00:01<00:22, 435.05it/s][1,13]<stderr>:#015  3%|▎         | 351/10570 [00:00<00:30, 336.27it/s][1,15]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:24, 414.25it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▍         | 456/10570 [00:01<00:26, 385.62it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 205/10570 [00:00<00:26, 395.75it/s]#033[A[1,7]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 419.65it/s][1,14]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:24, 409.27it/s][1,10]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 422.96it/s][1,11]<stderr>:#015  4%|▍         | 451/10570 [00:01<00:26, 379.54it/s][1,1]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:23, 423.51it/s][1,3]<stderr>:#015  4%|▍         | 452/10570 [00:01<00:26, 380.07it/s][1,2]<stderr>:#015  7%|▋         | 688/10570 [00:01<00:22, 433.55it/s][1,6]<stderr>:#015  4%|▍         | 407/10570 [00:01<00:27, 364.58it/s][1,5]<stderr>:#015  5%|▍         | 503/10570 [00:01<00:24, 403.30it/s][1,9]<stderr>:#015  6%|▌         | 588/10570 [00:01<00:24, 411.77it/s][1,4]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 423.70it/s][1,13]<stderr>:#015  4%|▎         | 393/10570 [00:01<00:28, 356.20it/s][1,12]<stderr>:#015  7%|▋         | 734/10570 [00:01<00:23, 421.73it/s][1,15]<stderr>:#015  6%|▌         | 636/10570 [00:01<00:23, 422.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  5%|▍         | 500/10570 [00:01<00:25, 398.42it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 239/10570 [00:00<00:28, 361.68it/s]#033[A[1,7]<stderr>:#015  7%|▋         | 771/10570 [00:01<00:23, 412.09it/s][1,14]<stderr>:#015  5%|▌         | 555/10570 [00:01<00:23, 417.39it/s][1,10]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 428.62it/s][1,11]<stderr>:#015  5%|▍         | 493/10570 [00:01<00:25, 390.69it/s][1,1]<stderr>:#015  6%|▋         | 681/10570 [00:01<00:23, 428.16it/s][1,3]<stderr>:#015  5%|▍         | 494/10570 [00:01<00:25, 390.53it/s][1,6]<stderr>:#015  4%|▍         | 448/10570 [00:01<00:26, 376.10it/s][1,2]<stderr>:#015  7%|▋         | 732/10570 [00:01<00:23, 420.03it/s][1,5]<stderr>:#015  5%|▌         | 549/10570 [00:01<00:24, 415.93it/s][1,9]<stderr>:#015  6%|▌         | 632/10570 [00:01<00:23, 419.33it/s][1,4]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 429.21it/s][1,13]<stderr>:#015  4%|▍         | 432/10570 [00:01<00:27, 363.91it/s][1,12]<stderr>:#015  7%|▋         | 777/10570 [00:01<00:23, 410.28it/s][1,15]<stderr>:#015  6%|▋         | 680/10570 [00:01<00:23, 421.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  5%|▌         | 545/10570 [00:01<00:24, 412.56it/s]#033[A[1,11]<stderr>:#015  5%|▌         | 539/10570 [00:01<00:24, 408.22it/s][1,3]<stderr>:#015  5%|▌         | 540/10570 [00:01<00:24, 407.40it/s][1,14]<stderr>:#015  6%|▌         | 598/10570 [00:01<00:24, 412.98it/s][1,10]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 417.36it/s][1,1]<stderr>:#015  7%|▋         | 725/10570 [00:01<00:23, 416.76it/s][1,7]<stderr>:#015  8%|▊         | 813/10570 [00:02<00:25, 386.81it/s][1,6]<stderr>:#015  5%|▍         | 490/10570 [00:01<00:26, 386.50it/s][1,9]<stderr>:#015  6%|▋         | 676/10570 [00:01<00:23, 423.02it/s][1,5]<stderr>:#015  6%|▌         | 592/10570 [00:01<00:24, 411.43it/s][1,2]<stderr>:#015  7%|▋         | 775/10570 [00:01<00:23, 408.75it/s][1,4]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 416.54it/s][1,13]<stderr>:#015  4%|▍         | 470/10570 [00:01<00:27, 366.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 272/10570 [00:00<00:36, 284.79it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▌         | 587/10570 [00:01<00:24, 411.80it/s]#033[A[1,15]<stderr>:#015  7%|▋         | 723/10570 [00:01<00:23, 411.80it/s][1,12]<stderr>:#015  8%|▊         | 819/10570 [00:02<00:25, 382.78it/s][1,11]<stderr>:#015  5%|▌         | 581/10570 [00:01<00:24, 407.17it/s][1,14]<stderr>:#015  6%|▌         | 643/10570 [00:01<00:23, 421.09it/s][1,3]<stderr>:#015  6%|▌         | 582/10570 [00:01<00:24, 407.47it/s][1,7]<stderr>:#015  8%|▊         | 853/10570 [00:02<00:25, 388.45it/s][1,10]<stderr>:#015  7%|▋         | 770/10570 [00:01<00:23, 409.12it/s][1,1]<stderr>:#015  7%|▋         | 767/10570 [00:01<00:23, 408.85it/s][1,6]<stderr>:#015  5%|▌         | 537/10570 [00:01<00:24, 405.67it/s][1,5]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:23, 420.56it/s][1,9]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:23, 414.77it/s][1,13]<stderr>:#015  5%|▍         | 513/10570 [00:01<00:26, 382.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 308/10570 [00:00<00:33, 302.68it/s]#033[A[1,2]<stderr>:#015  8%|▊         | 817/10570 [00:02<00:25, 382.24it/s][1,4]<stderr>:#015  7%|▋         | 770/10570 [00:01<00:24, 400.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▌         | 631/10570 [00:01<00:23, 418.26it/s]#033[A[1,15]<stderr>:#015  7%|▋         | 765/10570 [00:01<00:24, 404.77it/s][1,12]<stderr>:#015  8%|▊         | 860/10570 [00:02<00:24, 389.22it/s][1,11]<stderr>:#015  6%|▌         | 623/10570 [00:01<00:24, 408.87it/s][1,14]<stderr>:#015  7%|▋         | 689/10570 [00:01<00:22, 429.87it/s][1,3]<stderr>:#015  6%|▌         | 624/10570 [00:01<00:24, 399.66it/s][1,7]<stderr>:#015  8%|▊         | 896/10570 [00:02<00:24, 398.72it/s][1,6]<stderr>:#015  5%|▌         | 579/10570 [00:01<00:24, 403.58it/s][1,5]<stderr>:#015  6%|▋         | 681/10570 [00:01<00:23, 425.39it/s][1,1]<stderr>:#015  8%|▊         | 809/10570 [00:02<00:25, 382.22it/s][1,10]<stderr>:#015  8%|▊         | 812/10570 [00:02<00:25, 376.23it/s][1,13]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:25, 388.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 352/10570 [00:00<00:30, 333.16it/s]#033[A[1,9]<stderr>:#015  7%|▋         | 761/10570 [00:01<00:24, 393.51it/s][1,2]<stderr>:#015  8%|▊         | 857/10570 [00:02<00:25, 385.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▋         | 674/10570 [00:01<00:23, 421.40it/s]#033[A[1,4]<stderr>:#015  8%|▊         | 811/10570 [00:02<00:25, 376.62it/s][1,12]<stderr>:#015  9%|▊         | 902/10570 [00:02<00:24, 395.08it/s][1,11]<stderr>:#015  6%|▋         | 665/10570 [00:01<00:24, 411.60it/s][1,15]<stderr>:#015  8%|▊         | 806/10570 [00:02<00:25, 380.11it/s][1,3]<stderr>:#015  6%|▋         | 667/10570 [00:01<00:24, 406.10it/s][1,14]<stderr>:#015  7%|▋         | 733/10570 [00:01<00:23, 416.98it/s][1,7]<stderr>:#015  9%|▉         | 937/10570 [00:02<00:24, 393.31it/s][1,6]<stderr>:#015  6%|▌         | 620/10570 [00:01<00:24, 405.12it/s][1,1]<stderr>:#015  8%|▊         | 848/10570 [00:02<00:25, 381.29it/s][1,5]<stderr>:#015  7%|▋         | 724/10570 [00:01<00:23, 414.92it/s][1,10]<stderr>:#015  8%|▊         | 851/10570 [00:02<00:25, 376.46it/s][1,13]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:25, 386.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▎         | 394/10570 [00:01<00:28, 353.23it/s]#033[A[1,2]<stderr>:#015  9%|▊         | 899/10570 [00:02<00:24, 394.35it/s][1,9]<stderr>:#015  8%|▊         | 801/10570 [00:02<00:25, 375.74it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  7%|▋         | 717/10570 [00:01<00:23, 414.17it/s]#033[A[1,4]<stderr>:#015  8%|▊         | 850/10570 [00:02<00:25, 378.38it/s][1,12]<stderr>:#015  9%|▉         | 942/10570 [00:02<00:24, 389.66it/s][1,11]<stderr>:#015  7%|▋         | 707/10570 [00:01<00:23, 412.27it/s][1,3]<stderr>:#015  7%|▋         | 709/10570 [00:01<00:24, 407.38it/s][1,15]<stderr>:#015  8%|▊         | 845/10570 [00:02<00:25, 376.86it/s][1,7]<stderr>:#015  9%|▉         | 977/10570 [00:02<00:24, 394.67it/s][1,14]<stderr>:#015  7%|▋         | 775/10570 [00:01<00:24, 406.17it/s][1,6]<stderr>:#015  6%|▋         | 662/10570 [00:01<00:24, 408.57it/s][1,1]<stderr>:#015  8%|▊         | 890/10570 [00:02<00:24, 391.43it/s][1,10]<stderr>:#015  8%|▊         | 893/10570 [00:02<00:24, 387.99it/s][1,5]<stderr>:#015  7%|▋         | 766/10570 [00:01<00:24, 406.54it/s][1,13]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:25, 395.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▍         | 433/10570 [00:01<00:28, 361.19it/s]#033[A[1,2]<stderr>:#015  9%|▉         | 939/10570 [00:02<00:24, 387.30it/s][1,9]<stderr>:#015  8%|▊         | 839/10570 [00:02<00:26, 365.17it/s][1,4]<stderr>:#015  8%|▊         | 892/10570 [00:02<00:24, 389.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  7%|▋         | 759/10570 [00:01<00:24, 402.37it/s]#033[A[1,12]<stderr>:#015  9%|▉         | 982/10570 [00:02<00:25, 377.79it/s][1,15]<stderr>:#015  8%|▊         | 886/10570 [00:02<00:25, 385.74it/s][1,11]<stderr>:#015  7%|▋         | 749/10570 [00:01<00:24, 398.94it/s][1,7]<stderr>:#015 10%|▉         | 1017/10570 [00:02<00:24, 392.06it/s][1,3]<stderr>:#015  7%|▋         | 750/10570 [00:01<00:24, 395.28it/s][1,6]<stderr>:#015  7%|▋         | 704/10570 [00:01<00:24, 410.93it/s][1,14]<stderr>:#015  8%|▊         | 816/10570 [00:02<00:25, 381.52it/s][1,1]<stderr>:#015  9%|▉         | 930/10570 [00:02<00:25, 383.10it/s][1,13]<stderr>:#015  6%|▋         | 678/10570 [00:01<00:24, 399.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▍         | 470/10570 [00:01<00:27, 363.50it/s]#033[A[1,10]<stderr>:#015  9%|▉         | 933/10570 [00:02<00:25, 373.95it/s][1,2]<stderr>:#015  9%|▉         | 978/10570 [00:02<00:24, 386.70it/s][1,5]<stderr>:#015  8%|▊         | 807/10570 [00:02<00:25, 380.35it/s][1,9]<stderr>:#015  8%|▊         | 878/10570 [00:02<00:26, 370.46it/s][1,4]<stderr>:#015  9%|▉         | 932/10570 [00:02<00:25, 379.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 800/10570 [00:02<00:25, 382.97it/s]#033[A[1,12]<stderr>:#015 10%|▉         | 1021/10570 [00:02<00:25, 377.36it/s][1,15]<stderr>:#015  9%|▉         | 925/10570 [00:02<00:25, 378.91it/s][1,7]<stderr>:#015 10%|█         | 1057/10570 [00:02<00:24, 391.37it/s][1,11]<stderr>:#015  7%|▋         | 790/10570 [00:02<00:25, 385.04it/s][1,3]<stderr>:#015  7%|▋         | 790/10570 [00:02<00:25, 382.26it/s][1,6]<stderr>:#015  7%|▋         | 746/10570 [00:01<00:24, 398.25it/s][1,14]<stderr>:#015  8%|▊         | 855/10570 [00:02<00:25, 383.78it/s][1,1]<stderr>:#015  9%|▉         | 969/10570 [00:02<00:24, 384.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  5%|▍         | 513/10570 [00:01<00:26, 380.48it/s]#033[A[1,13]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:25, 389.94it/s][1,10]<stderr>:#015  9%|▉         | 972/10570 [00:02<00:26, 368.61it/s][1,2]<stderr>:#015 10%|▉         | 1017/10570 [00:02<00:24, 383.67it/s][1,5]<stderr>:#015  8%|▊         | 846/10570 [00:02<00:25, 376.18it/s][1,9]<stderr>:#015  9%|▊         | 916/10570 [00:02<00:26, 361.69it/s][1,4]<stderr>:#015  9%|▉         | 971/10570 [00:02<00:25, 380.35it/s][1,12]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:24, 381.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 839/10570 [00:02<00:26, 371.72it/s]#033[A[1,7]<stderr>:#015 10%|█         | 1097/10570 [00:02<00:24, 383.17it/s][1,15]<stderr>:#015  9%|▉         | 964/10570 [00:02<00:26, 367.57it/s][1,14]<stderr>:#015  8%|▊         | 897/10570 [00:02<00:24, 392.32it/s][1,11]<stderr>:#015  8%|▊         | 829/10570 [00:02<00:26, 364.15it/s][1,1]<stderr>:#015 10%|▉         | 1008/10570 [00:02<00:25, 382.23it/s][1,3]<stderr>:#015  8%|▊         | 829/10570 [00:02<00:26, 362.86it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:25, 386.93it/s]#033[A[1,6]<stderr>:#015  7%|▋         | 787/10570 [00:02<00:25, 380.63it/s][1,10]<stderr>:#015 10%|▉         | 1010/10570 [00:02<00:25, 371.15it/s][1,2]<stderr>:#015 10%|▉         | 1056/10570 [00:02<00:24, 384.51it/s][1,5]<stderr>:#015  8%|▊         | 888/10570 [00:02<00:25, 386.74it/s][1,13]<stderr>:#015  7%|▋         | 759/10570 [00:02<00:25, 379.28it/s][1,9]<stderr>:#015  9%|▉         | 955/10570 [00:02<00:26, 369.19it/s][1,4]<stderr>:#015 10%|▉         | 1010/10570 [00:02<00:25, 380.80it/s][1,12]<stderr>:#015 10%|█         | 1100/10570 [00:02<00:25, 375.87it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 881/10570 [00:02<00:25, 382.45it/s]#033[A[1,7]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:23, 393.69it/s][1,15]<stderr>:#015  9%|▉         | 1002/10570 [00:02<00:25, 369.86it/s][1,11]<stderr>:#015  8%|▊         | 869/10570 [00:02<00:26, 372.60it/s][1,14]<stderr>:#015  9%|▉         | 937/10570 [00:02<00:25, 385.10it/s][1,3]<stderr>:#015  8%|▊         | 869/10570 [00:02<00:26, 372.18it/s][1,1]<stderr>:#015 10%|▉         | 1047/10570 [00:02<00:24, 381.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:25, 385.45it/s]#033[A[1,10]<stderr>:#015 10%|▉         | 1049/10570 [00:02<00:25, 374.39it/s][1,2]<stderr>:#015 10%|█         | 1095/10570 [00:02<00:24, 380.25it/s][1,6]<stderr>:#015  8%|▊         | 826/10570 [00:02<00:27, 359.03it/s][1,5]<stderr>:#015  9%|▉         | 927/10570 [00:02<00:25, 379.00it/s][1,9]<stderr>:#015  9%|▉         | 993/10570 [00:02<00:26, 368.25it/s][1,13]<stderr>:#015  8%|▊         | 798/10570 [00:02<00:26, 362.38it/s][1,4]<stderr>:#015 10%|▉         | 1049/10570 [00:02<00:25, 380.63it/s][1,12]<stderr>:#015 11%|█         | 1142/10570 [00:02<00:24, 386.88it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▊         | 920/10570 [00:02<00:25, 376.56it/s]#033[A[1,7]<stderr>:#015 11%|█         | 1186/10570 [00:02<00:22, 411.44it/s][1,15]<stderr>:#015 10%|▉         | 1040/10570 [00:02<00:25, 368.86it/s][1,11]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:25, 374.47it/s][1,14]<stderr>:#015  9%|▉         | 976/10570 [00:02<00:24, 384.35it/s][1,1]<stderr>:#015 10%|█         | 1086/10570 [00:02<00:24, 383.17it/s][1,3]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:25, 374.43it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:25, 395.19it/s][1,8]<stderr>:#033[A[1,10]<stderr>:#015 10%|█         | 1089/10570 [00:02<00:24, 380.41it/s][1,2]<stderr>:#015 11%|█         | 1136/10570 [00:02<00:24, 387.37it/s][1,6]<stderr>:#015  8%|▊         | 865/10570 [00:02<00:26, 367.14it/s][1,5]<stderr>:#015  9%|▉         | 966/10570 [00:02<00:25, 381.92it/s][1,9]<stderr>:#015 10%|▉         | 1031/10570 [00:02<00:25, 370.44it/s][1,4]<stderr>:#015 10%|█         | 1088/10570 [00:02<00:24, 383.35it/s][1,13]<stderr>:#015  8%|▊         | 835/10570 [00:02<00:27, 349.17it/s][1,12]<stderr>:#015 11%|█         | 1188/10570 [00:02<00:23, 404.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▉         | 960/10570 [00:02<00:25, 380.76it/s]#033[A[1,7]<stderr>:#015 12%|█▏        | 1230/10570 [00:03<00:22, 418.08it/s][1,15]<stderr>:#015 10%|█         | 1080/10570 [00:02<00:25, 375.11it/s][1,11]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:25, 373.19it/s][1,14]<stderr>:#015 10%|▉         | 1015/10570 [00:02<00:25, 381.57it/s][1,3]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:25, 374.12it/s][1,1]<stderr>:#015 11%|█         | 1125/10570 [00:02<00:24, 378.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▋         | 678/10570 [00:01<00:24, 399.24it/s]#033[A[1,10]<stderr>:#015 11%|█         | 1128/10570 [00:02<00:24, 378.87it/s][1,2]<stderr>:#015 11%|█         | 1181/10570 [00:02<00:23, 404.23it/s][1,6]<stderr>:#015  9%|▊         | 905/10570 [00:02<00:25, 373.38it/s][1,5]<stderr>:#015 10%|▉         | 1005/10570 [00:02<00:25, 378.91it/s][1,9]<stderr>:#015 10%|█         | 1069/10570 [00:02<00:25, 371.22it/s][1,4]<stderr>:#015 11%|█         | 1127/10570 [00:02<00:24, 379.02it/s][1,13]<stderr>:#015  8%|▊         | 873/10570 [00:02<00:27, 357.78it/s][1,12]<stderr>:#015 12%|█▏        | 1232/10570 [00:03<00:22, 413.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▉         | 999/10570 [00:02<00:25, 377.23it/s]#033[A[1,7]<stderr>:#015 12%|█▏        | 1273/10570 [00:03<00:22, 417.91it/s][1,15]<stderr>:#015 11%|█         | 1118/10570 [00:02<00:25, 371.99it/s][1,11]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:25, 370.60it/s][1,14]<stderr>:#015 10%|▉         | 1054/10570 [00:02<00:24, 381.33it/s][1,3]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:25, 372.28it/s][1,1]<stderr>:#015 11%|█         | 1169/10570 [00:02<00:23, 395.14it/s][1,10]<stderr>:#015 11%|█         | 1173/10570 [00:02<00:23, 396.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:25, 389.65it/s]#033[A[1,2]<stderr>:#015 12%|█▏        | 1224/10570 [00:03<00:22, 411.21it/s][1,6]<stderr>:#015  9%|▉         | 943/10570 [00:02<00:26, 369.63it/s][1,5]<stderr>:#015 10%|▉         | 1044/10570 [00:02<00:25, 376.95it/s][1,9]<stderr>:#015 10%|█         | 1107/10570 [00:02<00:25, 368.04it/s][1,4]<stderr>:#015 11%|█         | 1172/10570 [00:02<00:23, 395.87it/s][1,13]<stderr>:#015  9%|▊         | 910/10570 [00:02<00:27, 357.10it/s][1,12]<stderr>:#015 12%|█▏        | 1274/10570 [00:03<00:22, 412.70it/s][1,7]<stderr>:#015 12%|█▏        | 1319/10570 [00:03<00:21, 429.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|▉         | 1037/10570 [00:02<00:25, 372.92it/s]#033[A[1,15]<stderr>:#015 11%|█         | 1161/10570 [00:02<00:24, 385.66it/s][1,14]<stderr>:#015 10%|█         | 1093/10570 [00:02<00:24, 383.37it/s][1,1]<stderr>:#015 11%|█▏        | 1212/10570 [00:03<00:23, 404.63it/s][1,11]<stderr>:#015 10%|▉         | 1022/10570 [00:02<00:26, 367.07it/s][1,3]<stderr>:#015 10%|▉         | 1022/10570 [00:02<00:25, 369.44it/s][1,10]<stderr>:#015 12%|█▏        | 1216/10570 [00:03<00:23, 403.82it/s][1,2]<stderr>:#015 12%|█▏        | 1266/10570 [00:03<00:22, 410.45it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  7%|▋         | 759/10570 [00:02<00:26, 373.36it/s]#033[A[1,6]<stderr>:#015  9%|▉         | 981/10570 [00:02<00:26, 368.52it/s][1,5]<stderr>:#015 10%|█         | 1084/10570 [00:02<00:24, 381.21it/s][1,9]<stderr>:#015 11%|█         | 1148/10570 [00:02<00:24, 378.59it/s][1,4]<stderr>:#015 11%|█▏        | 1215/10570 [00:03<00:23, 404.66it/s][1,13]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:27, 356.02it/s][1,12]<stderr>:#015 12%|█▏        | 1320/10570 [00:03<00:21, 424.55it/s][1,7]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:21, 429.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|█         | 1076/10570 [00:02<00:25, 377.46it/s]#033[A[1,15]<stderr>:#015 11%|█▏        | 1205/10570 [00:03<00:23, 398.30it/s][1,1]<stderr>:#015 12%|█▏        | 1255/10570 [00:03<00:22, 409.11it/s][1,14]<stderr>:#015 11%|█         | 1132/10570 [00:02<00:24, 379.12it/s][1,11]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:25, 371.17it/s][1,3]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:25, 372.68it/s][1,10]<stderr>:#015 12%|█▏        | 1257/10570 [00:03<00:22, 405.64it/s][1,2]<stderr>:#015 12%|█▏        | 1311/10570 [00:03<00:21, 421.44it/s][1,6]<stderr>:#015 10%|▉         | 1018/10570 [00:02<00:26, 365.79it/s][1,5]<stderr>:#015 11%|█         | 1123/10570 [00:02<00:25, 375.40it/s][1,9]<stderr>:#015 11%|█▏        | 1192/10570 [00:03<00:23, 394.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 797/10570 [00:02<00:27, 356.95it/s]#033[A[1,4]<stderr>:#015 12%|█▏        | 1257/10570 [00:03<00:22, 406.59it/s][1,13]<stderr>:#015  9%|▉         | 982/10570 [00:02<00:26, 355.56it/s][1,12]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:21, 424.47it/s][1,15]<stderr>:#015 12%|█▏        | 1250/10570 [00:03<00:22, 408.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█         | 1114/10570 [00:02<00:25, 367.88it/s]#033[A[1,7]<stderr>:#015 13%|█▎        | 1407/10570 [00:03<00:22, 415.01it/s][1,1]<stderr>:#015 12%|█▏        | 1299/10570 [00:03<00:22, 416.49it/s][1,14]<stderr>:#015 11%|█         | 1177/10570 [00:02<00:23, 396.01it/s][1,11]<stderr>:#015 10%|█         | 1099/10570 [00:02<00:26, 363.35it/s][1,3]<stderr>:#015 10%|█         | 1099/10570 [00:02<00:26, 363.65it/s][1,10]<stderr>:#015 12%|█▏        | 1302/10570 [00:03<00:22, 417.61it/s][1,2]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:21, 423.90it/s][1,6]<stderr>:#015 10%|▉         | 1055/10570 [00:02<00:25, 366.36it/s][1,5]<stderr>:#015 11%|█         | 1167/10570 [00:02<00:24, 391.72it/s][1,9]<stderr>:#015 12%|█▏        | 1235/10570 [00:03<00:23, 403.12it/s][1,4]<stderr>:#015 12%|█▏        | 1302/10570 [00:03<00:22, 417.51it/s][1,13]<stderr>:#015 10%|▉         | 1018/10570 [00:02<00:27, 351.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 834/10570 [00:02<00:28, 339.01it/s]#033[A[1,12]<stderr>:#015 13%|█▎        | 1406/10570 [00:03<00:22, 409.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█         | 1156/10570 [00:02<00:24, 381.73it/s]#033[A[1,15]<stderr>:#015 12%|█▏        | 1292/10570 [00:03<00:22, 408.64it/s][1,7]<stderr>:#015 14%|█▎        | 1451/10570 [00:03<00:21, 421.56it/s][1,1]<stderr>:#015 13%|█▎        | 1343/10570 [00:03<00:21, 422.35it/s][1,14]<stderr>:#015 12%|█▏        | 1219/10570 [00:03<00:23, 402.61it/s][1,11]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:25, 374.31it/s][1,10]<stderr>:#015 13%|█▎        | 1346/10570 [00:03<00:21, 422.54it/s][1,3]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:25, 373.99it/s][1,2]<stderr>:#015 13%|█▎        | 1397/10570 [00:03<00:22, 414.10it/s][1,6]<stderr>:#015 10%|█         | 1093/10570 [00:02<00:25, 369.00it/s][1,5]<stderr>:#015 11%|█▏        | 1210/10570 [00:03<00:23, 401.97it/s][1,9]<stderr>:#015 12%|█▏        | 1276/10570 [00:03<00:23, 401.68it/s][1,4]<stderr>:#015 13%|█▎        | 1346/10570 [00:03<00:21, 421.45it/s][1,13]<stderr>:#015 10%|▉         | 1054/10570 [00:02<00:27, 352.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 872/10570 [00:02<00:27, 349.16it/s]#033[A[1,12]<stderr>:#015 14%|█▎        | 1450/10570 [00:03<00:21, 416.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█▏        | 1199/10570 [00:03<00:23, 394.15it/s]#033[A[1,15]<stderr>:#015 13%|█▎        | 1336/10570 [00:03<00:22, 417.13it/s][1,7]<stderr>:#015 14%|█▍        | 1494/10570 [00:03<00:21, 423.52it/s][1,14]<stderr>:#015 12%|█▏        | 1260/10570 [00:03<00:23, 403.38it/s][1,1]<stderr>:#015 13%|█▎        | 1386/10570 [00:03<00:22, 414.05it/s][1,11]<stderr>:#015 11%|█         | 1185/10570 [00:03<00:23, 391.82it/s][1,3]<stderr>:#015 11%|█         | 1185/10570 [00:03<00:23, 391.45it/s][1,10]<stderr>:#015 13%|█▎        | 1389/10570 [00:03<00:22, 415.19it/s][1,2]<stderr>:#015 14%|█▎        | 1439/10570 [00:03<00:22, 406.10it/s][1,6]<stderr>:#015 11%|█         | 1130/10570 [00:02<00:25, 365.40it/s][1,5]<stderr>:#015 12%|█▏        | 1253/10570 [00:03<00:22, 408.23it/s][1,9]<stderr>:#015 12%|█▏        | 1321/10570 [00:03<00:22, 412.67it/s][1,4]<stderr>:#015 13%|█▎        | 1389/10570 [00:03<00:22, 413.54it/s][1,13]<stderr>:#015 10%|█         | 1091/10570 [00:02<00:26, 355.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:27, 351.62it/s][1,8]<stderr>:#033[A[1,12]<stderr>:#015 14%|█▍        | 1492/10570 [00:03<00:21, 417.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 12%|█▏        | 1243/10570 [00:03<00:22, 406.12it/s]#033[A[1,7]<stderr>:#015 15%|█▍        | 1538/10570 [00:03<00:21, 427.39it/s][1,15]<stderr>:#015 13%|█▎        | 1378/10570 [00:03<00:22, 410.97it/s][1,14]<stderr>:#015 12%|█▏        | 1305/10570 [00:03<00:22, 415.87it/s][1,11]<stderr>:#015 12%|█▏        | 1227/10570 [00:03<00:23, 397.88it/s][1,3]<stderr>:#015 12%|█▏        | 1227/10570 [00:03<00:23, 398.13it/s][1,1]<stderr>:#015 14%|█▎        | 1428/10570 [00:03<00:23, 394.65it/s][1,10]<stderr>:#015 14%|█▎        | 1431/10570 [00:03<00:22, 409.00it/s][1,2]<stderr>:#015 14%|█▍        | 1482/10570 [00:03<00:22, 411.65it/s][1,6]<stderr>:#015 11%|█         | 1173/10570 [00:03<00:24, 382.04it/s][1,5]<stderr>:#015 12%|█▏        | 1296/10570 [00:03<00:22, 411.98it/s][1,9]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:22, 411.51it/s][1,4]<stderr>:#015 14%|█▎        | 1431/10570 [00:03<00:22, 404.58it/s][1,13]<stderr>:#015 11%|█         | 1127/10570 [00:03<00:26, 351.61it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▉         | 944/10570 [00:02<00:27, 351.23it/s]#033[A[1,12]<stderr>:#015 15%|█▍        | 1536/10570 [00:03<00:21, 422.87it/s][1,7]<stderr>:#015 15%|█▍        | 1582/10570 [00:03<00:20, 430.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 12%|█▏        | 1284/10570 [00:03<00:23, 403.09it/s]#033[A[1,15]<stderr>:#015 13%|█▎        | 1420/10570 [00:03<00:22, 407.16it/s][1,14]<stderr>:#015 13%|█▎        | 1348/10570 [00:03<00:22, 418.40it/s][1,11]<stderr>:#015 12%|█▏        | 1268/10570 [00:03<00:23, 395.99it/s][1,3]<stderr>:#015 12%|█▏        | 1268/10570 [00:03<00:23, 397.01it/s][1,1]<stderr>:#015 14%|█▍        | 1470/10570 [00:03<00:22, 400.44it/s][1,10]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:21, 413.64it/s][1,2]<stderr>:#015 14%|█▍        | 1526/10570 [00:03<00:21, 417.29it/s][1,6]<stderr>:#015 11%|█▏        | 1215/10570 [00:03<00:23, 390.17it/s][1,5]<stderr>:#015 13%|█▎        | 1340/10570 [00:03<00:22, 419.08it/s][1,9]<stderr>:#015 13%|█▎        | 1405/10570 [00:03<00:23, 396.11it/s][1,4]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:22, 409.87it/s][1,13]<stderr>:#015 11%|█         | 1168/10570 [00:03<00:25, 365.75it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▉         | 980/10570 [00:02<00:27, 351.87it/s]#033[A[1,12]<stderr>:#015 15%|█▍        | 1580/10570 [00:03<00:21, 426.60it/s][1,7]<stderr>:#015 15%|█▌        | 1626/10570 [00:03<00:20, 432.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1328/10570 [00:03<00:22, 412.24it/s]#033[A[1,15]<stderr>:#015 14%|█▍        | 1462/10570 [00:03<00:22, 408.62it/s][1,14]<stderr>:#015 13%|█▎        | 1390/10570 [00:03<00:22, 410.40it/s][1,11]<stderr>:#015 12%|█▏        | 1312/10570 [00:03<00:22, 406.95it/s][1,3]<stderr>:#015 12%|█▏        | 1312/10570 [00:03<00:22, 407.37it/s][1,10]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 417.81it/s][1,1]<stderr>:#015 14%|█▍        | 1514/10570 [00:03<00:22, 408.80it/s][1,2]<stderr>:#015 15%|█▍        | 1571/10570 [00:03<00:21, 424.42it/s][1,6]<stderr>:#015 12%|█▏        | 1256/10570 [00:03<00:23, 392.95it/s][1,5]<stderr>:#015 13%|█▎        | 1383/10570 [00:03<00:22, 411.14it/s][1,9]<stderr>:#015 14%|█▎        | 1448/10570 [00:03<00:22, 404.01it/s][1,4]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 414.89it/s][1,13]<stderr>:#015 11%|█▏        | 1209/10570 [00:03<00:24, 375.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 15%|█▌        | 1624/10570 [00:03<00:20, 428.97it/s][1,8]<stderr>:#015 10%|▉         | 1016/10570 [00:02<00:27, 349.40it/s]#033[A[1,7]<stderr>:#015 16%|█▌        | 1674/10570 [00:04<00:19, 445.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1370/10570 [00:03<00:22, 411.45it/s]#033[A[1,15]<stderr>:#015 14%|█▍        | 1505/10570 [00:03<00:21, 413.58it/s][1,14]<stderr>:#015 14%|█▎        | 1432/10570 [00:03<00:22, 405.18it/s][1,11]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:22, 409.47it/s][1,3]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:22, 409.03it/s][1,1]<stderr>:#015 15%|█▍        | 1558/10570 [00:03<00:21, 416.43it/s][1,10]<stderr>:#015 15%|█▍        | 1562/10570 [00:03<00:21, 424.81it/s][1,2]<stderr>:#015 15%|█▌        | 1616/10570 [00:03<00:20, 428.48it/s][1,6]<stderr>:#015 12%|█▏        | 1299/10570 [00:03<00:23, 402.35it/s][1,5]<stderr>:#015 13%|█▎        | 1425/10570 [00:03<00:22, 404.81it/s][1,4]<stderr>:#015 15%|█▍        | 1561/10570 [00:03<00:21, 421.71it/s][1,9]<stderr>:#015 14%|█▍        | 1489/10570 [00:03<00:22, 402.17it/s][1,13]<stderr>:#015 12%|█▏        | 1250/10570 [00:03<00:24, 384.87it/s][1,12]<stderr>:#015 16%|█▌        | 1671/10570 [00:04<00:20, 440.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 10%|▉         | 1052/10570 [00:02<00:27, 349.66it/s]#033[A[1,7]<stderr>:#015 16%|█▋        | 1719/10570 [00:04<00:19, 445.42it/s][1,15]<stderr>:#015 15%|█▍        | 1548/10570 [00:03<00:21, 418.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1412/10570 [00:03<00:22, 400.52it/s]#033[A[1,14]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:22, 409.24it/s][1,11]<stderr>:#015 13%|█▎        | 1396/10570 [00:03<00:22, 400.59it/s][1,10]<stderr>:#015 15%|█▌        | 1606/10570 [00:03<00:20, 428.46it/s][1,1]<stderr>:#015 15%|█▌        | 1602/10570 [00:03<00:21, 421.30it/s][1,3]<stderr>:#015 13%|█▎        | 1396/10570 [00:03<00:22, 400.01it/s][1,2]<stderr>:#015 16%|█▌        | 1661/10570 [00:04<00:20, 432.61it/s][1,6]<stderr>:#015 13%|█▎        | 1342/10570 [00:03<00:22, 408.32it/s][1,5]<stderr>:#015 14%|█▍        | 1467/10570 [00:03<00:22, 406.67it/s][1,4]<stderr>:#015 15%|█▌        | 1605/10570 [00:03<00:21, 425.61it/s][1,9]<stderr>:#015 14%|█▍        | 1532/10570 [00:03<00:22, 409.14it/s][1,13]<stderr>:#015 12%|█▏        | 1289/10570 [00:03<00:24, 383.76it/s][1,12]<stderr>:#015 16%|█▌        | 1716/10570 [00:04<00:20, 440.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 10%|█         | 1088/10570 [00:02<00:26, 352.46it/s]#033[A[1,7]<stderr>:#015 17%|█▋        | 1764/10570 [00:04<00:20, 436.67it/s][1,15]<stderr>:#015 15%|█▌        | 1591/10570 [00:03<00:21, 419.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 14%|█▍        | 1454/10570 [00:03<00:22, 404.91it/s]#033[A[1,14]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 413.18it/s][1,10]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:20, 428.52it/s][1,1]<stderr>:#015 16%|█▌        | 1645/10570 [00:04<00:21, 423.06it/s][1,11]<stderr>:#015 14%|█▎        | 1437/10570 [00:03<00:22, 398.34it/s][1,3]<stderr>:#015 14%|█▎        | 1437/10570 [00:03<00:22, 397.70it/s][1,2]<stderr>:#015 16%|█▌        | 1707/10570 [00:04<00:20, 438.63it/s][1,6]<stderr>:#015 13%|█▎        | 1383/10570 [00:03<00:22, 400.10it/s][1,5]<stderr>:#015 14%|█▍        | 1511/10570 [00:03<00:21, 413.89it/s][1,4]<stderr>:#015 16%|█▌        | 1648/10570 [00:04<00:20, 426.56it/s][1,9]<stderr>:#015 15%|█▍        | 1575/10570 [00:04<00:21, 413.95it/s][1,13]<stderr>:#015 13%|█▎        | 1331/10570 [00:03<00:23, 391.91it/s][1,12]<stderr>:#015 17%|█▋        | 1761/10570 [00:04<00:20, 433.78it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█         | 1124/10570 [00:03<00:27, 342.60it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 17%|█▋        | 1809/10570 [00:04<00:19, 438.97it/s][1,15]<stderr>:#015 15%|█▌        | 1634/10570 [00:04<00:21, 420.48it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 14%|█▍        | 1496/10570 [00:03<00:22, 407.87it/s]#033[A[1,14]<stderr>:#015 15%|█▍        | 1561/10570 [00:03<00:21, 419.34it/s][1,10]<stderr>:#015 16%|█▌        | 1696/10570 [00:04<00:20, 439.38it/s][1,1]<stderr>:#015 16%|█▌        | 1692/10570 [00:04<00:20, 435.66it/s][1,11]<stderr>:#015 14%|█▍        | 1479/10570 [00:03<00:22, 402.45it/s][1,3]<stderr>:#015 14%|█▍        | 1479/10570 [00:03<00:22, 401.62it/s][1,2]<stderr>:#015 17%|█▋        | 1751/10570 [00:04<00:20, 433.74it/s][1,6]<stderr>:#015 13%|█▎        | 1424/10570 [00:03<00:23, 393.14it/s][1,5]<stderr>:#015 15%|█▍        | 1554/10570 [00:03<00:21, 416.28it/s][1,4]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 438.16it/s][1,9]<stderr>:#015 15%|█▌        | 1618/10570 [00:04<00:21, 417.47it/s][1,13]<stderr>:#015 13%|█▎        | 1371/10570 [00:03<00:23, 389.51it/s][1,12]<stderr>:#015 17%|█▋        | 1806/10570 [00:04<00:20, 436.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█         | 1165/10570 [00:03<00:26, 358.81it/s]#033[A[1,7]<stderr>:#015 18%|█▊        | 1853/10570 [00:04<00:20, 431.49it/s][1,15]<stderr>:#015 16%|█▌        | 1683/10570 [00:04<00:20, 437.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▍        | 1538/10570 [00:03<00:22, 410.25it/s]#033[A[1,14]<stderr>:#015 15%|█▌        | 1605/10570 [00:03<00:21, 423.13it/s][1,11]<stderr>:#015 14%|█▍        | 1521/10570 [00:03<00:22, 407.32it/s][1,1]<stderr>:#015 16%|█▋        | 1736/10570 [00:04<00:20, 427.83it/s][1,10]<stderr>:#015 16%|█▋        | 1741/10570 [00:04<00:20, 430.69it/s][1,3]<stderr>:#015 14%|█▍        | 1521/10570 [00:03<00:22, 406.26it/s][1,2]<stderr>:#015 17%|█▋        | 1796/10570 [00:04<00:20, 435.46it/s][1,6]<stderr>:#015 14%|█▍        | 1465/10570 [00:03<00:23, 395.42it/s][1,5]<stderr>:#015 15%|█▌        | 1597/10570 [00:03<00:21, 419.87it/s][1,9]<stderr>:#015 16%|█▌        | 1662/10570 [00:04<00:21, 422.96it/s][1,4]<stderr>:#015 16%|█▋        | 1739/10570 [00:04<00:20, 429.19it/s][1,13]<stderr>:#015 13%|█▎        | 1411/10570 [00:03<00:24, 378.28it/s][1,12]<stderr>:#015 18%|█▊        | 1850/10570 [00:04<00:20, 431.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█▏        | 1206/10570 [00:03<00:25, 371.14it/s]#033[A[1,7]<stderr>:#015 18%|█▊        | 1901/10570 [00:04<00:19, 443.90it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▍        | 1581/10570 [00:03<00:21, 413.80it/s]#033[A[1,15]<stderr>:#015 16%|█▋        | 1727/10570 [00:04<00:20, 430.78it/s][1,14]<stderr>:#015 16%|█▌        | 1648/10570 [00:04<00:21, 423.26it/s][1,11]<stderr>:#015 15%|█▍        | 1564/10570 [00:04<00:21, 413.19it/s][1,1]<stderr>:#015 17%|█▋        | 1780/10570 [00:04<00:20, 428.97it/s][1,10]<stderr>:#015 17%|█▋        | 1786/10570 [00:04<00:20, 434.35it/s][1,3]<stderr>:#015 15%|█▍        | 1564/10570 [00:04<00:21, 411.76it/s][1,2]<stderr>:#015 17%|█▋        | 1840/10570 [00:04<00:20, 428.81it/s][1,6]<stderr>:#015 14%|█▍        | 1508/10570 [00:03<00:22, 403.45it/s][1,5]<stderr>:#015 16%|█▌        | 1640/10570 [00:04<00:21, 421.95it/s][1,9]<stderr>:#015 16%|█▌        | 1707/10570 [00:04<00:20, 427.58it/s][1,4]<stderr>:#015 17%|█▋        | 1783/10570 [00:04<00:20, 430.59it/s][1,13]<stderr>:#015 14%|█▎        | 1451/10570 [00:03<00:23, 382.58it/s][1,12]<stderr>:#015 18%|█▊        | 1896/10570 [00:04<00:19, 439.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 12%|█▏        | 1248/10570 [00:03<00:24, 384.13it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 18%|█▊        | 1946/10570 [00:04<00:19, 443.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▌        | 1623/10570 [00:04<00:21, 415.51it/s]#033[A[1,15]<stderr>:#015 17%|█▋        | 1771/10570 [00:04<00:20, 424.00it/s][1,14]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 434.48it/s][1,11]<stderr>:#015 15%|█▌        | 1607/10570 [00:04<00:21, 417.36it/s][1,1]<stderr>:#015 17%|█▋        | 1823/10570 [00:04<00:20, 428.30it/s][1,3]<stderr>:#015 15%|█▌        | 1607/10570 [00:04<00:21, 415.06it/s][1,10]<stderr>:#015 17%|█▋        | 1830/10570 [00:04<00:20, 431.32it/s][1,2]<stderr>:#015 18%|█▊        | 1885/10570 [00:04<00:20, 434.13it/s][1,6]<stderr>:#015 15%|█▍        | 1550/10570 [00:04<00:22, 405.01it/s][1,5]<stderr>:#015 16%|█▌        | 1688/10570 [00:04<00:20, 436.44it/s][1,9]<stderr>:#015 17%|█▋        | 1750/10570 [00:04<00:20, 422.15it/s][1,4]<stderr>:#015 17%|█▋        | 1827/10570 [00:04<00:20, 429.57it/s][1,12]<stderr>:#015 18%|█▊        | 1941/10570 [00:04<00:19, 441.94it/s][1,13]<stderr>:#015 14%|█▍        | 1490/10570 [00:04<00:23, 382.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 12%|█▏        | 1287/10570 [00:03<00:24, 379.32it/s]#033[A[1,7]<stderr>:#015 19%|█▉        | 1993/10570 [00:04<00:19, 449.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 16%|█▌        | 1668/10570 [00:04<00:20, 424.58it/s]#033[A[1,15]<stderr>:#015 17%|█▋        | 1815/10570 [00:04<00:20, 428.06it/s][1,14]<stderr>:#015 16%|█▋        | 1739/10570 [00:04<00:20, 425.87it/s][1,11]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:21, 416.71it/s][1,1]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:20, 426.22it/s][1,3]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:21, 414.08it/s][1,10]<stderr>:#015 18%|█▊        | 1874/10570 [00:04<00:20, 430.91it/s][1,2]<stderr>:#015 18%|█▊        | 1932/10570 [00:04<00:19, 443.79it/s][1,6]<stderr>:#015 15%|█▌        | 1592/10570 [00:04<00:21, 408.90it/s][1,5]<stderr>:#015 16%|█▋        | 1732/10570 [00:04<00:20, 429.96it/s][1,9]<stderr>:#015 17%|█▋        | 1793/10570 [00:04<00:20, 422.90it/s][1,4]<stderr>:#015 18%|█▊        | 1871/10570 [00:04<00:20, 427.63it/s][1,12]<stderr>:#015 19%|█▉        | 1988/10570 [00:04<00:19, 449.44it/s][1,13]<stderr>:#015 14%|█▍        | 1530/10570 [00:04<00:23, 386.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1329/10570 [00:03<00:23, 388.21it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 16%|█▌        | 1711/10570 [00:04<00:20, 425.91it/s]#033[A[1,7]<stderr>:#015 19%|█▉        | 2039/10570 [00:04<00:19, 443.55it/s][1,15]<stderr>:#015 18%|█▊        | 1858/10570 [00:04<00:20, 420.95it/s][1,14]<stderr>:#015 17%|█▋        | 1783/10570 [00:04<00:20, 428.20it/s][1,11]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 427.86it/s][1,1]<stderr>:#015 18%|█▊        | 1913/10570 [00:04<00:19, 438.16it/s][1,3]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 425.50it/s][1,10]<stderr>:#015 18%|█▊        | 1921/10570 [00:04<00:19, 441.34it/s][1,2]<stderr>:#015 19%|█▊        | 1978/10570 [00:04<00:19, 448.35it/s][1,6]<stderr>:#015 15%|█▌        | 1633/10570 [00:04<00:21, 408.55it/s][1,5]<stderr>:#015 17%|█▋        | 1776/10570 [00:04<00:20, 427.36it/s][1,4]<stderr>:#015 18%|█▊        | 1919/10570 [00:04<00:19, 440.22it/s][1,9]<stderr>:#015 17%|█▋        | 1836/10570 [00:04<00:20, 416.78it/s][1,13]<stderr>:#015 15%|█▍        | 1569/10570 [00:04<00:23, 384.02it/s][1,12]<stderr>:#015 19%|█▉        | 2034/10570 [00:04<00:19, 440.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1369/10570 [00:03<00:23, 388.46it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1754/10570 [00:04<00:20, 420.95it/s]#033[A[1,7]<stderr>:#015 20%|█▉        | 2087/10570 [00:05<00:18, 451.99it/s][1,15]<stderr>:#015 18%|█▊        | 1906/10570 [00:04<00:19, 435.11it/s][1,14]<stderr>:#015 17%|█▋        | 1826/10570 [00:04<00:20, 427.24it/s][1,1]<stderr>:#015 19%|█▊        | 1958/10570 [00:04<00:19, 440.45it/s][1,11]<stderr>:#015 16%|█▋        | 1738/10570 [00:04<00:21, 418.58it/s][1,10]<stderr>:#015 19%|█▊        | 1966/10570 [00:04<00:19, 443.74it/s][1,3]<stderr>:#015 16%|█▋        | 1738/10570 [00:04<00:21, 417.62it/s][1,2]<stderr>:#015 19%|█▉        | 2023/10570 [00:04<00:19, 441.15it/s][1,6]<stderr>:#015 16%|█▌        | 1681/10570 [00:04<00:20, 425.26it/s][1,5]<stderr>:#015 17%|█▋        | 1819/10570 [00:04<00:20, 427.17it/s][1,9]<stderr>:#015 18%|█▊        | 1879/10570 [00:04<00:20, 419.73it/s][1,4]<stderr>:#015 19%|█▊        | 1964/10570 [00:04<00:19, 440.98it/s][1,13]<stderr>:#015 15%|█▌        | 1611/10570 [00:04<00:22, 392.77it/s][1,12]<stderr>:#015 20%|█▉        | 2080/10570 [00:05<00:19, 443.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1798/10570 [00:04<00:20, 426.08it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1408/10570 [00:03<00:24, 372.48it/s]#033[A[1,15]<stderr>:#015 18%|█▊        | 1950/10570 [00:04<00:19, 436.03it/s][1,14]<stderr>:#015 18%|█▊        | 1869/10570 [00:04<00:20, 425.90it/s][1,1]<stderr>:#015 19%|█▉        | 2003/10570 [00:04<00:19, 439.20it/s][1,11]<stderr>:#015 17%|█▋        | 1781/10570 [00:04<00:20, 419.86it/s][1,7]<stderr>:#015 20%|██        | 2133/10570 [00:05<00:20, 409.87it/s][1,3]<stderr>:#015 17%|█▋        | 1781/10570 [00:04<00:20, 419.67it/s][1,10]<stderr>:#015 19%|█▉        | 2011/10570 [00:04<00:19, 434.35it/s][1,2]<stderr>:#015 20%|█▉        | 2069/10570 [00:05<00:19, 445.02it/s][1,6]<stderr>:#015 16%|█▋        | 1724/10570 [00:04<00:21, 419.94it/s][1,5]<stderr>:#015 18%|█▊        | 1862/10570 [00:04<00:20, 423.23it/s][1,9]<stderr>:#015 18%|█▊        | 1926/10570 [00:04<00:19, 432.40it/s][1,4]<stderr>:#015 19%|█▉        | 2009/10570 [00:04<00:19, 437.70it/s][1,13]<stderr>:#015 16%|█▌        | 1651/10570 [00:04<00:22, 389.49it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1841/10570 [00:04<00:20, 418.48it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▎        | 1448/10570 [00:03<00:24, 379.57it/s]#033[A[1,15]<stderr>:#015 19%|█▉        | 1995/10570 [00:04<00:19, 438.51it/s][1,12]<stderr>:#015 20%|██        | 2125/10570 [00:05<00:20, 408.35it/s][1,14]<stderr>:#015 18%|█▊        | 1917/10570 [00:04<00:19, 438.83it/s][1,1]<stderr>:#015 19%|█▉        | 2047/10570 [00:05<00:19, 437.37it/s][1,11]<stderr>:#015 17%|█▋        | 1824/10570 [00:04<00:20, 418.66it/s][1,7]<stderr>:#015 21%|██        | 2175/10570 [00:05<00:20, 410.83it/s][1,10]<stderr>:#015 19%|█▉        | 2056/10570 [00:05<00:19, 436.77it/s][1,3]<stderr>:#015 17%|█▋        | 1824/10570 [00:04<00:20, 418.11it/s][1,2]<stderr>:#015 20%|██        | 2115/10570 [00:05<00:18, 449.33it/s][1,6]<stderr>:#015 17%|█▋        | 1767/10570 [00:04<00:21, 414.90it/s][1,5]<stderr>:#015 18%|█▊        | 1910/10570 [00:04<00:19, 436.91it/s][1,9]<stderr>:#015 19%|█▊        | 1970/10570 [00:04<00:19, 433.88it/s][1,4]<stderr>:#015 19%|█▉        | 2054/10570 [00:05<00:19, 439.54it/s][1,13]<stderr>:#015 16%|█▌        | 1694/10570 [00:04<00:22, 400.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 18%|█▊        | 1885/10570 [00:04<00:20, 423.21it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▍        | 1487/10570 [00:04<00:24, 372.92it/s]#033[A[1,15]<stderr>:#015 19%|█▉        | 2039/10570 [00:05<00:19, 429.84it/s][1,14]<stderr>:#015 19%|█▊        | 1962/10570 [00:04<00:19, 437.48it/s][1,12]<stderr>:#015 21%|██        | 2167/10570 [00:05<00:21, 398.77it/s][1,7]<stderr>:#015 21%|██        | 2220/10570 [00:05<00:19, 420.67it/s][1,1]<stderr>:#015 20%|█▉        | 2091/10570 [00:05<00:19, 431.81it/s][1,11]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:21, 413.51it/s][1,10]<stderr>:#015 20%|█▉        | 2101/10570 [00:05<00:19, 439.29it/s][1,3]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:21, 413.70it/s][1,5]<stderr>:#015 18%|█▊        | 1954/10570 [00:04<00:19, 435.64it/s][1,4]<stderr>:#015 20%|█▉        | 2100/10570 [00:05<00:19, 442.34it/s][1,9]<stderr>:#015 19%|█▉        | 2014/10570 [00:05<00:20, 425.01it/s][1,2]<stderr>:#015 20%|██        | 2160/10570 [00:05<00:21, 399.74it/s][1,13]<stderr>:#015 16%|█▋        | 1735/10570 [00:04<00:22, 390.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 18%|█▊        | 1931/10570 [00:04<00:20, 431.30it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▍        | 1527/10570 [00:04<00:23, 380.28it/s]#033[A[1,15]<stderr>:#015 20%|█▉        | 2084/10570 [00:05<00:19, 433.45it/s][1,6]<stderr>:#015 17%|█▋        | 1809/10570 [00:04<00:24, 354.16it/s][1,14]<stderr>:#015 19%|█▉        | 2006/10570 [00:04<00:19, 436.00it/s][1,12]<stderr>:#015 21%|██        | 2210/10570 [00:05<00:20, 406.41it/s][1,11]<stderr>:#015 18%|█▊        | 1912/10570 [00:04<00:20, 426.39it/s][1,7]<stderr>:#015 21%|██▏       | 2263/10570 [00:05<00:19, 416.17it/s][1,3]<stderr>:#015 18%|█▊        | 1912/10570 [00:04<00:20, 425.05it/s][1,5]<stderr>:#015 19%|█▉        | 1998/10570 [00:04<00:19, 433.98it/s][1,10]<stderr>:#015 20%|██        | 2145/10570 [00:05<00:21, 397.22it/s][1,1]<stderr>:#015 20%|██        | 2135/10570 [00:05<00:21, 385.69it/s][1,9]<stderr>:#015 19%|█▉        | 2058/10570 [00:05<00:19, 426.59it/s][1,2]<stderr>:#015 21%|██        | 2202/10570 [00:05<00:21, 391.93it/s][1,13]<stderr>:#015 17%|█▋        | 1775/10570 [00:04<00:22, 390.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 19%|█▊        | 1976/10570 [00:04<00:19, 434.13it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 15%|█▍        | 1568/10570 [00:04<00:23, 388.20it/s]#033[A[1,4]<stderr>:#015 20%|██        | 2145/10570 [00:05<00:21, 398.37it/s][1,6]<stderr>:#015 17%|█▋        | 1848/10570 [00:04<00:24, 363.11it/s][1,14]<stderr>:#015 19%|█▉        | 2051/10570 [00:05<00:19, 437.84it/s][1,12]<stderr>:#015 21%|██▏       | 2254/10570 [00:05<00:20, 415.13it/s][1,11]<stderr>:#015 19%|█▊        | 1956/10570 [00:04<00:20, 428.98it/s][1,7]<stderr>:#015 22%|██▏       | 2307/10570 [00:05<00:19, 422.12it/s][1,3]<stderr>:#015 19%|█▊        | 1956/10570 [00:04<00:20, 426.97it/s][1,15]<stderr>:#015 20%|██        | 2128/10570 [00:05<00:21, 396.41it/s][1,5]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:19, 431.02it/s][1,10]<stderr>:#015 21%|██        | 2186/10570 [00:05<00:21, 396.23it/s][1,1]<stderr>:#015 21%|██        | 2176/10570 [00:05<00:21, 389.64it/s][1,9]<stderr>:#015 20%|█▉        | 2103/10570 [00:05<00:19, 432.81it/s][1,13]<stderr>:#015 17%|█▋        | 1816/10570 [00:04<00:22, 396.34it/s][1,2]<stderr>:#015 21%|██▏       | 2247/10570 [00:05<00:20, 405.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 19%|█▉        | 2020/10570 [00:05<00:20, 427.17it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 15%|█▌        | 1609/10570 [00:04<00:22, 393.84it/s]#033[A[1,4]<stderr>:#015 21%|██        | 2186/10570 [00:05<00:21, 396.26it/s][1,6]<stderr>:#015 18%|█▊        | 1892/10570 [00:04<00:22, 381.86it/s][1,14]<stderr>:#015 20%|█▉        | 2098/10570 [00:05<00:19, 444.16it/s][1,12]<stderr>:#015 22%|██▏       | 2296/10570 [00:05<00:20, 409.81it/s][1,11]<stderr>:#015 19%|█▉        | 1999/10570 [00:05<00:20, 423.75it/s][1,7]<stderr>:#015 22%|██▏       | 2350/10570 [00:05<00:19, 422.91it/s][1,3]<stderr>:#015 19%|█▉        | 1999/10570 [00:05<00:20, 426.74it/s][1,15]<stderr>:#015 21%|██        | 2169/10570 [00:05<00:21, 399.60it/s][1,5]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 437.42it/s][1,1]<stderr>:#015 21%|██        | 2221/10570 [00:05<00:20, 404.41it/s][1,10]<stderr>:#015 21%|██        | 2233/10570 [00:05<00:20, 413.57it/s][1,2]<stderr>:#015 22%|██▏       | 2289/10570 [00:05<00:20, 409.86it/s][1,13]<stderr>:#015 18%|█▊        | 1856/10570 [00:04<00:22, 392.97it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|█▉        | 2065/10570 [00:05<00:19, 431.28it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:22, 392.93it/s]#033[A[1,9]<stderr>:#015 20%|██        | 2147/10570 [00:05<00:21, 389.99it/s][1,4]<stderr>:#015 21%|██        | 2232/10570 [00:05<00:20, 413.13it/s][1,6]<stderr>:#015 18%|█▊        | 1936/10570 [00:04<00:21, 396.17it/s][1,12]<stderr>:#015 22%|██▏       | 2339/10570 [00:05<00:19, 413.66it/s][1,11]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:20, 421.54it/s][1,15]<stderr>:#015 21%|██        | 2211/10570 [00:05<00:20, 405.25it/s][1,7]<stderr>:#015 23%|██▎       | 2393/10570 [00:05<00:19, 415.00it/s][1,3]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:20, 422.99it/s][1,14]<stderr>:#015 20%|██        | 2143/10570 [00:05<00:20, 402.06it/s][1,1]<stderr>:#015 21%|██▏       | 2263/10570 [00:05<00:20, 404.01it/s][1,10]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:20, 408.28it/s][1,2]<stderr>:#015 22%|██▏       | 2332/10570 [00:05<00:19, 413.14it/s][1,13]<stderr>:#015 18%|█▊        | 1901/10570 [00:05<00:21, 407.14it/s][1,5]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 395.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|█▉        | 2110/10570 [00:05<00:19, 436.40it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▌        | 1694/10570 [00:04<00:21, 405.52it/s][1,8]<stderr>:#033[A[1,9]<stderr>:#015 21%|██        | 2187/10570 [00:05<00:21, 390.20it/s][1,6]<stderr>:#015 19%|█▊        | 1981/10570 [00:05<00:20, 410.23it/s][1,4]<stderr>:#015 22%|██▏       | 2274/10570 [00:05<00:20, 408.29it/s][1,12]<stderr>:#015 23%|██▎       | 2381/10570 [00:05<00:19, 409.88it/s][1,11]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 430.59it/s][1,15]<stderr>:#015 21%|██▏       | 2254/10570 [00:05<00:20, 411.43it/s][1,3]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 431.60it/s][1,7]<stderr>:#015 23%|██▎       | 2435/10570 [00:05<00:19, 413.35it/s][1,14]<stderr>:#015 21%|██        | 2185/10570 [00:05<00:21, 398.97it/s][1,1]<stderr>:#015 22%|██▏       | 2306/10570 [00:05<00:20, 411.24it/s][1,10]<stderr>:#015 22%|██▏       | 2318/10570 [00:05<00:20, 412.58it/s][1,2]<stderr>:#015 22%|██▏       | 2374/10570 [00:05<00:19, 412.82it/s][1,13]<stderr>:#015 18%|█▊        | 1943/10570 [00:05<00:21, 409.58it/s][1,5]<stderr>:#015 21%|██        | 2173/10570 [00:05<00:21, 398.60it/s][1,9]<stderr>:#015 21%|██        | 2233/10570 [00:05<00:20, 407.72it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▋        | 1735/10570 [00:04<00:22, 399.12it/s]#033[A[1,6]<stderr>:#015 19%|█▉        | 2023/10570 [00:05<00:20, 408.56it/s][1,4]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:19, 412.66it/s][1,12]<stderr>:#015 23%|██▎       | 2423/10570 [00:05<00:20, 401.20it/s][1,15]<stderr>:#015 22%|██▏       | 2296/10570 [00:05<00:20, 412.09it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|██        | 2154/10570 [00:05<00:21, 390.92it/s]#033[A[1,7]<stderr>:#015 23%|██▎       | 2477/10570 [00:05<00:19, 409.96it/s][1,14]<stderr>:#015 21%|██        | 2231/10570 [00:05<00:20, 415.26it/s][1,11]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 387.68it/s][1,1]<stderr>:#015 22%|██▏       | 2348/10570 [00:05<00:19, 412.12it/s][1,10]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 417.09it/s][1,3]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 387.81it/s][1,13]<stderr>:#015 19%|█▉        | 1987/10570 [00:05<00:20, 417.24it/s][1,2]<stderr>:#015 23%|██▎       | 2416/10570 [00:05<00:20, 404.86it/s][1,5]<stderr>:#015 21%|██        | 2217/10570 [00:05<00:20, 407.84it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 17%|█▋        | 1776/10570 [00:04<00:22, 398.74it/s]#033[A[1,6]<stderr>:#015 20%|█▉        | 2067/10570 [00:05<00:20, 416.25it/s][1,9]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:20, 402.96it/s][1,4]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 417.71it/s][1,15]<stderr>:#015 22%|██▏       | 2338/10570 [00:05<00:19, 412.32it/s][1,12]<stderr>:#015 23%|██▎       | 2464/10570 [00:05<00:20, 399.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 21%|██        | 2195/10570 [00:05<00:21, 393.72it/s]#033[A[1,14]<stderr>:#015 22%|██▏       | 2274/10570 [00:05<00:20, 410.24it/s][1,7]<stderr>:#015 24%|██▍       | 2519/10570 [00:06<00:20, 388.74it/s][1,11]<stderr>:#015 21%|██        | 2172/10570 [00:05<00:21, 391.28it/s][1,1]<stderr>:#015 23%|██▎       | 2390/10570 [00:05<00:20, 406.90it/s][1,3]<stderr>:#015 21%|██        | 2172/10570 [00:05<00:21, 384.28it/s][1,10]<stderr>:#015 23%|██▎       | 2403/10570 [00:05<00:20, 405.24it/s][1,13]<stderr>:#015 19%|█▉        | 2029/10570 [00:05<00:20, 408.25it/s][1,2]<stderr>:#015 23%|██▎       | 2457/10570 [00:05<00:20, 400.96it/s][1,5]<stderr>:#015 21%|██▏       | 2259/10570 [00:05<00:20, 407.69it/s][1,8]<stderr>:#015100%|██████████| 169/169 [00:30<00:00, 12.65it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 17%|█▋        | 1816/10570 [00:04<00:21, 399.05it/s]#033[A[1,6]<stderr>:#015 20%|█▉        | 2110/10570 [00:05<00:20, 418.65it/s][1,9]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:20, 406.73it/s][1,4]<stderr>:#015 23%|██▎       | 2404/10570 [00:05<00:20, 404.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 21%|██        | 2239/10570 [00:05<00:20, 404.68it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2380/10570 [00:05<00:20, 407.40it/s][1,12]<stderr>:#015 24%|██▎       | 2505/10570 [00:06<00:20, 384.68it/s][1,14]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:19, 414.44it/s][1,11]<stderr>:#015 21%|██        | 2214/10570 [00:05<00:20, 399.02it/s][1,7]<stderr>:#015 24%|██▍       | 2559/10570 [00:06<00:20, 382.03it/s][1,1]<stderr>:#015 23%|██▎       | 2431/10570 [00:05<00:20, 403.01it/s][1,3]<stderr>:#015 21%|██        | 2214/10570 [00:05<00:21, 393.30it/s][1,10]<stderr>:#015 23%|██▎       | 2444/10570 [00:05<00:20, 404.26it/s][1,13]<stderr>:#015 20%|█▉        | 2072/10570 [00:05<00:20, 413.17it/s][1,5]<stderr>:#015 22%|██▏       | 2302/10570 [00:05<00:20, 412.36it/s][1,2]<stderr>:#015 24%|██▎       | 2498/10570 [00:06<00:20, 391.29it/s][1,9]<stderr>:#015 22%|██▏       | 2360/10570 [00:05<00:19, 412.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1856/10570 [00:04<00:22, 392.28it/s]#033[A[1,4]<stderr>:#015 23%|██▎       | 2445/10570 [00:05<00:20, 402.99it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2281/10570 [00:05<00:20, 404.16it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2421/10570 [00:05<00:20, 402.61it/s][1,6]<stderr>:#015 20%|██        | 2153/10570 [00:05<00:22, 376.21it/s][1,0]<stderr>:#015100%|██████████| 169/169 [00:30<00:00, 12.65it/s][1,12]<stderr>:#015 24%|██▍       | 2544/10570 [00:06<00:21, 378.56it/s][1,14]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 418.72it/s][1,7]<stderr>:#015 25%|██▍       | 2600/10570 [00:06<00:20, 388.54it/s][1,11]<stderr>:#015 21%|██▏       | 2256/10570 [00:05<00:20, 402.18it/s][1,1]<stderr>:#015 23%|██▎       | 2472/10570 [00:06<00:20, 399.08it/s][1,3]<stderr>:#015 21%|██▏       | 2256/10570 [00:05<00:20, 398.16it/s][1,10]<stderr>:#015 24%|██▎       | 2485/10570 [00:06<00:20, 403.99it/s][1,13]<stderr>:#015 20%|██        | 2115/10570 [00:05<00:20, 417.47it/s][1,5]<stderr>:#015 22%|██▏       | 2344/10570 [00:05<00:19, 412.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1900/10570 [00:05<00:21, 405.14it/s]#033[A[1,2]<stderr>:#015 24%|██▍       | 2538/10570 [00:06<00:20, 382.71it/s][1,9]<stderr>:#015 23%|██▎       | 2402/10570 [00:06<00:20, 400.59it/s][1,4]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:20, 402.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2322/10570 [00:05<00:20, 405.25it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2462/10570 [00:06<00:20, 395.44it/s][1,6]<stderr>:#015 21%|██        | 2192/10570 [00:05<00:22, 379.27it/s][1,12]<stderr>:#015 24%|██▍       | 2583/10570 [00:06<00:21, 376.19it/s][1,7]<stderr>:#015 25%|██▌       | 2645/10570 [00:06<00:19, 403.88it/s][1,14]<stderr>:#015 23%|██▎       | 2404/10570 [00:05<00:20, 406.34it/s][1,11]<stderr>:#015 22%|██▏       | 2297/10570 [00:05<00:21, 393.93it/s][1,3]<stderr>:#015 22%|██▏       | 2298/10570 [00:05<00:20, 401.59it/s][1,1]<stderr>:#015 24%|██▍       | 2513/10570 [00:06<00:21, 381.62it/s][1,10]<stderr>:#015 24%|██▍       | 2526/10570 [00:06<00:21, 380.67it/s][1,5]<stderr>:#015 23%|██▎       | 2386/10570 [00:05<00:20, 405.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1941/10570 [00:05<00:21, 406.41it/s]#033[A[1,2]<stderr>:#015 24%|██▍       | 2577/10570 [00:06<00:21, 375.23it/s][1,9]<stderr>:#015 23%|██▎       | 2443/10570 [00:06<00:20, 398.13it/s][1,13]<stderr>:#015 20%|██        | 2157/10570 [00:05<00:22, 372.70it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2364/10570 [00:05<00:20, 409.30it/s]#033[A[1,4]<stderr>:#015 24%|██▍       | 2527/10570 [00:06<00:21, 379.68it/s][1,6]<stderr>:#015 21%|██        | 2236/10570 [00:05<00:21, 394.64it/s][1,12]<stderr>:#015 25%|██▍       | 2627/10570 [00:06<00:20, 392.36it/s][1,15]<stderr>:#015 24%|██▎       | 2502/10570 [00:06<00:21, 380.65it/s][1,7]<stderr>:#015 25%|██▌       | 2689/10570 [00:06<00:19, 413.60it/s][1,14]<stderr>:#015 23%|██▎       | 2445/10570 [00:05<00:20, 405.30it/s][1,11]<stderr>:#015 22%|██▏       | 2338/10570 [00:05<00:20, 398.01it/s][1,3]<stderr>:#015 22%|██▏       | 2339/10570 [00:05<00:20, 402.72it/s][1,1]<stderr>:#015 24%|██▍       | 2552/10570 [00:06<00:21, 375.75it/s][1,5]<stderr>:#015 23%|██▎       | 2427/10570 [00:05<00:20, 403.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 19%|█▉        | 1985/10570 [00:05<00:20, 413.94it/s]#033[A[1,10]<stderr>:#015 24%|██▍       | 2565/10570 [00:06<00:21, 372.32it/s][1,2]<stderr>:#015 25%|██▍       | 2620/10570 [00:06<00:20, 388.53it/s][1,9]<stderr>:#015 23%|██▎       | 2483/10570 [00:06<00:20, 394.68it/s][1,13]<stderr>:#015 21%|██        | 2196/10570 [00:05<00:22, 376.25it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 23%|██▎       | 2406/10570 [00:05<00:20, 397.23it/s]#033[A[1,6]<stderr>:#015 22%|██▏       | 2277/10570 [00:05<00:21, 391.76it/s][1,12]<stderr>:#015 25%|██▌       | 2672/10570 [00:06<00:19, 406.34it/s][1,4]<stderr>:#015 24%|██▍       | 2566/10570 [00:06<00:21, 366.99it/s][1,7]<stderr>:#015 26%|██▌       | 2732/10570 [00:06<00:18, 416.79it/s][1,15]<stderr>:#015 24%|██▍       | 2541/10570 [00:06<00:21, 376.44it/s][1,14]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:19, 404.22it/s][1,3]<stderr>:#015 23%|██▎       | 2380/10570 [00:06<00:20, 397.45it/s][1,11]<stderr>:#015 23%|██▎       | 2379/10570 [00:06<00:21, 385.86it/s][1,1]<stderr>:#015 25%|██▍       | 2591/10570 [00:06<00:21, 378.53it/s][1,5]<stderr>:#015 23%|██▎       | 2468/10570 [00:06<00:20, 397.66it/s][1,10]<stderr>:#015 25%|██▍       | 2603/10570 [00:06<00:21, 374.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 19%|█▉        | 2027/10570 [00:05<00:21, 405.22it/s]#033[A[1,2]<stderr>:#015 25%|██▌       | 2666/10570 [00:06<00:19, 405.69it/s][1,13]<stderr>:#015 21%|██        | 2236/10570 [00:05<00:21, 381.22it/s][1,9]<stderr>:#015 24%|██▍       | 2523/10570 [00:06<00:21, 371.93it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 23%|██▎       | 2446/10570 [00:06<00:20, 394.73it/s]#033[A[1,6]<stderr>:#015 22%|██▏       | 2318/10570 [00:05<00:20, 396.17it/s][1,12]<stderr>:#015 26%|██▌       | 2715/10570 [00:06<00:19, 412.17it/s][1,4]<stderr>:#015 25%|██▍       | 2607/10570 [00:06<00:21, 378.73it/s][1,7]<stderr>:#015 26%|██▋       | 2780/10570 [00:06<00:18, 432.51it/s][1,15]<stderr>:#015 24%|██▍       | 2579/10570 [00:06<00:21, 370.90it/s][1,11]<stderr>:#015 23%|██▎       | 2418/10570 [00:06<00:21, 384.79it/s][1,3]<stderr>:#015 23%|██▎       | 2420/10570 [00:06<00:20, 390.95it/s][1,14]<stderr>:#015 24%|██▍       | 2527/10570 [00:06<00:21, 380.73it/s][1,1]<stderr>:#015 25%|██▍       | 2635/10570 [00:06<00:20, 393.16it/s][1,10]<stderr>:#015 25%|██▌       | 2647/10570 [00:06<00:20, 390.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|█▉        | 2069/10570 [00:05<00:20, 407.74it/s]#033[A[1,2]<stderr>:#015 26%|██▌       | 2707/10570 [00:06<00:19, 406.94it/s][1,5]<stderr>:#015 24%|██▎       | 2508/10570 [00:06<00:21, 377.47it/s][1,13]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:21, 379.22it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:20, 395.85it/s]#033[A[1,9]<stderr>:#015 24%|██▍       | 2561/10570 [00:06<00:21, 367.37it/s][1,6]<stderr>:#015 22%|██▏       | 2360/10570 [00:06<00:20, 402.48it/s][1,12]<stderr>:#015 26%|██▌       | 2763/10570 [00:06<00:18, 429.22it/s][1,4]<stderr>:#015 25%|██▌       | 2652/10570 [00:06<00:19, 397.59it/s][1,7]<stderr>:#015 27%|██▋       | 2825/10570 [00:06<00:17, 436.84it/s][1,15]<stderr>:#015 25%|██▍       | 2621/10570 [00:06<00:20, 383.98it/s][1,11]<stderr>:#015 23%|██▎       | 2457/10570 [00:06<00:21, 382.32it/s][1,3]<stderr>:#015 23%|██▎       | 2460/10570 [00:06<00:20, 387.52it/s][1,1]<stderr>:#015 25%|██▌       | 2678/10570 [00:06<00:19, 403.51it/s][1,14]<stderr>:#015 24%|██▍       | 2566/10570 [00:06<00:21, 371.70it/s][1,10]<stderr>:#015 25%|██▌       | 2690/10570 [00:06<00:19, 401.09it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|█▉        | 2112/10570 [00:05<00:20, 413.71it/s]#033[A[1,2]<stderr>:#015 26%|██▌       | 2754/10570 [00:06<00:18, 423.06it/s][1,5]<stderr>:#015 24%|██▍       | 2547/10570 [00:06<00:21, 373.39it/s][1,13]<stderr>:#015 22%|██▏       | 2316/10570 [00:06<00:21, 385.25it/s][1,9]<stderr>:#015 25%|██▍       | 2601/10570 [00:06<00:21, 375.48it/s][1,4]<stderr>:#015 25%|██▌       | 2694/10570 [00:06<00:19, 402.62it/s][1,12]<stderr>:#015 27%|██▋       | 2811/10570 [00:06<00:17, 440.83it/s][1,6]<stderr>:#015 23%|██▎       | 2401/10570 [00:06<00:20, 391.04it/s][1,15]<stderr>:#015 25%|██▌       | 2666/10570 [00:06<00:19, 400.27it/s][1,7]<stderr>:#015 27%|██▋       | 2869/10570 [00:06<00:17, 429.75it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▍       | 2526/10570 [00:06<00:21, 373.18it/s]#033[A[1,11]<stderr>:#015 24%|██▎       | 2496/10570 [00:06<00:21, 381.40it/s][1,1]<stderr>:#015 26%|██▌       | 2721/10570 [00:06<00:19, 411.11it/s][1,14]<stderr>:#015 25%|██▍       | 2608/10570 [00:06<00:20, 383.00it/s][1,3]<stderr>:#015 24%|██▎       | 2499/10570 [00:06<00:21, 372.79it/s][1,10]<stderr>:#015 26%|██▌       | 2734/10570 [00:06<00:19, 410.58it/s][1,2]<stderr>:#015 27%|██▋       | 2802/10570 [00:06<00:17, 438.35it/s][1,5]<stderr>:#015 24%|██▍       | 2585/10570 [00:06<00:21, 374.23it/s][1,13]<stderr>:#015 22%|██▏       | 2357/10570 [00:06<00:21, 390.57it/s][1,9]<stderr>:#015 25%|██▌       | 2645/10570 [00:06<00:20, 390.67it/s][1,4]<stderr>:#015 26%|██▌       | 2739/10570 [00:06<00:18, 413.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|██        | 2154/10570 [00:05<00:23, 365.85it/s]#033[A[1,6]<stderr>:#015 23%|██▎       | 2441/10570 [00:06<00:20, 389.00it/s][1,12]<stderr>:#015 27%|██▋       | 2856/10570 [00:06<00:17, 429.92it/s][1,15]<stderr>:#015 26%|██▌       | 2707/10570 [00:06<00:19, 402.16it/s][1,7]<stderr>:#015 28%|██▊       | 2913/10570 [00:07<00:18, 422.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▍       | 2564/10570 [00:06<00:21, 365.77it/s]#033[A[1,1]<stderr>:#015 26%|██▌       | 2768/10570 [00:06<00:18, 426.09it/s][1,14]<stderr>:#015 25%|██▌       | 2654/10570 [00:06<00:19, 401.95it/s][1,10]<stderr>:#015 26%|██▋       | 2781/10570 [00:06<00:18, 426.48it/s][1,3]<stderr>:#015 24%|██▍       | 2537/10570 [00:06<00:21, 369.14it/s][1,11]<stderr>:#015 24%|██▍       | 2535/10570 [00:06<00:22, 356.27it/s][1,2]<stderr>:#015 27%|██▋       | 2847/10570 [00:06<00:17, 430.06it/s][1,5]<stderr>:#015 25%|██▍       | 2628/10570 [00:06<00:20, 388.76it/s][1,13]<stderr>:#015 23%|██▎       | 2397/10570 [00:06<00:21, 378.19it/s][1,9]<stderr>:#015 25%|██▌       | 2688/10570 [00:06<00:19, 401.57it/s][1,4]<stderr>:#015 26%|██▋       | 2786/10570 [00:06<00:18, 428.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 21%|██        | 2192/10570 [00:05<00:22, 366.84it/s]#033[A[1,15]<stderr>:#015 26%|██▌       | 2753/10570 [00:06<00:18, 417.46it/s][1,6]<stderr>:#015 23%|██▎       | 2481/10570 [00:06<00:20, 386.75it/s][1,12]<stderr>:#015 27%|██▋       | 2900/10570 [00:07<00:18, 420.49it/s][1,7]<stderr>:#015 28%|██▊       | 2956/10570 [00:07<00:18, 420.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▍       | 2605/10570 [00:06<00:21, 375.45it/s]#033[A[1,1]<stderr>:#015 27%|██▋       | 2814/10570 [00:06<00:17, 434.30it/s][1,14]<stderr>:#015 26%|██▌       | 2696/10570 [00:06<00:19, 405.20it/s][1,10]<stderr>:#015 27%|██▋       | 2825/10570 [00:06<00:18, 429.06it/s][1,3]<stderr>:#015 24%|██▍       | 2575/10570 [00:06<00:22, 360.43it/s][1,11]<stderr>:#015 24%|██▍       | 2572/10570 [00:06<00:22, 349.53it/s][1,5]<stderr>:#015 25%|██▌       | 2672/10570 [00:06<00:19, 401.73it/s][1,2]<stderr>:#015 27%|██▋       | 2891/10570 [00:07<00:18, 419.49it/s][1,13]<stderr>:#015 23%|██▎       | 2436/10570 [00:06<00:21, 378.36it/s][1,9]<stderr>:#015 26%|██▌       | 2732/10570 [00:06<00:19, 410.68it/s][1,4]<stderr>:#015 27%|██▋       | 2831/10570 [00:06<00:17, 432.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 21%|██        | 2235/10570 [00:05<00:21, 381.07it/s]#033[A[1,15]<stderr>:#015 26%|██▋       | 2800/10570 [00:06<00:17, 431.94it/s][1,12]<stderr>:#015 28%|██▊       | 2943/10570 [00:07<00:18, 420.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▌       | 2649/10570 [00:06<00:20, 391.11it/s]#033[A[1,7]<stderr>:#015 28%|██▊       | 2999/10570 [00:07<00:18, 418.94it/s][1,6]<stderr>:#015 24%|██▍       | 2520/10570 [00:06<00:22, 364.07it/s][1,14]<stderr>:#015 26%|██▌       | 2741/10570 [00:06<00:18, 416.81it/s][1,1]<stderr>:#015 27%|██▋       | 2858/10570 [00:07<00:18, 424.15it/s][1,10]<stderr>:#015 27%|██▋       | 2869/10570 [00:07<00:18, 420.65it/s][1,3]<stderr>:#015 25%|██▍       | 2617/10570 [00:06<00:21, 374.20it/s][1,11]<stderr>:#015 25%|██▍       | 2614/10570 [00:06<00:21, 367.26it/s][1,5]<stderr>:#015 26%|██▌       | 2714/10570 [00:06<00:19, 405.99it/s][1,2]<stderr>:#015 28%|██▊       | 2934/10570 [00:07<00:18, 419.20it/s][1,13]<stderr>:#015 23%|██▎       | 2474/10570 [00:06<00:21, 375.94it/s][1,9]<stderr>:#015 26%|██▋       | 2779/10570 [00:06<00:18, 425.19it/s][1,4]<stderr>:#015 27%|██▋       | 2875/10570 [00:07<00:18, 421.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2274/10570 [00:06<00:22, 375.51it/s]#033[A[1,15]<stderr>:#015 27%|██▋       | 2844/10570 [00:07<00:18, 423.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▌       | 2691/10570 [00:06<00:19, 398.46it/s]#033[A[1,7]<stderr>:#015 29%|██▉       | 3042/10570 [00:07<00:17, 420.73it/s][1,12]<stderr>:#015 28%|██▊       | 2986/10570 [00:07<00:18, 413.82it/s][1,6]<stderr>:#015 24%|██▍       | 2557/10570 [00:06<00:22, 357.38it/s][1,14]<stderr>:#015 26%|██▋       | 2788/10570 [00:06<00:18, 430.15it/s][1,1]<stderr>:#015 27%|██▋       | 2901/10570 [00:07<00:18, 415.06it/s][1,3]<stderr>:#015 25%|██▌       | 2661/10570 [00:06<00:20, 390.99it/s][1,10]<stderr>:#015 28%|██▊       | 2912/10570 [00:07<00:18, 411.73it/s][1,11]<stderr>:#015 25%|██▌       | 2657/10570 [00:06<00:20, 382.98it/s][1,5]<stderr>:#015 26%|██▌       | 2761/10570 [00:06<00:18, 422.65it/s][1,2]<stderr>:#015 28%|██▊       | 2977/10570 [00:07<00:18, 413.39it/s][1,9]<stderr>:#015 27%|██▋       | 2823/10570 [00:07<00:18, 427.48it/s][1,13]<stderr>:#015 24%|██▍       | 2512/10570 [00:06<00:22, 355.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2314/10570 [00:06<00:21, 379.53it/s]#033[A[1,4]<stderr>:#015 28%|██▊       | 2918/10570 [00:07<00:18, 413.72it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 26%|██▌       | 2735/10570 [00:06<00:19, 407.98it/s]#033[A[1,15]<stderr>:#015 27%|██▋       | 2887/10570 [00:07<00:18, 414.32it/s][1,12]<stderr>:#015 29%|██▊       | 3029/10570 [00:07<00:18, 416.45it/s][1,7]<stderr>:#015 29%|██▉       | 3085/10570 [00:07<00:18, 414.68it/s][1,6]<stderr>:#015 25%|██▍       | 2595/10570 [00:06<00:22, 362.40it/s][1,14]<stderr>:#015 27%|██▋       | 2833/10570 [00:06<00:17, 434.14it/s][1,1]<stderr>:#015 28%|██▊       | 2943/10570 [00:07<00:18, 415.29it/s][1,3]<stderr>:#015 26%|██▌       | 2701/10570 [00:06<00:20, 392.78it/s][1,11]<stderr>:#015 26%|██▌       | 2697/10570 [00:06<00:20, 387.23it/s][1,10]<stderr>:#015 28%|██▊       | 2954/10570 [00:07<00:18, 408.57it/s][1,5]<stderr>:#015 27%|██▋       | 2808/10570 [00:06<00:17, 434.58it/s][1,2]<stderr>:#015 29%|██▊       | 3019/10570 [00:07<00:18, 414.94it/s][1,9]<stderr>:#015 27%|██▋       | 2866/10570 [00:07<00:18, 419.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2354/10570 [00:06<00:21, 383.11it/s]#033[A[1,13]<stderr>:#015 24%|██▍       | 2548/10570 [00:06<00:22, 351.20it/s][1,4]<stderr>:#015 28%|██▊       | 2960/10570 [00:07<00:18, 412.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 26%|██▋       | 2782/10570 [00:06<00:18, 424.50it/s]#033[A[1,15]<stderr>:#015 28%|██▊       | 2929/10570 [00:07<00:18, 413.25it/s][1,7]<stderr>:#015 30%|██▉       | 3127/10570 [00:07<00:17, 413.76it/s][1,12]<stderr>:#015 29%|██▉       | 3071/10570 [00:07<00:18, 405.33it/s][1,6]<stderr>:#015 25%|██▍       | 2637/10570 [00:06<00:21, 377.61it/s][1,14]<stderr>:#015 27%|██▋       | 2877/10570 [00:07<00:18, 422.76it/s][1,1]<stderr>:#015 28%|██▊       | 2985/10570 [00:07<00:18, 410.19it/s][1,3]<stderr>:#015 26%|██▌       | 2746/10570 [00:06<00:19, 406.32it/s][1,11]<stderr>:#015 26%|██▌       | 2741/10570 [00:06<00:19, 400.19it/s][1,10]<stderr>:#015 28%|██▊       | 2995/10570 [00:07<00:18, 406.83it/s][1,5]<stderr>:#015 27%|██▋       | 2852/10570 [00:07<00:18, 421.90it/s][1,2]<stderr>:#015 29%|██▉       | 3061/10570 [00:07<00:18, 413.84it/s][1,9]<stderr>:#015 28%|██▊       | 2909/10570 [00:07<00:18, 411.21it/s][1,13]<stderr>:#015 24%|██▍       | 2584/10570 [00:06<00:22, 352.59it/s][1,4]<stderr>:#015 28%|██▊       | 3002/10570 [00:07<00:18, 410.34it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 27%|██▋       | 2826/10570 [00:07<00:18, 426.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2393/10570 [00:06<00:21, 374.35it/s][1,8]<stderr>:#033[A[1,15]<stderr>:#015 28%|██▊       | 2971/10570 [00:07<00:18, 410.38it/s][1,7]<stderr>:#015 30%|██▉       | 3169/10570 [00:07<00:18, 409.66it/s][1,12]<stderr>:#015 29%|██▉       | 3113/10570 [00:07<00:18, 408.80it/s][1,6]<stderr>:#015 25%|██▌       | 2680/10570 [00:06<00:20, 388.97it/s][1,14]<stderr>:#015 28%|██▊       | 2920/10570 [00:07<00:18, 416.45it/s][1,1]<stderr>:#015 29%|██▊       | 3027/10570 [00:07<00:18, 412.63it/s][1,3]<stderr>:#015 26%|██▋       | 2793/10570 [00:07<00:18, 421.53it/s][1,11]<stderr>:#015 26%|██▋       | 2787/10570 [00:07<00:18, 415.63it/s][1,10]<stderr>:#015 29%|██▊       | 3038/10570 [00:07<00:18, 411.44it/s][1,2]<stderr>:#015 29%|██▉       | 3103/10570 [00:07<00:18, 411.46it/s][1,5]<stderr>:#015 27%|██▋       | 2895/10570 [00:07<00:18, 411.01it/s][1,9]<stderr>:#015 28%|██▊       | 2951/10570 [00:07<00:18, 409.09it/s][1,13]<stderr>:#015 25%|██▍       | 2625/10570 [00:06<00:21, 366.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2431/10570 [00:06<00:21, 372.88it/s]#033[A[1,4]<stderr>:#015 29%|██▉       | 3044/10570 [00:07<00:18, 407.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 27%|██▋       | 2869/10570 [00:07<00:18, 416.70it/s]#033[A[1,15]<stderr>:#015 29%|██▊       | 3013/10570 [00:07<00:18, 407.35it/s][1,12]<stderr>:#015 30%|██▉       | 3154/10570 [00:07<00:18, 404.36it/s][1,7]<stderr>:#015 30%|███       | 3211/10570 [00:07<00:18, 405.63it/s][1,6]<stderr>:#015 26%|██▌       | 2721/10570 [00:06<00:19, 394.10it/s][1,14]<stderr>:#015 28%|██▊       | 2962/10570 [00:07<00:18, 414.91it/s][1,11]<stderr>:#015 27%|██▋       | 2831/10570 [00:07<00:18, 421.03it/s][1,3]<stderr>:#015 27%|██▋       | 2836/10570 [00:07<00:18, 420.37it/s][1,1]<stderr>:#015 29%|██▉       | 3069/10570 [00:07<00:18, 405.30it/s][1,10]<stderr>:#015 29%|██▉       | 3080/10570 [00:07<00:18, 405.64it/s][1,5]<stderr>:#015 28%|██▊       | 2937/10570 [00:07<00:18, 411.75it/s][1,2]<stderr>:#015 30%|██▉       | 3145/10570 [00:07<00:18, 403.01it/s][1,9]<stderr>:#015 28%|██▊       | 2993/10570 [00:07<00:18, 408.25it/s][1,13]<stderr>:#015 25%|██▌       | 2667/10570 [00:07<00:20, 380.62it/s][1,4]<stderr>:#015 29%|██▉       | 3085/10570 [00:07<00:18, 405.52it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2469/10570 [00:06<00:22, 365.83it/s][1,8]<stderr>:#033[A[1,15]<stderr>:#015 29%|██▉       | 3054/10570 [00:07<00:18, 406.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2911/10570 [00:07<00:18, 408.35it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 26%|██▌       | 2765/10570 [00:07<00:19, 406.56it/s][1,12]<stderr>:#015 30%|███       | 3195/10570 [00:07<00:18, 401.43it/s][1,7]<stderr>:#015 31%|███       | 3252/10570 [00:07<00:18, 398.96it/s][1,14]<stderr>:#015 28%|██▊       | 3004/10570 [00:07<00:18, 412.15it/s][1,1]<stderr>:#015 29%|██▉       | 3110/10570 [00:07<00:18, 406.47it/s][1,11]<stderr>:#015 27%|██▋       | 2874/10570 [00:07<00:18, 411.47it/s][1,10]<stderr>:#015 30%|██▉       | 3121/10570 [00:07<00:18, 406.00it/s][1,3]<stderr>:#015 27%|██▋       | 2879/10570 [00:07<00:18, 406.78it/s][1,5]<stderr>:#015 28%|██▊       | 2979/10570 [00:07<00:18, 406.73it/s][1,2]<stderr>:#015 30%|███       | 3186/10570 [00:07<00:18, 394.29it/s][1,9]<stderr>:#015 29%|██▊       | 3035/10570 [00:07<00:18, 410.00it/s][1,13]<stderr>:#015 26%|██▌       | 2706/10570 [00:07<00:20, 381.44it/s][1,4]<stderr>:#015 30%|██▉       | 3126/10570 [00:07<00:18, 404.31it/s][1,15]<stderr>:#015 29%|██▉       | 3095/10570 [00:07<00:18, 405.09it/s][1,6]<stderr>:#015 27%|██▋       | 2811/10570 [00:07<00:18, 418.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2953/10570 [00:07<00:18, 405.78it/s]#033[A[1,12]<stderr>:#015 31%|███       | 3236/10570 [00:07<00:18, 398.06it/s][1,7]<stderr>:#015 31%|███       | 3294/10570 [00:07<00:18, 402.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▎       | 2506/10570 [00:06<00:23, 349.28it/s]#033[A[1,14]<stderr>:#015 29%|██▉       | 3046/10570 [00:07<00:18, 410.49it/s][1,1]<stderr>:#015 30%|██▉       | 3151/10570 [00:07<00:18, 400.31it/s][1,10]<stderr>:#015 30%|██▉       | 3162/10570 [00:07<00:18, 402.95it/s][1,11]<stderr>:#015 28%|██▊       | 2916/10570 [00:07<00:18, 404.13it/s][1,3]<stderr>:#015 28%|██▊       | 2920/10570 [00:07<00:19, 402.41it/s][1,5]<stderr>:#015 29%|██▊       | 3021/10570 [00:07<00:18, 409.42it/s][1,2]<stderr>:#015 31%|███       | 3226/10570 [00:07<00:18, 395.19it/s][1,13]<stderr>:#015 26%|██▌       | 2750/10570 [00:07<00:19, 395.15it/s][1,9]<stderr>:#015 29%|██▉       | 3077/10570 [00:07<00:18, 402.76it/s][1,4]<stderr>:#015 30%|██▉       | 3167/10570 [00:07<00:18, 399.91it/s][1,15]<stderr>:#015 30%|██▉       | 3136/10570 [00:07<00:18, 403.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2994/10570 [00:07<00:18, 404.18it/s]#033[A[1,7]<stderr>:#015 32%|███▏      | 3337/10570 [00:08<00:17, 408.11it/s][1,12]<stderr>:#015 31%|███       | 3276/10570 [00:07<00:18, 393.82it/s][1,6]<stderr>:#015 27%|██▋       | 2854/10570 [00:07<00:18, 407.72it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▍       | 2542/10570 [00:06<00:23, 346.29it/s]#033[A[1,14]<stderr>:#015 29%|██▉       | 3088/10570 [00:07<00:18, 409.95it/s][1,1]<stderr>:#015 30%|███       | 3192/10570 [00:07<00:18, 397.33it/s][1,10]<stderr>:#015 30%|███       | 3203/10570 [00:07<00:18, 400.11it/s][1,11]<stderr>:#015 28%|██▊       | 2957/10570 [00:07<00:18, 403.39it/s][1,3]<stderr>:#015 28%|██▊       | 2961/10570 [00:07<00:18, 401.47it/s][1,5]<stderr>:#015 29%|██▉       | 3063/10570 [00:07<00:18, 406.46it/s][1,2]<stderr>:#015 31%|███       | 3266/10570 [00:07<00:18, 394.29it/s][1,13]<stderr>:#015 26%|██▋       | 2795/10570 [00:07<00:19, 408.67it/s][1,9]<stderr>:#015 29%|██▉       | 3118/10570 [00:07<00:18, 403.59it/s][1,4]<stderr>:#015 30%|███       | 3208/10570 [00:07<00:18, 394.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▊       | 3036/10570 [00:07<00:18, 406.90it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 32%|███▏      | 3379/10570 [00:08<00:17, 411.42it/s][1,12]<stderr>:#015 31%|███▏      | 3317/10570 [00:08<00:18, 397.35it/s][1,15]<stderr>:#015 30%|███       | 3177/10570 [00:07<00:19, 388.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▍       | 2577/10570 [00:06<00:23, 340.41it/s]#033[A[1,6]<stderr>:#015 27%|██▋       | 2896/10570 [00:07<00:19, 398.95it/s][1,14]<stderr>:#015 30%|██▉       | 3130/10570 [00:07<00:18, 408.32it/s][1,1]<stderr>:#015 31%|███       | 3232/10570 [00:07<00:18, 394.84it/s][1,11]<stderr>:#015 28%|██▊       | 2998/10570 [00:07<00:18, 400.46it/s][1,3]<stderr>:#015 28%|██▊       | 3002/10570 [00:07<00:18, 399.33it/s][1,10]<stderr>:#015 31%|███       | 3244/10570 [00:07<00:18, 394.56it/s][1,5]<stderr>:#015 29%|██▉       | 3104/10570 [00:07<00:18, 405.86it/s][1,2]<stderr>:#015 31%|███▏      | 3307/10570 [00:08<00:18, 396.53it/s][1,13]<stderr>:#015 27%|██▋       | 2837/10570 [00:07<00:18, 409.02it/s][1,9]<stderr>:#015 30%|██▉       | 3159/10570 [00:07<00:18, 398.99it/s][1,4]<stderr>:#015 31%|███       | 3248/10570 [00:07<00:18, 386.98it/s][1,12]<stderr>:#015 32%|███▏      | 3359/10570 [00:08<00:17, 403.13it/s][1,7]<stderr>:#015 32%|███▏      | 3421/10570 [00:08<00:17, 409.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▉       | 3077/10570 [00:07<00:18, 398.54it/s]#033[A[1,15]<stderr>:#015 30%|███       | 3217/10570 [00:07<00:18, 389.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 25%|██▍       | 2616/10570 [00:06<00:22, 353.88it/s]#033[A[1,6]<stderr>:#015 28%|██▊       | 2937/10570 [00:07<00:19, 400.35it/s][1,1]<stderr>:#015 31%|███       | 3272/10570 [00:08<00:18, 395.71it/s][1,14]<stderr>:#015 30%|███       | 3171/10570 [00:07<00:18, 396.44it/s][1,11]<stderr>:#015 29%|██▉       | 3040/10570 [00:07<00:18, 404.04it/s][1,3]<stderr>:#015 29%|██▉       | 3043/10570 [00:07<00:18, 400.15it/s][1,10]<stderr>:#015 31%|███       | 3284/10570 [00:08<00:18, 394.49it/s][1,2]<stderr>:#015 32%|███▏      | 3347/10570 [00:08<00:18, 397.40it/s][1,5]<stderr>:#015 30%|██▉       | 3145/10570 [00:07<00:18, 397.57it/s][1,9]<stderr>:#015 30%|███       | 3199/10570 [00:07<00:18, 390.40it/s][1,13]<stderr>:#015 27%|██▋       | 2879/10570 [00:07<00:19, 396.35it/s][1,4]<stderr>:#015 31%|███       | 3289/10570 [00:08<00:18, 391.75it/s][1,12]<stderr>:#015 32%|███▏      | 3400/10570 [00:08<00:17, 404.10it/s][1,7]<stderr>:#015 33%|███▎      | 3464/10570 [00:08<00:17, 413.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▉       | 3118/10570 [00:07<00:18, 399.35it/s]#033[A[1,15]<stderr>:#015 31%|███       | 3257/10570 [00:08<00:18, 386.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 25%|██▌       | 2659/10570 [00:07<00:21, 370.96it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 28%|██▊       | 2978/10570 [00:07<00:19, 394.73it/s][1,1]<stderr>:#015 31%|███▏      | 3312/10570 [00:08<00:18, 396.86it/s][1,14]<stderr>:#015 30%|███       | 3211/10570 [00:07<00:18, 394.21it/s][1,10]<stderr>:#015 31%|███▏      | 3324/10570 [00:08<00:18, 391.94it/s][1,3]<stderr>:#015 29%|██▉       | 3084/10570 [00:07<00:18, 395.80it/s][1,11]<stderr>:#015 29%|██▉       | 3081/10570 [00:07<00:18, 395.66it/s][1,2]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:17, 400.55it/s][1,5]<stderr>:#015 30%|███       | 3185/10570 [00:07<00:19, 388.00it/s][1,9]<stderr>:#015 31%|███       | 3239/10570 [00:08<00:18, 387.66it/s][1,13]<stderr>:#015 28%|██▊       | 2919/10570 [00:07<00:19, 391.15it/s][1,4]<stderr>:#015 32%|███▏      | 3330/10570 [00:08<00:18, 396.54it/s][1,12]<stderr>:#015 33%|███▎      | 3441/10570 [00:08<00:17, 405.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 30%|██▉       | 3158/10570 [00:07<00:18, 395.62it/s]#033[A[1,15]<stderr>:#015 31%|███       | 3297/10570 [00:08<00:18, 389.16it/s][1,7]<stderr>:#015 33%|███▎      | 3506/10570 [00:08<00:17, 404.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▌       | 2697/10570 [00:07<00:21, 372.49it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 29%|██▊       | 3018/10570 [00:07<00:19, 395.89it/s][1,1]<stderr>:#015 32%|███▏      | 3354/10570 [00:08<00:17, 401.19it/s][1,14]<stderr>:#015 31%|███       | 3251/10570 [00:07<00:18, 388.54it/s][1,10]<stderr>:#015 32%|███▏      | 3366/10570 [00:08<00:18, 397.61it/s][1,3]<stderr>:#015 30%|██▉       | 3124/10570 [00:07<00:18, 394.43it/s][1,11]<stderr>:#015 30%|██▉       | 3121/10570 [00:07<00:18, 395.14it/s][1,2]<stderr>:#015 32%|███▏      | 3429/10570 [00:08<00:17, 399.92it/s][1,5]<stderr>:#015 31%|███       | 3225/10570 [00:07<00:18, 391.05it/s][1,9]<stderr>:#015 31%|███       | 3278/10570 [00:08<00:18, 388.31it/s][1,13]<stderr>:#015 28%|██▊       | 2959/10570 [00:07<00:19, 389.77it/s][1,4]<stderr>:#015 32%|███▏      | 3372/10570 [00:08<00:17, 401.17it/s][1,15]<stderr>:#015 32%|███▏      | 3339/10570 [00:08<00:18, 395.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 30%|███       | 3198/10570 [00:07<00:18, 391.38it/s]#033[A[1,12]<stderr>:#015 33%|███▎      | 3482/10570 [00:08<00:17, 394.13it/s][1,7]<stderr>:#015 34%|███▎      | 3549/10570 [00:08<00:17, 410.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▌       | 2739/10570 [00:07<00:20, 383.90it/s]#033[A[1,6]<stderr>:#015 29%|██▉       | 3058/10570 [00:07<00:18, 395.67it/s][1,1]<stderr>:#015 32%|███▏      | 3395/10570 [00:08<00:17, 400.63it/s][1,14]<stderr>:#015 31%|███       | 3292/10570 [00:08<00:18, 393.44it/s][1,10]<stderr>:#015 32%|███▏      | 3408/10570 [00:08<00:17, 401.46it/s][1,11]<stderr>:#015 30%|██▉       | 3161/10570 [00:07<00:18, 391.07it/s][1,3]<stderr>:#015 30%|██▉       | 3164/10570 [00:07<00:19, 389.34it/s][1,2]<stderr>:#015 33%|███▎      | 3470/10570 [00:08<00:17, 402.04it/s][1,5]<stderr>:#015 31%|███       | 3265/10570 [00:08<00:18, 389.59it/s][1,9]<stderr>:#015 31%|███▏      | 3319/10570 [00:08<00:18, 392.85it/s][1,13]<stderr>:#015 28%|██▊       | 2999/10570 [00:07<00:19, 387.63it/s][1,4]<stderr>:#015 32%|███▏      | 3413/10570 [00:08<00:17, 400.82it/s][1,15]<stderr>:#015 32%|███▏      | 3380/10570 [00:08<00:17, 399.46it/s][1,12]<stderr>:#015 33%|███▎      | 3522/10570 [00:08<00:17, 394.91it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▋       | 2784/10570 [00:07<00:19, 399.69it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███       | 3238/10570 [00:08<00:18, 387.29it/s]#033[A[1,7]<stderr>:#015 34%|███▍      | 3591/10570 [00:08<00:17, 402.66it/s][1,6]<stderr>:#015 29%|██▉       | 3098/10570 [00:07<00:18, 393.41it/s][1,1]<stderr>:#015 33%|███▎      | 3436/10570 [00:08<00:17, 402.59it/s][1,14]<stderr>:#015 32%|███▏      | 3334/10570 [00:08<00:18, 398.78it/s][1,10]<stderr>:#015 33%|███▎      | 3449/10570 [00:08<00:17, 401.94it/s][1,3]<stderr>:#015 30%|███       | 3203/10570 [00:08<00:19, 384.46it/s][1,11]<stderr>:#015 30%|███       | 3201/10570 [00:08<00:19, 387.60it/s][1,2]<stderr>:#015 33%|███▎      | 3511/10570 [00:08<00:17, 393.83it/s][1,5]<stderr>:#015 31%|███▏      | 3305/10570 [00:08<00:18, 388.72it/s][1,9]<stderr>:#015 32%|███▏      | 3359/10570 [00:08<00:18, 392.09it/s][1,13]<stderr>:#015 29%|██▉       | 3039/10570 [00:07<00:19, 390.46it/s][1,4]<stderr>:#015 33%|███▎      | 3454/10570 [00:08<00:18, 394.25it/s][1,15]<stderr>:#015 32%|███▏      | 3420/10570 [00:08<00:17, 397.60it/s][1,12]<stderr>:#015 34%|███▎      | 3563/10570 [00:08<00:17, 397.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2825/10570 [00:07<00:19, 402.44it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███       | 3277/10570 [00:08<00:18, 385.31it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 34%|███▍      | 3632/10570 [00:08<00:17, 402.67it/s][1,6]<stderr>:#015 30%|██▉       | 3138/10570 [00:08<00:19, 387.99it/s][1,1]<stderr>:#015 33%|███▎      | 3477/10570 [00:08<00:17, 401.32it/s][1,14]<stderr>:#015 32%|███▏      | 3376/10570 [00:08<00:17, 402.34it/s][1,10]<stderr>:#015 33%|███▎      | 3490/10570 [00:08<00:17, 396.41it/s][1,11]<stderr>:#015 31%|███       | 3240/10570 [00:08<00:19, 382.64it/s][1,3]<stderr>:#015 31%|███       | 3242/10570 [00:08<00:19, 379.68it/s][1,2]<stderr>:#015 34%|███▎      | 3553/10570 [00:08<00:17, 399.96it/s][1,5]<stderr>:#015 32%|███▏      | 3347/10570 [00:08<00:18, 396.84it/s][1,9]<stderr>:#015 32%|███▏      | 3399/10570 [00:08<00:18, 394.25it/s][1,13]<stderr>:#015 29%|██▉       | 3079/10570 [00:08<00:19, 383.58it/s][1,15]<stderr>:#015 33%|███▎      | 3462/10570 [00:08<00:17, 402.67it/s][1,4]<stderr>:#015 33%|███▎      | 3494/10570 [00:08<00:18, 388.85it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███▏      | 3317/10570 [00:08<00:18, 389.02it/s]#033[A[1,12]<stderr>:#015 34%|███▍      | 3603/10570 [00:08<00:17, 390.71it/s][1,7]<stderr>:#015 35%|███▍      | 3673/10570 [00:08<00:17, 404.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2866/10570 [00:07<00:19, 393.72it/s]#033[A[1,6]<stderr>:#015 30%|███       | 3177/10570 [00:08<00:19, 381.28it/s][1,1]<stderr>:#015 33%|███▎      | 3518/10570 [00:08<00:17, 397.24it/s][1,14]<stderr>:#015 32%|███▏      | 3417/10570 [00:08<00:17, 399.75it/s][1,10]<stderr>:#015 33%|███▎      | 3532/10570 [00:08<00:17, 401.28it/s][1,11]<stderr>:#015 31%|███       | 3279/10570 [00:08<00:19, 382.48it/s][1,3]<stderr>:#015 31%|███       | 3281/10570 [00:08<00:19, 379.46it/s][1,2]<stderr>:#015 34%|███▍      | 3594/10570 [00:08<00:17, 393.57it/s][1,5]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:18, 398.72it/s][1,9]<stderr>:#015 33%|███▎      | 3439/10570 [00:08<00:18, 395.87it/s][1,13]<stderr>:#015 29%|██▉       | 3118/10570 [00:08<00:19, 383.74it/s][1,4]<stderr>:#015 33%|███▎      | 3536/10570 [00:08<00:17, 395.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 32%|███▏      | 3358/10570 [00:08<00:18, 394.34it/s]#033[A[1,15]<stderr>:#015 33%|███▎      | 3503/10570 [00:08<00:17, 394.76it/s][1,12]<stderr>:#015 34%|███▍      | 3645/10570 [00:08<00:17, 396.57it/s][1,7]<stderr>:#015 35%|███▌      | 3714/10570 [00:08<00:16, 405.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2906/10570 [00:07<00:19, 383.81it/s]#033[A[1,6]<stderr>:#015 30%|███       | 3216/10570 [00:08<00:19, 379.83it/s][1,1]<stderr>:#015 34%|███▎      | 3559/10570 [00:08<00:17, 399.30it/s][1,14]<stderr>:#015 33%|███▎      | 3460/10570 [00:08<00:17, 406.19it/s][1,10]<stderr>:#015 34%|███▍      | 3573/10570 [00:08<00:17, 397.92it/s][1,11]<stderr>:#015 31%|███▏      | 3319/10570 [00:08<00:18, 386.75it/s][1,3]<stderr>:#015 31%|███▏      | 3321/10570 [00:08<00:18, 385.20it/s][1,2]<stderr>:#015 34%|███▍      | 3635/10570 [00:08<00:17, 396.84it/s][1,5]<stderr>:#015 32%|███▏      | 3428/10570 [00:08<00:17, 397.37it/s][1,9]<stderr>:#015 33%|███▎      | 3479/10570 [00:08<00:17, 394.91it/s][1,13]<stderr>:#015 30%|██▉       | 3157/10570 [00:08<00:19, 379.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 32%|███▏      | 3398/10570 [00:08<00:18, 394.88it/s]#033[A[1,15]<stderr>:#015 34%|███▎      | 3544/10570 [00:08<00:17, 398.98it/s][1,4]<stderr>:#015 34%|███▍      | 3576/10570 [00:08<00:17, 391.32it/s][1,12]<stderr>:#015 35%|███▍      | 3686/10570 [00:09<00:17, 400.09it/s][1,7]<stderr>:#015 36%|███▌      | 3755/10570 [00:09<00:16, 405.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 28%|██▊       | 2945/10570 [00:07<00:19, 383.29it/s]#033[A[1,6]<stderr>:#015 31%|███       | 3255/10570 [00:08<00:19, 375.75it/s][1,1]<stderr>:#015 34%|███▍      | 3599/10570 [00:08<00:17, 392.50it/s][1,14]<stderr>:#015 33%|███▎      | 3501/10570 [00:08<00:17, 397.19it/s][1,10]<stderr>:#015 34%|███▍      | 3613/10570 [00:08<00:17, 395.22it/s][1,11]<stderr>:#015 32%|███▏      | 3360/10570 [00:08<00:18, 391.12it/s][1,3]<stderr>:#015 32%|███▏      | 3362/10570 [00:08<00:18, 389.86it/s][1,2]<stderr>:#015 35%|███▍      | 3676/10570 [00:09<00:17, 400.22it/s][1,5]<stderr>:#015 33%|███▎      | 3469/10570 [00:08<00:17, 399.20it/s][1,9]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:17, 392.03it/s][1,13]<stderr>:#015 30%|███       | 3195/10570 [00:08<00:19, 375.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3439/10570 [00:08<00:17, 396.51it/s]#033[A[1,4]<stderr>:#015 34%|███▍      | 3616/10570 [00:08<00:17, 390.35it/s][1,12]<stderr>:#015 35%|███▌      | 3727/10570 [00:09<00:17, 399.16it/s][1,7]<stderr>:#015 36%|███▌      | 3797/10570 [00:09<00:16, 406.11it/s][1,15]<stderr>:#015 34%|███▍      | 3584/10570 [00:08<00:17, 392.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 28%|██▊       | 2984/10570 [00:07<00:20, 378.82it/s]#033[A[1,6]<stderr>:#015 31%|███       | 3294/10570 [00:08<00:19, 378.45it/s][1,1]<stderr>:#015 34%|███▍      | 3640/10570 [00:08<00:17, 396.20it/s][1,14]<stderr>:#015 34%|███▎      | 3543/10570 [00:08<00:17, 401.69it/s][1,10]<stderr>:#015 35%|███▍      | 3655/10570 [00:09<00:17, 399.81it/s][1,11]<stderr>:#015 32%|███▏      | 3400/10570 [00:08<00:18, 391.64it/s][1,3]<stderr>:#015 32%|███▏      | 3402/10570 [00:08<00:18, 391.40it/s][1,2]<stderr>:#015 35%|███▌      | 3717/10570 [00:09<00:17, 401.32it/s][1,5]<stderr>:#015 33%|███▎      | 3509/10570 [00:08<00:17, 392.81it/s][1,9]<stderr>:#015 34%|███▎      | 3560/10570 [00:08<00:17, 394.38it/s][1,13]<stderr>:#015 31%|███       | 3233/10570 [00:08<00:19, 372.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3479/10570 [00:08<00:17, 393.96it/s]#033[A[1,12]<stderr>:#015 36%|███▌      | 3767/10570 [00:09<00:17, 398.08it/s][1,4]<stderr>:#015 35%|███▍      | 3658/10570 [00:09<00:17, 395.99it/s][1,7]<stderr>:#015 36%|███▋      | 3839/10570 [00:09<00:16, 408.38it/s][1,15]<stderr>:#015 34%|███▍      | 3624/10570 [00:08<00:17, 393.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▊       | 3023/10570 [00:08<00:19, 381.56it/s]#033[A[1,6]<stderr>:#015 32%|███▏      | 3334/10570 [00:08<00:18, 384.38it/s][1,1]<stderr>:#015 35%|███▍      | 3681/10570 [00:09<00:17, 399.29it/s][1,14]<stderr>:#015 34%|███▍      | 3584/10570 [00:08<00:17, 394.35it/s][1,11]<stderr>:#015 33%|███▎      | 3440/10570 [00:08<00:18, 392.63it/s][1,10]<stderr>:#015 35%|███▍      | 3696/10570 [00:09<00:17, 398.25it/s][1,3]<stderr>:#015 33%|███▎      | 3442/10570 [00:08<00:18, 390.51it/s][1,2]<stderr>:#015 36%|███▌      | 3758/10570 [00:09<00:16, 401.69it/s][1,5]<stderr>:#015 34%|███▎      | 3551/10570 [00:08<00:17, 398.39it/s][1,9]<stderr>:#015 34%|███▍      | 3600/10570 [00:09<00:17, 388.63it/s][1,13]<stderr>:#015 31%|███       | 3271/10570 [00:08<00:19, 372.70it/s][1,12]<stderr>:#015 36%|███▌      | 3807/10570 [00:09<00:16, 398.52it/s][1,4]<stderr>:#015 35%|███▍      | 3698/10570 [00:09<00:17, 395.28it/s][1,7]<stderr>:#015 37%|███▋      | 3880/10570 [00:09<00:16, 406.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:18, 390.56it/s]#033[A[1,15]<stderr>:#015 35%|███▍      | 3664/10570 [00:09<00:17, 392.54it/s][1,6]<stderr>:#015 32%|███▏      | 3374/10570 [00:08<00:18, 387.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▉       | 3062/10570 [00:08<00:19, 378.30it/s]#033[A[1,1]<stderr>:#015 35%|███▌      | 3722/10570 [00:09<00:17, 399.78it/s][1,14]<stderr>:#015 34%|███▍      | 3624/10570 [00:08<00:17, 395.05it/s][1,10]<stderr>:#015 35%|███▌      | 3737/10570 [00:09<00:17, 398.98it/s][1,3]<stderr>:#015 33%|███▎      | 3482/10570 [00:08<00:18, 388.06it/s][1,11]<stderr>:#015 33%|███▎      | 3480/10570 [00:08<00:18, 385.23it/s][1,2]<stderr>:#015 36%|███▌      | 3799/10570 [00:09<00:16, 401.47it/s][1,5]<stderr>:#015 34%|███▍      | 3591/10570 [00:08<00:17, 391.68it/s][1,9]<stderr>:#015 34%|███▍      | 3641/10570 [00:09<00:17, 392.75it/s][1,13]<stderr>:#015 31%|███▏      | 3309/10570 [00:08<00:19, 373.39it/s][1,12]<stderr>:#015 36%|███▋      | 3848/10570 [00:09<00:16, 399.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▎      | 3559/10570 [00:08<00:17, 392.65it/s]#033[A[1,15]<stderr>:#015 35%|███▌      | 3704/10570 [00:09<00:17, 394.29it/s][1,4]<stderr>:#015 35%|███▌      | 3739/10570 [00:09<00:17, 396.34it/s][1,7]<stderr>:#015 37%|███▋      | 3921/10570 [00:09<00:16, 403.61it/s][1,6]<stderr>:#015 32%|███▏      | 3413/10570 [00:08<00:18, 388.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▉       | 3100/10570 [00:08<00:19, 377.03it/s]#033[A[1,1]<stderr>:#015 36%|███▌      | 3763/10570 [00:09<00:16, 400.52it/s][1,14]<stderr>:#015 35%|███▍      | 3665/10570 [00:09<00:17, 397.06it/s][1,10]<stderr>:#015 36%|███▌      | 3777/10570 [00:09<00:17, 397.23it/s][1,3]<stderr>:#015 33%|███▎      | 3521/10570 [00:08<00:18, 385.65it/s][1,11]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:18, 383.32it/s][1,2]<stderr>:#015 36%|███▋      | 3840/10570 [00:09<00:16, 402.38it/s][1,5]<stderr>:#015 34%|███▍      | 3631/10570 [00:09<00:17, 393.69it/s][1,9]<stderr>:#015 35%|███▍      | 3682/10570 [00:09<00:17, 395.93it/s][1,13]<stderr>:#015 32%|███▏      | 3349/10570 [00:08<00:19, 379.35it/s][1,12]<stderr>:#015 37%|███▋      | 3889/10570 [00:09<00:16, 401.60it/s][1,4]<stderr>:#015 36%|███▌      | 3782/10570 [00:09<00:16, 404.95it/s][1,7]<stderr>:#015 37%|███▋      | 3962/10570 [00:09<00:16, 404.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▍      | 3599/10570 [00:09<00:18, 386.65it/s]#033[A[1,15]<stderr>:#015 35%|███▌      | 3744/10570 [00:09<00:17, 383.89it/s][1,6]<stderr>:#015 33%|███▎      | 3452/10570 [00:08<00:18, 385.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|██▉       | 3138/10570 [00:08<00:19, 373.51it/s]#033[A[1,1]<stderr>:#015 36%|███▌      | 3804/10570 [00:09<00:16, 399.59it/s][1,14]<stderr>:#015 35%|███▌      | 3706/10570 [00:09<00:17, 398.95it/s][1,10]<stderr>:#015 36%|███▌      | 3817/10570 [00:09<00:17, 393.93it/s][1,3]<stderr>:#015 34%|███▎      | 3561/10570 [00:09<00:18, 386.74it/s][1,11]<stderr>:#015 34%|███▎      | 3559/10570 [00:09<00:18, 386.02it/s][1,2]<stderr>:#015 37%|███▋      | 3881/10570 [00:09<00:16, 403.67it/s][1,5]<stderr>:#015 35%|███▍      | 3672/10570 [00:09<00:17, 395.81it/s][1,9]<stderr>:#015 35%|███▌      | 3722/10570 [00:09<00:17, 396.36it/s][1,13]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:18, 380.32it/s][1,12]<stderr>:#015 37%|███▋      | 3930/10570 [00:09<00:16, 400.08it/s][1,7]<stderr>:#015 38%|███▊      | 4003/10570 [00:09<00:16, 404.14it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▍      | 3640/10570 [00:09<00:17, 391.02it/s]#033[A[1,4]<stderr>:#015 36%|███▌      | 3823/10570 [00:09<00:17, 396.56it/s][1,15]<stderr>:#015 36%|███▌      | 3787/10570 [00:09<00:17, 395.01it/s][1,6]<stderr>:#015 33%|███▎      | 3491/10570 [00:08<00:18, 378.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|███       | 3176/10570 [00:08<00:20, 366.91it/s]#033[A[1,1]<stderr>:#015 36%|███▋      | 3845/10570 [00:09<00:16, 399.62it/s][1,14]<stderr>:#015 35%|███▌      | 3746/10570 [00:09<00:17, 396.77it/s][1,10]<stderr>:#015 36%|███▋      | 3858/10570 [00:09<00:16, 397.18it/s][1,3]<stderr>:#015 34%|███▍      | 3600/10570 [00:09<00:18, 379.57it/s][1,2]<stderr>:#015 37%|███▋      | 3922/10570 [00:09<00:16, 399.74it/s][1,11]<stderr>:#015 34%|███▍      | 3598/10570 [00:09<00:18, 375.22it/s][1,5]<stderr>:#015 35%|███▌      | 3713/10570 [00:09<00:17, 397.96it/s][1,9]<stderr>:#015 36%|███▌      | 3762/10570 [00:09<00:17, 396.68it/s][1,13]<stderr>:#015 32%|███▏      | 3427/10570 [00:09<00:18, 377.98it/s][1,12]<stderr>:#015 38%|███▊      | 3971/10570 [00:09<00:16, 401.45it/s][1,7]<stderr>:#015 38%|███▊      | 4044/10570 [00:09<00:16, 405.23it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 35%|███▍      | 3680/10570 [00:09<00:17, 393.45it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3864/10570 [00:09<00:16, 399.27it/s][1,15]<stderr>:#015 36%|███▌      | 3827/10570 [00:09<00:17, 391.96it/s][1,6]<stderr>:#015 33%|███▎      | 3531/10570 [00:09<00:18, 384.27it/s][1,1]<stderr>:#015 37%|███▋      | 3886/10570 [00:09<00:16, 401.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|███       | 3213/10570 [00:08<00:20, 364.12it/s]#033[A[1,14]<stderr>:#015 36%|███▌      | 3789/10570 [00:09<00:16, 405.38it/s][1,10]<stderr>:#015 37%|███▋      | 3899/10570 [00:09<00:16, 397.83it/s][1,3]<stderr>:#015 34%|███▍      | 3640/10570 [00:09<00:18, 384.28it/s][1,2]<stderr>:#015 37%|███▋      | 3963/10570 [00:09<00:16, 400.08it/s][1,11]<stderr>:#015 34%|███▍      | 3638/10570 [00:09<00:18, 380.62it/s][1,5]<stderr>:#015 36%|███▌      | 3753/10570 [00:09<00:17, 396.42it/s][1,9]<stderr>:#015 36%|███▌      | 3802/10570 [00:09<00:17, 395.85it/s][1,13]<stderr>:#015 33%|███▎      | 3466/10570 [00:09<00:18, 380.85it/s][1,7]<stderr>:#015 39%|███▊      | 4088/10570 [00:09<00:15, 414.31it/s][1,12]<stderr>:#015 38%|███▊      | 4012/10570 [00:09<00:16, 399.99it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 35%|███▌      | 3720/10570 [00:09<00:17, 393.36it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3904/10570 [00:09<00:16, 397.64it/s][1,15]<stderr>:#015 37%|███▋      | 3867/10570 [00:09<00:17, 380.88it/s][1,6]<stderr>:#015 34%|███▍      | 3570/10570 [00:09<00:18, 381.02it/s][1,1]<stderr>:#015 37%|███▋      | 3927/10570 [00:09<00:16, 397.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███       | 3250/10570 [00:08<00:20, 357.93it/s][1,8]<stderr>:#033[A[1,14]<stderr>:#015 36%|███▌      | 3830/10570 [00:09<00:16, 401.07it/s][1,10]<stderr>:#015 37%|███▋      | 3939/10570 [00:09<00:16, 396.26it/s][1,3]<stderr>:#015 35%|███▍      | 3680/10570 [00:09<00:17, 387.28it/s][1,11]<stderr>:#015 35%|███▍      | 3678/10570 [00:09<00:17, 383.77it/s][1,5]<stderr>:#015 36%|███▌      | 3794/10570 [00:09<00:16, 400.38it/s][1,2]<stderr>:#015 38%|███▊      | 4004/10570 [00:09<00:16, 393.42it/s][1,9]<stderr>:#015 36%|███▋      | 3842/10570 [00:09<00:16, 395.98it/s][1,12]<stderr>:#015 38%|███▊      | 4053/10570 [00:09<00:16, 402.31it/s][1,13]<stderr>:#015 33%|███▎      | 3505/10570 [00:09<00:18, 373.80it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▌      | 3760/10570 [00:09<00:17, 393.54it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3944/10570 [00:09<00:16, 395.66it/s][1,15]<stderr>:#015 37%|███▋      | 3906/10570 [00:09<00:17, 382.56it/s][1,7]<stderr>:#015 39%|███▉      | 4130/10570 [00:10<00:16, 388.98it/s][1,6]<stderr>:#015 34%|███▍      | 3609/10570 [00:09<00:18, 377.54it/s][1,1]<stderr>:#015 38%|███▊      | 3968/10570 [00:09<00:16, 398.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███       | 3288/10570 [00:08<00:20, 362.03it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3871/10570 [00:09<00:16, 401.56it/s][1,10]<stderr>:#015 38%|███▊      | 3980/10570 [00:09<00:16, 399.90it/s][1,3]<stderr>:#015 35%|███▌      | 3719/10570 [00:09<00:17, 387.63it/s][1,11]<stderr>:#015 35%|███▌      | 3718/10570 [00:09<00:17, 385.99it/s][1,2]<stderr>:#015 38%|███▊      | 4045/10570 [00:09<00:16, 396.28it/s][1,5]<stderr>:#015 36%|███▋      | 3835/10570 [00:09<00:16, 398.47it/s][1,9]<stderr>:#015 37%|███▋      | 3883/10570 [00:09<00:16, 398.22it/s][1,12]<stderr>:#015 39%|███▊      | 4095/10570 [00:10<00:15, 406.47it/s][1,13]<stderr>:#015 34%|███▎      | 3544/10570 [00:09<00:18, 378.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▌      | 3800/10570 [00:09<00:17, 393.00it/s]#033[A[1,4]<stderr>:#015 38%|███▊      | 3984/10570 [00:09<00:16, 395.67it/s][1,15]<stderr>:#015 37%|███▋      | 3945/10570 [00:09<00:17, 383.75it/s][1,6]<stderr>:#015 35%|███▍      | 3649/10570 [00:09<00:18, 382.41it/s][1,1]<stderr>:#015 38%|███▊      | 4008/10570 [00:09<00:16, 396.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███▏      | 3326/10570 [00:08<00:19, 365.86it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3912/10570 [00:09<00:16, 398.81it/s][1,10]<stderr>:#015 38%|███▊      | 4021/10570 [00:09<00:16, 398.10it/s][1,3]<stderr>:#015 36%|███▌      | 3758/10570 [00:09<00:17, 387.36it/s][1,11]<stderr>:#015 36%|███▌      | 3757/10570 [00:09<00:17, 386.04it/s][1,5]<stderr>:#015 37%|███▋      | 3875/10570 [00:09<00:16, 398.35it/s][1,2]<stderr>:#015 39%|███▊      | 4089/10570 [00:10<00:15, 406.19it/s][1,9]<stderr>:#015 37%|███▋      | 3923/10570 [00:09<00:16, 393.86it/s][1,7]<stderr>:#015 39%|███▉      | 4170/10570 [00:10<00:19, 322.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▋      | 3840/10570 [00:09<00:17, 393.61it/s]#033[A[1,13]<stderr>:#015 34%|███▍      | 3582/10570 [00:09<00:18, 371.59it/s][1,4]<stderr>:#015 38%|███▊      | 4024/10570 [00:09<00:16, 392.41it/s][1,12]<stderr>:#015 39%|███▉      | 4136/10570 [00:10<00:16, 383.06it/s][1,15]<stderr>:#015 38%|███▊      | 3986/10570 [00:09<00:16, 389.62it/s][1,6]<stderr>:#015 35%|███▍      | 3689/10570 [00:09<00:17, 384.89it/s][1,1]<stderr>:#015 38%|███▊      | 4049/10570 [00:10<00:16, 397.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 32%|███▏      | 3365/10570 [00:08<00:19, 370.17it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3952/10570 [00:09<00:16, 398.09it/s][1,10]<stderr>:#015 38%|███▊      | 4063/10570 [00:10<00:16, 402.45it/s][1,3]<stderr>:#015 36%|███▌      | 3797/10570 [00:09<00:17, 386.61it/s][1,11]<stderr>:#015 36%|███▌      | 3796/10570 [00:09<00:17, 379.95it/s][1,5]<stderr>:#015 37%|███▋      | 3915/10570 [00:09<00:16, 395.50it/s][1,9]<stderr>:#015 37%|███▋      | 3963/10570 [00:09<00:16, 394.08it/s][1,2]<stderr>:#015 39%|███▉      | 4130/10570 [00:10<00:16, 381.02it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3880/10570 [00:09<00:16, 395.36it/s][1,0]<stderr>:#033[A[1,4]<stderr>:#015 38%|███▊      | 4066/10570 [00:10<00:16, 398.42it/s][1,13]<stderr>:#015 34%|███▍      | 3620/10570 [00:09<00:19, 365.61it/s][1,15]<stderr>:#015 38%|███▊      | 4026/10570 [00:10<00:16, 386.09it/s][1,6]<stderr>:#015 35%|███▌      | 3728/10570 [00:09<00:17, 383.07it/s][1,1]<stderr>:#015 39%|███▊      | 4092/10570 [00:10<00:16, 404.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 32%|███▏      | 3403/10570 [00:09<00:19, 372.26it/s]#033[A[1,14]<stderr>:#015 38%|███▊      | 3992/10570 [00:09<00:16, 396.87it/s][1,7]<stderr>:#015 40%|███▉      | 4205/10570 [00:10<00:22, 283.52it/s][1,3]<stderr>:#015 36%|███▋      | 3837/10570 [00:09<00:17, 389.34it/s][1,10]<stderr>:#015 39%|███▉      | 4104/10570 [00:10<00:16, 394.74it/s][1,5]<stderr>:#015 37%|███▋      | 3955/10570 [00:09<00:16, 395.22it/s][1,11]<stderr>:#015 36%|███▋      | 3836/10570 [00:09<00:17, 383.40it/s][1,9]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:16, 388.19it/s][1,12]<stderr>:#015 39%|███▉      | 4175/10570 [00:10<00:20, 316.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3920/10570 [00:09<00:16, 391.82it/s]#033[A[1,13]<stderr>:#015 35%|███▍      | 3659/10570 [00:09<00:18, 369.74it/s][1,4]<stderr>:#015 39%|███▉      | 4106/10570 [00:10<00:16, 388.87it/s][1,15]<stderr>:#015 38%|███▊      | 4067/10570 [00:10<00:16, 390.88it/s][1,6]<stderr>:#015 36%|███▌      | 3768/10570 [00:09<00:17, 386.86it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3441/10570 [00:09<00:19, 371.02it/s]#033[A[1,7]<stderr>:#015 40%|████      | 4239/10570 [00:10<00:21, 298.08it/s][1,14]<stderr>:#015 38%|███▊      | 4032/10570 [00:09<00:16, 393.88it/s][1,3]<stderr>:#015 37%|███▋      | 3876/10570 [00:09<00:17, 389.19it/s][1,1]<stderr>:#015 39%|███▉      | 4133/10570 [00:10<00:16, 379.94it/s][1,2]<stderr>:#015 39%|███▉      | 4169/10570 [00:10<00:20, 315.54it/s][1,11]<stderr>:#015 37%|███▋      | 3875/10570 [00:09<00:17, 385.22it/s][1,5]<stderr>:#015 38%|███▊      | 3995/10570 [00:09<00:16, 394.94it/s][1,10]<stderr>:#015 39%|███▉      | 4144/10570 [00:10<00:17, 366.39it/s][1,9]<stderr>:#015 38%|███▊      | 4043/10570 [00:10<00:16, 390.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3960/10570 [00:09<00:16, 391.82it/s]#033[A[1,13]<stderr>:#015 35%|███▍      | 3697/10570 [00:09<00:18, 370.88it/s][1,6]<stderr>:#015 36%|███▌      | 3807/10570 [00:09<00:17, 384.68it/s][1,15]<stderr>:#015 39%|███▉      | 4107/10570 [00:10<00:16, 380.66it/s][1,4]<stderr>:#015 39%|███▉      | 4145/10570 [00:10<00:17, 358.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3479/10570 [00:09<00:19, 364.04it/s]#033[A[1,14]<stderr>:#015 39%|███▊      | 4075/10570 [00:10<00:16, 402.40it/s][1,12]<stderr>:#015 40%|███▉      | 4209/10570 [00:10<00:23, 272.72it/s][1,3]<stderr>:#015 37%|███▋      | 3915/10570 [00:09<00:17, 385.60it/s][1,11]<stderr>:#015 37%|███▋      | 3914/10570 [00:09<00:17, 383.56it/s][1,5]<stderr>:#015 38%|███▊      | 4035/10570 [00:10<00:16, 394.17it/s][1,9]<stderr>:#015 39%|███▊      | 4086/10570 [00:10<00:16, 400.57it/s][1,2]<stderr>:#015 40%|███▉      | 4203/10570 [00:10<00:22, 281.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 38%|███▊      | 4000/10570 [00:10<00:16, 391.19it/s]#033[A[1,7]<stderr>:#015 40%|████      | 4271/10570 [00:10<00:24, 256.90it/s][1,13]<stderr>:#015 35%|███▌      | 3735/10570 [00:09<00:18, 373.31it/s][1,1]<stderr>:#015 39%|███▉      | 4172/10570 [00:10<00:20, 312.49it/s][1,6]<stderr>:#015 36%|███▋      | 3846/10570 [00:09<00:17, 384.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3516/10570 [00:09<00:19, 361.27it/s]#033[A[1,12]<stderr>:#015 40%|████      | 4247/10570 [00:10<00:21, 297.83it/s][1,3]<stderr>:#015 37%|███▋      | 3954/10570 [00:10<00:17, 385.04it/s][1,10]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:21, 302.72it/s][1,14]<stderr>:#015 39%|███▉      | 4116/10570 [00:10<00:16, 387.82it/s][1,11]<stderr>:#015 37%|███▋      | 3953/10570 [00:10<00:17, 384.00it/s][1,5]<stderr>:#015 39%|███▊      | 4078/10570 [00:10<00:16, 401.86it/s][1,15]<stderr>:#015 39%|███▉      | 4146/10570 [00:10<00:18, 345.79it/s][1,9]<stderr>:#015 39%|███▉      | 4127/10570 [00:10<00:16, 379.97it/s][1,4]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:20, 304.54it/s][1,2]<stderr>:#015 40%|████      | 4234/10570 [00:10<00:22, 288.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 38%|███▊      | 4040/10570 [00:10<00:16, 391.51it/s]#033[A[1,13]<stderr>:#015 36%|███▌      | 3775/10570 [00:09<00:17, 379.44it/s][1,6]<stderr>:#015 37%|███▋      | 3886/10570 [00:09<00:17, 386.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▎      | 3554/10570 [00:09<00:19, 366.27it/s][1,8]<stderr>:#033[A[1,3]<stderr>:#015 38%|███▊      | 3993/10570 [00:10<00:17, 381.17it/s][1,7]<stderr>:#015 41%|████      | 4299/10570 [00:10<00:27, 225.82it/s][1,11]<stderr>:#015 38%|███▊      | 3993/10570 [00:10<00:17, 386.26it/s][1,5]<stderr>:#015 39%|███▉      | 4119/10570 [00:10<00:16, 386.74it/s][1,1]<stderr>:#015 40%|███▉      | 4206/10570 [00:10<00:23, 273.86it/s][1,10]<stderr>:#015 40%|███▉      | 4215/10570 [00:10<00:23, 270.30it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▊      | 4082/10570 [00:10<00:16, 399.54it/s]#033[A[1,13]<stderr>:#015 36%|███▌      | 3813/10570 [00:10<00:18, 374.28it/s][1,15]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:21, 299.92it/s][1,14]<stderr>:#015 39%|███▉      | 4155/10570 [00:10<00:19, 322.96it/s][1,2]<stderr>:#015 40%|████      | 4265/10570 [00:10<00:23, 270.87it/s][1,6]<stderr>:#015 37%|███▋      | 3925/10570 [00:10<00:17, 382.31it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▍      | 3591/10570 [00:09<00:19, 360.04it/s]#033[A[1,4]<stderr>:#015 40%|███▉      | 4215/10570 [00:10<00:23, 270.49it/s][1,12]<stderr>:#015 40%|████      | 4280/10570 [00:10<00:26, 234.29it/s][1,3]<stderr>:#015 38%|███▊      | 4032/10570 [00:10<00:17, 380.03it/s][1,7]<stderr>:#015 41%|████      | 4325/10570 [00:10<00:26, 234.71it/s][1,11]<stderr>:#015 38%|███▊      | 4032/10570 [00:10<00:17, 383.87it/s][1,9]<stderr>:#015 39%|███▉      | 4166/10570 [00:10<00:20, 313.92it/s][1,1]<stderr>:#015 40%|████      | 4241/10570 [00:10<00:21, 292.37it/s][1,10]<stderr>:#015 40%|████      | 4253/10570 [00:10<00:21, 294.39it/s][1,13]<stderr>:#015 36%|███▋      | 3851/10570 [00:10<00:17, 374.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▉      | 4122/10570 [00:10<00:16, 382.15it/s]#033[A[1,6]<stderr>:#015 38%|███▊      | 3964/10570 [00:10<00:17, 382.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▍      | 3628/10570 [00:09<00:19, 362.59it/s][1,8]<stderr>:#033[A[1,5]<stderr>:#015 39%|███▉      | 4158/10570 [00:10<00:20, 318.61it/s][1,4]<stderr>:#015 40%|████      | 4253/10570 [00:10<00:21, 294.28it/s][1,3]<stderr>:#015 39%|███▊      | 4074/10570 [00:10<00:16, 389.02it/s][1,7]<stderr>:#015 41%|████▏     | 4366/10570 [00:10<00:23, 268.70it/s][1,11]<stderr>:#015 39%|███▊      | 4074/10570 [00:10<00:16, 392.84it/s][1,14]<stderr>:#015 40%|███▉      | 4190/10570 [00:10<00:21, 294.82it/s][1,15]<stderr>:#015 40%|███▉      | 4214/10570 [00:10<00:23, 265.23it/s][1,12]<stderr>:#015 41%|████      | 4308/10570 [00:10<00:29, 212.78it/s][1,13]<stderr>:#015 37%|███▋      | 3889/10570 [00:10<00:17, 373.52it/s][1,2]<stderr>:#015 41%|████      | 4294/10570 [00:10<00:28, 222.59it/s][1,9]<stderr>:#015 40%|███▉      | 4200/10570 [00:10<00:22, 278.74it/s][1,6]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:17, 381.70it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▍      | 3665/10570 [00:09<00:18, 363.79it/s]#033[A[1,1]<stderr>:#015 40%|████      | 4273/10570 [00:10<00:25, 247.67it/s][1,7]<stderr>:#015 42%|████▏     | 4407/10570 [00:11<00:20, 298.26it/s][1,3]<stderr>:#015 39%|███▉      | 4113/10570 [00:10<00:17, 372.52it/s][1,15]<stderr>:#015 40%|████      | 4251/10570 [00:10<00:21, 289.45it/s][1,11]<stderr>:#015 39%|███▉      | 4114/10570 [00:10<00:17, 376.32it/s][1,14]<stderr>:#015 40%|███▉      | 4222/10570 [00:10<00:22, 283.94it/s][1,5]<stderr>:#015 40%|███▉      | 4192/10570 [00:10<00:22, 288.30it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▉      | 4161/10570 [00:10<00:20, 312.99it/s]#033[A[1,12]<stderr>:#015 41%|████      | 4348/10570 [00:11<00:25, 246.50it/s][1,13]<stderr>:#015 37%|███▋      | 3927/10570 [00:10<00:17, 372.12it/s][1,9]<stderr>:#015 40%|████      | 4231/10570 [00:10<00:22, 284.73it/s][1,2]<stderr>:#015 41%|████      | 4319/10570 [00:11<00:28, 221.31it/s][1,6]<stderr>:#015 38%|███▊      | 4042/10570 [00:10<00:17, 382.42it/s][1,10]<stderr>:#015 41%|████      | 4285/10570 [00:10<00:28, 217.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▌      | 3702/10570 [00:09<00:18, 364.45it/s]#033[A[1,7]<stderr>:#015 42%|████▏     | 4449/10570 [00:11<00:18, 326.06it/s][1,4]<stderr>:#015 41%|████      | 4285/10570 [00:10<00:28, 222.31it/s][1,14]<stderr>:#015 40%|████      | 4257/10570 [00:10<00:21, 300.37it/s][1,5]<stderr>:#015 40%|███▉      | 4223/10570 [00:10<00:22, 279.50it/s][1,12]<stderr>:#015 42%|████▏     | 4388/10570 [00:11<00:22, 277.95it/s][1,13]<stderr>:#015 38%|███▊      | 3965/10570 [00:10<00:17, 368.32it/s][1,1]<stderr>:#015 41%|████      | 4301/10570 [00:11<00:29, 215.70it/s][1,2]<stderr>:#015 41%|████      | 4360/10570 [00:11<00:24, 255.55it/s][1,3]<stderr>:#015 39%|███▉      | 4151/10570 [00:10<00:20, 314.15it/s][1,6]<stderr>:#015 39%|███▊      | 4084/10570 [00:10<00:16, 391.11it/s][1,9]<stderr>:#015 40%|████      | 4262/10570 [00:10<00:22, 281.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|███▉      | 4195/10570 [00:10<00:22, 283.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▌      | 3739/10570 [00:09<00:18, 364.73it/s]#033[A[1,11]<stderr>:#015 39%|███▉      | 4152/10570 [00:10<00:20, 311.70it/s][1,7]<stderr>:#015 42%|████▏     | 4492/10570 [00:11<00:17, 350.72it/s][1,10]<stderr>:#015 41%|████      | 4312/10570 [00:11<00:29, 211.72it/s][1,15]<stderr>:#015 41%|████      | 4282/10570 [00:11<00:28, 222.56it/s][1,4]<stderr>:#015 41%|████      | 4312/10570 [00:11<00:29, 215.21it/s][1,5]<stderr>:#015 40%|████      | 4258/10570 [00:10<00:21, 293.61it/s][1,12]<stderr>:#015 42%|████▏     | 4429/10570 [00:11<00:20, 306.41it/s][1,13]<stderr>:#015 38%|███▊      | 4002/10570 [00:10<00:17, 368.45it/s][1,1]<stderr>:#015 41%|████      | 4332/10570 [00:11<00:26, 236.83it/s][1,2]<stderr>:#015 42%|████▏     | 4400/10570 [00:11<00:21, 285.33it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▌      | 3779/10570 [00:10<00:18, 373.00it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|███▉      | 4226/10570 [00:10<00:22, 276.77it/s]#033[A[1,6]<stderr>:#015 39%|███▉      | 4124/10570 [00:10<00:17, 371.47it/s][1,3]<stderr>:#015 40%|███▉      | 4185/10570 [00:10<00:22, 290.16it/s][1,7]<stderr>:#015 43%|████▎     | 4530/10570 [00:11<00:17, 349.41it/s][1,10]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:25, 245.90it/s][1,11]<stderr>:#015 40%|███▉      | 4186/10570 [00:10<00:22, 289.41it/s][1,14]<stderr>:#015 41%|████      | 4289/10570 [00:10<00:27, 228.54it/s][1,4]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:24, 249.04it/s][1,12]<stderr>:#015 42%|████▏     | 4470/10570 [00:11<00:18, 331.18it/s][1,13]<stderr>:#015 38%|███▊      | 4039/10570 [00:10<00:17, 368.77it/s][1,1]<stderr>:#015 41%|████▏     | 4372/10570 [00:11<00:22, 269.82it/s][1,15]<stderr>:#015 41%|████      | 4308/10570 [00:11<00:30, 206.53it/s][1,2]<stderr>:#015 42%|████▏     | 4442/10570 [00:11<00:19, 314.20it/s][1,9]<stderr>:#015 41%|████      | 4292/10570 [00:11<00:28, 221.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▌      | 3817/10570 [00:10<00:18, 364.98it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|████      | 4259/10570 [00:10<00:21, 287.83it/s]#033[A[1,7]<stderr>:#015 43%|████▎     | 4570/10570 [00:11<00:16, 361.72it/s][1,10]<stderr>:#015 42%|████▏     | 4392/10570 [00:11<00:22, 276.68it/s][1,3]<stderr>:#015 40%|███▉      | 4216/10570 [00:10<00:24, 264.13it/s][1,4]<stderr>:#015 42%|████▏     | 4392/10570 [00:11<00:22, 279.40it/s][1,12]<stderr>:#015 43%|████▎     | 4508/10570 [00:11<00:17, 343.39it/s][1,13]<stderr>:#015 39%|███▊      | 4079/10570 [00:10<00:17, 377.50it/s][1,11]<stderr>:#015 40%|███▉      | 4217/10570 [00:10<00:23, 266.35it/s][1,1]<stderr>:#015 42%|████▏     | 4412/10570 [00:11<00:20, 297.69it/s][1,15]<stderr>:#015 41%|████      | 4347/10570 [00:11<00:25, 239.40it/s][1,2]<stderr>:#015 42%|████▏     | 4485/10570 [00:11<00:17, 340.92it/s][1,14]<stderr>:#015 41%|████      | 4316/10570 [00:11<00:28, 220.83it/s][1,6]<stderr>:#015 39%|███▉      | 4162/10570 [00:10<00:21, 302.71it/s][1,5]<stderr>:#015 41%|████      | 4289/10570 [00:11<00:28, 217.61it/s][1,9]<stderr>:#015 41%|████      | 4317/10570 [00:11<00:28, 218.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▋      | 3854/10570 [00:10<00:18, 365.83it/s]#033[A[1,10]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:20, 305.15it/s][1,7]<stderr>:#015 44%|████▎     | 4608/10570 [00:11<00:16, 352.05it/s][1,3]<stderr>:#015 40%|████      | 4252/10570 [00:11<00:22, 286.97it/s][1,4]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:19, 307.49it/s][1,12]<stderr>:#015 43%|████▎     | 4545/10570 [00:11<00:17, 347.30it/s][1,1]<stderr>:#015 42%|████▏     | 4453/10570 [00:11<00:18, 324.21it/s][1,11]<stderr>:#015 40%|████      | 4253/10570 [00:11<00:21, 287.32it/s][1,15]<stderr>:#015 41%|████▏     | 4386/10570 [00:11<00:22, 270.40it/s][1,13]<stderr>:#015 39%|███▉      | 4117/10570 [00:10<00:17, 362.45it/s][1,14]<stderr>:#015 41%|████      | 4356/10570 [00:11<00:24, 254.78it/s][1,2]<stderr>:#015 43%|████▎     | 4522/10570 [00:11<00:17, 342.19it/s][1,9]<stderr>:#015 41%|████      | 4357/10570 [00:11<00:24, 252.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 37%|███▋      | 3892/10570 [00:10<00:18, 367.34it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4289/10570 [00:11<00:28, 220.21it/s]#033[A[1,5]<stderr>:#015 41%|████      | 4315/10570 [00:11<00:29, 211.74it/s][1,10]<stderr>:#015 42%|████▏     | 4476/10570 [00:11<00:18, 333.22it/s][1,6]<stderr>:#015 40%|███▉      | 4195/10570 [00:10<00:23, 273.93it/s][1,7]<stderr>:#015 44%|████▍     | 4645/10570 [00:11<00:16, 348.62it/s][1,4]<stderr>:#015 42%|████▏     | 4476/10570 [00:11<00:18, 334.81it/s][1,12]<stderr>:#015 43%|████▎     | 4584/10570 [00:11<00:16, 357.35it/s][1,1]<stderr>:#015 43%|████▎     | 4495/10570 [00:11<00:17, 345.78it/s][1,15]<stderr>:#015 42%|████▏     | 4426/10570 [00:11<00:20, 297.87it/s][1,14]<stderr>:#015 42%|████▏     | 4396/10570 [00:11<00:21, 284.81it/s][1,2]<stderr>:#015 43%|████▎     | 4560/10570 [00:11<00:17, 351.94it/s][1,9]<stderr>:#015 42%|████▏     | 4396/10570 [00:11<00:21, 281.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 37%|███▋      | 3929/10570 [00:10<00:18, 365.14it/s]#033[A[1,5]<stderr>:#015 41%|████      | 4355/10570 [00:11<00:25, 245.73it/s][1,10]<stderr>:#015 43%|████▎     | 4513/10570 [00:11<00:18, 331.01it/s][1,7]<stderr>:#015 44%|████▍     | 4682/10570 [00:11<00:16, 352.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4315/10570 [00:11<00:29, 212.65it/s]#033[A[1,6]<stderr>:#015 40%|███▉      | 4225/10570 [00:11<00:23, 266.99it/s][1,13]<stderr>:#015 39%|███▉      | 4154/10570 [00:11<00:21, 299.46it/s][1,4]<stderr>:#015 43%|████▎     | 4513/10570 [00:11<00:17, 340.88it/s][1,12]<stderr>:#015 44%|████▎     | 4621/10570 [00:11<00:16, 350.94it/s][1,15]<stderr>:#015 42%|████▏     | 4468/10570 [00:11<00:18, 325.23it/s][1,3]<stderr>:#015 41%|████      | 4283/10570 [00:11<00:29, 216.58it/s][1,14]<stderr>:#015 42%|████▏     | 4437/10570 [00:11<00:19, 312.83it/s][1,1]<stderr>:#015 43%|████▎     | 4533/10570 [00:11<00:17, 345.10it/s][1,2]<stderr>:#015 43%|████▎     | 4597/10570 [00:11<00:17, 348.97it/s][1,11]<stderr>:#015 41%|████      | 4284/10570 [00:11<00:29, 214.35it/s][1,9]<stderr>:#015 42%|████▏     | 4437/10570 [00:11<00:19, 309.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 3966/10570 [00:10<00:18, 366.00it/s]#033[A[1,5]<stderr>:#015 42%|████▏     | 4395/10570 [00:11<00:22, 276.39it/s][1,10]<stderr>:#015 43%|████▎     | 4549/10570 [00:11<00:17, 338.50it/s][1,7]<stderr>:#015 45%|████▍     | 4725/10570 [00:11<00:15, 370.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4354/10570 [00:11<00:25, 246.21it/s]#033[A[1,6]<stderr>:#015 40%|████      | 4258/10570 [00:11<00:22, 281.09it/s][1,4]<stderr>:#015 43%|████▎     | 4550/10570 [00:11<00:17, 345.10it/s][1,15]<stderr>:#015 43%|████▎     | 4505/10570 [00:11<00:18, 336.53it/s][1,1]<stderr>:#015 43%|████▎     | 4572/10570 [00:11<00:16, 355.80it/s][1,13]<stderr>:#015 40%|███▉      | 4186/10570 [00:11<00:22, 280.10it/s][1,14]<stderr>:#015 42%|████▏     | 4480/10570 [00:11<00:17, 339.01it/s][1,12]<stderr>:#015 44%|████▍     | 4658/10570 [00:11<00:17, 343.11it/s][1,2]<stderr>:#015 44%|████▍     | 4634/10570 [00:11<00:16, 354.54it/s][1,9]<stderr>:#015 42%|████▏     | 4479/10570 [00:11<00:18, 335.85it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:18, 359.35it/s]#033[A[1,5]<stderr>:#015 42%|████▏     | 4436/10570 [00:11<00:20, 304.74it/s][1,3]<stderr>:#015 41%|████      | 4309/10570 [00:11<00:30, 205.05it/s][1,11]<stderr>:#015 41%|████      | 4310/10570 [00:11<00:30, 207.18it/s][1,10]<stderr>:#015 43%|████▎     | 4587/10570 [00:11<00:17, 349.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4393/10570 [00:11<00:22, 276.37it/s]#033[A[1,7]<stderr>:#015 45%|████▌     | 4763/10570 [00:12<00:16, 361.63it/s][1,4]<stderr>:#015 43%|████▎     | 4589/10570 [00:11<00:16, 355.11it/s][1,14]<stderr>:#015 43%|████▎     | 4517/10570 [00:11<00:17, 343.36it/s][1,12]<stderr>:#015 44%|████▍     | 4698/10570 [00:11<00:16, 357.24it/s][1,15]<stderr>:#015 43%|████▎     | 4542/10570 [00:11<00:17, 339.00it/s][1,1]<stderr>:#015 44%|████▎     | 4609/10570 [00:11<00:17, 346.40it/s][1,9]<stderr>:#015 43%|████▎     | 4516/10570 [00:11<00:17, 340.20it/s][1,2]<stderr>:#015 44%|████▍     | 4671/10570 [00:11<00:17, 339.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 4040/10570 [00:10<00:18, 362.02it/s]#033[A[1,13]<stderr>:#015 40%|███▉      | 4216/10570 [00:11<00:24, 257.78it/s][1,3]<stderr>:#015 41%|████      | 4347/10570 [00:11<00:26, 237.65it/s][1,5]<stderr>:#015 42%|████▏     | 4479/10570 [00:11<00:18, 332.03it/s][1,11]<stderr>:#015 41%|████      | 4348/10570 [00:11<00:25, 239.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:20, 304.12it/s]#033[A[1,10]<stderr>:#015 44%|████▎     | 4624/10570 [00:11<00:17, 343.16it/s][1,7]<stderr>:#015 45%|████▌     | 4800/10570 [00:12<00:15, 363.71it/s][1,6]<stderr>:#015 41%|████      | 4288/10570 [00:11<00:29, 213.89it/s][1,4]<stderr>:#015 44%|████▍     | 4626/10570 [00:11<00:17, 348.04it/s][1,14]<stderr>:#015 43%|████▎     | 4554/10570 [00:11<00:17, 349.89it/s][1,15]<stderr>:#015 43%|████▎     | 4579/10570 [00:11<00:17, 347.32it/s][1,12]<stderr>:#015 45%|████▍     | 4739/10570 [00:12<00:15, 369.55it/s][1,1]<stderr>:#015 44%|████▍     | 4645/10570 [00:11<00:17, 342.73it/s][1,9]<stderr>:#015 43%|████▎     | 4553/10570 [00:11<00:17, 345.97it/s][1,2]<stderr>:#015 45%|████▍     | 4714/10570 [00:12<00:16, 360.26it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▊      | 4080/10570 [00:10<00:17, 371.32it/s]#033[A[1,13]<stderr>:#015 40%|████      | 4251/10570 [00:11<00:22, 279.79it/s][1,3]<stderr>:#015 41%|████▏     | 4386/10570 [00:11<00:23, 268.02it/s][1,5]<stderr>:#015 43%|████▎     | 4516/10570 [00:11<00:17, 337.87it/s][1,11]<stderr>:#015 42%|████▏     | 4387/10570 [00:11<00:22, 269.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4475/10570 [00:11<00:18, 330.98it/s]#033[A[1,7]<stderr>:#015 46%|████▌     | 4843/10570 [00:12<00:15, 380.62it/s][1,10]<stderr>:#015 44%|████▍     | 4660/10570 [00:12<00:17, 335.87it/s][1,14]<stderr>:#015 43%|████▎     | 4593/10570 [00:11<00:16, 359.09it/s][1,4]<stderr>:#015 44%|████▍     | 4662/10570 [00:12<00:17, 338.73it/s][1,15]<stderr>:#015 44%|████▎     | 4615/10570 [00:11<00:17, 341.35it/s][1,6]<stderr>:#015 41%|████      | 4313/10570 [00:11<00:30, 203.49it/s][1,1]<stderr>:#015 44%|████▍     | 4681/10570 [00:12<00:16, 346.84it/s][1,12]<stderr>:#015 45%|████▌     | 4777/10570 [00:12<00:16, 353.10it/s][1,9]<stderr>:#015 43%|████▎     | 4591/10570 [00:11<00:16, 354.96it/s][1,2]<stderr>:#015 45%|████▍     | 4751/10570 [00:12<00:16, 361.92it/s][1,3]<stderr>:#015 42%|████▏     | 4425/10570 [00:11<00:20, 294.77it/s][1,5]<stderr>:#015 43%|████▎     | 4553/10570 [00:11<00:17, 344.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▉      | 4118/10570 [00:11<00:18, 353.87it/s]#033[A[1,11]<stderr>:#015 42%|████▏     | 4426/10570 [00:11<00:20, 296.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4512/10570 [00:11<00:17, 337.22it/s]#033[A[1,10]<stderr>:#015 44%|████▍     | 4700/10570 [00:12<00:16, 351.10it/s][1,7]<stderr>:#015 46%|████▌     | 4882/10570 [00:12<00:15, 360.83it/s][1,4]<stderr>:#015 44%|████▍     | 4702/10570 [00:12<00:16, 354.11it/s][1,14]<stderr>:#015 44%|████▍     | 4631/10570 [00:11<00:16, 351.88it/s][1,6]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:26, 236.62it/s][1,1]<stderr>:#015 45%|████▍     | 4723/10570 [00:12<00:16, 363.96it/s][1,15]<stderr>:#015 44%|████▍     | 4651/10570 [00:12<00:17, 335.03it/s][1,12]<stderr>:#015 46%|████▌     | 4818/10570 [00:12<00:15, 366.92it/s][1,9]<stderr>:#015 44%|████▍     | 4628/10570 [00:12<00:17, 346.84it/s][1,3]<stderr>:#015 42%|████▏     | 4466/10570 [00:11<00:19, 321.25it/s][1,5]<stderr>:#015 43%|████▎     | 4591/10570 [00:11<00:16, 353.84it/s][1,13]<stderr>:#015 41%|████      | 4281/10570 [00:11<00:29, 215.81it/s][1,2]<stderr>:#015 45%|████▌     | 4788/10570 [00:12<00:16, 343.94it/s][1,11]<stderr>:#015 42%|████▏     | 4467/10570 [00:11<00:18, 322.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4548/10570 [00:11<00:17, 340.45it/s]#033[A[1,10]<stderr>:#015 45%|████▍     | 4737/10570 [00:12<00:16, 354.71it/s][1,7]<stderr>:#015 47%|████▋     | 4923/10570 [00:12<00:15, 373.20it/s][1,4]<stderr>:#015 45%|████▍     | 4742/10570 [00:12<00:15, 366.73it/s][1,6]<stderr>:#015 42%|████▏     | 4390/10570 [00:11<00:23, 266.55it/s][1,15]<stderr>:#015 44%|████▍     | 4688/10570 [00:12<00:17, 344.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▉      | 4154/10570 [00:11<00:22, 287.05it/s]#033[A[1,1]<stderr>:#015 45%|████▌     | 4760/10570 [00:12<00:16, 353.61it/s][1,14]<stderr>:#015 44%|████▍     | 4668/10570 [00:11<00:17, 340.24it/s][1,3]<stderr>:#015 43%|████▎     | 4504/10570 [00:11<00:18, 334.80it/s][1,12]<stderr>:#015 46%|████▌     | 4856/10570 [00:12<00:16, 350.21it/s][1,2]<stderr>:#015 46%|████▌     | 4831/10570 [00:12<00:15, 364.56it/s][1,9]<stderr>:#015 44%|████▍     | 4664/10570 [00:12<00:17, 337.11it/s][1,11]<stderr>:#015 43%|████▎     | 4504/10570 [00:11<00:18, 335.52it/s][1,5]<stderr>:#015 44%|████▍     | 4628/10570 [00:11<00:17, 346.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4586/10570 [00:11<00:17, 350.30it/s]#033[A[1,10]<stderr>:#015 45%|████▌     | 4773/10570 [00:12<00:16, 342.31it/s][1,7]<stderr>:#015 47%|████▋     | 4964/10570 [00:12<00:14, 380.98it/s][1,13]<stderr>:#015 41%|████      | 4306/10570 [00:11<00:31, 195.97it/s][1,6]<stderr>:#015 42%|████▏     | 4429/10570 [00:11<00:20, 293.61it/s][1,15]<stderr>:#015 45%|████▍     | 4729/10570 [00:12<00:16, 361.50it/s][1,4]<stderr>:#015 45%|████▌     | 4780/10570 [00:12<00:16, 348.08it/s][1,1]<stderr>:#015 45%|████▌     | 4797/10570 [00:12<00:16, 356.65it/s][1,14]<stderr>:#015 45%|████▍     | 4710/10570 [00:12<00:16, 358.90it/s][1,12]<stderr>:#015 46%|████▋     | 4898/10570 [00:12<00:15, 367.75it/s][1,3]<stderr>:#015 43%|████▎     | 4540/10570 [00:11<00:17, 336.51it/s][1,9]<stderr>:#015 45%|████▍     | 4704/10570 [00:12<00:16, 352.51it/s][1,11]<stderr>:#015 43%|████▎     | 4540/10570 [00:12<00:17, 336.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|███▉      | 4185/10570 [00:11<00:23, 269.08it/s]#033[A[1,5]<stderr>:#015 44%|████▍     | 4664/10570 [00:12<00:17, 337.66it/s][1,2]<stderr>:#015 46%|████▌     | 4869/10570 [00:12<00:16, 349.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▎     | 4623/10570 [00:12<00:17, 343.20it/s]#033[A[1,10]<stderr>:#015 46%|████▌     | 4812/10570 [00:12<00:16, 354.00it/s][1,7]<stderr>:#015 47%|████▋     | 5003/10570 [00:12<00:14, 376.84it/s][1,13]<stderr>:#015 41%|████      | 4342/10570 [00:11<00:27, 226.54it/s][1,6]<stderr>:#015 42%|████▏     | 4470/10570 [00:11<00:19, 320.56it/s][1,4]<stderr>:#015 46%|████▌     | 4821/10570 [00:12<00:15, 364.47it/s][1,1]<stderr>:#015 46%|████▌     | 4840/10570 [00:12<00:15, 373.86it/s][1,14]<stderr>:#015 45%|████▍     | 4750/10570 [00:12<00:15, 369.00it/s][1,15]<stderr>:#015 45%|████▌     | 4766/10570 [00:12<00:16, 350.51it/s][1,12]<stderr>:#015 47%|████▋     | 4939/10570 [00:12<00:14, 377.51it/s][1,3]<stderr>:#015 43%|████▎     | 4577/10570 [00:12<00:17, 344.36it/s][1,9]<stderr>:#015 45%|████▍     | 4744/10570 [00:12<00:15, 364.72it/s][1,11]<stderr>:#015 43%|████▎     | 4577/10570 [00:12<00:17, 344.28it/s][1,5]<stderr>:#015 45%|████▍     | 4704/10570 [00:12<00:16, 353.47it/s][1,2]<stderr>:#015 46%|████▋     | 4910/10570 [00:12<00:15, 364.02it/s][1,10]<stderr>:#015 46%|████▌     | 4851/10570 [00:12<00:15, 361.66it/s][1,7]<stderr>:#015 48%|████▊     | 5042/10570 [00:12<00:14, 379.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▍     | 4659/10570 [00:12<00:18, 328.07it/s]#033[A[1,13]<stderr>:#015 41%|████▏     | 4380/10570 [00:11<00:24, 257.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|███▉      | 4214/10570 [00:11<00:26, 242.87it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 43%|████▎     | 4506/10570 [00:12<00:18, 331.08it/s][1,15]<stderr>:#015 45%|████▌     | 4803/10570 [00:12<00:16, 354.98it/s][1,12]<stderr>:#015 47%|████▋     | 4978/10570 [00:12<00:14, 379.96it/s][1,4]<stderr>:#015 46%|████▌     | 4859/10570 [00:12<00:16, 347.06it/s][1,1]<stderr>:#015 46%|████▌     | 4878/10570 [00:12<00:16, 352.55it/s][1,14]<stderr>:#015 45%|████▌     | 4788/10570 [00:12<00:16, 349.08it/s][1,3]<stderr>:#015 44%|████▎     | 4613/10570 [00:12<00:17, 336.76it/s][1,5]<stderr>:#015 45%|████▍     | 4744/10570 [00:12<00:15, 365.02it/s][1,11]<stderr>:#015 44%|████▎     | 4613/10570 [00:12<00:17, 336.63it/s][1,2]<stderr>:#015 47%|████▋     | 4948/10570 [00:12<00:15, 363.96it/s][1,9]<stderr>:#015 45%|████▌     | 4782/10570 [00:12<00:16, 345.27it/s][1,7]<stderr>:#015 48%|████▊     | 5081/10570 [00:12<00:14, 380.95it/s][1,13]<stderr>:#015 42%|████▏     | 4417/10570 [00:12<00:21, 283.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▍     | 4697/10570 [00:12<00:17, 340.53it/s]#033[A[1,10]<stderr>:#015 46%|████▌     | 4888/10570 [00:12<00:16, 353.17it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|████      | 4249/10570 [00:11<00:23, 266.50it/s]#033[A[1,6]<stderr>:#015 43%|████▎     | 4542/10570 [00:12<00:18, 333.10it/s][1,15]<stderr>:#015 46%|████▌     | 4845/10570 [00:12<00:15, 370.16it/s][1,12]<stderr>:#015 47%|████▋     | 5017/10570 [00:12<00:14, 374.90it/s][1,4]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 364.34it/s][1,1]<stderr>:#015 47%|████▋     | 4919/10570 [00:12<00:15, 366.40it/s][1,14]<stderr>:#015 46%|████▌     | 4831/10570 [00:12<00:15, 368.06it/s][1,3]<stderr>:#015 44%|████▍     | 4648/10570 [00:12<00:17, 331.23it/s][1,2]<stderr>:#015 47%|████▋     | 4987/10570 [00:12<00:15, 370.07it/s][1,9]<stderr>:#015 46%|████▌     | 4823/10570 [00:12<00:15, 362.04it/s][1,11]<stderr>:#015 44%|████▍     | 4648/10570 [00:12<00:17, 331.53it/s][1,5]<stderr>:#015 45%|████▌     | 4782/10570 [00:12<00:16, 345.30it/s][1,7]<stderr>:#015 48%|████▊     | 5122/10570 [00:12<00:14, 388.59it/s][1,13]<stderr>:#015 42%|████▏     | 4456/10570 [00:12<00:19, 308.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 45%|████▍     | 4737/10570 [00:12<00:16, 355.94it/s]#033[A[1,10]<stderr>:#015 47%|████▋     | 4929/10570 [00:12<00:15, 366.31it/s][1,6]<stderr>:#015 43%|████▎     | 4578/10570 [00:12<00:17, 340.68it/s][1,12]<stderr>:#015 48%|████▊     | 5057/10570 [00:12<00:14, 381.56it/s][1,4]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 373.38it/s][1,1]<stderr>:#015 47%|████▋     | 4959/10570 [00:12<00:15, 373.83it/s][1,15]<stderr>:#015 46%|████▌     | 4883/10570 [00:12<00:16, 351.43it/s][1,3]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 336.99it/s][1,14]<stderr>:#015 46%|████▌     | 4869/10570 [00:12<00:16, 352.34it/s][1,11]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 337.53it/s][1,2]<stderr>:#015 48%|████▊     | 5025/10570 [00:12<00:15, 365.43it/s][1,5]<stderr>:#015 46%|████▌     | 4823/10570 [00:12<00:15, 362.24it/s][1,9]<stderr>:#015 46%|████▌     | 4860/10570 [00:12<00:16, 344.56it/s][1,7]<stderr>:#015 49%|████▉     | 5163/10570 [00:13<00:13, 393.26it/s][1,13]<stderr>:#015 43%|████▎     | 4494/10570 [00:12<00:18, 324.56it/s][1,10]<stderr>:#015 47%|████▋     | 4968/10570 [00:12<00:15, 372.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|████      | 4278/10570 [00:11<00:29, 211.24it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 45%|████▌     | 4774/10570 [00:12<00:16, 341.86it/s]#033[A[1,6]<stderr>:#015 44%|████▎     | 4614/10570 [00:12<00:17, 334.13it/s][1,12]<stderr>:#015 48%|████▊     | 5096/10570 [00:13<00:14, 381.98it/s][1,4]<stderr>:#015 47%|████▋     | 4979/10570 [00:12<00:14, 374.87it/s][1,15]<stderr>:#015 47%|████▋     | 4920/10570 [00:12<00:15, 356.64it/s][1,1]<stderr>:#015 47%|████▋     | 4997/10570 [00:12<00:15, 371.33it/s][1,3]<stderr>:#015 45%|████▍     | 4725/10570 [00:12<00:16, 353.66it/s][1,14]<stderr>:#015 46%|████▋     | 4910/10570 [00:12<00:15, 365.82it/s][1,11]<stderr>:#015 45%|████▍     | 4725/10570 [00:12<00:16, 354.57it/s][1,2]<stderr>:#015 48%|████▊     | 5066/10570 [00:13<00:14, 375.86it/s][1,9]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 361.83it/s][1,7]<stderr>:#015 49%|████▉     | 5204/10570 [00:13<00:13, 396.56it/s][1,10]<stderr>:#015 47%|████▋     | 5006/10570 [00:12<00:15, 368.49it/s][1,5]<stderr>:#015 46%|████▌     | 4860/10570 [00:12<00:16, 344.54it/s][1,13]<stderr>:#015 43%|████▎     | 4529/10570 [00:12<00:18, 323.29it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4813/10570 [00:12<00:16, 353.06it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 44%|████▍     | 4649/10570 [00:12<00:18, 327.92it/s][1,12]<stderr>:#015 49%|████▊     | 5139/10570 [00:13<00:13, 392.21it/s][1,4]<stderr>:#015 47%|████▋     | 5017/10570 [00:12<00:15, 369.39it/s][1,15]<stderr>:#015 47%|████▋     | 4960/10570 [00:12<00:15, 366.09it/s][1,1]<stderr>:#015 48%|████▊     | 5035/10570 [00:13<00:14, 372.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████      | 4303/10570 [00:11<00:33, 187.06it/s]#033[A[1,14]<stderr>:#015 47%|████▋     | 4948/10570 [00:12<00:15, 364.37it/s][1,3]<stderr>:#015 45%|████▌     | 4761/10570 [00:12<00:16, 342.85it/s][1,2]<stderr>:#015 48%|████▊     | 5104/10570 [00:13<00:14, 370.51it/s][1,9]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 371.04it/s][1,11]<stderr>:#015 45%|████▌     | 4762/10570 [00:12<00:16, 344.74it/s][1,7]<stderr>:#015 50%|████▉     | 5244/10570 [00:13<00:13, 393.11it/s][1,5]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 361.79it/s][1,10]<stderr>:#015 48%|████▊     | 5045/10570 [00:13<00:14, 372.49it/s][1,13]<stderr>:#015 43%|████▎     | 4566/10570 [00:12<00:17, 334.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4851/10570 [00:12<00:15, 359.06it/s]#033[A[1,6]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 333.87it/s][1,12]<stderr>:#015 49%|████▉     | 5179/10570 [00:13<00:13, 391.92it/s][1,15]<stderr>:#015 47%|████▋     | 4997/10570 [00:13<00:15, 365.52it/s][1,4]<stderr>:#015 48%|████▊     | 5057/10570 [00:13<00:14, 376.04it/s][1,1]<stderr>:#015 48%|████▊     | 5075/10570 [00:13<00:14, 377.92it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████      | 4333/10570 [00:12<00:29, 210.62it/s]#033[A[1,14]<stderr>:#015 47%|████▋     | 4987/10570 [00:12<00:15, 370.62it/s][1,3]<stderr>:#015 45%|████▌     | 4796/10570 [00:12<00:16, 343.52it/s][1,2]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 379.81it/s][1,9]<stderr>:#015 47%|████▋     | 4979/10570 [00:13<00:14, 373.09it/s][1,11]<stderr>:#015 45%|████▌     | 4797/10570 [00:12<00:16, 345.81it/s][1,7]<stderr>:#015 50%|█████     | 5285/10570 [00:13<00:13, 397.61it/s][1,5]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 370.34it/s][1,10]<stderr>:#015 48%|████▊     | 5085/10570 [00:13<00:14, 377.28it/s][1,13]<stderr>:#015 44%|████▎     | 4601/10570 [00:12<00:18, 323.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4888/10570 [00:12<00:16, 349.35it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 45%|████▍     | 4724/10570 [00:12<00:16, 350.99it/s][1,12]<stderr>:#015 49%|████▉     | 5220/10570 [00:13<00:13, 394.75it/s][1,15]<stderr>:#015 48%|████▊     | 5034/10570 [00:13<00:15, 366.30it/s][1,4]<stderr>:#015 48%|████▊     | 5095/10570 [00:13<00:14, 376.29it/s][1,1]<stderr>:#015 48%|████▊     | 5115/10570 [00:13<00:14, 384.01it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████▏     | 4370/10570 [00:12<00:25, 241.37it/s]#033[A[1,3]<stderr>:#015 46%|████▌     | 4838/10570 [00:12<00:15, 361.27it/s][1,14]<stderr>:#015 48%|████▊     | 5025/10570 [00:12<00:15, 366.17it/s][1,2]<stderr>:#015 49%|████▉     | 5185/10570 [00:13<00:14, 383.38it/s][1,11]<stderr>:#015 46%|████▌     | 4838/10570 [00:12<00:15, 362.34it/s][1,9]<stderr>:#015 47%|████▋     | 5017/10570 [00:13<00:15, 367.61it/s][1,7]<stderr>:#015 50%|█████     | 5328/10570 [00:13<00:12, 406.47it/s][1,5]<stderr>:#015 47%|████▋     | 4979/10570 [00:12<00:14, 372.74it/s][1,10]<stderr>:#015 48%|████▊     | 5126/10570 [00:13<00:14, 385.39it/s][1,13]<stderr>:#015 44%|████▍     | 4638/10570 [00:12<00:17, 335.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 4928/10570 [00:12<00:15, 361.97it/s]#033[A[1,6]<stderr>:#015 45%|████▌     | 4760/10570 [00:12<00:17, 339.66it/s][1,12]<stderr>:#015 50%|████▉     | 5260/10570 [00:13<00:13, 394.14it/s][1,4]<stderr>:#015 49%|████▊     | 5137/10570 [00:13<00:14, 387.19it/s][1,15]<stderr>:#015 48%|████▊     | 5072/10570 [00:13<00:14, 368.09it/s][1,1]<stderr>:#015 49%|████▉     | 5155/10570 [00:13<00:14, 386.02it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4406/10570 [00:12<00:23, 267.54it/s]#033[A[1,14]<stderr>:#015 48%|████▊     | 5066/10570 [00:13<00:14, 376.35it/s][1,2]<stderr>:#015 49%|████▉     | 5226/10570 [00:13<00:13, 388.99it/s][1,9]<stderr>:#015 48%|████▊     | 5056/10570 [00:13<00:14, 373.91it/s][1,7]<stderr>:#015 51%|█████     | 5369/10570 [00:13<00:12, 403.33it/s][1,3]<stderr>:#015 46%|████▌     | 4875/10570 [00:12<00:16, 341.55it/s][1,10]<stderr>:#015 49%|████▉     | 5166/10570 [00:13<00:13, 388.89it/s][1,11]<stderr>:#015 46%|████▌     | 4875/10570 [00:12<00:16, 342.68it/s][1,5]<stderr>:#015 47%|████▋     | 5017/10570 [00:13<00:15, 368.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 4967/10570 [00:13<00:15, 367.57it/s]#033[A[1,13]<stderr>:#015 44%|████▍     | 4673/10570 [00:12<00:18, 318.68it/s][1,12]<stderr>:#015 50%|█████     | 5303/10570 [00:13<00:13, 403.02it/s][1,6]<stderr>:#015 45%|████▌     | 4795/10570 [00:12<00:16, 341.03it/s][1,4]<stderr>:#015 49%|████▉     | 5176/10570 [00:13<00:13, 386.19it/s][1,15]<stderr>:#015 48%|████▊     | 5112/10570 [00:13<00:14, 374.87it/s][1,1]<stderr>:#015 49%|████▉     | 5195/10570 [00:13<00:13, 389.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4445/10570 [00:12<00:20, 293.86it/s]#033[A[1,2]<stderr>:#015 50%|████▉     | 5266/10570 [00:13<00:13, 389.61it/s][1,14]<stderr>:#015 48%|████▊     | 5104/10570 [00:13<00:14, 373.52it/s][1,9]<stderr>:#015 48%|████▊     | 5094/10570 [00:13<00:14, 373.96it/s][1,7]<stderr>:#015 51%|█████     | 5411/10570 [00:13<00:12, 406.37it/s][1,3]<stderr>:#015 46%|████▋     | 4911/10570 [00:13<00:16, 345.75it/s][1,10]<stderr>:#015 49%|████▉     | 5206/10570 [00:13<00:13, 391.33it/s][1,11]<stderr>:#015 46%|████▋     | 4914/10570 [00:13<00:15, 355.10it/s][1,5]<stderr>:#015 48%|████▊     | 5057/10570 [00:13<00:14, 374.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 5005/10570 [00:13<00:15, 363.97it/s]#033[A[1,13]<stderr>:#015 45%|████▍     | 4714/10570 [00:12<00:17, 339.81it/s][1,6]<stderr>:#015 46%|████▌     | 4836/10570 [00:12<00:15, 358.99it/s][1,12]<stderr>:#015 51%|█████     | 5344/10570 [00:13<00:12, 403.69it/s][1,4]<stderr>:#015 49%|████▉     | 5216/10570 [00:13<00:13, 387.83it/s][1,1]<stderr>:#015 50%|████▉     | 5235/10570 [00:13<00:13, 392.36it/s][1,15]<stderr>:#015 49%|████▊     | 5152/10570 [00:13<00:14, 379.17it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4485/10570 [00:12<00:19, 317.89it/s][1,8]<stderr>:#033[A[1,2]<stderr>:#015 50%|█████     | 5309/10570 [00:13<00:13, 399.57it/s][1,14]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 381.76it/s][1,9]<stderr>:#015 49%|████▊     | 5133/10570 [00:13<00:14, 377.25it/s][1,3]<stderr>:#015 47%|████▋     | 4949/10570 [00:13<00:15, 354.95it/s][1,10]<stderr>:#015 50%|████▉     | 5246/10570 [00:13<00:13, 391.50it/s][1,11]<stderr>:#015 47%|████▋     | 4953/10570 [00:13<00:15, 362.74it/s][1,5]<stderr>:#015 48%|████▊     | 5095/10570 [00:13<00:14, 373.64it/s][1,7]<stderr>:#015 52%|█████▏    | 5452/10570 [00:13<00:13, 382.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5043/10570 [00:13<00:15, 366.97it/s]#033[A[1,13]<stderr>:#015 45%|████▍     | 4751/10570 [00:13<00:16, 347.20it/s][1,12]<stderr>:#015 51%|█████     | 5385/10570 [00:13<00:12, 402.14it/s][1,1]<stderr>:#015 50%|████▉     | 5275/10570 [00:13<00:13, 393.21it/s][1,4]<stderr>:#015 50%|████▉     | 5255/10570 [00:13<00:13, 386.33it/s][1,15]<stderr>:#015 49%|████▉     | 5192/10570 [00:13<00:13, 384.64it/s][1,6]<stderr>:#015 46%|████▌     | 4873/10570 [00:13<00:16, 340.02it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4520/10570 [00:12<00:19, 314.98it/s]#033[A[1,14]<stderr>:#015 49%|████▉     | 5185/10570 [00:13<00:13, 384.64it/s][1,2]<stderr>:#015 51%|█████     | 5350/10570 [00:13<00:13, 397.53it/s][1,9]<stderr>:#015 49%|████▉     | 5173/10570 [00:13<00:14, 381.45it/s][1,3]<stderr>:#015 47%|████▋     | 4987/10570 [00:13<00:15, 359.64it/s][1,10]<stderr>:#015 50%|█████     | 5287/10570 [00:13<00:13, 395.90it/s][1,11]<stderr>:#015 47%|████▋     | 4991/10570 [00:13<00:15, 364.67it/s][1,5]<stderr>:#015 49%|████▊     | 5136/10570 [00:13<00:14, 383.85it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5082/10570 [00:13<00:14, 372.14it/s]#033[A[1,7]<stderr>:#015 52%|█████▏    | 5491/10570 [00:13<00:13, 363.44it/s][1,1]<stderr>:#015 50%|█████     | 5317/10570 [00:13<00:13, 399.61it/s][1,4]<stderr>:#015 50%|█████     | 5297/10570 [00:13<00:13, 394.38it/s][1,12]<stderr>:#015 51%|█████▏    | 5426/10570 [00:13<00:12, 399.15it/s][1,15]<stderr>:#015 50%|████▉     | 5233/10570 [00:13<00:13, 388.85it/s][1,13]<stderr>:#015 45%|████▌     | 4787/10570 [00:13<00:17, 327.38it/s][1,6]<stderr>:#015 46%|████▋     | 4912/10570 [00:13<00:16, 351.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4554/10570 [00:12<00:18, 320.29it/s]#033[A[1,14]<stderr>:#015 49%|████▉     | 5226/10570 [00:13<00:13, 390.09it/s][1,2]<stderr>:#015 51%|█████     | 5392/10570 [00:13<00:12, 400.81it/s][1,9]<stderr>:#015 49%|████▉     | 5213/10570 [00:13<00:13, 384.28it/s][1,10]<stderr>:#015 50%|█████     | 5329/10570 [00:13<00:13, 402.53it/s][1,3]<stderr>:#015 48%|████▊     | 5024/10570 [00:13<00:15, 354.13it/s][1,5]<stderr>:#015 49%|████▉     | 5175/10570 [00:13<00:14, 384.88it/s][1,11]<stderr>:#015 48%|████▊     | 5028/10570 [00:13<00:15, 359.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5122/10570 [00:13<00:14, 378.22it/s]#033[A[1,7]<stderr>:#015 52%|█████▏    | 5528/10570 [00:14<00:13, 364.28it/s][1,4]<stderr>:#015 51%|█████     | 5339/10570 [00:13<00:13, 399.94it/s][1,1]<stderr>:#015 51%|█████     | 5358/10570 [00:13<00:13, 399.45it/s][1,15]<stderr>:#015 50%|████▉     | 5273/10570 [00:13<00:13, 389.44it/s][1,13]<stderr>:#015 46%|████▌     | 4827/10570 [00:13<00:16, 345.54it/s][1,6]<stderr>:#015 47%|████▋     | 4950/10570 [00:13<00:15, 359.43it/s][1,12]<stderr>:#015 52%|█████▏    | 5466/10570 [00:13<00:13, 374.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4588/10570 [00:12<00:18, 322.16it/s]#033[A[1,14]<stderr>:#015 50%|████▉     | 5266/10570 [00:13<00:13, 390.84it/s][1,2]<stderr>:#015 51%|█████▏    | 5433/10570 [00:13<00:12, 398.07it/s][1,9]<stderr>:#015 50%|████▉     | 5252/10570 [00:13<00:13, 383.20it/s][1,3]<stderr>:#015 48%|████▊     | 5063/10570 [00:13<00:15, 363.37it/s][1,10]<stderr>:#015 51%|█████     | 5370/10570 [00:13<00:13, 397.37it/s][1,5]<stderr>:#015 49%|████▉     | 5215/10570 [00:13<00:13, 386.40it/s][1,11]<stderr>:#015 48%|████▊     | 5067/10570 [00:13<00:14, 367.75it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 49%|████▉     | 5161/10570 [00:13<00:14, 381.66it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5566/10570 [00:14<00:13, 368.83it/s][1,1]<stderr>:#015 51%|█████     | 5399/10570 [00:13<00:12, 400.90it/s][1,15]<stderr>:#015 50%|█████     | 5315/10570 [00:13<00:13, 397.28it/s][1,4]<stderr>:#015 51%|█████     | 5380/10570 [00:13<00:13, 396.98it/s][1,6]<stderr>:#015 47%|████▋     | 4988/10570 [00:13<00:15, 362.84it/s][1,13]<stderr>:#015 46%|████▌     | 4863/10570 [00:13<00:17, 328.10it/s][1,14]<stderr>:#015 50%|█████     | 5309/10570 [00:13<00:13, 400.61it/s][1,12]<stderr>:#015 52%|█████▏    | 5504/10570 [00:14<00:13, 362.10it/s][1,9]<stderr>:#015 50%|█████     | 5294/10570 [00:13<00:13, 392.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▎     | 4622/10570 [00:12<00:18, 315.07it/s][1,8]<stderr>:#033[A[1,3]<stderr>:#015 48%|████▊     | 5100/10570 [00:13<00:15, 364.09it/s][1,10]<stderr>:#015 51%|█████     | 5411/10570 [00:13<00:12, 400.61it/s][1,5]<stderr>:#015 50%|████▉     | 5254/10570 [00:13<00:13, 385.98it/s][1,11]<stderr>:#015 48%|████▊     | 5105/10570 [00:13<00:14, 368.88it/s][1,2]<stderr>:#015 52%|█████▏    | 5473/10570 [00:14<00:13, 370.70it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 49%|████▉     | 5201/10570 [00:13<00:13, 384.58it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5605/10570 [00:14<00:13, 373.90it/s][1,15]<stderr>:#015 51%|█████     | 5355/10570 [00:13<00:13, 396.44it/s][1,4]<stderr>:#015 51%|█████▏    | 5420/10570 [00:13<00:12, 396.83it/s][1,1]<stderr>:#015 51%|█████▏    | 5440/10570 [00:14<00:12, 398.16it/s][1,6]<stderr>:#015 48%|████▊     | 5025/10570 [00:13<00:15, 355.26it/s][1,13]<stderr>:#015 46%|████▋     | 4902/10570 [00:13<00:16, 343.34it/s][1,12]<stderr>:#015 52%|█████▏    | 5541/10570 [00:14<00:13, 363.13it/s][1,14]<stderr>:#015 51%|█████     | 5350/10570 [00:13<00:13, 398.21it/s][1,9]<stderr>:#015 50%|█████     | 5336/10570 [00:13<00:13, 398.57it/s][1,3]<stderr>:#015 49%|████▊     | 5140/10570 [00:13<00:14, 373.14it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▍     | 4655/10570 [00:13<00:19, 308.00it/s]#033[A[1,5]<stderr>:#015 50%|█████     | 5296/10570 [00:13<00:13, 394.36it/s][1,11]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 375.22it/s][1,10]<stderr>:#015 52%|█████▏    | 5452/10570 [00:14<00:13, 376.80it/s][1,2]<stderr>:#015 52%|█████▏    | 5511/10570 [00:14<00:14, 355.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|████▉     | 5241/10570 [00:13<00:13, 386.85it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5643/10570 [00:14<00:13, 371.66it/s][1,15]<stderr>:#015 51%|█████     | 5396/10570 [00:14<00:13, 397.81it/s][1,4]<stderr>:#015 52%|█████▏    | 5460/10570 [00:14<00:13, 372.32it/s][1,6]<stderr>:#015 48%|████▊     | 5064/10570 [00:13<00:15, 363.89it/s][1,13]<stderr>:#015 47%|████▋     | 4940/10570 [00:13<00:16, 351.78it/s][1,1]<stderr>:#015 52%|█████▏    | 5480/10570 [00:14<00:13, 365.20it/s][1,14]<stderr>:#015 51%|█████     | 5391/10570 [00:13<00:12, 401.66it/s][1,12]<stderr>:#015 53%|█████▎    | 5579/10570 [00:14<00:13, 365.20it/s][1,9]<stderr>:#015 51%|█████     | 5376/10570 [00:14<00:13, 395.70it/s][1,3]<stderr>:#015 49%|████▉     | 5178/10570 [00:13<00:14, 373.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▍     | 4690/10570 [00:13<00:18, 318.79it/s]#033[A[1,5]<stderr>:#015 51%|█████     | 5338/10570 [00:13<00:13, 399.37it/s][1,11]<stderr>:#015 49%|████▉     | 5183/10570 [00:13<00:14, 376.57it/s][1,2]<stderr>:#015 52%|█████▏    | 5548/10570 [00:14<00:14, 358.18it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|████▉     | 5281/10570 [00:13<00:13, 388.49it/s]#033[A[1,7]<stderr>:#015 54%|█████▎    | 5681/10570 [00:14<00:13, 361.31it/s][1,10]<stderr>:#015 52%|█████▏    | 5491/10570 [00:14<00:14, 358.55it/s][1,15]<stderr>:#015 51%|█████▏    | 5436/10570 [00:14<00:12, 396.05it/s][1,6]<stderr>:#015 48%|████▊     | 5101/10570 [00:13<00:15, 363.38it/s][1,13]<stderr>:#015 47%|████▋     | 4976/10570 [00:13<00:15, 352.79it/s][1,12]<stderr>:#015 53%|█████▎    | 5617/10570 [00:14<00:13, 367.40it/s][1,14]<stderr>:#015 51%|█████▏    | 5432/10570 [00:13<00:12, 397.55it/s][1,4]<stderr>:#015 52%|█████▏    | 5498/10570 [00:14<00:14, 356.13it/s][1,9]<stderr>:#015 51%|█████     | 5416/10570 [00:14<00:13, 395.10it/s][1,1]<stderr>:#015 52%|█████▏    | 5518/10570 [00:14<00:14, 353.87it/s][1,3]<stderr>:#015 49%|████▉     | 5217/10570 [00:13<00:14, 376.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▍     | 4728/10570 [00:13<00:17, 333.74it/s]#033[A[1,11]<stderr>:#015 49%|████▉     | 5222/10570 [00:13<00:14, 379.23it/s][1,5]<stderr>:#015 51%|█████     | 5379/10570 [00:13<00:13, 395.74it/s][1,2]<stderr>:#015 53%|█████▎    | 5586/10570 [00:14<00:13, 362.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|█████     | 5323/10570 [00:13<00:13, 395.07it/s]#033[A[1,7]<stderr>:#015 54%|█████▍    | 5723/10570 [00:14<00:12, 375.78it/s][1,10]<stderr>:#015 52%|█████▏    | 5528/10570 [00:14<00:14, 349.58it/s][1,6]<stderr>:#015 49%|████▊     | 5141/10570 [00:13<00:14, 371.29it/s][1,15]<stderr>:#015 52%|█████▏    | 5476/10570 [00:14<00:13, 367.67it/s][1,13]<stderr>:#015 47%|████▋     | 5012/10570 [00:13<00:16, 347.09it/s][1,12]<stderr>:#015 53%|█████▎    | 5654/10570 [00:14<00:13, 362.72it/s][1,4]<stderr>:#015 52%|█████▏    | 5534/10570 [00:14<00:14, 355.96it/s][1,1]<stderr>:#015 53%|█████▎    | 5555/10570 [00:14<00:13, 358.47it/s][1,3]<stderr>:#015 50%|████▉     | 5255/10570 [00:13<00:14, 377.03it/s][1,11]<stderr>:#015 50%|████▉     | 5260/10570 [00:13<00:14, 377.24it/s][1,5]<stderr>:#015 51%|█████▏    | 5419/10570 [00:14<00:13, 395.78it/s][1,14]<stderr>:#015 52%|█████▏    | 5472/10570 [00:14<00:13, 371.29it/s][1,9]<stderr>:#015 52%|█████▏    | 5456/10570 [00:14<00:13, 371.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▌     | 4762/10570 [00:13<00:18, 322.48it/s]#033[A[1,2]<stderr>:#015 53%|█████▎    | 5623/10570 [00:14<00:13, 357.72it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████     | 5363/10570 [00:14<00:13, 393.62it/s]#033[A[1,7]<stderr>:#015 55%|█████▍    | 5762/10570 [00:14<00:12, 378.67it/s][1,10]<stderr>:#015 53%|█████▎    | 5566/10570 [00:14<00:14, 356.17it/s][1,6]<stderr>:#015 49%|████▉     | 5179/10570 [00:13<00:14, 370.83it/s][1,13]<stderr>:#015 48%|████▊     | 5049/10570 [00:13<00:15, 353.49it/s][1,12]<stderr>:#015 54%|█████▍    | 5692/10570 [00:14<00:13, 365.39it/s][1,4]<stderr>:#015 53%|█████▎    | 5572/10570 [00:14<00:13, 360.90it/s][1,1]<stderr>:#015 53%|█████▎    | 5593/10570 [00:14<00:13, 363.95it/s][1,15]<stderr>:#015 52%|█████▏    | 5514/10570 [00:14<00:14, 352.32it/s][1,3]<stderr>:#015 50%|█████     | 5296/10570 [00:14<00:13, 385.27it/s][1,11]<stderr>:#015 50%|█████     | 5301/10570 [00:14<00:13, 386.40it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▌     | 4795/10570 [00:13<00:17, 322.69it/s]#033[A[1,14]<stderr>:#015 52%|█████▏    | 5510/10570 [00:14<00:14, 356.56it/s][1,9]<stderr>:#015 52%|█████▏    | 5494/10570 [00:14<00:14, 355.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████     | 5403/10570 [00:14<00:13, 394.99it/s]#033[A[1,5]<stderr>:#015 52%|█████▏    | 5459/10570 [00:14<00:13, 371.39it/s][1,2]<stderr>:#015 54%|█████▎    | 5659/10570 [00:14<00:14, 346.96it/s][1,7]<stderr>:#015 55%|█████▍    | 5801/10570 [00:14<00:12, 373.65it/s][1,10]<stderr>:#015 53%|█████▎    | 5603/10570 [00:14<00:13, 358.44it/s][1,6]<stderr>:#015 49%|████▉     | 5217/10570 [00:14<00:14, 370.39it/s][1,13]<stderr>:#015 48%|████▊     | 5086/10570 [00:14<00:15, 355.17it/s][1,12]<stderr>:#015 54%|█████▍    | 5732/10570 [00:14<00:12, 373.56it/s][1,4]<stderr>:#015 53%|█████▎    | 5610/10570 [00:14<00:13, 364.81it/s][1,1]<stderr>:#015 53%|█████▎    | 5630/10570 [00:14<00:13, 362.51it/s][1,15]<stderr>:#015 53%|█████▎    | 5551/10570 [00:14<00:14, 355.62it/s][1,3]<stderr>:#015 50%|█████     | 5337/10570 [00:14<00:13, 389.70it/s][1,11]<stderr>:#015 51%|█████     | 5341/10570 [00:14<00:13, 389.34it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▌     | 4835/10570 [00:13<00:16, 340.32it/s]#033[A[1,14]<stderr>:#015 52%|█████▏    | 5547/10570 [00:14<00:13, 358.95it/s][1,9]<stderr>:#015 52%|█████▏    | 5530/10570 [00:14<00:14, 354.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████▏    | 5443/10570 [00:14<00:13, 389.67it/s]#033[A[1,2]<stderr>:#015 54%|█████▍    | 5699/10570 [00:14<00:13, 356.23it/s][1,5]<stderr>:#015 52%|█████▏    | 5497/10570 [00:14<00:14, 355.08it/s][1,7]<stderr>:#015 55%|█████▌    | 5842/10570 [00:14<00:12, 380.04it/s][1,10]<stderr>:#015 53%|█████▎    | 5640/10570 [00:14<00:13, 357.01it/s][1,6]<stderr>:#015 50%|████▉     | 5255/10570 [00:14<00:14, 371.25it/s][1,13]<stderr>:#015 48%|████▊     | 5125/10570 [00:14<00:14, 363.64it/s][1,12]<stderr>:#015 55%|█████▍    | 5770/10570 [00:14<00:12, 373.23it/s][1,4]<stderr>:#015 53%|█████▎    | 5647/10570 [00:14<00:13, 360.59it/s][1,15]<stderr>:#015 53%|█████▎    | 5589/10570 [00:14<00:13, 360.27it/s][1,3]<stderr>:#015 51%|█████     | 5377/10570 [00:14<00:13, 385.24it/s][1,1]<stderr>:#015 54%|█████▎    | 5667/10570 [00:14<00:13, 351.38it/s][1,11]<stderr>:#015 51%|█████     | 5381/10570 [00:14<00:13, 385.88it/s][1,14]<stderr>:#015 53%|█████▎    | 5585/10570 [00:14<00:13, 362.49it/s][1,9]<stderr>:#015 53%|█████▎    | 5567/10570 [00:14<00:13, 358.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▌     | 4870/10570 [00:13<00:17, 321.20it/s]#033[A[1,2]<stderr>:#015 54%|█████▍    | 5735/10570 [00:14<00:13, 352.29it/s][1,5]<stderr>:#015 52%|█████▏    | 5533/10570 [00:14<00:14, 353.99it/s][1,7]<stderr>:#015 56%|█████▌    | 5881/10570 [00:14<00:12, 381.30it/s][1,10]<stderr>:#015 54%|█████▎    | 5676/10570 [00:14<00:13, 351.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 52%|█████▏    | 5483/10570 [00:14<00:14, 350.47it/s]#033[A[1,6]<stderr>:#015 50%|█████     | 5296/10570 [00:14<00:13, 379.77it/s][1,13]<stderr>:#015 49%|████▉     | 5163/10570 [00:14<00:14, 366.61it/s][1,12]<stderr>:#015 55%|█████▍    | 5808/10570 [00:14<00:12, 372.53it/s][1,15]<stderr>:#015 53%|█████▎    | 5626/10570 [00:14<00:13, 359.99it/s][1,4]<stderr>:#015 54%|█████▍    | 5684/10570 [00:14<00:13, 353.44it/s][1,3]<stderr>:#015 51%|█████     | 5416/10570 [00:14<00:13, 385.05it/s][1,1]<stderr>:#015 54%|█████▍    | 5708/10570 [00:14<00:13, 365.30it/s][1,11]<stderr>:#015 51%|█████▏    | 5420/10570 [00:14<00:13, 386.18it/s][1,14]<stderr>:#015 53%|█████▎    | 5622/10570 [00:14<00:13, 362.93it/s][1,9]<stderr>:#015 53%|█████▎    | 5605/10570 [00:14<00:13, 363.84it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▋     | 4907/10570 [00:13<00:16, 334.38it/s]#033[A[1,2]<stderr>:#015 55%|█████▍    | 5773/10570 [00:14<00:13, 358.15it/s][1,5]<stderr>:#015 53%|█████▎    | 5571/10570 [00:14<00:13, 359.77it/s][1,7]<stderr>:#015 56%|█████▌    | 5920/10570 [00:15<00:12, 382.60it/s][1,10]<stderr>:#015 54%|█████▍    | 5718/10570 [00:14<00:13, 367.07it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 52%|█████▏    | 5519/10570 [00:14<00:14, 349.93it/s]#033[A[1,6]<stderr>:#015 50%|█████     | 5337/10570 [00:14<00:13, 386.21it/s][1,13]<stderr>:#015 49%|████▉     | 5201/10570 [00:14<00:14, 368.77it/s][1,12]<stderr>:#015 55%|█████▌    | 5847/10570 [00:15<00:12, 376.57it/s][1,4]<stderr>:#015 54%|█████▍    | 5720/10570 [00:14<00:13, 353.50it/s][1,1]<stderr>:#015 54%|█████▍    | 5747/10570 [00:14<00:13, 370.40it/s][1,15]<stderr>:#015 54%|█████▎    | 5663/10570 [00:14<00:14, 349.84it/s][1,3]<stderr>:#015 52%|█████▏    | 5455/10570 [00:14<00:14, 361.26it/s][1,9]<stderr>:#015 53%|█████▎    | 5642/10570 [00:14<00:13, 360.72it/s][1,11]<stderr>:#015 52%|█████▏    | 5459/10570 [00:14<00:14, 362.46it/s][1,14]<stderr>:#015 54%|█████▎    | 5659/10570 [00:14<00:13, 351.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 4944/10570 [00:13<00:16, 342.78it/s]#033[A[1,5]<stderr>:#015 53%|█████▎    | 5609/10570 [00:14<00:13, 364.24it/s][1,7]<stderr>:#015 56%|█████▋    | 5961/10570 [00:15<00:11, 387.57it/s][1,2]<stderr>:#015 55%|█████▍    | 5809/10570 [00:15<00:13, 354.07it/s][1,10]<stderr>:#015 54%|█████▍    | 5756/10570 [00:14<00:12, 370.48it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5556/10570 [00:14<00:14, 354.07it/s]#033[A[1,6]<stderr>:#015 51%|█████     | 5376/10570 [00:14<00:13, 383.19it/s][1,13]<stderr>:#015 50%|████▉     | 5239/10570 [00:14<00:14, 370.62it/s][1,12]<stderr>:#015 56%|█████▌    | 5887/10570 [00:15<00:12, 382.10it/s][1,4]<stderr>:#015 54%|█████▍    | 5758/10570 [00:14<00:13, 360.04it/s][1,1]<stderr>:#015 55%|█████▍    | 5785/10570 [00:14<00:12, 371.14it/s][1,15]<stderr>:#015 54%|█████▍    | 5704/10570 [00:14<00:13, 363.94it/s][1,14]<stderr>:#015 54%|█████▍    | 5700/10570 [00:14<00:13, 365.90it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 4979/10570 [00:13<00:16, 343.12it/s]#033[A[1,9]<stderr>:#015 54%|█████▎    | 5679/10570 [00:14<00:13, 350.81it/s][1,3]<stderr>:#015 52%|█████▏    | 5492/10570 [00:14<00:14, 344.68it/s][1,11]<stderr>:#015 52%|█████▏    | 5496/10570 [00:14<00:14, 346.37it/s][1,7]<stderr>:#015 57%|█████▋    | 6001/10570 [00:15<00:11, 390.66it/s][1,5]<stderr>:#015 53%|█████▎    | 5646/10570 [00:14<00:13, 361.92it/s][1,2]<stderr>:#015 55%|█████▌    | 5848/10570 [00:15<00:13, 360.98it/s][1,10]<stderr>:#015 55%|█████▍    | 5794/10570 [00:15<00:13, 366.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5594/10570 [00:14<00:13, 360.25it/s]#033[A[1,6]<stderr>:#015 51%|█████     | 5415/10570 [00:14<00:13, 383.16it/s][1,13]<stderr>:#015 50%|████▉     | 5277/10570 [00:14<00:14, 371.59it/s][1,12]<stderr>:#015 56%|█████▌    | 5926/10570 [00:15<00:12, 381.08it/s][1,4]<stderr>:#015 55%|█████▍    | 5795/10570 [00:15<00:13, 358.37it/s][1,15]<stderr>:#015 54%|█████▍    | 5743/10570 [00:15<00:13, 369.12it/s][1,1]<stderr>:#015 55%|█████▌    | 5823/10570 [00:15<00:13, 361.64it/s][1,14]<stderr>:#015 54%|█████▍    | 5739/10570 [00:14<00:13, 370.20it/s][1,3]<stderr>:#015 52%|█████▏    | 5527/10570 [00:14<00:14, 345.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 5014/10570 [00:14<00:16, 336.69it/s]#033[A[1,9]<stderr>:#015 54%|█████▍    | 5715/10570 [00:15<00:13, 347.63it/s][1,2]<stderr>:#015 56%|█████▌    | 5888/10570 [00:15<00:12, 371.07it/s][1,11]<stderr>:#015 52%|█████▏    | 5532/10570 [00:14<00:14, 345.07it/s][1,7]<stderr>:#015 57%|█████▋    | 6041/10570 [00:15<00:11, 386.31it/s][1,5]<stderr>:#015 54%|█████▍    | 5683/10570 [00:14<00:13, 353.24it/s][1,10]<stderr>:#015 55%|█████▌    | 5833/10570 [00:15<00:12, 373.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5631/10570 [00:14<00:13, 358.23it/s]#033[A[1,13]<stderr>:#015 50%|█████     | 5317/10570 [00:14<00:13, 377.30it/s][1,12]<stderr>:#015 56%|█████▋    | 5966/10570 [00:15<00:11, 386.31it/s][1,4]<stderr>:#015 55%|█████▌    | 5834/10570 [00:15<00:12, 366.34it/s][1,6]<stderr>:#015 52%|█████▏    | 5454/10570 [00:14<00:14, 358.88it/s][1,15]<stderr>:#015 55%|█████▍    | 5781/10570 [00:15<00:12, 370.25it/s][1,1]<stderr>:#015 55%|█████▌    | 5861/10570 [00:15<00:12, 365.06it/s][1,14]<stderr>:#015 55%|█████▍    | 5777/10570 [00:14<00:12, 371.87it/s][1,3]<stderr>:#015 53%|█████▎    | 5563/10570 [00:14<00:14, 349.35it/s][1,9]<stderr>:#015 54%|█████▍    | 5752/10570 [00:15<00:13, 353.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5050/10570 [00:14<00:16, 341.65it/s]#033[A[1,2]<stderr>:#015 56%|█████▌    | 5926/10570 [00:15<00:12, 372.52it/s][1,11]<stderr>:#015 53%|█████▎    | 5569/10570 [00:14<00:14, 350.11it/s][1,5]<stderr>:#015 54%|█████▍    | 5719/10570 [00:14<00:13, 354.38it/s][1,7]<stderr>:#015 58%|█████▊    | 6080/10570 [00:15<00:11, 379.58it/s][1,10]<stderr>:#015 56%|█████▌    | 5871/10570 [00:15<00:12, 372.61it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▎    | 5668/10570 [00:14<00:14, 347.48it/s]#033[A[1,12]<stderr>:#015 57%|█████▋    | 6005/10570 [00:15<00:11, 385.27it/s][1,13]<stderr>:#015 51%|█████     | 5355/10570 [00:14<00:13, 374.87it/s][1,4]<stderr>:#015 56%|█████▌    | 5871/10570 [00:15<00:12, 367.18it/s][1,15]<stderr>:#015 55%|█████▌    | 5819/10570 [00:15<00:12, 368.76it/s][1,1]<stderr>:#015 56%|█████▌    | 5901/10570 [00:15<00:12, 373.26it/s][1,6]<stderr>:#015 52%|█████▏    | 5491/10570 [00:14<00:14, 341.29it/s][1,3]<stderr>:#015 53%|█████▎    | 5599/10570 [00:14<00:14, 352.06it/s][1,9]<stderr>:#015 55%|█████▍    | 5789/10570 [00:15<00:13, 357.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5085/10570 [00:14<00:15, 343.57it/s]#033[A[1,14]<stderr>:#015 55%|█████▌    | 5815/10570 [00:15<00:13, 363.41it/s][1,2]<stderr>:#015 56%|█████▋    | 5966/10570 [00:15<00:12, 379.31it/s][1,11]<stderr>:#015 53%|█████▎    | 5606/10570 [00:14<00:13, 355.29it/s][1,5]<stderr>:#015 54%|█████▍    | 5757/10570 [00:15<00:13, 359.80it/s][1,10]<stderr>:#015 56%|█████▌    | 5911/10570 [00:15<00:12, 377.70it/s][1,7]<stderr>:#015 58%|█████▊    | 6119/10570 [00:15<00:12, 369.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▍    | 5708/10570 [00:15<00:13, 361.37it/s]#033[A[1,12]<stderr>:#015 57%|█████▋    | 6044/10570 [00:15<00:11, 384.98it/s][1,13]<stderr>:#015 51%|█████     | 5394/10570 [00:14<00:13, 377.00it/s][1,4]<stderr>:#015 56%|█████▌    | 5910/10570 [00:15<00:12, 373.31it/s][1,15]<stderr>:#015 55%|█████▌    | 5857/10570 [00:15<00:12, 370.73it/s][1,1]<stderr>:#015 56%|█████▌    | 5940/10570 [00:15<00:12, 375.42it/s][1,6]<stderr>:#015 52%|█████▏    | 5526/10570 [00:14<00:14, 341.68it/s][1,9]<stderr>:#015 55%|█████▌    | 5825/10570 [00:15<00:13, 355.99it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5123/10570 [00:14<00:15, 351.49it/s]#033[A[1,14]<stderr>:#015 55%|█████▌    | 5854/10570 [00:15<00:12, 369.46it/s][1,3]<stderr>:#015 53%|█████▎    | 5635/10570 [00:15<00:14, 348.32it/s][1,2]<stderr>:#015 57%|█████▋    | 6005/10570 [00:15<00:11, 380.89it/s][1,11]<stderr>:#015 53%|█████▎    | 5642/10570 [00:15<00:13, 352.12it/s][1,5]<stderr>:#015 55%|█████▍    | 5794/10570 [00:15<00:13, 358.21it/s][1,10]<stderr>:#015 56%|█████▋    | 5950/10570 [00:15<00:12, 380.83it/s][1,7]<stderr>:#015 58%|█████▊    | 6160/10570 [00:15<00:11, 379.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▍    | 5746/10570 [00:15<00:13, 366.76it/s]#033[A[1,13]<stderr>:#015 51%|█████▏    | 5432/10570 [00:14<00:13, 374.63it/s][1,12]<stderr>:#015 58%|█████▊    | 6083/10570 [00:15<00:11, 377.52it/s][1,4]<stderr>:#015 56%|█████▋    | 5949/10570 [00:15<00:12, 376.79it/s][1,15]<stderr>:#015 56%|█████▌    | 5897/10570 [00:15<00:12, 377.29it/s][1,1]<stderr>:#015 57%|█████▋    | 5980/10570 [00:15<00:12, 382.00it/s][1,6]<stderr>:#015 53%|█████▎    | 5562/10570 [00:14<00:14, 345.23it/s][1,9]<stderr>:#015 55%|█████▌    | 5862/10570 [00:15<00:13, 357.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 49%|████▉     | 5160/10570 [00:14<00:15, 355.06it/s]#033[A[1,14]<stderr>:#015 56%|█████▌    | 5894/10570 [00:15<00:12, 376.92it/s][1,2]<stderr>:#015 57%|█████▋    | 6044/10570 [00:15<00:11, 381.42it/s][1,3]<stderr>:#015 54%|█████▎    | 5670/10570 [00:15<00:14, 337.93it/s][1,5]<stderr>:#015 55%|█████▌    | 5833/10570 [00:15<00:12, 365.67it/s][1,11]<stderr>:#015 54%|█████▎    | 5678/10570 [00:15<00:14, 343.84it/s][1,10]<stderr>:#015 57%|█████▋    | 5990/10570 [00:15<00:11, 385.92it/s][1,7]<stderr>:#015 59%|█████▊    | 6200/10570 [00:15<00:11, 383.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▍    | 5783/10570 [00:15<00:13, 367.45it/s]#033[A[1,12]<stderr>:#015 58%|█████▊    | 6122/10570 [00:15<00:11, 380.30it/s][1,4]<stderr>:#015 57%|█████▋    | 5989/10570 [00:15<00:11, 381.93it/s][1,15]<stderr>:#015 56%|█████▌    | 5935/10570 [00:15<00:12, 375.79it/s][1,1]<stderr>:#015 57%|█████▋    | 6019/10570 [00:15<00:11, 381.35it/s][1,13]<stderr>:#015 52%|█████▏    | 5470/10570 [00:15<00:14, 345.47it/s][1,6]<stderr>:#015 53%|█████▎    | 5600/10570 [00:15<00:14, 353.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 49%|████▉     | 5197/10570 [00:14<00:15, 357.50it/s][1,8]<stderr>:#033[A[1,9]<stderr>:#015 56%|█████▌    | 5902/10570 [00:15<00:12, 367.00it/s][1,14]<stderr>:#015 56%|█████▌    | 5932/10570 [00:15<00:12, 375.38it/s][1,3]<stderr>:#015 54%|█████▍    | 5704/10570 [00:15<00:14, 332.08it/s][1,11]<stderr>:#015 54%|█████▍    | 5718/10570 [00:15<00:13, 358.39it/s][1,5]<stderr>:#015 56%|█████▌    | 5870/10570 [00:15<00:12, 363.48it/s][1,10]<stderr>:#015 57%|█████▋    | 6029/10570 [00:15<00:11, 385.24it/s][1,2]<stderr>:#015 58%|█████▊    | 6083/10570 [00:15<00:12, 349.82it/s][1,7]<stderr>:#015 59%|█████▉    | 6239/10570 [00:15<00:11, 367.69it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▌    | 5820/10570 [00:15<00:12, 365.87it/s]#033[A[1,12]<stderr>:#015 58%|█████▊    | 6162/10570 [00:15<00:11, 385.96it/s][1,4]<stderr>:#015 57%|█████▋    | 6028/10570 [00:15<00:11, 381.82it/s][1,15]<stderr>:#015 57%|█████▋    | 5975/10570 [00:15<00:12, 381.40it/s][1,1]<stderr>:#015 57%|█████▋    | 6058/10570 [00:15<00:12, 373.45it/s][1,6]<stderr>:#015 53%|█████▎    | 5636/10570 [00:15<00:14, 347.90it/s][1,13]<stderr>:#015 52%|█████▏    | 5506/10570 [00:15<00:15, 333.74it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|████▉     | 5234/10570 [00:14<00:14, 360.84it/s]#033[A[1,9]<stderr>:#015 56%|█████▌    | 5940/10570 [00:15<00:12, 370.08it/s][1,14]<stderr>:#015 56%|█████▋    | 5972/10570 [00:15<00:12, 381.78it/s][1,3]<stderr>:#015 54%|█████▍    | 5741/10570 [00:15<00:14, 342.22it/s][1,11]<stderr>:#015 54%|█████▍    | 5755/10570 [00:15<00:13, 360.14it/s][1,5]<stderr>:#015 56%|█████▌    | 5909/10570 [00:15<00:12, 369.72it/s][1,10]<stderr>:#015 57%|█████▋    | 6068/10570 [00:15<00:12, 373.68it/s][1,2]<stderr>:#015 58%|█████▊    | 6122/10570 [00:15<00:12, 359.52it/s][1,7]<stderr>:#015 59%|█████▉    | 6278/10570 [00:16<00:11, 373.79it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▌    | 5858/10570 [00:15<00:12, 367.36it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 59%|█████▊    | 6201/10570 [00:15<00:11, 386.09it/s][1,4]<stderr>:#015 57%|█████▋    | 6067/10570 [00:15<00:12, 370.56it/s][1,15]<stderr>:#015 57%|█████▋    | 6014/10570 [00:15<00:11, 380.78it/s][1,1]<stderr>:#015 58%|█████▊    | 6096/10570 [00:15<00:12, 365.13it/s][1,13]<stderr>:#015 52%|█████▏    | 5541/10570 [00:15<00:14, 336.53it/s][1,6]<stderr>:#015 54%|█████▎    | 5671/10570 [00:15<00:14, 337.54it/s][1,9]<stderr>:#015 57%|█████▋    | 5980/10570 [00:15<00:12, 377.10it/s][1,14]<stderr>:#015 57%|█████▋    | 6011/10570 [00:15<00:11, 382.53it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|████▉     | 5271/10570 [00:14<00:14, 360.45it/s]#033[A[1,3]<stderr>:#015 55%|█████▍    | 5778/10570 [00:15<00:13, 348.32it/s][1,5]<stderr>:#015 56%|█████▋    | 5948/10570 [00:15<00:12, 373.51it/s][1,11]<stderr>:#015 55%|█████▍    | 5792/10570 [00:15<00:13, 357.02it/s][1,10]<stderr>:#015 58%|█████▊    | 6106/10570 [00:15<00:11, 374.83it/s][1,2]<stderr>:#015 58%|█████▊    | 6162/10570 [00:15<00:11, 370.29it/s][1,7]<stderr>:#015 60%|█████▉    | 6320/10570 [00:16<00:11, 383.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 56%|█████▌    | 5897/10570 [00:15<00:12, 373.69it/s]#033[A[1,12]<stderr>:#015 59%|█████▉    | 6240/10570 [00:16<00:11, 370.23it/s][1,4]<stderr>:#015 58%|█████▊    | 6105/10570 [00:15<00:12, 371.42it/s][1,15]<stderr>:#015 57%|█████▋    | 6053/10570 [00:15<00:11, 377.57it/s][1,1]<stderr>:#015 58%|█████▊    | 6136/10570 [00:15<00:11, 374.71it/s][1,13]<stderr>:#015 53%|█████▎    | 5576/10570 [00:15<00:14, 340.08it/s][1,6]<stderr>:#015 54%|█████▍    | 5705/10570 [00:15<00:14, 337.64it/s][1,9]<stderr>:#015 57%|█████▋    | 6018/10570 [00:15<00:12, 377.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|█████     | 5310/10570 [00:14<00:14, 368.39it/s]#033[A[1,14]<stderr>:#015 57%|█████▋    | 6050/10570 [00:15<00:11, 380.73it/s][1,5]<stderr>:#015 57%|█████▋    | 5988/10570 [00:15<00:12, 379.70it/s][1,3]<stderr>:#015 55%|█████▍    | 5813/10570 [00:15<00:13, 342.69it/s][1,11]<stderr>:#015 55%|█████▌    | 5830/10570 [00:15<00:13, 361.60it/s][1,10]<stderr>:#015 58%|█████▊    | 6147/10570 [00:15<00:11, 383.62it/s][1,2]<stderr>:#015 59%|█████▊    | 6201/10570 [00:16<00:11, 374.25it/s][1,7]<stderr>:#015 60%|██████    | 6360/10570 [00:16<00:10, 387.07it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 56%|█████▌    | 5935/10570 [00:15<00:12, 371.85it/s]#033[A[1,12]<stderr>:#015 59%|█████▉    | 6279/10570 [00:16<00:11, 374.75it/s][1,4]<stderr>:#015 58%|█████▊    | 6146/10570 [00:15<00:11, 381.26it/s][1,15]<stderr>:#015 58%|█████▊    | 6091/10570 [00:15<00:12, 370.59it/s][1,1]<stderr>:#015 58%|█████▊    | 6176/10570 [00:16<00:11, 379.43it/s][1,13]<stderr>:#015 53%|█████▎    | 5612/10570 [00:15<00:14, 344.14it/s][1,6]<stderr>:#015 54%|█████▍    | 5742/10570 [00:15<00:13, 345.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████     | 5347/10570 [00:14<00:14, 365.14it/s]#033[A[1,9]<stderr>:#015 57%|█████▋    | 6056/10570 [00:15<00:12, 368.49it/s][1,14]<stderr>:#015 58%|█████▊    | 6089/10570 [00:15<00:11, 374.56it/s][1,3]<stderr>:#015 55%|█████▌    | 5850/10570 [00:15<00:13, 349.32it/s][1,5]<stderr>:#015 57%|█████▋    | 6027/10570 [00:15<00:11, 379.05it/s][1,11]<stderr>:#015 56%|█████▌    | 5867/10570 [00:15<00:12, 361.79it/s][1,10]<stderr>:#015 59%|█████▊    | 6186/10570 [00:16<00:11, 384.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 5974/10570 [00:15<00:12, 376.71it/s]#033[A[1,2]<stderr>:#015 59%|█████▉    | 6239/10570 [00:16<00:11, 361.27it/s][1,7]<stderr>:#015 61%|██████    | 6399/10570 [00:16<00:11, 374.06it/s][1,12]<stderr>:#015 60%|█████▉    | 6319/10570 [00:16<00:11, 380.74it/s][1,4]<stderr>:#015 59%|█████▊    | 6185/10570 [00:16<00:11, 381.49it/s][1,15]<stderr>:#015 58%|█████▊    | 6130/10570 [00:16<00:11, 375.92it/s][1,1]<stderr>:#015 59%|█████▉    | 6215/10570 [00:16<00:11, 379.50it/s][1,13]<stderr>:#015 53%|█████▎    | 5647/10570 [00:15<00:14, 342.64it/s][1,6]<stderr>:#015 55%|█████▍    | 5778/10570 [00:15<00:13, 349.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████     | 5384/10570 [00:15<00:14, 365.24it/s]#033[A[1,9]<stderr>:#015 58%|█████▊    | 6093/10570 [00:16<00:12, 365.57it/s][1,14]<stderr>:#015 58%|█████▊    | 6129/10570 [00:15<00:11, 379.13it/s][1,3]<stderr>:#015 56%|█████▌    | 5889/10570 [00:15<00:13, 358.26it/s][1,11]<stderr>:#015 56%|█████▌    | 5906/10570 [00:15<00:12, 367.32it/s][1,5]<stderr>:#015 57%|█████▋    | 6065/10570 [00:15<00:12, 366.16it/s][1,10]<stderr>:#015 59%|█████▉    | 6225/10570 [00:16<00:11, 376.98it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 6012/10570 [00:15<00:12, 376.01it/s]#033[A[1,2]<stderr>:#015 59%|█████▉    | 6278/10570 [00:16<00:11, 367.55it/s][1,7]<stderr>:#015 61%|██████    | 6442/10570 [00:16<00:10, 387.10it/s][1,12]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:10, 383.24it/s][1,4]<stderr>:#015 59%|█████▉    | 6224/10570 [00:16<00:11, 378.37it/s][1,15]<stderr>:#015 58%|█████▊    | 6170/10570 [00:16<00:11, 381.14it/s][1,6]<stderr>:#015 55%|█████▌    | 5814/10570 [00:15<00:13, 347.97it/s][1,13]<stderr>:#015 54%|█████▍    | 5682/10570 [00:15<00:14, 334.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████▏    | 5421/10570 [00:15<00:14, 364.06it/s]#033[A[1,9]<stderr>:#015 58%|█████▊    | 6133/10570 [00:16<00:11, 373.85it/s][1,1]<stderr>:#015 59%|█████▉    | 6254/10570 [00:16<00:11, 362.62it/s][1,14]<stderr>:#015 58%|█████▊    | 6169/10570 [00:15<00:11, 383.79it/s][1,3]<stderr>:#015 56%|█████▌    | 5925/10570 [00:15<00:12, 358.50it/s][1,11]<stderr>:#015 56%|█████▌    | 5944/10570 [00:15<00:12, 369.15it/s][1,5]<stderr>:#015 58%|█████▊    | 6103/10570 [00:15<00:12, 367.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 6050/10570 [00:15<00:12, 373.89it/s]#033[A[1,10]<stderr>:#015 59%|█████▉    | 6263/10570 [00:16<00:11, 366.07it/s][1,2]<stderr>:#015 60%|█████▉    | 6319/10570 [00:16<00:11, 378.49it/s][1,7]<stderr>:#015 61%|██████▏   | 6483/10570 [00:16<00:10, 393.29it/s][1,12]<stderr>:#015 61%|██████    | 6397/10570 [00:16<00:11, 370.16it/s][1,15]<stderr>:#015 59%|█████▊    | 6209/10570 [00:16<00:11, 380.16it/s][1,4]<stderr>:#015 59%|█████▉    | 6262/10570 [00:16<00:11, 363.11it/s][1,6]<stderr>:#015 55%|█████▌    | 5851/10570 [00:15<00:13, 353.82it/s][1,13]<stderr>:#015 54%|█████▍    | 5721/10570 [00:15<00:13, 348.23it/s][1,9]<stderr>:#015 58%|█████▊    | 6173/10570 [00:16<00:11, 379.32it/s][1,1]<stderr>:#015 60%|█████▉    | 6294/10570 [00:16<00:11, 370.98it/s][1,14]<stderr>:#015 59%|█████▊    | 6208/10570 [00:16<00:11, 383.13it/s][1,3]<stderr>:#015 56%|█████▋    | 5964/10570 [00:15<00:12, 365.67it/s][1,11]<stderr>:#015 57%|█████▋    | 5983/10570 [00:15<00:12, 374.16it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5458/10570 [00:15<00:15, 340.33it/s]#033[A[1,5]<stderr>:#015 58%|█████▊    | 6144/10570 [00:16<00:11, 378.33it/s][1,10]<stderr>:#015 60%|█████▉    | 6304/10570 [00:16<00:11, 375.94it/s][1,2]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:11, 380.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6088/10570 [00:16<00:12, 367.79it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 62%|██████▏   | 6523/10570 [00:16<00:10, 392.21it/s][1,12]<stderr>:#015 61%|██████    | 6440/10570 [00:16<00:10, 384.96it/s][1,4]<stderr>:#015 60%|█████▉    | 6303/10570 [00:16<00:11, 374.07it/s][1,15]<stderr>:#015 59%|█████▉    | 6248/10570 [00:16<00:11, 364.81it/s][1,6]<stderr>:#015 56%|█████▌    | 5890/10570 [00:15<00:12, 361.78it/s][1,13]<stderr>:#015 54%|█████▍    | 5757/10570 [00:15<00:13, 349.68it/s][1,1]<stderr>:#015 60%|█████▉    | 6334/10570 [00:16<00:11, 376.96it/s][1,9]<stderr>:#015 59%|█████▉    | 6212/10570 [00:16<00:11, 378.47it/s][1,3]<stderr>:#015 57%|█████▋    | 6002/10570 [00:16<00:12, 368.06it/s][1,14]<stderr>:#015 59%|█████▉    | 6247/10570 [00:16<00:11, 367.95it/s][1,11]<stderr>:#015 57%|█████▋    | 6021/10570 [00:16<00:12, 372.61it/s][1,5]<stderr>:#015 58%|█████▊    | 6183/10570 [00:16<00:11, 380.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5493/10570 [00:15<00:15, 323.96it/s][1,8]<stderr>:#033[A[1,10]<stderr>:#015 60%|██████    | 6344/10570 [00:16<00:11, 380.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6127/10570 [00:16<00:11, 372.18it/s]#033[A[1,7]<stderr>:#015 62%|██████▏   | 6564/10570 [00:16<00:10, 396.03it/s][1,2]<stderr>:#015 61%|██████    | 6397/10570 [00:16<00:11, 367.52it/s][1,12]<stderr>:#015 61%|██████▏   | 6481/10570 [00:16<00:10, 390.92it/s][1,4]<stderr>:#015 60%|██████    | 6343/10570 [00:16<00:11, 379.35it/s][1,15]<stderr>:#015 59%|█████▉    | 6287/10570 [00:16<00:11, 371.25it/s][1,6]<stderr>:#015 56%|█████▌    | 5927/10570 [00:16<00:12, 360.35it/s][1,13]<stderr>:#015 55%|█████▍    | 5793/10570 [00:16<00:13, 341.58it/s][1,1]<stderr>:#015 60%|██████    | 6374/10570 [00:16<00:11, 377.92it/s][1,3]<stderr>:#015 57%|█████▋    | 6039/10570 [00:16<00:12, 366.86it/s][1,14]<stderr>:#015 59%|█████▉    | 6286/10570 [00:16<00:11, 374.03it/s][1,9]<stderr>:#015 59%|█████▉    | 6250/10570 [00:16<00:11, 362.67it/s][1,5]<stderr>:#015 59%|█████▉    | 6222/10570 [00:16<00:11, 380.34it/s][1,11]<stderr>:#015 57%|█████▋    | 6059/10570 [00:16<00:12, 362.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5526/10570 [00:15<00:15, 323.33it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6166/10570 [00:16<00:11, 376.93it/s]#033[A[1,7]<stderr>:#015 62%|██████▏   | 6604/10570 [00:16<00:10, 390.49it/s][1,10]<stderr>:#015 60%|██████    | 6383/10570 [00:16<00:11, 366.71it/s][1,2]<stderr>:#015 61%|██████    | 6440/10570 [00:16<00:10, 382.70it/s][1,12]<stderr>:#015 62%|██████▏   | 6521/10570 [00:16<00:10, 389.41it/s][1,15]<stderr>:#015 60%|█████▉    | 6326/10570 [00:16<00:11, 375.31it/s][1,6]<stderr>:#015 56%|█████▋    | 5966/10570 [00:16<00:12, 366.17it/s][1,4]<stderr>:#015 60%|██████    | 6382/10570 [00:16<00:11, 364.71it/s][1,13]<stderr>:#015 55%|█████▌    | 5830/10570 [00:16<00:13, 348.09it/s][1,1]<stderr>:#015 61%|██████    | 6412/10570 [00:16<00:11, 370.29it/s][1,14]<stderr>:#015 60%|█████▉    | 6325/10570 [00:16<00:11, 378.68it/s][1,9]<stderr>:#015 59%|█████▉    | 6289/10570 [00:16<00:11, 369.23it/s][1,3]<stderr>:#015 57%|█████▋    | 6076/10570 [00:16<00:12, 358.91it/s][1,11]<stderr>:#015 58%|█████▊    | 6096/10570 [00:16<00:12, 361.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5560/10570 [00:15<00:15, 327.84it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▊    | 6204/10570 [00:16<00:11, 376.07it/s]#033[A[1,5]<stderr>:#015 59%|█████▉    | 6261/10570 [00:16<00:12, 358.61it/s][1,7]<stderr>:#015 63%|██████▎   | 6646/10570 [00:16<00:09, 397.85it/s][1,2]<stderr>:#015 61%|██████▏   | 6481/10570 [00:16<00:10, 388.92it/s][1,10]<stderr>:#015 61%|██████    | 6425/10570 [00:16<00:10, 379.43it/s][1,12]<stderr>:#015 62%|██████▏   | 6562/10570 [00:16<00:10, 393.37it/s][1,6]<stderr>:#015 57%|█████▋    | 6003/10570 [00:16<00:12, 367.24it/s][1,15]<stderr>:#015 60%|██████    | 6364/10570 [00:16<00:11, 368.65it/s][1,4]<stderr>:#015 61%|██████    | 6423/10570 [00:16<00:10, 377.16it/s][1,13]<stderr>:#015 55%|█████▌    | 5866/10570 [00:16<00:13, 349.89it/s][1,1]<stderr>:#015 61%|██████    | 6454/10570 [00:16<00:10, 382.76it/s][1,14]<stderr>:#015 60%|██████    | 6365/10570 [00:16<00:10, 382.64it/s][1,9]<stderr>:#015 60%|█████▉    | 6328/10570 [00:16<00:11, 373.51it/s][1,3]<stderr>:#015 58%|█████▊    | 6113/10570 [00:16<00:12, 361.14it/s][1,11]<stderr>:#015 58%|█████▊    | 6135/10570 [00:16<00:12, 368.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5596/10570 [00:15<00:14, 335.76it/s]#033[A[1,7]<stderr>:#015 63%|██████▎   | 6686/10570 [00:17<00:09, 397.92it/s][1,5]<stderr>:#015 60%|█████▉    | 6302/10570 [00:16<00:11, 370.29it/s][1,10]<stderr>:#015 61%|██████    | 6466/10570 [00:16<00:10, 388.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▉    | 6242/10570 [00:16<00:12, 360.32it/s]#033[A[1,2]<stderr>:#015 62%|██████▏   | 6521/10570 [00:16<00:10, 386.74it/s][1,12]<stderr>:#015 62%|██████▏   | 6602/10570 [00:16<00:10, 387.01it/s][1,6]<stderr>:#015 57%|█████▋    | 6040/10570 [00:16<00:12, 365.85it/s][1,4]<stderr>:#015 61%|██████    | 6466/10570 [00:16<00:10, 389.52it/s][1,15]<stderr>:#015 61%|██████    | 6401/10570 [00:16<00:11, 358.97it/s][1,13]<stderr>:#015 56%|█████▌    | 5904/10570 [00:16<00:13, 356.26it/s][1,1]<stderr>:#015 61%|██████▏   | 6494/10570 [00:16<00:10, 386.38it/s][1,9]<stderr>:#015 60%|██████    | 6368/10570 [00:16<00:11, 379.41it/s][1,3]<stderr>:#015 58%|█████▊    | 6152/10570 [00:16<00:11, 368.73it/s][1,14]<stderr>:#015 61%|██████    | 6404/10570 [00:16<00:11, 370.46it/s][1,11]<stderr>:#015 58%|█████▊    | 6174/10570 [00:16<00:11, 372.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5630/10570 [00:15<00:14, 331.86it/s]#033[A[1,5]<stderr>:#015 60%|█████▉    | 6341/10570 [00:16<00:11, 375.67it/s][1,7]<stderr>:#015 64%|██████▎   | 6726/10570 [00:17<00:09, 388.16it/s][1,10]<stderr>:#015 62%|██████▏   | 6506/10570 [00:16<00:10, 387.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▉    | 6280/10570 [00:16<00:11, 365.41it/s]#033[A[1,2]<stderr>:#015 62%|██████▏   | 6561/10570 [00:17<00:10, 390.58it/s][1,12]<stderr>:#015 63%|██████▎   | 6644/10570 [00:17<00:09, 395.52it/s][1,4]<stderr>:#015 62%|██████▏   | 6506/10570 [00:16<00:10, 388.21it/s][1,6]<stderr>:#015 57%|█████▋    | 6077/10570 [00:16<00:12, 358.73it/s][1,15]<stderr>:#015 61%|██████    | 6443/10570 [00:16<00:11, 374.69it/s][1,13]<stderr>:#015 56%|█████▌    | 5941/10570 [00:16<00:12, 358.03it/s][1,1]<stderr>:#015 62%|██████▏   | 6533/10570 [00:16<00:10, 386.67it/s][1,3]<stderr>:#015 59%|█████▊    | 6190/10570 [00:16<00:11, 370.83it/s][1,14]<stderr>:#015 61%|██████    | 6446/10570 [00:16<00:10, 383.49it/s][1,9]<stderr>:#015 61%|██████    | 6407/10570 [00:16<00:11, 367.18it/s][1,11]<stderr>:#015 59%|█████▉    | 6212/10570 [00:16<00:11, 370.92it/s][1,7]<stderr>:#015 64%|██████▍   | 6768/10570 [00:17<00:09, 396.19it/s][1,10]<stderr>:#015 62%|██████▏   | 6546/10570 [00:17<00:10, 390.93it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▎    | 5664/10570 [00:15<00:15, 319.56it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 60%|█████▉    | 6320/10570 [00:16<00:11, 372.95it/s]#033[A[1,5]<stderr>:#015 60%|██████    | 6379/10570 [00:16<00:11, 361.01it/s][1,2]<stderr>:#015 62%|██████▏   | 6601/10570 [00:17<00:10, 384.53it/s][1,12]<stderr>:#015 63%|██████▎   | 6684/10570 [00:17<00:09, 396.64it/s][1,4]<stderr>:#015 62%|██████▏   | 6546/10570 [00:17<00:10, 390.30it/s][1,6]<stderr>:#015 58%|█████▊    | 6114/10570 [00:16<00:12, 360.41it/s][1,15]<stderr>:#015 61%|██████▏   | 6483/10570 [00:16<00:10, 381.31it/s][1,13]<stderr>:#015 57%|█████▋    | 5979/10570 [00:16<00:12, 363.34it/s][1,1]<stderr>:#015 62%|██████▏   | 6573/10570 [00:17<00:10, 387.35it/s][1,14]<stderr>:#015 61%|██████▏   | 6486/10570 [00:16<00:10, 388.15it/s][1,9]<stderr>:#015 61%|██████    | 6449/10570 [00:16<00:10, 379.48it/s][1,3]<stderr>:#015 59%|█████▉    | 6228/10570 [00:16<00:12, 352.64it/s][1,11]<stderr>:#015 59%|█████▉    | 6250/10570 [00:16<00:12, 354.44it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▍    | 5702/10570 [00:16<00:14, 334.87it/s][1,8]<stderr>:#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:11, 374.42it/s]#033[A[1,7]<stderr>:#015 64%|██████▍   | 6808/10570 [00:17<00:09, 391.28it/s][1,10]<stderr>:#015 62%|██████▏   | 6586/10570 [00:17<00:10, 387.74it/s][1,5]<stderr>:#015 61%|██████    | 6420/10570 [00:16<00:11, 374.13it/s][1,2]<stderr>:#015 63%|██████▎   | 6643/10570 [00:17<00:09, 393.02it/s][1,12]<stderr>:#015 64%|██████▎   | 6724/10570 [00:17<00:09, 384.71it/s][1,6]<stderr>:#015 58%|█████▊    | 6153/10570 [00:16<00:12, 366.85it/s][1,4]<stderr>:#015 62%|██████▏   | 6586/10570 [00:17<00:10, 386.63it/s][1,15]<stderr>:#015 62%|██████▏   | 6522/10570 [00:17<00:10, 374.99it/s][1,13]<stderr>:#015 57%|█████▋    | 6016/10570 [00:16<00:12, 361.91it/s][1,1]<stderr>:#015 63%|██████▎   | 6612/10570 [00:17<00:10, 382.29it/s][1,14]<stderr>:#015 62%|██████▏   | 6525/10570 [00:16<00:10, 385.82it/s][1,9]<stderr>:#015 61%|██████▏   | 6489/10570 [00:17<00:10, 384.44it/s][1,3]<stderr>:#015 59%|█████▉    | 6265/10570 [00:16<00:12, 356.55it/s][1,11]<stderr>:#015 59%|█████▉    | 6289/10570 [00:16<00:11, 361.46it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▍    | 5738/10570 [00:16<00:14, 339.35it/s]#033[A[1,10]<stderr>:#015 63%|██████▎   | 6625/10570 [00:17<00:10, 387.99it/s][1,5]<stderr>:#015 61%|██████    | 6462/10570 [00:16<00:10, 385.56it/s][1,2]<stderr>:#015 63%|██████▎   | 6683/10570 [00:17<00:09, 394.99it/s][1,7]<stderr>:#015 65%|██████▍   | 6848/10570 [00:17<00:09, 382.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████    | 6396/10570 [00:16<00:11, 360.93it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 64%|██████▍   | 6766/10570 [00:17<00:09, 392.92it/s][1,6]<stderr>:#015 59%|█████▊    | 6191/10570 [00:16<00:11, 369.24it/s][1,4]<stderr>:#015 63%|██████▎   | 6625/10570 [00:17<00:10, 386.44it/s][1,15]<stderr>:#015 62%|██████▏   | 6562/10570 [00:17<00:10, 380.94it/s][1,13]<stderr>:#015 57%|█████▋    | 6053/10570 [00:16<00:12, 358.37it/s][1,1]<stderr>:#015 63%|██████▎   | 6651/10570 [00:17<00:10, 384.17it/s][1,14]<stderr>:#015 62%|██████▏   | 6566/10570 [00:17<00:10, 390.28it/s][1,9]<stderr>:#015 62%|██████▏   | 6528/10570 [00:17<00:10, 383.02it/s][1,3]<stderr>:#015 60%|█████▉    | 6305/10570 [00:16<00:11, 366.35it/s][1,11]<stderr>:#015 60%|█████▉    | 6327/10570 [00:16<00:11, 365.45it/s][1,10]<stderr>:#015 63%|██████▎   | 6665/10570 [00:17<00:09, 390.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▍    | 5773/10570 [00:16<00:14, 339.53it/s]#033[A[1,5]<stderr>:#015 62%|██████▏   | 6501/10570 [00:16<00:10, 385.37it/s][1,7]<stderr>:#015 65%|██████▌   | 6887/10570 [00:17<00:09, 381.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████    | 6436/10570 [00:16<00:11, 369.96it/s]#033[A[1,2]<stderr>:#015 64%|██████▎   | 6723/10570 [00:17<00:10, 381.64it/s][1,12]<stderr>:#015 64%|██████▍   | 6806/10570 [00:17<00:09, 388.54it/s][1,4]<stderr>:#015 63%|██████▎   | 6666/10570 [00:17<00:09, 390.79it/s][1,15]<stderr>:#015 62%|██████▏   | 6601/10570 [00:17<00:10, 376.40it/s][1,13]<stderr>:#015 58%|█████▊    | 6089/10570 [00:16<00:12, 352.17it/s][1,6]<stderr>:#015 59%|█████▉    | 6228/10570 [00:16<00:12, 349.95it/s][1,1]<stderr>:#015 63%|██████▎   | 6690/10570 [00:17<00:10, 382.88it/s][1,9]<stderr>:#015 62%|██████▏   | 6568/10570 [00:17<00:10, 386.19it/s][1,14]<stderr>:#015 62%|██████▏   | 6606/10570 [00:17<00:10, 383.46it/s][1,3]<stderr>:#015 60%|██████    | 6343/10570 [00:17<00:11, 369.40it/s][1,11]<stderr>:#015 60%|██████    | 6366/10570 [00:17<00:11, 371.20it/s][1,5]<stderr>:#015 62%|██████▏   | 6541/10570 [00:17<00:10, 387.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▍    | 5808/10570 [00:16<00:14, 338.53it/s]#033[A[1,10]<stderr>:#015 63%|██████▎   | 6705/10570 [00:17<00:10, 383.95it/s][1,7]<stderr>:#015 66%|██████▌   | 6927/10570 [00:17<00:09, 385.64it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████▏   | 6476/10570 [00:17<00:10, 378.37it/s]#033[A[1,2]<stderr>:#015 64%|██████▍   | 6764/10570 [00:17<00:09, 389.65it/s][1,12]<stderr>:#015 65%|██████▍   | 6845/10570 [00:17<00:09, 380.58it/s][1,4]<stderr>:#015 63%|██████▎   | 6706/10570 [00:17<00:10, 382.80it/s][1,15]<stderr>:#015 63%|██████▎   | 6642/10570 [00:17<00:10, 385.55it/s][1,13]<stderr>:#015 58%|█████▊    | 6126/10570 [00:16<00:12, 356.78it/s][1,6]<stderr>:#015 59%|█████▉    | 6265/10570 [00:16<00:12, 353.57it/s][1,1]<stderr>:#015 64%|██████▎   | 6729/10570 [00:17<00:10, 378.06it/s][1,14]<stderr>:#015 63%|██████▎   | 6646/10570 [00:17<00:10, 387.47it/s][1,9]<stderr>:#015 63%|██████▎   | 6607/10570 [00:17<00:10, 377.74it/s][1,3]<stderr>:#015 60%|██████    | 6381/10570 [00:17<00:11, 353.85it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▌    | 5844/10570 [00:16<00:13, 344.69it/s]#033[A[1,5]<stderr>:#015 62%|██████▏   | 6580/10570 [00:17<00:10, 386.34it/s][1,11]<stderr>:#015 61%|██████    | 6404/10570 [00:17<00:11, 358.21it/s][1,10]<stderr>:#015 64%|██████▍   | 6744/10570 [00:17<00:09, 382.94it/s][1,7]<stderr>:#015 66%|██████▌   | 6969/10570 [00:17<00:09, 394.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6515/10570 [00:17<00:10, 380.00it/s]#033[A[1,2]<stderr>:#015 64%|██████▍   | 6804/10570 [00:17<00:09, 386.46it/s][1,12]<stderr>:#015 65%|██████▌   | 6884/10570 [00:17<00:09, 380.28it/s][1,15]<stderr>:#015 63%|██████▎   | 6682/10570 [00:17<00:10, 388.49it/s][1,4]<stderr>:#015 64%|██████▍   | 6745/10570 [00:17<00:09, 382.55it/s][1,13]<stderr>:#015 58%|█████▊    | 6164/10570 [00:17<00:12, 361.22it/s][1,6]<stderr>:#015 60%|█████▉    | 6304/10570 [00:17<00:11, 363.34it/s][1,1]<stderr>:#015 64%|██████▍   | 6770/10570 [00:17<00:09, 386.62it/s][1,14]<stderr>:#015 63%|██████▎   | 6686/10570 [00:17<00:10, 388.13it/s][1,9]<stderr>:#015 63%|██████▎   | 6649/10570 [00:17<00:10, 386.80it/s][1,3]<stderr>:#015 61%|██████    | 6421/10570 [00:17<00:11, 364.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▌    | 5879/10570 [00:16<00:13, 345.70it/s]#033[A[1,5]<stderr>:#015 63%|██████▎   | 6619/10570 [00:17<00:10, 381.44it/s][1,11]<stderr>:#015 61%|██████    | 6445/10570 [00:17<00:11, 370.87it/s][1,10]<stderr>:#015 64%|██████▍   | 6784/10570 [00:17<00:09, 386.25it/s][1,7]<stderr>:#015 66%|██████▋   | 7010/10570 [00:17<00:08, 398.88it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6554/10570 [00:17<00:10, 382.82it/s]#033[A[1,2]<stderr>:#015 65%|██████▍   | 6843/10570 [00:17<00:09, 375.65it/s][1,12]<stderr>:#015 65%|██████▌   | 6923/10570 [00:17<00:09, 382.07it/s][1,4]<stderr>:#015 64%|██████▍   | 6784/10570 [00:17<00:09, 384.09it/s][1,6]<stderr>:#015 60%|██████    | 6342/10570 [00:17<00:11, 367.04it/s][1,13]<stderr>:#015 59%|█████▊    | 6201/10570 [00:17<00:12, 360.72it/s][1,15]<stderr>:#015 64%|██████▎   | 6721/10570 [00:17<00:10, 376.61it/s][1,1]<stderr>:#015 64%|██████▍   | 6809/10570 [00:17<00:09, 381.48it/s][1,9]<stderr>:#015 63%|██████▎   | 6688/10570 [00:17<00:10, 385.04it/s][1,14]<stderr>:#015 64%|██████▎   | 6725/10570 [00:17<00:10, 379.46it/s][1,3]<stderr>:#015 61%|██████    | 6462/10570 [00:17<00:10, 377.02it/s][1,11]<stderr>:#015 61%|██████▏   | 6484/10570 [00:17<00:10, 376.00it/s][1,5]<stderr>:#015 63%|██████▎   | 6658/10570 [00:17<00:10, 382.55it/s][1,7]<stderr>:#015 67%|██████▋   | 7052/10570 [00:17<00:08, 403.83it/s][1,10]<stderr>:#015 65%|██████▍   | 6823/10570 [00:17<00:09, 382.68it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▌    | 5914/10570 [00:16<00:14, 332.51it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6593/10570 [00:17<00:10, 377.54it/s]#033[A[1,2]<stderr>:#015 65%|██████▌   | 6881/10570 [00:17<00:09, 375.75it/s][1,12]<stderr>:#015 66%|██████▌   | 6965/10570 [00:17<00:09, 391.81it/s][1,4]<stderr>:#015 65%|██████▍   | 6823/10570 [00:17<00:09, 380.13it/s][1,15]<stderr>:#015 64%|██████▍   | 6762/10570 [00:17<00:09, 384.29it/s][1,1]<stderr>:#015 65%|██████▍   | 6848/10570 [00:17<00:09, 375.16it/s][1,6]<stderr>:#015 60%|██████    | 6379/10570 [00:17<00:11, 350.07it/s][1,13]<stderr>:#015 59%|█████▉    | 6238/10570 [00:17<00:12, 345.32it/s][1,14]<stderr>:#015 64%|██████▍   | 6766/10570 [00:17<00:09, 387.30it/s][1,9]<stderr>:#015 64%|██████▎   | 6727/10570 [00:17<00:10, 377.53it/s][1,3]<stderr>:#015 61%|██████▏   | 6500/10570 [00:17<00:10, 376.50it/s][1,11]<stderr>:#015 62%|██████▏   | 6522/10570 [00:17<00:10, 373.72it/s][1,7]<stderr>:#015 67%|██████▋   | 7093/10570 [00:18<00:08, 403.98it/s][1,5]<stderr>:#015 63%|██████▎   | 6697/10570 [00:17<00:10, 378.28it/s][1,10]<stderr>:#015 65%|██████▍   | 6862/10570 [00:17<00:09, 379.69it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 63%|██████▎   | 6633/10570 [00:17<00:10, 383.37it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▋    | 5950/10570 [00:16<00:13, 338.53it/s]#033[A[1,2]<stderr>:#015 65%|██████▌   | 6919/10570 [00:17<00:09, 366.37it/s][1,12]<stderr>:#015 66%|██████▋   | 7007/10570 [00:18<00:08, 397.64it/s][1,4]<stderr>:#015 65%|██████▍   | 6862/10570 [00:17<00:09, 377.05it/s][1,15]<stderr>:#015 64%|██████▍   | 6801/10570 [00:17<00:09, 378.88it/s][1,6]<stderr>:#015 61%|██████    | 6419/10570 [00:17<00:11, 362.56it/s][1,1]<stderr>:#015 65%|██████▌   | 6886/10570 [00:17<00:09, 374.04it/s][1,13]<stderr>:#015 59%|█████▉    | 6274/10570 [00:17<00:12, 349.15it/s][1,14]<stderr>:#015 64%|██████▍   | 6805/10570 [00:17<00:09, 383.87it/s][1,9]<stderr>:#015 64%|██████▍   | 6768/10570 [00:17<00:09, 384.77it/s][1,3]<stderr>:#015 62%|██████▏   | 6539/10570 [00:17<00:10, 378.44it/s][1,11]<stderr>:#015 62%|██████▏   | 6561/10570 [00:17<00:10, 377.35it/s][1,7]<stderr>:#015 68%|██████▊   | 7135/10570 [00:18<00:08, 407.25it/s][1,5]<stderr>:#015 64%|██████▎   | 6735/10570 [00:17<00:10, 377.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 5987/10570 [00:16<00:13, 347.19it/s]#033[A[1,10]<stderr>:#015 65%|██████▌   | 6900/10570 [00:17<00:09, 376.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 63%|██████▎   | 6673/10570 [00:17<00:10, 385.64it/s]#033[A[1,2]<stderr>:#015 66%|██████▌   | 6958/10570 [00:18<00:09, 372.72it/s][1,12]<stderr>:#015 67%|██████▋   | 7049/10570 [00:18<00:08, 402.15it/s][1,4]<stderr>:#015 65%|██████▌   | 6900/10570 [00:17<00:10, 366.75it/s][1,6]<stderr>:#015 61%|██████    | 6460/10570 [00:17<00:10, 375.34it/s][1,15]<stderr>:#015 65%|██████▍   | 6840/10570 [00:17<00:10, 371.68it/s][1,1]<stderr>:#015 66%|██████▌   | 6925/10570 [00:18<00:09, 376.47it/s][1,13]<stderr>:#015 60%|█████▉    | 6313/10570 [00:17<00:11, 358.47it/s][1,14]<stderr>:#015 65%|██████▍   | 6844/10570 [00:17<00:09, 376.59it/s][1,9]<stderr>:#015 64%|██████▍   | 6807/10570 [00:17<00:09, 379.67it/s][1,3]<stderr>:#015 62%|██████▏   | 6577/10570 [00:17<00:10, 377.58it/s][1,7]<stderr>:#015 68%|██████▊   | 7176/10570 [00:18<00:08, 407.72it/s][1,5]<stderr>:#015 64%|██████▍   | 6776/10570 [00:17<00:09, 384.09it/s][1,11]<stderr>:#015 62%|██████▏   | 6599/10570 [00:17<00:10, 370.89it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 6022/10570 [00:16<00:13, 346.81it/s]#033[A[1,10]<stderr>:#015 66%|██████▌   | 6941/10570 [00:18<00:09, 384.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▎   | 6712/10570 [00:17<00:10, 373.29it/s][1,0]<stderr>:#033[A[1,2]<stderr>:#015 66%|██████▌   | 7000/10570 [00:18<00:09, 384.74it/s][1,12]<stderr>:#015 67%|██████▋   | 7090/10570 [00:18<00:08, 400.48it/s][1,4]<stderr>:#015 66%|██████▌   | 6939/10570 [00:18<00:09, 371.84it/s][1,6]<stderr>:#015 61%|██████▏   | 6498/10570 [00:17<00:10, 374.89it/s][1,1]<stderr>:#015 66%|██████▌   | 6966/10570 [00:18<00:09, 385.56it/s][1,15]<stderr>:#015 65%|██████▌   | 6878/10570 [00:18<00:09, 371.83it/s][1,13]<stderr>:#015 60%|██████    | 6350/10570 [00:17<00:11, 359.99it/s][1,14]<stderr>:#015 65%|██████▌   | 6882/10570 [00:17<00:09, 376.64it/s][1,3]<stderr>:#015 63%|██████▎   | 6615/10570 [00:17<00:10, 372.55it/s][1,9]<stderr>:#015 65%|██████▍   | 6846/10570 [00:18<00:09, 373.04it/s][1,11]<stderr>:#015 63%|██████▎   | 6639/10570 [00:17<00:10, 378.81it/s][1,10]<stderr>:#015 66%|██████▌   | 6982/10570 [00:18<00:09, 390.49it/s][1,7]<stderr>:#015 68%|██████▊   | 7217/10570 [00:18<00:08, 395.10it/s][1,5]<stderr>:#015 64%|██████▍   | 6815/10570 [00:17<00:09, 378.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 6057/10570 [00:17<00:13, 339.25it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▍   | 6752/10570 [00:17<00:10, 379.49it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7041/10570 [00:18<00:09, 390.97it/s][1,12]<stderr>:#015 67%|██████▋   | 7131/10570 [00:18<00:08, 393.69it/s][1,4]<stderr>:#015 66%|██████▌   | 6980/10570 [00:18<00:09, 380.51it/s][1,6]<stderr>:#015 62%|██████▏   | 6536/10570 [00:17<00:10, 375.66it/s][1,1]<stderr>:#015 66%|██████▋   | 7007/10570 [00:18<00:09, 391.66it/s][1,15]<stderr>:#015 65%|██████▌   | 6916/10570 [00:18<00:09, 372.46it/s][1,13]<stderr>:#015 60%|██████    | 6387/10570 [00:17<00:12, 347.46it/s][1,14]<stderr>:#015 65%|██████▌   | 6920/10570 [00:17<00:09, 374.05it/s][1,3]<stderr>:#015 63%|██████▎   | 6655/10570 [00:17<00:10, 378.71it/s][1,9]<stderr>:#015 65%|██████▌   | 6884/10570 [00:18<00:09, 372.48it/s][1,11]<stderr>:#015 63%|██████▎   | 6678/10570 [00:17<00:10, 380.26it/s][1,10]<stderr>:#015 66%|██████▋   | 7024/10570 [00:18<00:08, 398.42it/s][1,7]<stderr>:#015 69%|██████▊   | 7257/10570 [00:18<00:08, 393.16it/s][1,5]<stderr>:#015 65%|██████▍   | 6853/10570 [00:17<00:09, 374.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6092/10570 [00:17<00:13, 337.67it/s][1,8]<stderr>:#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▍   | 6792/10570 [00:17<00:09, 383.07it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7081/10570 [00:18<00:08, 391.43it/s][1,12]<stderr>:#015 68%|██████▊   | 7172/10570 [00:18<00:08, 397.89it/s][1,4]<stderr>:#015 66%|██████▋   | 7022/10570 [00:18<00:09, 390.21it/s][1,1]<stderr>:#015 67%|██████▋   | 7048/10570 [00:18<00:08, 396.73it/s][1,6]<stderr>:#015 62%|██████▏   | 6574/10570 [00:17<00:10, 374.55it/s][1,15]<stderr>:#015 66%|██████▌   | 6956/10570 [00:18<00:09, 379.99it/s][1,13]<stderr>:#015 61%|██████    | 6426/10570 [00:17<00:11, 358.13it/s][1,14]<stderr>:#015 66%|██████▌   | 6960/10570 [00:18<00:09, 380.83it/s][1,3]<stderr>:#015 63%|██████▎   | 6693/10570 [00:17<00:10, 373.51it/s][1,9]<stderr>:#015 65%|██████▌   | 6922/10570 [00:18<00:10, 364.41it/s][1,10]<stderr>:#015 67%|██████▋   | 7065/10570 [00:18<00:08, 399.30it/s][1,7]<stderr>:#015 69%|██████▉   | 7297/10570 [00:18<00:08, 393.70it/s][1,5]<stderr>:#015 65%|██████▌   | 6891/10570 [00:18<00:09, 371.97it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6128/10570 [00:17<00:12, 344.03it/s]#033[A[1,11]<stderr>:#015 64%|██████▎   | 6717/10570 [00:17<00:10, 366.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▍   | 6831/10570 [00:18<00:10, 373.61it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7124/10570 [00:18<00:08, 399.38it/s][1,12]<stderr>:#015 68%|██████▊   | 7212/10570 [00:18<00:08, 392.81it/s][1,4]<stderr>:#015 67%|██████▋   | 7062/10570 [00:18<00:08, 392.94it/s][1,1]<stderr>:#015 67%|██████▋   | 7088/10570 [00:18<00:08, 395.14it/s][1,15]<stderr>:#015 66%|██████▌   | 6997/10570 [00:18<00:09, 388.50it/s][1,6]<stderr>:#015 63%|██████▎   | 6612/10570 [00:17<00:10, 369.20it/s][1,13]<stderr>:#015 61%|██████    | 6466/10570 [00:17<00:11, 369.23it/s][1,14]<stderr>:#015 66%|██████▌   | 7002/10570 [00:18<00:09, 389.33it/s][1,3]<stderr>:#015 64%|██████▎   | 6731/10570 [00:18<00:10, 369.86it/s][1,9]<stderr>:#015 66%|██████▌   | 6961/10570 [00:18<00:09, 371.45it/s][1,10]<stderr>:#015 67%|██████▋   | 7105/10570 [00:18<00:08, 398.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6165/10570 [00:17<00:12, 349.81it/s]#033[A[1,5]<stderr>:#015 66%|██████▌   | 6929/10570 [00:18<00:09, 369.46it/s][1,11]<stderr>:#015 64%|██████▍   | 6757/10570 [00:18<00:10, 373.93it/s][1,2]<stderr>:#015 68%|██████▊   | 7165/10570 [00:18<00:08, 401.11it/s][1,7]<stderr>:#015 69%|██████▉   | 7337/10570 [00:18<00:08, 371.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▍   | 6869/10570 [00:18<00:10, 366.52it/s]#033[A[1,12]<stderr>:#015 69%|██████▊   | 7252/10570 [00:18<00:08, 390.80it/s][1,4]<stderr>:#015 67%|██████▋   | 7102/10570 [00:18<00:08, 394.36it/s][1,15]<stderr>:#015 67%|██████▋   | 7038/10570 [00:18<00:08, 394.48it/s][1,1]<stderr>:#015 67%|██████▋   | 7129/10570 [00:18<00:08, 398.73it/s][1,6]<stderr>:#015 63%|██████▎   | 6652/10570 [00:17<00:10, 376.44it/s][1,13]<stderr>:#015 62%|██████▏   | 6504/10570 [00:17<00:11, 367.80it/s][1,14]<stderr>:#015 67%|██████▋   | 7043/10570 [00:18<00:08, 394.69it/s][1,3]<stderr>:#015 64%|██████▍   | 6771/10570 [00:18<00:10, 377.36it/s][1,9]<stderr>:#015 66%|██████▌   | 7002/10570 [00:18<00:09, 380.52it/s][1,10]<stderr>:#015 68%|██████▊   | 7147/10570 [00:18<00:08, 403.96it/s][1,5]<stderr>:#015 66%|██████▌   | 6969/10570 [00:18<00:09, 376.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▊    | 6201/10570 [00:17<00:12, 348.82it/s]#033[A[1,11]<stderr>:#015 64%|██████▍   | 6796/10570 [00:18<00:10, 375.50it/s][1,7]<stderr>:#015 70%|██████▉   | 7375/10570 [00:18<00:08, 370.40it/s][1,2]<stderr>:#015 68%|██████▊   | 7206/10570 [00:18<00:08, 394.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▌   | 6906/10570 [00:18<00:10, 363.83it/s]#033[A[1,12]<stderr>:#015 69%|██████▉   | 7292/10570 [00:18<00:08, 390.51it/s][1,4]<stderr>:#015 68%|██████▊   | 7144/10570 [00:18<00:08, 399.85it/s][1,1]<stderr>:#015 68%|██████▊   | 7170/10570 [00:18<00:08, 401.13it/s][1,15]<stderr>:#015 67%|██████▋   | 7078/10570 [00:18<00:08, 391.82it/s][1,6]<stderr>:#015 63%|██████▎   | 6690/10570 [00:18<00:10, 373.00it/s][1,13]<stderr>:#015 62%|██████▏   | 6542/10570 [00:18<00:10, 369.99it/s][1,14]<stderr>:#015 67%|██████▋   | 7083/10570 [00:18<00:08, 393.06it/s][1,9]<stderr>:#015 67%|██████▋   | 7043/10570 [00:18<00:09, 387.40it/s][1,3]<stderr>:#015 64%|██████▍   | 6809/10570 [00:18<00:10, 371.19it/s][1,10]<stderr>:#015 68%|██████▊   | 7188/10570 [00:18<00:08, 401.41it/s][1,5]<stderr>:#015 66%|██████▋   | 7010/10570 [00:18<00:09, 384.45it/s][1,11]<stderr>:#015 65%|██████▍   | 6834/10570 [00:18<00:10, 366.63it/s][1,7]<stderr>:#015 70%|███████   | 7414/10570 [00:18<00:08, 375.76it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▉    | 6236/10570 [00:17<00:13, 332.36it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▌   | 6947/10570 [00:18<00:09, 375.86it/s]#033[A[1,2]<stderr>:#015 69%|██████▊   | 7246/10570 [00:18<00:08, 388.51it/s][1,12]<stderr>:#015 69%|██████▉   | 7332/10570 [00:18<00:08, 386.63it/s][1,4]<stderr>:#015 68%|██████▊   | 7185/10570 [00:18<00:08, 395.46it/s][1,15]<stderr>:#015 67%|██████▋   | 7120/10570 [00:18<00:08, 397.46it/s][1,1]<stderr>:#015 68%|██████▊   | 7211/10570 [00:18<00:08, 393.35it/s][1,6]<stderr>:#015 64%|██████▎   | 6728/10570 [00:18<00:10, 366.24it/s][1,14]<stderr>:#015 67%|██████▋   | 7125/10570 [00:18<00:08, 399.81it/s][1,13]<stderr>:#015 62%|██████▏   | 6580/10570 [00:18<00:10, 367.54it/s][1,9]<stderr>:#015 67%|██████▋   | 7082/10570 [00:18<00:09, 387.06it/s][1,3]<stderr>:#015 65%|██████▍   | 6847/10570 [00:18<00:10, 364.25it/s][1,10]<stderr>:#015 68%|██████▊   | 7229/10570 [00:18<00:08, 392.46it/s][1,5]<stderr>:#015 67%|██████▋   | 7051/10570 [00:18<00:09, 390.26it/s][1,11]<stderr>:#015 65%|██████▌   | 6871/10570 [00:18<00:10, 364.37it/s][1,7]<stderr>:#015 71%|███████   | 7455/10570 [00:19<00:08, 382.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▉    | 6271/10570 [00:17<00:12, 336.11it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▌   | 6987/10570 [00:18<00:09, 382.64it/s]#033[A[1,2]<stderr>:#015 69%|██████▉   | 7285/10570 [00:18<00:08, 387.52it/s][1,4]<stderr>:#015 68%|██████▊   | 7225/10570 [00:18<00:08, 390.74it/s][1,15]<stderr>:#015 68%|██████▊   | 7161/10570 [00:18<00:08, 399.59it/s][1,12]<stderr>:#015 70%|██████▉   | 7371/10570 [00:18<00:08, 363.14it/s][1,1]<stderr>:#015 69%|██████▊   | 7251/10570 [00:18<00:08, 388.89it/s][1,6]<stderr>:#015 64%|██████▍   | 6768/10570 [00:18<00:10, 373.97it/s][1,14]<stderr>:#015 68%|██████▊   | 7166/10570 [00:18<00:08, 401.67it/s][1,13]<stderr>:#015 63%|██████▎   | 6617/10570 [00:18<00:10, 363.29it/s][1,9]<stderr>:#015 67%|██████▋   | 7124/10570 [00:18<00:08, 395.01it/s][1,3]<stderr>:#015 65%|██████▌   | 6884/10570 [00:18<00:10, 362.77it/s][1,5]<stderr>:#015 67%|██████▋   | 7091/10570 [00:18<00:08, 391.01it/s][1,10]<stderr>:#015 69%|██████▉   | 7269/10570 [00:18<00:08, 388.76it/s][1,7]<stderr>:#015 71%|███████   | 7495/10570 [00:19<00:07, 386.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|█████▉    | 6309/10570 [00:17<00:12, 347.28it/s]#033[A[1,11]<stderr>:#015 65%|██████▌   | 6908/10570 [00:18<00:10, 360.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▋   | 7027/10570 [00:18<00:09, 386.27it/s]#033[A[1,2]<stderr>:#015 69%|██████▉   | 7325/10570 [00:18<00:08, 389.72it/s][1,12]<stderr>:#015 70%|███████   | 7410/10570 [00:19<00:08, 368.90it/s][1,4]<stderr>:#015 69%|██████▊   | 7265/10570 [00:18<00:08, 384.63it/s][1,15]<stderr>:#015 68%|██████▊   | 7202/10570 [00:18<00:08, 392.81it/s][1,1]<stderr>:#015 69%|██████▉   | 7290/10570 [00:18<00:08, 387.96it/s][1,6]<stderr>:#015 64%|██████▍   | 6806/10570 [00:18<00:10, 368.71it/s][1,13]<stderr>:#015 63%|██████▎   | 6656/10570 [00:18<00:10, 369.58it/s][1,14]<stderr>:#015 68%|██████▊   | 7207/10570 [00:18<00:08, 394.69it/s][1,9]<stderr>:#015 68%|██████▊   | 7165/10570 [00:18<00:08, 396.99it/s][1,5]<stderr>:#015 67%|██████▋   | 7132/10570 [00:18<00:08, 394.40it/s][1,10]<stderr>:#015 69%|██████▉   | 7308/10570 [00:18<00:08, 382.07it/s][1,3]<stderr>:#015 65%|██████▌   | 6921/10570 [00:18<00:10, 350.08it/s][1,11]<stderr>:#015 66%|██████▌   | 6949/10570 [00:18<00:09, 373.56it/s][1,7]<stderr>:#015 71%|███████▏  | 7537/10570 [00:19<00:07, 393.89it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|██████    | 6345/10570 [00:17<00:12, 349.68it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 67%|██████▋   | 7067/10570 [00:18<00:09, 388.22it/s]#033[A[1,2]<stderr>:#015 70%|██████▉   | 7365/10570 [00:19<00:08, 361.12it/s][1,12]<stderr>:#015 70%|███████   | 7451/10570 [00:19<00:08, 378.09it/s][1,4]<stderr>:#015 69%|██████▉   | 7305/10570 [00:18<00:08, 387.63it/s][1,1]<stderr>:#015 69%|██████▉   | 7329/10570 [00:19<00:08, 387.65it/s][1,15]<stderr>:#015 69%|██████▊   | 7242/10570 [00:18<00:08, 386.97it/s][1,6]<stderr>:#015 65%|██████▍   | 6843/10570 [00:18<00:10, 359.77it/s][1,13]<stderr>:#015 63%|██████▎   | 6694/10570 [00:18<00:10, 364.88it/s][1,14]<stderr>:#015 69%|██████▊   | 7247/10570 [00:18<00:08, 388.97it/s][1,9]<stderr>:#015 68%|██████▊   | 7205/10570 [00:18<00:08, 391.06it/s][1,5]<stderr>:#015 68%|██████▊   | 7173/10570 [00:18<00:08, 396.46it/s][1,3]<stderr>:#015 66%|██████▌   | 6959/10570 [00:18<00:10, 357.91it/s][1,11]<stderr>:#015 66%|██████▌   | 6989/10570 [00:18<00:09, 379.97it/s][1,7]<stderr>:#015 72%|███████▏  | 7579/10570 [00:19<00:07, 399.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 67%|██████▋   | 7107/10570 [00:18<00:08, 389.33it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|██████    | 6381/10570 [00:18<00:12, 333.21it/s]#033[A[1,10]<stderr>:#015 70%|██████▉   | 7347/10570 [00:19<00:09, 349.57it/s][1,2]<stderr>:#015 70%|███████   | 7403/10570 [00:19<00:08, 364.07it/s][1,12]<stderr>:#015 71%|███████   | 7491/10570 [00:19<00:08, 382.00it/s][1,15]<stderr>:#015 69%|██████▉   | 7281/10570 [00:19<00:08, 385.40it/s][1,6]<stderr>:#015 65%|██████▌   | 6880/10570 [00:18<00:10, 359.58it/s][1,14]<stderr>:#015 69%|██████▉   | 7286/10570 [00:18<00:08, 387.77it/s][1,1]<stderr>:#015 70%|██████▉   | 7368/10570 [00:19<00:08, 358.61it/s][1,13]<stderr>:#015 64%|██████▎   | 6731/10570 [00:18<00:10, 359.78it/s][1,4]<stderr>:#015 69%|██████▉   | 7344/10570 [00:19<00:09, 351.96it/s][1,9]<stderr>:#015 69%|██████▊   | 7245/10570 [00:19<00:08, 383.96it/s][1,3]<stderr>:#015 66%|██████▌   | 6999/10570 [00:18<00:09, 369.56it/s][1,5]<stderr>:#015 68%|██████▊   | 7213/10570 [00:18<00:08, 390.49it/s][1,7]<stderr>:#015 72%|███████▏  | 7620/10570 [00:19<00:07, 401.32it/s][1,11]<stderr>:#015 67%|██████▋   | 7030/10570 [00:18<00:09, 386.23it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7148/10570 [00:18<00:08, 394.35it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████    | 6419/10570 [00:18<00:12, 344.96it/s]#033[A[1,10]<stderr>:#015 70%|██████▉   | 7387/10570 [00:19<00:08, 361.68it/s][1,2]<stderr>:#015 70%|███████   | 7444/10570 [00:19<00:08, 373.98it/s][1,12]<stderr>:#015 71%|███████▏  | 7533/10570 [00:19<00:07, 390.44it/s][1,15]<stderr>:#015 69%|██████▉   | 7321/10570 [00:19<00:08, 387.94it/s][1,6]<stderr>:#015 65%|██████▌   | 6917/10570 [00:18<00:10, 360.55it/s][1,14]<stderr>:#015 69%|██████▉   | 7325/10570 [00:18<00:08, 384.89it/s][1,13]<stderr>:#015 64%|██████▍   | 6768/10570 [00:18<00:10, 362.30it/s][1,1]<stderr>:#015 70%|███████   | 7405/10570 [00:19<00:08, 361.16it/s][1,4]<stderr>:#015 70%|██████▉   | 7384/10570 [00:19<00:08, 364.80it/s][1,9]<stderr>:#015 69%|██████▉   | 7284/10570 [00:19<00:08, 383.56it/s][1,3]<stderr>:#015 67%|██████▋   | 7039/10570 [00:18<00:09, 377.43it/s][1,5]<stderr>:#015 69%|██████▊   | 7253/10570 [00:18<00:08, 385.51it/s][1,11]<stderr>:#015 67%|██████▋   | 7069/10570 [00:18<00:09, 386.12it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7188/10570 [00:18<00:08, 392.79it/s]#033[A[1,7]<stderr>:#015 72%|███████▏  | 7661/10570 [00:19<00:07, 386.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████    | 6459/10570 [00:18<00:11, 357.55it/s]#033[A[1,10]<stderr>:#015 70%|███████   | 7428/10570 [00:19<00:08, 374.91it/s][1,2]<stderr>:#015 71%|███████   | 7485/10570 [00:19<00:08, 381.46it/s][1,12]<stderr>:#015 72%|███████▏  | 7573/10570 [00:19<00:07, 392.65it/s][1,6]<stderr>:#015 66%|██████▌   | 6957/10570 [00:18<00:09, 370.48it/s][1,1]<stderr>:#015 70%|███████   | 7445/10570 [00:19<00:08, 370.65it/s][1,4]<stderr>:#015 70%|███████   | 7424/10570 [00:19<00:08, 374.17it/s][1,15]<stderr>:#015 70%|██████▉   | 7360/10570 [00:19<00:09, 354.51it/s][1,13]<stderr>:#015 64%|██████▍   | 6805/10570 [00:18<00:10, 351.17it/s][1,9]<stderr>:#015 69%|██████▉   | 7323/10570 [00:19<00:08, 378.71it/s][1,3]<stderr>:#015 67%|██████▋   | 7077/10570 [00:18<00:09, 377.13it/s][1,14]<stderr>:#015 70%|██████▉   | 7364/10570 [00:19<00:08, 357.10it/s][1,5]<stderr>:#015 69%|██████▉   | 7292/10570 [00:19<00:08, 384.52it/s][1,11]<stderr>:#015 67%|██████▋   | 7108/10570 [00:18<00:08, 385.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7228/10570 [00:19<00:08, 384.97it/s]#033[A[1,7]<stderr>:#015 73%|███████▎  | 7703/10570 [00:19<00:07, 394.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████▏   | 6496/10570 [00:18<00:11, 357.55it/s]#033[A[1,10]<stderr>:#015 71%|███████   | 7466/10570 [00:19<00:08, 374.88it/s][1,2]<stderr>:#015 71%|███████   | 7526/10570 [00:19<00:07, 388.58it/s][1,12]<stderr>:#015 72%|███████▏  | 7615/10570 [00:19<00:07, 398.35it/s][1,6]<stderr>:#015 66%|██████▌   | 6998/10570 [00:18<00:09, 379.12it/s][1,1]<stderr>:#015 71%|███████   | 7485/10570 [00:19<00:08, 378.00it/s][1,4]<stderr>:#015 71%|███████   | 7462/10570 [00:19<00:08, 374.36it/s][1,15]<stderr>:#015 70%|██████▉   | 7398/10570 [00:19<00:08, 360.36it/s][1,13]<stderr>:#015 65%|██████▍   | 6841/10570 [00:18<00:10, 346.14it/s][1,3]<stderr>:#015 67%|██████▋   | 7118/10570 [00:19<00:08, 384.36it/s][1,14]<stderr>:#015 70%|███████   | 7402/10570 [00:19<00:08, 362.11it/s][1,5]<stderr>:#015 69%|██████▉   | 7331/10570 [00:19<00:08, 383.34it/s][1,11]<stderr>:#015 68%|██████▊   | 7149/10570 [00:19<00:08, 390.41it/s][1,9]<stderr>:#015 70%|██████▉   | 7361/10570 [00:19<00:09, 350.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7267/10570 [00:19<00:08, 380.42it/s]#033[A[1,7]<stderr>:#015 73%|███████▎  | 7744/10570 [00:19<00:07, 394.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6532/10570 [00:18<00:11, 357.06it/s]#033[A[1,10]<stderr>:#015 71%|███████   | 7507/10570 [00:19<00:07, 383.15it/s][1,2]<stderr>:#015 72%|███████▏  | 7566/10570 [00:19<00:07, 391.54it/s][1,12]<stderr>:#015 72%|███████▏  | 7655/10570 [00:19<00:07, 392.91it/s][1,6]<stderr>:#015 67%|██████▋   | 7038/10570 [00:19<00:09, 383.63it/s][1,1]<stderr>:#015 71%|███████   | 7526/10570 [00:19<00:07, 385.19it/s][1,4]<stderr>:#015 71%|███████   | 7502/10570 [00:19<00:08, 381.53it/s][1,15]<stderr>:#015 70%|███████   | 7438/10570 [00:19<00:08, 370.56it/s][1,13]<stderr>:#015 65%|██████▌   | 6877/10570 [00:19<00:10, 348.69it/s][1,3]<stderr>:#015 68%|██████▊   | 7158/10570 [00:19<00:08, 387.10it/s][1,14]<stderr>:#015 70%|███████   | 7442/10570 [00:19<00:08, 371.99it/s][1,11]<stderr>:#015 68%|██████▊   | 7189/10570 [00:19<00:08, 387.49it/s][1,9]<stderr>:#015 70%|███████   | 7399/10570 [00:19<00:08, 356.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7307/10570 [00:19<00:08, 383.93it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6569/10570 [00:18<00:11, 358.13it/s]#033[A[1,10]<stderr>:#015 71%|███████▏  | 7548/10570 [00:19<00:07, 389.13it/s][1,5]<stderr>:#015 70%|██████▉   | 7370/10570 [00:19<00:09, 354.89it/s][1,7]<stderr>:#015 74%|███████▎  | 7784/10570 [00:19<00:07, 372.69it/s][1,2]<stderr>:#015 72%|███████▏  | 7606/10570 [00:19<00:07, 393.81it/s][1,12]<stderr>:#015 73%|███████▎  | 7695/10570 [00:19<00:07, 389.11it/s][1,6]<stderr>:#015 67%|██████▋   | 7077/10570 [00:19<00:09, 380.48it/s][1,1]<stderr>:#015 72%|███████▏  | 7566/10570 [00:19<00:07, 387.65it/s][1,4]<stderr>:#015 71%|███████▏  | 7542/10570 [00:19<00:07, 386.57it/s][1,15]<stderr>:#015 71%|███████   | 7478/10570 [00:19<00:08, 376.53it/s][1,13]<stderr>:#015 65%|██████▌   | 6912/10570 [00:19<00:10, 348.21it/s][1,14]<stderr>:#015 71%|███████   | 7482/10570 [00:19<00:08, 379.66it/s][1,3]<stderr>:#015 68%|██████▊   | 7197/10570 [00:19<00:08, 380.93it/s][1,11]<stderr>:#015 68%|██████▊   | 7228/10570 [00:19<00:08, 378.77it/s][1,9]<stderr>:#015 70%|███████   | 7438/10570 [00:19<00:08, 366.16it/s][1,10]<stderr>:#015 72%|███████▏  | 7589/10570 [00:19<00:07, 394.74it/s][1,5]<stderr>:#015 70%|███████   | 7408/10570 [00:19<00:08, 359.78it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6605/10570 [00:18<00:11, 349.85it/s]#033[A[1,7]<stderr>:#015 74%|███████▍  | 7823/10570 [00:19<00:07, 376.34it/s][1,2]<stderr>:#015 72%|███████▏  | 7646/10570 [00:19<00:07, 381.09it/s][1,12]<stderr>:#015 73%|███████▎  | 7736/10570 [00:19<00:07, 392.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7346/10570 [00:19<00:09, 347.18it/s]#033[A[1,6]<stderr>:#015 67%|██████▋   | 7117/10570 [00:19<00:08, 385.73it/s][1,1]<stderr>:#015 72%|███████▏  | 7607/10570 [00:19<00:07, 393.35it/s][1,4]<stderr>:#015 72%|███████▏  | 7583/10570 [00:19<00:07, 392.45it/s][1,15]<stderr>:#015 71%|███████   | 7518/10570 [00:19<00:07, 382.68it/s][1,13]<stderr>:#015 66%|██████▌   | 6952/10570 [00:19<00:10, 360.90it/s][1,14]<stderr>:#015 71%|███████   | 7522/10570 [00:19<00:07, 384.63it/s][1,3]<stderr>:#015 68%|██████▊   | 7236/10570 [00:19<00:08, 373.60it/s][1,9]<stderr>:#015 71%|███████   | 7477/10570 [00:19<00:08, 371.33it/s][1,11]<stderr>:#015 69%|██████▊   | 7266/10570 [00:19<00:08, 374.12it/s][1,10]<stderr>:#015 72%|███████▏  | 7630/10570 [00:19<00:07, 398.78it/s][1,5]<stderr>:#015 70%|███████   | 7448/10570 [00:19<00:08, 369.90it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 63%|██████▎   | 6644/10570 [00:18<00:10, 358.71it/s]#033[A[1,7]<stderr>:#015 74%|███████▍  | 7864/10570 [00:20<00:07, 383.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 70%|██████▉   | 7386/10570 [00:19<00:08, 358.80it/s]#033[A[1,2]<stderr>:#015 73%|███████▎  | 7685/10570 [00:19<00:07, 373.37it/s][1,6]<stderr>:#015 68%|██████▊   | 7157/10570 [00:19<00:08, 387.50it/s][1,12]<stderr>:#015 74%|███████▎  | 7776/10570 [00:20<00:07, 371.07it/s][1,4]<stderr>:#015 72%|███████▏  | 7623/10570 [00:19<00:07, 393.85it/s][1,1]<stderr>:#015 72%|███████▏  | 7647/10570 [00:19<00:07, 385.09it/s][1,15]<stderr>:#015 72%|███████▏  | 7558/10570 [00:19<00:07, 385.54it/s][1,13]<stderr>:#015 66%|██████▌   | 6991/10570 [00:19<00:09, 368.67it/s][1,14]<stderr>:#015 72%|███████▏  | 7561/10570 [00:19<00:07, 384.86it/s][1,3]<stderr>:#015 69%|██████▉   | 7274/10570 [00:19<00:08, 373.28it/s][1,9]<stderr>:#015 71%|███████   | 7517/10570 [00:19<00:08, 378.75it/s][1,11]<stderr>:#015 69%|██████▉   | 7305/10570 [00:19<00:08, 376.68it/s][1,5]<stderr>:#015 71%|███████   | 7487/10570 [00:19<00:08, 375.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 63%|██████▎   | 6681/10570 [00:18<00:10, 360.09it/s]#033[A[1,7]<stderr>:#015 75%|███████▍  | 7904/10570 [00:20<00:06, 386.81it/s][1,10]<stderr>:#015 73%|███████▎  | 7671/10570 [00:19<00:07, 383.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 70%|███████   | 7427/10570 [00:19<00:08, 370.78it/s]#033[A[1,2]<stderr>:#015 73%|███████▎  | 7725/10570 [00:20<00:07, 379.32it/s][1,12]<stderr>:#015 74%|███████▍  | 7814/10570 [00:20<00:07, 373.38it/s][1,6]<stderr>:#015 68%|██████▊   | 7196/10570 [00:19<00:08, 381.94it/s][1,15]<stderr>:#015 72%|███████▏  | 7599/10570 [00:19<00:07, 391.64it/s][1,13]<stderr>:#015 67%|██████▋   | 7030/10570 [00:19<00:09, 374.51it/s][1,1]<stderr>:#015 73%|███████▎  | 7686/10570 [00:19<00:07, 369.75it/s][1,4]<stderr>:#015 72%|███████▏  | 7663/10570 [00:19<00:07, 370.01it/s][1,14]<stderr>:#015 72%|███████▏  | 7603/10570 [00:19<00:07, 392.25it/s][1,3]<stderr>:#015 69%|██████▉   | 7312/10570 [00:19<00:08, 369.99it/s][1,9]<stderr>:#015 71%|███████▏  | 7556/10570 [00:19<00:07, 377.95it/s][1,5]<stderr>:#015 71%|███████   | 7528/10570 [00:19<00:07, 383.31it/s][1,7]<stderr>:#015 75%|███████▌  | 7946/10570 [00:20<00:06, 394.63it/s][1,10]<stderr>:#015 73%|███████▎  | 7712/10570 [00:20<00:07, 388.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▎   | 6718/10570 [00:18<00:11, 346.46it/s]#033[A[1,11]<stderr>:#015 69%|██████▉   | 7343/10570 [00:19<00:09, 341.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████   | 7465/10570 [00:19<00:08, 370.43it/s]#033[A[1,12]<stderr>:#015 74%|███████▍  | 7854/10570 [00:20<00:07, 378.59it/s][1,2]<stderr>:#015 73%|███████▎  | 7764/10570 [00:20<00:07, 365.40it/s][1,6]<stderr>:#015 68%|██████▊   | 7235/10570 [00:19<00:08, 373.67it/s][1,15]<stderr>:#015 72%|███████▏  | 7639/10570 [00:20<00:07, 392.82it/s][1,13]<stderr>:#015 67%|██████▋   | 7068/10570 [00:19<00:09, 374.90it/s][1,1]<stderr>:#015 73%|███████▎  | 7726/10570 [00:20<00:07, 376.22it/s][1,4]<stderr>:#015 73%|███████▎  | 7704/10570 [00:20<00:07, 380.08it/s][1,14]<stderr>:#015 72%|███████▏  | 7643/10570 [00:19<00:07, 390.07it/s][1,9]<stderr>:#015 72%|███████▏  | 7597/10570 [00:19<00:07, 385.83it/s][1,5]<stderr>:#015 72%|███████▏  | 7567/10570 [00:19<00:07, 382.51it/s][1,7]<stderr>:#015 76%|███████▌  | 7986/10570 [00:20<00:06, 392.17it/s][1,3]<stderr>:#015 70%|██████▉   | 7350/10570 [00:19<00:09, 337.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▍   | 6756/10570 [00:19<00:10, 353.87it/s]#033[A[1,10]<stderr>:#015 73%|███████▎  | 7751/10570 [00:20<00:07, 376.48it/s][1,11]<stderr>:#015 70%|██████▉   | 7382/10570 [00:19<00:09, 354.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████   | 7505/10570 [00:19<00:08, 377.68it/s]#033[A[1,12]<stderr>:#015 75%|███████▍  | 7894/10570 [00:20<00:06, 384.58it/s][1,2]<stderr>:#015 74%|███████▍  | 7802/10570 [00:20<00:07, 368.27it/s][1,6]<stderr>:#015 69%|██████▉   | 7273/10570 [00:19<00:08, 373.11it/s][1,13]<stderr>:#015 67%|██████▋   | 7106/10570 [00:19<00:09, 374.84it/s][1,15]<stderr>:#015 73%|███████▎  | 7679/10570 [00:20<00:07, 380.08it/s][1,4]<stderr>:#015 73%|███████▎  | 7744/10570 [00:20<00:07, 382.13it/s][1,1]<stderr>:#015 73%|███████▎  | 7764/10570 [00:20<00:07, 361.40it/s][1,14]<stderr>:#015 73%|███████▎  | 7683/10570 [00:19<00:07, 380.11it/s][1,9]<stderr>:#015 72%|███████▏  | 7637/10570 [00:20<00:07, 388.18it/s][1,5]<stderr>:#015 72%|███████▏  | 7608/10570 [00:19<00:07, 388.38it/s][1,7]<stderr>:#015 76%|███████▌  | 8026/10570 [00:20<00:06, 391.54it/s][1,3]<stderr>:#015 70%|██████▉   | 7388/10570 [00:19<00:09, 347.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▍   | 6792/10570 [00:19<00:10, 352.04it/s]#033[A[1,11]<stderr>:#015 70%|███████   | 7422/10570 [00:19<00:08, 364.57it/s][1,10]<stderr>:#015 74%|███████▎  | 7789/10570 [00:20<00:07, 373.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████▏  | 7545/10570 [00:19<00:07, 383.26it/s]#033[A[1,12]<stderr>:#015 75%|███████▌  | 7935/10570 [00:20<00:06, 391.72it/s][1,2]<stderr>:#015 74%|███████▍  | 7840/10570 [00:20<00:07, 370.19it/s][1,6]<stderr>:#015 69%|██████▉   | 7311/10570 [00:19<00:08, 374.67it/s][1,13]<stderr>:#015 68%|██████▊   | 7146/10570 [00:19<00:09, 380.20it/s][1,15]<stderr>:#015 73%|███████▎  | 7718/10570 [00:20<00:07, 382.03it/s][1,1]<stderr>:#015 74%|███████▍  | 7802/10570 [00:20<00:07, 365.39it/s][1,14]<stderr>:#015 73%|███████▎  | 7723/10570 [00:20<00:07, 383.65it/s][1,4]<stderr>:#015 74%|███████▎  | 7783/10570 [00:20<00:07, 363.24it/s][1,5]<stderr>:#015 72%|███████▏  | 7647/10570 [00:20<00:07, 377.95it/s][1,9]<stderr>:#015 73%|███████▎  | 7676/10570 [00:20<00:07, 363.00it/s][1,3]<stderr>:#015 70%|███████   | 7428/10570 [00:19<00:08, 361.83it/s][1,7]<stderr>:#015 76%|███████▋  | 8066/10570 [00:20<00:06, 386.23it/s][1,10]<stderr>:#015 74%|███████▍  | 7827/10570 [00:20<00:07, 374.66it/s][1,11]<stderr>:#015 71%|███████   | 7459/10570 [00:19<00:08, 364.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 72%|███████▏  | 7586/10570 [00:19<00:07, 388.24it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▍   | 6828/10570 [00:19<00:10, 342.00it/s]#033[A[1,12]<stderr>:#015 75%|███████▌  | 7975/10570 [00:20<00:06, 389.47it/s][1,2]<stderr>:#015 75%|███████▍  | 7880/10570 [00:20<00:07, 377.99it/s][1,13]<stderr>:#015 68%|██████▊   | 7185/10570 [00:19<00:09, 374.60it/s][1,15]<stderr>:#015 73%|███████▎  | 7757/10570 [00:20<00:07, 369.58it/s][1,1]<stderr>:#015 74%|███████▍  | 7840/10570 [00:20<00:07, 368.17it/s][1,4]<stderr>:#015 74%|███████▍  | 7822/10570 [00:20<00:07, 367.98it/s][1,6]<stderr>:#015 70%|██████▉   | 7349/10570 [00:19<00:09, 338.24it/s][1,14]<stderr>:#015 73%|███████▎  | 7762/10570 [00:20<00:07, 368.77it/s][1,9]<stderr>:#015 73%|███████▎  | 7716/10570 [00:20<00:07, 370.99it/s][1,3]<stderr>:#015 71%|███████   | 7465/10570 [00:20<00:08, 360.95it/s][1,5]<stderr>:#015 73%|███████▎  | 7685/10570 [00:20<00:07, 369.44it/s][1,7]<stderr>:#015 77%|███████▋  | 8109/10570 [00:20<00:06, 395.96it/s][1,11]<stderr>:#015 71%|███████   | 7497/10570 [00:20<00:08, 368.85it/s][1,10]<stderr>:#015 74%|███████▍  | 7867/10570 [00:20<00:07, 380.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 72%|███████▏  | 7627/10570 [00:20<00:07, 391.41it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▍   | 6863/10570 [00:19<00:10, 339.99it/s]#033[A[1,2]<stderr>:#015 75%|███████▍  | 7921/10570 [00:20<00:06, 386.66it/s][1,12]<stderr>:#015 76%|███████▌  | 8015/10570 [00:20<00:06, 390.05it/s][1,13]<stderr>:#015 68%|██████▊   | 7223/10570 [00:19<00:09, 370.18it/s][1,1]<stderr>:#015 75%|███████▍  | 7880/10570 [00:20<00:07, 376.12it/s][1,15]<stderr>:#015 74%|███████▎  | 7795/10570 [00:20<00:07, 369.34it/s][1,4]<stderr>:#015 74%|███████▍  | 7862/10570 [00:20<00:07, 375.35it/s][1,6]<stderr>:#015 70%|██████▉   | 7387/10570 [00:19<00:09, 348.46it/s][1,14]<stderr>:#015 74%|███████▍  | 7800/10570 [00:20<00:07, 370.09it/s][1,5]<stderr>:#015 73%|███████▎  | 7724/10570 [00:20<00:07, 375.20it/s][1,3]<stderr>:#015 71%|███████   | 7505/10570 [00:20<00:08, 369.72it/s][1,9]<stderr>:#015 73%|███████▎  | 7754/10570 [00:20<00:07, 363.45it/s][1,10]<stderr>:#015 75%|███████▍  | 7906/10570 [00:20<00:06, 382.90it/s][1,11]<stderr>:#015 71%|███████▏  | 7537/10570 [00:20<00:08, 376.14it/s][1,7]<stderr>:#015 77%|███████▋  | 8149/10570 [00:20<00:06, 388.45it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▌   | 6898/10570 [00:19<00:10, 339.62it/s][1,8]<stderr>:#033[A[1,2]<stderr>:#015 75%|███████▌  | 7962/10570 [00:20<00:06, 391.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7667/10570 [00:20<00:07, 375.89it/s]#033[A[1,12]<stderr>:#015 76%|███████▌  | 8055/10570 [00:20<00:06, 382.46it/s][1,13]<stderr>:#015 69%|██████▊   | 7261/10570 [00:20<00:09, 364.83it/s][1,1]<stderr>:#015 75%|███████▍  | 7921/10570 [00:20<00:06, 384.41it/s][1,15]<stderr>:#015 74%|███████▍  | 7833/10570 [00:20<00:07, 368.29it/s][1,4]<stderr>:#015 75%|███████▍  | 7901/10570 [00:20<00:07, 378.75it/s][1,6]<stderr>:#015 70%|███████   | 7427/10570 [00:20<00:08, 360.82it/s][1,14]<stderr>:#015 74%|███████▍  | 7838/10570 [00:20<00:07, 371.30it/s][1,3]<stderr>:#015 71%|███████▏  | 7545/10570 [00:20<00:08, 375.92it/s][1,10]<stderr>:#015 75%|███████▌  | 7947/10570 [00:20<00:06, 390.47it/s][1,9]<stderr>:#015 74%|███████▎  | 7791/10570 [00:20<00:07, 362.55it/s][1,11]<stderr>:#015 72%|███████▏  | 7576/10570 [00:20<00:07, 380.19it/s][1,5]<stderr>:#015 73%|███████▎  | 7762/10570 [00:20<00:07, 362.06it/s][1,7]<stderr>:#015 77%|███████▋  | 8188/10570 [00:20<00:06, 380.43it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▌   | 6935/10570 [00:19<00:10, 347.60it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7707/10570 [00:20<00:07, 382.47it/s]#033[A[1,12]<stderr>:#015 77%|███████▋  | 8097/10570 [00:20<00:06, 392.10it/s][1,2]<stderr>:#015 76%|███████▌  | 8002/10570 [00:20<00:06, 380.90it/s][1,13]<stderr>:#015 69%|██████▉   | 7298/10570 [00:20<00:08, 366.11it/s][1,1]<stderr>:#015 75%|███████▌  | 7961/10570 [00:20<00:06, 388.01it/s][1,15]<stderr>:#015 74%|███████▍  | 7874/10570 [00:20<00:07, 377.78it/s][1,6]<stderr>:#015 71%|███████   | 7464/10570 [00:20<00:08, 360.74it/s][1,4]<stderr>:#015 75%|███████▌  | 7941/10570 [00:20<00:06, 378.81it/s][1,14]<stderr>:#015 75%|███████▍  | 7878/10570 [00:20<00:07, 378.50it/s][1,3]<stderr>:#015 72%|███████▏  | 7584/10570 [00:20<00:07, 379.15it/s][1,9]<stderr>:#015 74%|███████▍  | 7829/10570 [00:20<00:07, 365.91it/s][1,11]<stderr>:#015 72%|███████▏  | 7616/10570 [00:20<00:07, 384.24it/s][1,10]<stderr>:#015 76%|███████▌  | 7987/10570 [00:20<00:06, 388.79it/s][1,5]<stderr>:#015 74%|███████▍  | 7799/10570 [00:20<00:07, 363.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▌   | 6973/10570 [00:19<00:10, 354.85it/s]#033[A[1,7]<stderr>:#015 78%|███████▊  | 8227/10570 [00:21<00:06, 365.15it/s][1,12]<stderr>:#015 77%|███████▋  | 8137/10570 [00:20<00:06, 390.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7746/10570 [00:20<00:07, 374.75it/s]#033[A[1,2]<stderr>:#015 76%|███████▌  | 8042/10570 [00:20<00:06, 384.19it/s][1,15]<stderr>:#015 75%|███████▍  | 7913/10570 [00:20<00:06, 380.99it/s][1,1]<stderr>:#015 76%|███████▌  | 8000/10570 [00:20<00:06, 381.81it/s][1,13]<stderr>:#015 69%|██████▉   | 7335/10570 [00:20<00:09, 351.44it/s][1,6]<stderr>:#015 71%|███████   | 7503/10570 [00:20<00:08, 367.96it/s][1,4]<stderr>:#015 75%|███████▌  | 7980/10570 [00:20<00:06, 380.21it/s][1,14]<stderr>:#015 75%|███████▍  | 7919/10570 [00:20<00:06, 385.52it/s][1,3]<stderr>:#015 72%|███████▏  | 7624/10570 [00:20<00:07, 383.02it/s][1,9]<stderr>:#015 74%|███████▍  | 7869/10570 [00:20<00:07, 373.63it/s][1,10]<stderr>:#015 76%|███████▌  | 8026/10570 [00:20<00:06, 387.69it/s][1,11]<stderr>:#015 72%|███████▏  | 7655/10570 [00:20<00:07, 378.55it/s][1,5]<stderr>:#015 74%|███████▍  | 7837/10570 [00:20<00:07, 365.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▋   | 7012/10570 [00:19<00:09, 362.54it/s]#033[A[1,7]<stderr>:#015 78%|███████▊  | 8264/10570 [00:21<00:06, 361.70it/s][1,2]<stderr>:#015 76%|███████▋  | 8081/10570 [00:20<00:06, 383.02it/s][1,12]<stderr>:#015 77%|███████▋  | 8177/10570 [00:21<00:06, 381.87it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▎  | 7784/10570 [00:20<00:07, 362.79it/s]#033[A[1,1]<stderr>:#015 76%|███████▌  | 8039/10570 [00:20<00:06, 383.56it/s][1,15]<stderr>:#015 75%|███████▌  | 7954/10570 [00:20<00:06, 387.48it/s][1,4]<stderr>:#015 76%|███████▌  | 8019/10570 [00:20<00:06, 382.39it/s][1,6]<stderr>:#015 71%|███████▏  | 7543/10570 [00:20<00:08, 374.45it/s][1,14]<stderr>:#015 75%|███████▌  | 7958/10570 [00:20<00:06, 384.93it/s][1,13]<stderr>:#015 70%|██████▉   | 7371/10570 [00:20<00:09, 341.85it/s][1,9]<stderr>:#015 75%|███████▍  | 7908/10570 [00:20<00:07, 376.44it/s][1,5]<stderr>:#015 75%|███████▍  | 7877/10570 [00:20<00:07, 374.14it/s][1,10]<stderr>:#015 76%|███████▋  | 8065/10570 [00:20<00:06, 381.13it/s][1,11]<stderr>:#015 73%|███████▎  | 7693/10570 [00:20<00:07, 373.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7050/10570 [00:19<00:09, 366.05it/s]#033[A[1,3]<stderr>:#015 72%|███████▏  | 7663/10570 [00:20<00:08, 358.15it/s][1,2]<stderr>:#015 77%|███████▋  | 8123/10570 [00:21<00:06, 392.21it/s][1,7]<stderr>:#015 79%|███████▊  | 8301/10570 [00:21<00:06, 353.82it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▍  | 7822/10570 [00:20<00:07, 366.25it/s]#033[A[1,12]<stderr>:#015 78%|███████▊  | 8216/10570 [00:21<00:06, 373.49it/s][1,1]<stderr>:#015 76%|███████▋  | 8078/10570 [00:21<00:06, 381.37it/s][1,15]<stderr>:#015 76%|███████▌  | 7993/10570 [00:20<00:06, 383.40it/s][1,6]<stderr>:#015 72%|███████▏  | 7583/10570 [00:20<00:07, 379.65it/s][1,14]<stderr>:#015 76%|███████▌  | 7997/10570 [00:20<00:06, 381.44it/s][1,13]<stderr>:#015 70%|███████   | 7407/10570 [00:20<00:09, 346.04it/s][1,4]<stderr>:#015 76%|███████▌  | 8058/10570 [00:21<00:06, 374.91it/s][1,9]<stderr>:#015 75%|███████▌  | 7946/10570 [00:20<00:06, 377.33it/s][1,5]<stderr>:#015 75%|███████▍  | 7917/10570 [00:20<00:06, 380.32it/s][1,10]<stderr>:#015 77%|███████▋  | 8105/10570 [00:21<00:06, 384.99it/s][1,11]<stderr>:#015 73%|███████▎  | 7732/10570 [00:20<00:07, 377.49it/s][1,3]<stderr>:#015 73%|███████▎  | 7703/10570 [00:20<00:07, 368.33it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7087/10570 [00:20<00:09, 363.09it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 79%|███████▉  | 8340/10570 [00:21<00:06, 363.35it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▍  | 7859/10570 [00:20<00:07, 366.80it/s]#033[A[1,2]<stderr>:#015 77%|███████▋  | 8163/10570 [00:21<00:06, 378.01it/s][1,12]<stderr>:#015 78%|███████▊  | 8254/10570 [00:21<00:06, 370.47it/s][1,1]<stderr>:#015 77%|███████▋  | 8120/10570 [00:21<00:06, 390.12it/s][1,15]<stderr>:#015 76%|███████▌  | 8032/10570 [00:21<00:06, 382.98it/s][1,6]<stderr>:#015 72%|███████▏  | 7622/10570 [00:20<00:07, 382.43it/s][1,4]<stderr>:#015 77%|███████▋  | 8099/10570 [00:21<00:06, 384.67it/s][1,13]<stderr>:#015 70%|███████   | 7444/10570 [00:20<00:08, 352.32it/s][1,14]<stderr>:#015 76%|███████▌  | 8036/10570 [00:20<00:06, 381.90it/s][1,9]<stderr>:#015 76%|███████▌  | 7984/10570 [00:21<00:06, 376.97it/s][1,5]<stderr>:#015 75%|███████▌  | 7957/10570 [00:20<00:06, 385.68it/s][1,10]<stderr>:#015 77%|███████▋  | 8144/10570 [00:21<00:06, 382.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7125/10570 [00:20<00:09, 367.47it/s]#033[A[1,3]<stderr>:#015 73%|███████▎  | 7742/10570 [00:20<00:07, 373.24it/s][1,11]<stderr>:#015 74%|███████▎  | 7770/10570 [00:20<00:07, 355.79it/s][1,7]<stderr>:#015 79%|███████▉  | 8381/10570 [00:21<00:05, 373.76it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▍  | 7897/10570 [00:20<00:07, 370.50it/s]#033[A[1,2]<stderr>:#015 78%|███████▊  | 8201/10570 [00:21<00:06, 371.08it/s][1,15]<stderr>:#015 76%|███████▋  | 8071/10570 [00:21<00:06, 379.41it/s][1,12]<stderr>:#015 78%|███████▊  | 8292/10570 [00:21<00:06, 353.65it/s][1,1]<stderr>:#015 77%|███████▋  | 8160/10570 [00:21<00:06, 376.54it/s][1,13]<stderr>:#015 71%|███████   | 7482/10570 [00:20<00:08, 358.64it/s][1,4]<stderr>:#015 77%|███████▋  | 8138/10570 [00:21<00:06, 384.08it/s][1,14]<stderr>:#015 76%|███████▋  | 8075/10570 [00:20<00:06, 379.83it/s][1,6]<stderr>:#015 72%|███████▏  | 7661/10570 [00:20<00:07, 364.96it/s][1,9]<stderr>:#015 76%|███████▌  | 8023/10570 [00:21<00:06, 378.85it/s][1,5]<stderr>:#015 76%|███████▌  | 7996/10570 [00:20<00:06, 381.49it/s][1,10]<stderr>:#015 77%|███████▋  | 8183/10570 [00:21<00:06, 375.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7163/10570 [00:20<00:09, 369.47it/s]#033[A[1,11]<stderr>:#015 74%|███████▍  | 7808/10570 [00:20<00:07, 361.11it/s][1,7]<stderr>:#015 80%|███████▉  | 8423/10570 [00:21<00:05, 385.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▌  | 7938/10570 [00:20<00:06, 379.38it/s]#033[A[1,3]<stderr>:#015 74%|███████▎  | 7780/10570 [00:20<00:07, 350.85it/s][1,2]<stderr>:#015 78%|███████▊  | 8239/10570 [00:21<00:06, 367.90it/s][1,15]<stderr>:#015 77%|███████▋  | 8112/10570 [00:21<00:06, 387.01it/s][1,12]<stderr>:#015 79%|███████▉  | 8331/10570 [00:21<00:06, 361.13it/s][1,13]<stderr>:#015 71%|███████   | 7519/10570 [00:20<00:08, 361.94it/s][1,1]<stderr>:#015 78%|███████▊  | 8198/10570 [00:21<00:06, 370.83it/s][1,14]<stderr>:#015 77%|███████▋  | 8117/10570 [00:21<00:06, 388.67it/s][1,4]<stderr>:#015 77%|███████▋  | 8177/10570 [00:21<00:06, 375.72it/s][1,6]<stderr>:#015 73%|███████▎  | 7701/10570 [00:20<00:07, 373.70it/s][1,9]<stderr>:#015 76%|███████▋  | 8061/10570 [00:21<00:06, 371.25it/s][1,5]<stderr>:#015 76%|███████▌  | 8035/10570 [00:21<00:06, 381.34it/s][1,10]<stderr>:#015 78%|███████▊  | 8221/10570 [00:21<00:06, 367.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7200/10570 [00:20<00:09, 363.23it/s]#033[A[1,11]<stderr>:#015 74%|███████▍  | 7845/10570 [00:21<00:07, 359.85it/s][1,7]<stderr>:#015 80%|████████  | 8466/10570 [00:21<00:05, 396.82it/s][1,3]<stderr>:#015 74%|███████▍  | 7817/10570 [00:21<00:07, 355.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▌  | 7977/10570 [00:21<00:06, 378.06it/s]#033[A[1,2]<stderr>:#015 78%|███████▊  | 8276/10570 [00:21<00:06, 354.56it/s][1,12]<stderr>:#015 79%|███████▉  | 8372/10570 [00:21<00:05, 374.48it/s][1,15]<stderr>:#015 77%|███████▋  | 8151/10570 [00:21<00:06, 377.23it/s][1,13]<stderr>:#015 71%|███████▏  | 7557/10570 [00:20<00:08, 365.44it/s][1,1]<stderr>:#015 78%|███████▊  | 8236/10570 [00:21<00:06, 366.85it/s][1,6]<stderr>:#015 73%|███████▎  | 7740/10570 [00:20<00:07, 376.22it/s][1,14]<stderr>:#015 77%|███████▋  | 8156/10570 [00:21<00:06, 376.87it/s][1,4]<stderr>:#015 78%|███████▊  | 8215/10570 [00:21<00:06, 367.20it/s][1,9]<stderr>:#015 77%|███████▋  | 8103/10570 [00:21<00:06, 382.12it/s][1,5]<stderr>:#015 76%|███████▋  | 8074/10570 [00:21<00:06, 378.76it/s][1,10]<stderr>:#015 78%|███████▊  | 8258/10570 [00:21<00:06, 363.93it/s][1,11]<stderr>:#015 75%|███████▍  | 7884/10570 [00:21<00:07, 366.70it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7237/10570 [00:20<00:09, 356.09it/s]#033[A[1,7]<stderr>:#015 80%|████████  | 8506/10570 [00:21<00:05, 396.23it/s][1,3]<stderr>:#015 74%|███████▍  | 7855/10570 [00:21<00:07, 360.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 76%|███████▌  | 8015/10570 [00:21<00:06, 375.47it/s]#033[A[1,2]<stderr>:#015 79%|███████▊  | 8313/10570 [00:21<00:06, 358.11it/s][1,12]<stderr>:#015 80%|███████▉  | 8413/10570 [00:21<00:05, 383.67it/s][1,13]<stderr>:#015 72%|███████▏  | 7596/10570 [00:20<00:08, 371.40it/s][1,15]<stderr>:#015 77%|███████▋  | 8189/10570 [00:21<00:06, 372.36it/s][1,1]<stderr>:#015 78%|███████▊  | 8273/10570 [00:21<00:06, 363.98it/s][1,4]<stderr>:#015 78%|███████▊  | 8252/10570 [00:21<00:06, 365.40it/s][1,14]<stderr>:#015 78%|███████▊  | 8194/10570 [00:21<00:06, 367.88it/s][1,6]<stderr>:#015 74%|███████▎  | 7778/10570 [00:21<00:07, 352.82it/s][1,9]<stderr>:#015 77%|███████▋  | 8142/10570 [00:21<00:06, 379.64it/s][1,5]<stderr>:#015 77%|███████▋  | 8116/10570 [00:21<00:06, 388.47it/s][1,11]<stderr>:#015 75%|███████▍  | 7924/10570 [00:21<00:07, 374.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7273/10570 [00:20<00:09, 355.90it/s]#033[A[1,10]<stderr>:#015 78%|███████▊  | 8295/10570 [00:21<00:06, 351.73it/s][1,7]<stderr>:#015 81%|████████  | 8547/10570 [00:21<00:05, 397.65it/s][1,3]<stderr>:#015 75%|███████▍  | 7894/10570 [00:21<00:07, 367.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 76%|███████▌  | 8053/10570 [00:21<00:06, 369.21it/s]#033[A[1,2]<stderr>:#015 79%|███████▉  | 8352/10570 [00:21<00:06, 366.50it/s][1,12]<stderr>:#015 80%|████████  | 8456/10570 [00:21<00:05, 395.41it/s][1,13]<stderr>:#015 72%|███████▏  | 7635/10570 [00:21<00:07, 374.46it/s][1,15]<stderr>:#015 78%|███████▊  | 8227/10570 [00:21<00:06, 367.08it/s][1,1]<stderr>:#015 79%|███████▊  | 8310/10570 [00:21<00:06, 354.54it/s][1,14]<stderr>:#015 78%|███████▊  | 8231/10570 [00:21<00:06, 365.83it/s][1,6]<stderr>:#015 74%|███████▍  | 7814/10570 [00:21<00:07, 354.37it/s][1,4]<stderr>:#015 78%|███████▊  | 8289/10570 [00:21<00:06, 347.55it/s][1,9]<stderr>:#015 77%|███████▋  | 8181/10570 [00:21<00:06, 372.04it/s][1,5]<stderr>:#015 77%|███████▋  | 8155/10570 [00:21<00:06, 375.13it/s][1,11]<stderr>:#015 75%|███████▌  | 7963/10570 [00:21<00:06, 377.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7310/10570 [00:20<00:09, 358.31it/s]#033[A[1,10]<stderr>:#015 79%|███████▉  | 8333/10570 [00:21<00:06, 358.39it/s][1,3]<stderr>:#015 75%|███████▌  | 7934/10570 [00:21<00:07, 374.23it/s][1,7]<stderr>:#015 81%|████████  | 8587/10570 [00:21<00:05, 390.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8093/10570 [00:21<00:06, 376.98it/s]#033[A[1,12]<stderr>:#015 80%|████████  | 8496/10570 [00:21<00:05, 396.37it/s][1,2]<stderr>:#015 79%|███████▉  | 8392/10570 [00:21<00:05, 373.66it/s][1,15]<stderr>:#015 78%|███████▊  | 8264/10570 [00:21<00:06, 360.27it/s][1,13]<stderr>:#015 73%|███████▎  | 7673/10570 [00:21<00:08, 358.96it/s][1,1]<stderr>:#015 79%|███████▉  | 8349/10570 [00:21<00:06, 362.34it/s][1,14]<stderr>:#015 78%|███████▊  | 8268/10570 [00:21<00:06, 362.06it/s][1,6]<stderr>:#015 74%|███████▍  | 7851/10570 [00:21<00:07, 358.89it/s][1,4]<stderr>:#015 79%|███████▉  | 8327/10570 [00:21<00:06, 355.74it/s][1,9]<stderr>:#015 78%|███████▊  | 8219/10570 [00:21<00:06, 364.72it/s][1,5]<stderr>:#015 78%|███████▊  | 8193/10570 [00:21<00:06, 369.48it/s][1,11]<stderr>:#015 76%|███████▌  | 8001/10570 [00:21<00:06, 370.99it/s][1,10]<stderr>:#015 79%|███████▉  | 8374/10570 [00:21<00:05, 372.25it/s][1,7]<stderr>:#015 82%|████████▏ | 8629/10570 [00:22<00:04, 397.60it/s][1,3]<stderr>:#015 75%|███████▌  | 7972/10570 [00:21<00:06, 372.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8133/10570 [00:21<00:06, 380.28it/s]#033[A[1,12]<stderr>:#015 81%|████████  | 8537/10570 [00:21<00:05, 398.68it/s][1,2]<stderr>:#015 80%|███████▉  | 8434/10570 [00:21<00:05, 384.68it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7346/10570 [00:20<00:10, 320.28it/s]#033[A[1,13]<stderr>:#015 73%|███████▎  | 7711/10570 [00:21<00:07, 364.53it/s][1,1]<stderr>:#015 79%|███████▉  | 8389/10570 [00:21<00:05, 371.62it/s][1,15]<stderr>:#015 79%|███████▊  | 8301/10570 [00:21<00:06, 350.29it/s][1,6]<stderr>:#015 75%|███████▍  | 7890/10570 [00:21<00:07, 365.42it/s][1,4]<stderr>:#015 79%|███████▉  | 8367/10570 [00:21<00:06, 366.54it/s][1,14]<stderr>:#015 79%|███████▊  | 8305/10570 [00:21<00:06, 352.20it/s][1,9]<stderr>:#015 78%|███████▊  | 8256/10570 [00:21<00:06, 361.87it/s][1,5]<stderr>:#015 78%|███████▊  | 8231/10570 [00:21<00:06, 366.34it/s][1,11]<stderr>:#015 76%|███████▌  | 8039/10570 [00:21<00:06, 372.90it/s][1,10]<stderr>:#015 80%|███████▉  | 8415/10570 [00:21<00:05, 381.17it/s][1,7]<stderr>:#015 82%|████████▏ | 8669/10570 [00:22<00:04, 396.15it/s][1,3]<stderr>:#015 76%|███████▌  | 8010/10570 [00:21<00:06, 372.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8172/10570 [00:21<00:06, 369.31it/s]#033[A[1,2]<stderr>:#015 80%|████████  | 8475/10570 [00:22<00:05, 391.27it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 70%|██████▉   | 7383/10570 [00:20<00:09, 333.26it/s]#033[A[1,12]<stderr>:#015 81%|████████  | 8577/10570 [00:22<00:05, 393.75it/s][1,1]<stderr>:#015 80%|███████▉  | 8431/10570 [00:21<00:05, 382.54it/s][1,15]<stderr>:#015 79%|███████▉  | 8339/10570 [00:21<00:06, 358.24it/s][1,13]<stderr>:#015 73%|███████▎  | 7748/10570 [00:21<00:08, 352.22it/s][1,6]<stderr>:#015 75%|███████▌  | 7928/10570 [00:21<00:07, 367.48it/s][1,4]<stderr>:#015 80%|███████▉  | 8407/10570 [00:21<00:05, 375.32it/s][1,14]<stderr>:#015 79%|███████▉  | 8344/10570 [00:21<00:06, 360.86it/s][1,5]<stderr>:#015 78%|███████▊  | 8268/10570 [00:21<00:06, 361.73it/s][1,10]<stderr>:#015 80%|████████  | 8458/10570 [00:22<00:05, 392.54it/s][1,11]<stderr>:#015 76%|███████▋  | 8077/10570 [00:21<00:06, 370.16it/s][1,9]<stderr>:#015 78%|███████▊  | 8293/10570 [00:21<00:06, 346.84it/s][1,7]<stderr>:#015 82%|████████▏ | 8710/10570 [00:22<00:04, 398.31it/s][1,3]<stderr>:#015 76%|███████▌  | 8048/10570 [00:21<00:06, 367.53it/s][1,2]<stderr>:#015 81%|████████  | 8515/10570 [00:22<00:05, 392.35it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 70%|███████   | 7421/10570 [00:20<00:09, 343.70it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8210/10570 [00:21<00:06, 362.65it/s]#033[A[1,12]<stderr>:#015 82%|████████▏ | 8618/10570 [00:22<00:04, 395.61it/s][1,1]<stderr>:#015 80%|████████  | 8472/10570 [00:22<00:05, 388.98it/s][1,15]<stderr>:#015 79%|███████▉  | 8379/10570 [00:22<00:05, 369.60it/s][1,13]<stderr>:#015 74%|███████▎  | 7784/10570 [00:21<00:08, 348.21it/s][1,6]<stderr>:#015 75%|███████▌  | 7966/10570 [00:21<00:07, 371.04it/s][1,4]<stderr>:#015 80%|███████▉  | 8449/10570 [00:22<00:05, 387.56it/s][1,14]<stderr>:#015 79%|███████▉  | 8384/10570 [00:21<00:05, 371.47it/s][1,10]<stderr>:#015 80%|████████  | 8498/10570 [00:22<00:05, 394.55it/s][1,11]<stderr>:#015 77%|███████▋  | 8118/10570 [00:21<00:06, 379.14it/s][1,9]<stderr>:#015 79%|███████▉  | 8331/10570 [00:21<00:06, 354.42it/s][1,5]<stderr>:#015 79%|███████▊  | 8305/10570 [00:21<00:06, 351.34it/s][1,7]<stderr>:#015 83%|████████▎ | 8750/10570 [00:22<00:04, 392.47it/s][1,3]<stderr>:#015 77%|███████▋  | 8087/10570 [00:21<00:06, 373.82it/s][1,2]<stderr>:#015 81%|████████  | 8556/10570 [00:22<00:05, 395.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8247/10570 [00:21<00:06, 361.84it/s]#033[A[1,12]<stderr>:#015 82%|████████▏ | 8658/10570 [00:22<00:04, 393.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7456/10570 [00:21<00:09, 339.90it/s]#033[A[1,15]<stderr>:#015 80%|███████▉  | 8420/10570 [00:22<00:05, 379.90it/s][1,1]<stderr>:#015 81%|████████  | 8512/10570 [00:22<00:05, 390.15it/s][1,4]<stderr>:#015 80%|████████  | 8489/10570 [00:22<00:05, 390.89it/s][1,13]<stderr>:#015 74%|███████▍  | 7821/10570 [00:21<00:07, 351.70it/s][1,14]<stderr>:#015 80%|███████▉  | 8425/10570 [00:21<00:05, 381.75it/s][1,6]<stderr>:#015 76%|███████▌  | 8004/10570 [00:21<00:07, 365.34it/s][1,10]<stderr>:#015 81%|████████  | 8538/10570 [00:22<00:05, 396.11it/s][1,9]<stderr>:#015 79%|███████▉  | 8372/10570 [00:22<00:05, 367.20it/s][1,5]<stderr>:#015 79%|███████▉  | 8343/10570 [00:21<00:06, 359.07it/s][1,3]<stderr>:#015 77%|███████▋  | 8127/10570 [00:21<00:06, 379.30it/s][1,11]<stderr>:#015 77%|███████▋  | 8157/10570 [00:21<00:06, 365.85it/s][1,7]<stderr>:#015 83%|████████▎ | 8790/10570 [00:22<00:04, 384.81it/s][1,2]<stderr>:#015 81%|████████▏ | 8596/10570 [00:22<00:05, 391.68it/s][1,12]<stderr>:#015 82%|████████▏ | 8700/10570 [00:22<00:04, 399.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7492/10570 [00:21<00:08, 344.39it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8284/10570 [00:21<00:06, 340.84it/s]#033[A[1,1]<stderr>:#015 81%|████████  | 8553/10570 [00:22<00:05, 394.48it/s][1,15]<stderr>:#015 80%|████████  | 8462/10570 [00:22<00:05, 389.00it/s][1,13]<stderr>:#015 74%|███████▍  | 7858/10570 [00:21<00:07, 356.74it/s][1,4]<stderr>:#015 81%|████████  | 8530/10570 [00:22<00:05, 394.24it/s][1,14]<stderr>:#015 80%|████████  | 8468/10570 [00:21<00:05, 392.45it/s][1,6]<stderr>:#015 76%|███████▌  | 8042/10570 [00:21<00:06, 368.05it/s][1,10]<stderr>:#015 81%|████████  | 8578/10570 [00:22<00:05, 392.08it/s][1,9]<stderr>:#015 80%|███████▉  | 8412/10570 [00:22<00:05, 376.09it/s][1,5]<stderr>:#015 79%|███████▉  | 8383/10570 [00:21<00:05, 369.13it/s][1,7]<stderr>:#015 84%|████████▎ | 8829/10570 [00:22<00:04, 385.60it/s][1,11]<stderr>:#015 78%|███████▊  | 8194/10570 [00:21<00:06, 358.54it/s][1,3]<stderr>:#015 77%|███████▋  | 8166/10570 [00:21<00:06, 367.09it/s][1,2]<stderr>:#015 82%|████████▏ | 8638/10570 [00:22<00:04, 398.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7530/10570 [00:21<00:08, 353.59it/s]#033[A[1,12]<stderr>:#015 83%|████████▎ | 8741/10570 [00:22<00:04, 390.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▊  | 8323/10570 [00:22<00:06, 352.54it/s][1,0]<stderr>:#033[A[1,15]<stderr>:#015 80%|████████  | 8502/10570 [00:22<00:05, 388.51it/s][1,1]<stderr>:#015 81%|████████▏ | 8593/10570 [00:22<00:05, 389.30it/s][1,13]<stderr>:#015 75%|███████▍  | 7895/10570 [00:21<00:07, 358.86it/s][1,4]<stderr>:#015 81%|████████  | 8570/10570 [00:22<00:05, 391.76it/s][1,6]<stderr>:#015 76%|███████▋  | 8079/10570 [00:21<00:06, 366.31it/s][1,14]<stderr>:#015 80%|████████  | 8508/10570 [00:22<00:05, 390.90it/s][1,10]<stderr>:#015 82%|████████▏ | 8618/10570 [00:22<00:04, 393.25it/s][1,9]<stderr>:#015 80%|███████▉  | 8454/10570 [00:22<00:05, 386.52it/s][1,5]<stderr>:#015 80%|███████▉  | 8424/10570 [00:22<00:05, 379.17it/s][1,7]<stderr>:#015 84%|████████▍ | 8869/10570 [00:22<00:04, 389.18it/s][1,11]<stderr>:#015 78%|███████▊  | 8231/10570 [00:22<00:06, 354.93it/s][1,3]<stderr>:#015 78%|███████▊  | 8203/10570 [00:22<00:06, 359.45it/s][1,2]<stderr>:#015 82%|████████▏ | 8678/10570 [00:22<00:04, 392.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7567/10570 [00:21<00:08, 355.36it/s]#033[A[1,12]<stderr>:#015 83%|████████▎ | 8781/10570 [00:22<00:04, 382.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▉  | 8362/10570 [00:22<00:06, 362.26it/s]#033[A[1,15]<stderr>:#015 81%|████████  | 8543/10570 [00:22<00:05, 392.46it/s][1,1]<stderr>:#015 82%|████████▏ | 8634/10570 [00:22<00:04, 395.22it/s][1,13]<stderr>:#015 75%|███████▌  | 7934/10570 [00:21<00:07, 366.13it/s][1,4]<stderr>:#015 81%|████████▏ | 8610/10570 [00:22<00:04, 392.84it/s][1,6]<stderr>:#015 77%|███████▋  | 8119/10570 [00:21<00:06, 375.72it/s][1,14]<stderr>:#015 81%|████████  | 8550/10570 [00:22<00:05, 396.29it/s][1,10]<stderr>:#015 82%|████████▏ | 8658/10570 [00:22<00:04, 390.99it/s][1,9]<stderr>:#015 80%|████████  | 8494/10570 [00:22<00:05, 387.91it/s][1,5]<stderr>:#015 80%|████████  | 8467/10570 [00:22<00:05, 390.50it/s][1,7]<stderr>:#015 84%|████████▍ | 8908/10570 [00:22<00:04, 387.62it/s][1,11]<stderr>:#015 78%|███████▊  | 8267/10570 [00:22<00:06, 350.43it/s][1,3]<stderr>:#015 78%|███████▊  | 8240/10570 [00:22<00:06, 355.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7605/10570 [00:21<00:08, 361.93it/s]#033[A[1,2]<stderr>:#015 82%|████████▏ | 8718/10570 [00:22<00:04, 382.48it/s][1,12]<stderr>:#015 83%|████████▎ | 8820/10570 [00:22<00:04, 382.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▉  | 8402/10570 [00:22<00:05, 370.04it/s]#033[A[1,1]<stderr>:#015 82%|████████▏ | 8674/10570 [00:22<00:04, 390.73it/s][1,15]<stderr>:#015 81%|████████  | 8583/10570 [00:22<00:05, 387.27it/s][1,13]<stderr>:#015 75%|███████▌  | 7971/10570 [00:22<00:07, 364.42it/s][1,4]<stderr>:#015 82%|████████▏ | 8650/10570 [00:22<00:04, 390.29it/s][1,14]<stderr>:#015 81%|████████▏ | 8590/10570 [00:22<00:05, 389.29it/s][1,6]<stderr>:#015 77%|███████▋  | 8157/10570 [00:22<00:06, 361.74it/s][1,10]<stderr>:#015 82%|████████▏ | 8700/10570 [00:22<00:04, 397.48it/s][1,9]<stderr>:#015 81%|████████  | 8534/10570 [00:22<00:05, 390.57it/s][1,5]<stderr>:#015 80%|████████  | 8507/10570 [00:22<00:05, 388.72it/s][1,7]<stderr>:#015 85%|████████▍ | 8948/10570 [00:22<00:04, 390.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7642/10570 [00:21<00:08, 362.22it/s]#033[A[1,11]<stderr>:#015 79%|███████▊  | 8303/10570 [00:22<00:06, 339.98it/s][1,2]<stderr>:#015 83%|████████▎ | 8757/10570 [00:22<00:04, 378.31it/s][1,3]<stderr>:#015 78%|███████▊  | 8276/10570 [00:22<00:06, 341.97it/s][1,12]<stderr>:#015 84%|████████▍ | 8860/10570 [00:22<00:04, 385.64it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 80%|███████▉  | 8443/10570 [00:22<00:05, 379.46it/s]#033[A[1,1]<stderr>:#015 82%|████████▏ | 8714/10570 [00:22<00:04, 391.18it/s][1,15]<stderr>:#015 82%|████████▏ | 8623/10570 [00:22<00:05, 389.18it/s][1,13]<stderr>:#015 76%|███████▌  | 8008/10570 [00:22<00:07, 362.63it/s][1,4]<stderr>:#015 82%|████████▏ | 8690/10570 [00:22<00:04, 391.24it/s][1,14]<stderr>:#015 82%|████████▏ | 8632/10570 [00:22<00:04, 395.57it/s][1,6]<stderr>:#015 78%|███████▊  | 8194/10570 [00:22<00:06, 356.21it/s][1,10]<stderr>:#015 83%|████████▎ | 8740/10570 [00:22<00:04, 388.50it/s][1,9]<stderr>:#015 81%|████████  | 8574/10570 [00:22<00:05, 386.91it/s][1,5]<stderr>:#015 81%|████████  | 8548/10570 [00:22<00:05, 393.40it/s][1,7]<stderr>:#015 85%|████████▌ | 8989/10570 [00:22<00:04, 393.21it/s][1,11]<stderr>:#015 79%|███████▉  | 8338/10570 [00:22<00:06, 341.50it/s][1,3]<stderr>:#015 79%|███████▊  | 8311/10570 [00:22<00:06, 342.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7679/10570 [00:21<00:08, 349.12it/s]#033[A[1,12]<stderr>:#015 84%|████████▍ | 8899/10570 [00:22<00:04, 385.23it/s][1,2]<stderr>:#015 83%|████████▎ | 8795/10570 [00:22<00:04, 372.58it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 80%|████████  | 8483/10570 [00:22<00:05, 385.08it/s]#033[A[1,15]<stderr>:#015 82%|████████▏ | 8662/10570 [00:22<00:04, 387.89it/s][1,13]<stderr>:#015 76%|███████▌  | 8045/10570 [00:22<00:07, 359.83it/s][1,1]<stderr>:#015 83%|████████▎ | 8754/10570 [00:22<00:04, 384.06it/s][1,4]<stderr>:#015 83%|████████▎ | 8730/10570 [00:22<00:04, 384.98it/s][1,14]<stderr>:#015 82%|████████▏ | 8672/10570 [00:22<00:04, 390.40it/s][1,6]<stderr>:#015 78%|███████▊  | 8230/10570 [00:22<00:06, 352.81it/s][1,9]<stderr>:#015 81%|████████▏ | 8614/10570 [00:22<00:05, 389.24it/s][1,10]<stderr>:#015 83%|████████▎ | 8779/10570 [00:22<00:04, 380.76it/s][1,7]<stderr>:#015 85%|████████▌ | 9029/10570 [00:23<00:03, 394.87it/s][1,5]<stderr>:#015 81%|████████  | 8588/10570 [00:22<00:05, 385.80it/s][1,11]<stderr>:#015 79%|███████▉  | 8378/10570 [00:22<00:06, 355.21it/s][1,3]<stderr>:#015 79%|███████▉  | 8348/10570 [00:22<00:06, 350.61it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7715/10570 [00:21<00:08, 351.49it/s]#033[A[1,2]<stderr>:#015 84%|████████▎ | 8833/10570 [00:22<00:04, 373.92it/s][1,12]<stderr>:#015 85%|████████▍ | 8939/10570 [00:23<00:04, 387.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████  | 8522/10570 [00:22<00:05, 385.84it/s]#033[A[1,15]<stderr>:#015 82%|████████▏ | 8703/10570 [00:22<00:04, 393.00it/s][1,13]<stderr>:#015 76%|███████▋  | 8082/10570 [00:22<00:06, 362.13it/s][1,1]<stderr>:#015 83%|████████▎ | 8793/10570 [00:22<00:04, 378.12it/s][1,4]<stderr>:#015 83%|████████▎ | 8769/10570 [00:22<00:04, 382.32it/s][1,14]<stderr>:#015 82%|████████▏ | 8712/10570 [00:22<00:04, 391.66it/s][1,6]<stderr>:#015 78%|███████▊  | 8266/10570 [00:22<00:06, 347.83it/s][1,9]<stderr>:#015 82%|████████▏ | 8653/10570 [00:22<00:04, 385.42it/s][1,10]<stderr>:#015 83%|████████▎ | 8818/10570 [00:22<00:04, 380.58it/s][1,7]<stderr>:#015 86%|████████▌ | 9069/10570 [00:23<00:03, 393.98it/s][1,5]<stderr>:#015 82%|████████▏ | 8629/10570 [00:22<00:04, 392.08it/s][1,11]<stderr>:#015 80%|███████▉  | 8414/10570 [00:22<00:06, 355.05it/s][1,3]<stderr>:#015 79%|███████▉  | 8387/10570 [00:22<00:06, 359.12it/s][1,2]<stderr>:#015 84%|████████▍ | 8873/10570 [00:23<00:04, 380.01it/s][1,12]<stderr>:#015 85%|████████▍ | 8979/10570 [00:23<00:04, 389.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████  | 8562/10570 [00:22<00:05, 388.61it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7751/10570 [00:21<00:08, 340.76it/s]#033[A[1,13]<stderr>:#015 77%|███████▋  | 8121/10570 [00:22<00:06, 369.97it/s][1,15]<stderr>:#015 83%|████████▎ | 8743/10570 [00:22<00:04, 384.21it/s][1,1]<stderr>:#015 84%|████████▎ | 8831/10570 [00:23<00:04, 377.66it/s][1,4]<stderr>:#015 83%|████████▎ | 8808/10570 [00:22<00:04, 378.37it/s][1,14]<stderr>:#015 83%|████████▎ | 8752/10570 [00:22<00:04, 385.38it/s][1,6]<stderr>:#015 79%|███████▊  | 8301/10570 [00:22<00:06, 336.74it/s][1,9]<stderr>:#015 82%|████████▏ | 8694/10570 [00:22<00:04, 390.27it/s][1,10]<stderr>:#015 84%|████████▍ | 8857/10570 [00:23<00:04, 382.55it/s][1,7]<stderr>:#015 86%|████████▌ | 9109/10570 [00:23<00:03, 391.04it/s][1,5]<stderr>:#015 82%|████████▏ | 8669/10570 [00:22<00:04, 388.84it/s][1,11]<stderr>:#015 80%|███████▉  | 8453/10570 [00:22<00:05, 360.70it/s][1,3]<stderr>:#015 80%|███████▉  | 8427/10570 [00:22<00:05, 369.29it/s][1,12]<stderr>:#015 85%|████████▌ | 9020/10570 [00:23<00:03, 393.29it/s][1,2]<stderr>:#015 84%|████████▍ | 8912/10570 [00:23<00:04, 379.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████▏ | 8601/10570 [00:22<00:05, 384.63it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▎  | 7786/10570 [00:22<00:08, 327.93it/s]#033[A[1,1]<stderr>:#015 84%|████████▍ | 8871/10570 [00:23<00:04, 382.20it/s][1,15]<stderr>:#015 83%|████████▎ | 8782/10570 [00:23<00:04, 376.43it/s][1,4]<stderr>:#015 84%|████████▎ | 8846/10570 [00:23<00:04, 377.72it/s][1,13]<stderr>:#015 77%|███████▋  | 8159/10570 [00:22<00:06, 356.93it/s][1,14]<stderr>:#015 83%|████████▎ | 8791/10570 [00:22<00:04, 378.96it/s][1,6]<stderr>:#015 79%|███████▉  | 8338/10570 [00:22<00:06, 343.84it/s][1,10]<stderr>:#015 84%|████████▍ | 8896/10570 [00:23<00:04, 382.73it/s][1,7]<stderr>:#015 87%|████████▋ | 9149/10570 [00:23<00:03, 391.62it/s][1,5]<stderr>:#015 82%|████████▏ | 8709/10570 [00:22<00:04, 384.09it/s][1,9]<stderr>:#015 83%|████████▎ | 8734/10570 [00:23<00:04, 374.34it/s][1,11]<stderr>:#015 80%|████████  | 8492/10570 [00:22<00:05, 366.58it/s][1,3]<stderr>:#015 80%|████████  | 8468/10570 [00:22<00:05, 379.38it/s][1,2]<stderr>:#015 85%|████████▍ | 8952/10570 [00:23<00:04, 383.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 82%|████████▏ | 8642/10570 [00:22<00:04, 389.99it/s]#033[A[1,12]<stderr>:#015 86%|████████▌ | 9060/10570 [00:23<00:03, 383.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▍  | 7821/10570 [00:22<00:08, 333.52it/s]#033[A[1,1]<stderr>:#015 84%|████████▍ | 8910/10570 [00:23<00:04, 380.94it/s][1,15]<stderr>:#015 83%|████████▎ | 8820/10570 [00:23<00:04, 377.28it/s][1,4]<stderr>:#015 84%|████████▍ | 8886/10570 [00:23<00:04, 382.51it/s][1,13]<stderr>:#015 78%|███████▊  | 8195/10570 [00:22<00:06, 351.07it/s][1,14]<stderr>:#015 84%|████████▎ | 8829/10570 [00:22<00:04, 378.92it/s][1,6]<stderr>:#015 79%|███████▉  | 8377/10570 [00:22<00:06, 356.34it/s][1,10]<stderr>:#015 85%|████████▍ | 8936/10570 [00:23<00:04, 385.96it/s][1,7]<stderr>:#015 87%|████████▋ | 9190/10570 [00:23<00:03, 395.65it/s][1,9]<stderr>:#015 83%|████████▎ | 8772/10570 [00:23<00:04, 373.13it/s][1,5]<stderr>:#015 83%|████████▎ | 8748/10570 [00:22<00:04, 379.21it/s][1,11]<stderr>:#015 81%|████████  | 8531/10570 [00:22<00:05, 373.30it/s][1,3]<stderr>:#015 80%|████████  | 8507/10570 [00:22<00:05, 378.05it/s][1,2]<stderr>:#015 85%|████████▌ | 8992/10570 [00:23<00:04, 387.42it/s][1,12]<stderr>:#015 86%|████████▌ | 9100/10570 [00:23<00:03, 385.44it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 82%|████████▏ | 8682/10570 [00:22<00:04, 383.76it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▍  | 7857/10570 [00:22<00:07, 340.05it/s]#033[A[1,1]<stderr>:#015 85%|████████▍ | 8949/10570 [00:23<00:04, 383.29it/s][1,15]<stderr>:#015 84%|████████▍ | 8859/10570 [00:23<00:04, 379.94it/s][1,4]<stderr>:#015 84%|████████▍ | 8925/10570 [00:23<00:04, 381.74it/s][1,13]<stderr>:#015 78%|███████▊  | 8231/10570 [00:22<00:06, 347.96it/s][1,14]<stderr>:#015 84%|████████▍ | 8869/10570 [00:23<00:04, 383.44it/s][1,6]<stderr>:#015 80%|███████▉  | 8416/10570 [00:22<00:05, 365.68it/s][1,10]<stderr>:#015 85%|████████▍ | 8975/10570 [00:23<00:04, 385.46it/s][1,7]<stderr>:#015 87%|████████▋ | 9231/10570 [00:23<00:03, 397.21it/s][1,9]<stderr>:#015 83%|████████▎ | 8810/10570 [00:23<00:04, 371.56it/s][1,5]<stderr>:#015 83%|████████▎ | 8786/10570 [00:23<00:04, 372.91it/s][1,11]<stderr>:#015 81%|████████  | 8569/10570 [00:22<00:05, 369.70it/s][1,3]<stderr>:#015 81%|████████  | 8547/10570 [00:22<00:05, 383.64it/s][1,2]<stderr>:#015 85%|████████▌ | 9032/10570 [00:23<00:03, 388.77it/s][1,12]<stderr>:#015 86%|████████▋ | 9140/10570 [00:23<00:03, 387.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8721/10570 [00:23<00:04, 382.16it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▍  | 7893/10570 [00:22<00:07, 344.94it/s]#033[A[1,1]<stderr>:#015 85%|████████▌ | 8989/10570 [00:23<00:04, 386.53it/s][1,15]<stderr>:#015 84%|████████▍ | 8898/10570 [00:23<00:04, 378.83it/s][1,4]<stderr>:#015 85%|████████▍ | 8964/10570 [00:23<00:04, 382.59it/s][1,13]<stderr>:#015 78%|███████▊  | 8266/10570 [00:22<00:06, 341.56it/s][1,14]<stderr>:#015 84%|████████▍ | 8908/10570 [00:23<00:04, 379.63it/s][1,6]<stderr>:#015 80%|████████  | 8457/10570 [00:22<00:05, 376.42it/s][1,10]<stderr>:#015 85%|████████▌ | 9016/10570 [00:23<00:03, 389.95it/s][1,7]<stderr>:#015 88%|████████▊ | 9273/10570 [00:23<00:03, 401.71it/s][1,9]<stderr>:#015 84%|████████▎ | 8848/10570 [00:23<00:04, 372.18it/s][1,5]<stderr>:#015 83%|████████▎ | 8824/10570 [00:23<00:04, 374.37it/s][1,11]<stderr>:#015 81%|████████▏ | 8607/10570 [00:23<00:05, 372.36it/s][1,2]<stderr>:#015 86%|████████▌ | 9071/10570 [00:23<00:03, 388.37it/s][1,3]<stderr>:#015 81%|████████  | 8586/10570 [00:23<00:05, 376.25it/s][1,12]<stderr>:#015 87%|████████▋ | 9179/10570 [00:23<00:03, 387.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8760/10570 [00:23<00:04, 377.20it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▌  | 7929/10570 [00:22<00:07, 349.19it/s]#033[A[1,1]<stderr>:#015 85%|████████▌ | 9029/10570 [00:23<00:03, 388.58it/s][1,15]<stderr>:#015 85%|████████▍ | 8937/10570 [00:23<00:04, 381.85it/s][1,4]<stderr>:#015 85%|████████▌ | 9005/10570 [00:23<00:04, 390.11it/s][1,14]<stderr>:#015 85%|████████▍ | 8948/10570 [00:23<00:04, 383.05it/s][1,6]<stderr>:#015 80%|████████  | 8495/10570 [00:23<00:05, 377.11it/s][1,13]<stderr>:#015 79%|███████▊  | 8301/10570 [00:22<00:06, 330.43it/s][1,10]<stderr>:#015 86%|████████▌ | 9056/10570 [00:23<00:03, 387.89it/s][1,7]<stderr>:#015 88%|████████▊ | 9314/10570 [00:23<00:03, 398.52it/s][1,9]<stderr>:#015 84%|████████▍ | 8888/10570 [00:23<00:04, 377.80it/s][1,5]<stderr>:#015 84%|████████▍ | 8863/10570 [00:23<00:04, 378.26it/s][1,11]<stderr>:#015 82%|████████▏ | 8646/10570 [00:23<00:05, 376.52it/s][1,2]<stderr>:#015 86%|████████▌ | 9110/10570 [00:23<00:03, 386.90it/s][1,3]<stderr>:#015 82%|████████▏ | 8625/10570 [00:23<00:05, 380.23it/s][1,12]<stderr>:#015 87%|████████▋ | 9219/10570 [00:23<00:03, 390.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8798/10570 [00:23<00:04, 373.54it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▌  | 7966/10570 [00:22<00:07, 353.15it/s]#033[A[1,1]<stderr>:#015 86%|████████▌ | 9068/10570 [00:23<00:03, 386.20it/s][1,15]<stderr>:#015 85%|████████▍ | 8976/10570 [00:23<00:04, 383.17it/s][1,4]<stderr>:#015 86%|████████▌ | 9045/10570 [00:23<00:03, 381.68it/s][1,14]<stderr>:#015 85%|████████▌ | 8988/10570 [00:23<00:04, 386.53it/s][1,6]<stderr>:#015 81%|████████  | 8534/10570 [00:23<00:05, 379.44it/s][1,13]<stderr>:#015 79%|███████▉  | 8337/10570 [00:23<00:06, 337.37it/s][1,10]<stderr>:#015 86%|████████▌ | 9096/10570 [00:23<00:03, 389.06it/s][1,7]<stderr>:#015 88%|████████▊ | 9354/10570 [00:23<00:03, 394.71it/s][1,9]<stderr>:#015 84%|████████▍ | 8926/10570 [00:23<00:04, 376.42it/s][1,5]<stderr>:#015 84%|████████▍ | 8902/10570 [00:23<00:04, 379.11it/s][1,11]<stderr>:#015 82%|████████▏ | 8684/10570 [00:23<00:05, 375.05it/s][1,2]<stderr>:#015 87%|████████▋ | 9149/10570 [00:23<00:03, 387.61it/s][1,3]<stderr>:#015 82%|████████▏ | 8664/10570 [00:23<00:05, 378.46it/s][1,12]<stderr>:#015 88%|████████▊ | 9261/10570 [00:23<00:03, 397.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 84%|████████▎ | 8836/10570 [00:23<00:04, 371.97it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▌  | 8002/10570 [00:22<00:07, 346.98it/s]#033[A[1,15]<stderr>:#015 85%|████████▌ | 9016/10570 [00:23<00:04, 387.39it/s][1,1]<stderr>:#015 86%|████████▌ | 9107/10570 [00:23<00:03, 383.95it/s][1,4]<stderr>:#015 86%|████████▌ | 9085/10570 [00:23<00:03, 384.74it/s][1,14]<stderr>:#015 85%|████████▌ | 9028/10570 [00:23<00:03, 388.96it/s][1,6]<stderr>:#015 81%|████████  | 8573/10570 [00:23<00:05, 375.83it/s][1,13]<stderr>:#015 79%|███████▉  | 8376/10570 [00:23<00:06, 349.12it/s][1,10]<stderr>:#015 86%|████████▋ | 9136/10570 [00:23<00:03, 389.72it/s][1,7]<stderr>:#015 89%|████████▉ | 9394/10570 [00:23<00:02, 393.49it/s][1,9]<stderr>:#015 85%|████████▍ | 8965/10570 [00:23<00:04, 377.88it/s][1,5]<stderr>:#015 85%|████████▍ | 8941/10570 [00:23<00:04, 380.19it/s][1,11]<stderr>:#015 83%|████████▎ | 8722/10570 [00:23<00:04, 373.40it/s][1,2]<stderr>:#015 87%|████████▋ | 9188/10570 [00:23<00:03, 388.14it/s][1,3]<stderr>:#015 82%|████████▏ | 8703/10570 [00:23<00:04, 381.79it/s][1,12]<stderr>:#015 88%|████████▊ | 9301/10570 [00:23<00:03, 395.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▌  | 8038/10570 [00:22<00:07, 349.98it/s]#033[A[1,0]<stderr>:#015 84%|████████▍ | 8874/10570 [00:23<00:04, 368.21it/s]#033[A[1,1]<stderr>:#015 87%|████████▋ | 9147/10570 [00:23<00:03, 385.93it/s][1,15]<stderr>:#015 86%|████████▌ | 9055/10570 [00:23<00:03, 380.37it/s][1,4]<stderr>:#015 86%|████████▋ | 9124/10570 [00:23<00:03, 384.66it/s][1,14]<stderr>:#015 86%|████████▌ | 9067/10570 [00:23<00:03, 384.16it/s][1,6]<stderr>:#015 81%|████████▏ | 8612/10570 [00:23<00:05, 378.68it/s][1,13]<stderr>:#015 80%|███████▉  | 8414/10570 [00:23<00:06, 357.24it/s][1,10]<stderr>:#015 87%|████████▋ | 9175/10570 [00:23<00:03, 387.91it/s][1,7]<stderr>:#015 89%|████████▉ | 9434/10570 [00:24<00:02, 387.18it/s][1,9]<stderr>:#015 85%|████████▌ | 9006/10570 [00:23<00:04, 385.74it/s][1,5]<stderr>:#015 85%|████████▍ | 8980/10570 [00:23<00:04, 383.03it/s][1,2]<stderr>:#015 87%|████████▋ | 9228/10570 [00:23<00:03, 390.35it/s][1,11]<stderr>:#015 83%|████████▎ | 8760/10570 [00:23<00:04, 369.61it/s][1,12]<stderr>:#015 88%|████████▊ | 9341/10570 [00:24<00:03, 392.35it/s][1,3]<stderr>:#015 83%|████████▎ | 8742/10570 [00:23<00:05, 361.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 84%|████████▍ | 8912/10570 [00:23<00:04, 369.87it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▋  | 8074/10570 [00:22<00:07, 347.43it/s]#033[A[1,1]<stderr>:#015 87%|████████▋ | 9187/10570 [00:23<00:03, 388.29it/s][1,15]<stderr>:#015 86%|████████▌ | 9094/10570 [00:23<00:03, 383.19it/s][1,4]<stderr>:#015 87%|████████▋ | 9163/10570 [00:23<00:03, 383.39it/s][1,14]<stderr>:#015 86%|████████▌ | 9106/10570 [00:23<00:03, 383.22it/s][1,13]<stderr>:#015 80%|███████▉  | 8454/10570 [00:23<00:05, 368.09it/s][1,6]<stderr>:#015 82%|████████▏ | 8650/10570 [00:23<00:05, 376.23it/s][1,10]<stderr>:#015 87%|████████▋ | 9215/10570 [00:23<00:03, 391.17it/s][1,7]<stderr>:#015 90%|████████▉ | 9475/10570 [00:24<00:02, 393.65it/s][1,9]<stderr>:#015 86%|████████▌ | 9045/10570 [00:23<00:03, 381.37it/s][1,5]<stderr>:#015 85%|████████▌ | 9019/10570 [00:23<00:04, 380.96it/s][1,2]<stderr>:#015 88%|████████▊ | 9269/10570 [00:24<00:03, 392.79it/s][1,11]<stderr>:#015 83%|████████▎ | 8797/10570 [00:23<00:04, 366.05it/s][1,12]<stderr>:#015 89%|████████▉ | 9381/10570 [00:24<00:03, 392.31it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▍ | 8950/10570 [00:23<00:04, 370.98it/s]#033[A[1,3]<stderr>:#015 83%|████████▎ | 8779/10570 [00:23<00:05, 355.98it/s][1,1]<stderr>:#015 87%|████████▋ | 9226/10570 [00:24<00:03, 388.39it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 86%|████████▋ | 9133/10570 [00:23<00:03, 384.68it/s][1,8]<stderr>:#015 77%|███████▋  | 8113/10570 [00:22<00:06, 357.16it/s]#033[A[1,4]<stderr>:#015 87%|████████▋ | 9203/10570 [00:24<00:03, 385.80it/s][1,14]<stderr>:#015 87%|████████▋ | 9146/10570 [00:23<00:03, 385.95it/s][1,13]<stderr>:#015 80%|████████  | 8492/10570 [00:23<00:05, 369.49it/s][1,6]<stderr>:#015 82%|████████▏ | 8688/10570 [00:23<00:05, 376.01it/s][1,10]<stderr>:#015 88%|████████▊ | 9257/10570 [00:24<00:03, 396.78it/s][1,7]<stderr>:#015 90%|█████████ | 9515/10570 [00:24<00:02, 395.15it/s][1,9]<stderr>:#015 86%|████████▌ | 9084/10570 [00:23<00:03, 383.83it/s][1,5]<stderr>:#015 86%|████████▌ | 9058/10570 [00:23<00:03, 378.12it/s][1,2]<stderr>:#015 88%|████████▊ | 9309/10570 [00:24<00:03, 391.68it/s][1,11]<stderr>:#015 84%|████████▎ | 8834/10570 [00:23<00:04, 364.51it/s][1,12]<stderr>:#015 89%|████████▉ | 9421/10570 [00:24<00:02, 391.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▌ | 8989/10570 [00:23<00:04, 375.64it/s]#033[A[1,3]<stderr>:#015 83%|████████▎ | 8816/10570 [00:23<00:04, 359.00it/s][1,1]<stderr>:#015 88%|████████▊ | 9267/10570 [00:24<00:03, 392.97it/s][1,15]<stderr>:#015 87%|████████▋ | 9172/10570 [00:24<00:03, 381.57it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 77%|███████▋  | 8149/10570 [00:23<00:06, 347.83it/s]#033[A[1,4]<stderr>:#015 87%|████████▋ | 9243/10570 [00:24<00:03, 389.77it/s][1,14]<stderr>:#015 87%|████████▋ | 9185/10570 [00:23<00:03, 385.24it/s][1,6]<stderr>:#015 83%|████████▎ | 8726/10570 [00:23<00:04, 370.33it/s][1,13]<stderr>:#015 81%|████████  | 8530/10570 [00:23<00:05, 365.56it/s][1,10]<stderr>:#015 88%|████████▊ | 9297/10570 [00:24<00:03, 392.02it/s][1,7]<stderr>:#015 90%|█████████ | 9557/10570 [00:24<00:02, 402.27it/s][1,9]<stderr>:#015 86%|████████▋ | 9123/10570 [00:24<00:03, 381.95it/s][1,5]<stderr>:#015 86%|████████▌ | 9097/10570 [00:23<00:03, 380.71it/s][1,11]<stderr>:#015 84%|████████▍ | 8872/10570 [00:23<00:04, 367.73it/s][1,2]<stderr>:#015 88%|████████▊ | 9349/10570 [00:24<00:03, 386.97it/s][1,12]<stderr>:#015 90%|████████▉ | 9463/10570 [00:24<00:02, 397.04it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▌ | 9028/10570 [00:23<00:04, 379.28it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8854/10570 [00:23<00:04, 362.42it/s][1,1]<stderr>:#015 88%|████████▊ | 9307/10570 [00:24<00:03, 391.59it/s][1,15]<stderr>:#015 87%|████████▋ | 9212/10570 [00:24<00:03, 384.13it/s][1,4]<stderr>:#015 88%|████████▊ | 9283/10570 [00:24<00:03, 392.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 77%|███████▋  | 8184/10570 [00:23<00:06, 341.76it/s]#033[A[1,14]<stderr>:#015 87%|████████▋ | 9224/10570 [00:23<00:03, 377.31it/s][1,13]<stderr>:#015 81%|████████  | 8567/10570 [00:23<00:05, 366.08it/s][1,6]<stderr>:#015 83%|████████▎ | 8764/10570 [00:23<00:04, 367.17it/s][1,10]<stderr>:#015 88%|████████▊ | 9337/10570 [00:24<00:03, 386.95it/s][1,7]<stderr>:#015 91%|█████████ | 9598/10570 [00:24<00:02, 399.69it/s][1,5]<stderr>:#015 86%|████████▋ | 9136/10570 [00:23<00:03, 382.78it/s][1,9]<stderr>:#015 87%|████████▋ | 9162/10570 [00:24<00:03, 380.65it/s][1,11]<stderr>:#015 84%|████████▍ | 8909/10570 [00:23<00:04, 365.96it/s][1,2]<stderr>:#015 89%|████████▉ | 9388/10570 [00:24<00:03, 387.61it/s][1,12]<stderr>:#015 90%|████████▉ | 9503/10570 [00:24<00:02, 395.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▌ | 9066/10570 [00:23<00:03, 378.59it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8892/10570 [00:23<00:04, 365.11it/s][1,15]<stderr>:#015 88%|████████▊ | 9253/10570 [00:24<00:03, 389.38it/s][1,1]<stderr>:#015 88%|████████▊ | 9347/10570 [00:24<00:03, 386.24it/s][1,4]<stderr>:#015 88%|████████▊ | 9323/10570 [00:24<00:03, 385.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8219/10570 [00:23<00:07, 333.32it/s]#033[A[1,14]<stderr>:#015 88%|████████▊ | 9265/10570 [00:24<00:03, 385.94it/s][1,13]<stderr>:#015 81%|████████▏ | 8604/10570 [00:23<00:05, 365.56it/s][1,6]<stderr>:#015 83%|████████▎ | 8801/10570 [00:23<00:04, 363.38it/s][1,10]<stderr>:#015 89%|████████▊ | 9377/10570 [00:24<00:03, 388.20it/s][1,7]<stderr>:#015 91%|█████████ | 9640/10570 [00:24<00:02, 404.67it/s][1,9]<stderr>:#015 87%|████████▋ | 9202/10570 [00:24<00:03, 385.43it/s][1,5]<stderr>:#015 87%|████████▋ | 9175/10570 [00:24<00:03, 381.73it/s][1,11]<stderr>:#015 85%|████████▍ | 8946/10570 [00:24<00:04, 366.94it/s][1,2]<stderr>:#015 89%|████████▉ | 9428/10570 [00:24<00:02, 390.72it/s][1,12]<stderr>:#015 90%|█████████ | 9545/10570 [00:24<00:02, 401.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▌ | 9104/10570 [00:24<00:03, 376.97it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8930/10570 [00:24<00:04, 368.24it/s][1,15]<stderr>:#015 88%|████████▊ | 9292/10570 [00:24<00:03, 388.30it/s][1,1]<stderr>:#015 89%|████████▉ | 9386/10570 [00:24<00:03, 387.08it/s][1,4]<stderr>:#015 89%|████████▊ | 9363/10570 [00:24<00:03, 387.14it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8253/10570 [00:23<00:06, 331.76it/s]#033[A[1,14]<stderr>:#015 88%|████████▊ | 9305/10570 [00:24<00:03, 387.29it/s][1,13]<stderr>:#015 82%|████████▏ | 8641/10570 [00:23<00:05, 364.32it/s][1,6]<stderr>:#015 84%|████████▎ | 8838/10570 [00:23<00:04, 361.37it/s][1,10]<stderr>:#015 89%|████████▉ | 9416/10570 [00:24<00:02, 388.01it/s][1,7]<stderr>:#015 92%|█████████▏| 9682/10570 [00:24<00:02, 406.75it/s][1,9]<stderr>:#015 87%|████████▋ | 9242/10570 [00:24<00:03, 389.12it/s][1,5]<stderr>:#015 87%|████████▋ | 9215/10570 [00:24<00:03, 385.28it/s][1,2]<stderr>:#015 90%|████████▉ | 9469/10570 [00:24<00:02, 396.04it/s][1,11]<stderr>:#015 85%|████████▍ | 8983/10570 [00:24<00:04, 362.95it/s][1,12]<stderr>:#015 91%|█████████ | 9586/10570 [00:24<00:02, 400.98it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▋ | 9143/10570 [00:24<00:03, 380.39it/s][1,0]<stderr>:#033[A[1,3]<stderr>:#015 85%|████████▍ | 8968/10570 [00:24<00:04, 369.79it/s][1,1]<stderr>:#015 89%|████████▉ | 9426/10570 [00:24<00:02, 390.02it/s][1,15]<stderr>:#015 88%|████████▊ | 9331/10570 [00:24<00:03, 385.10it/s][1,4]<stderr>:#015 89%|████████▉ | 9402/10570 [00:24<00:03, 386.18it/s][1,14]<stderr>:#015 88%|████████▊ | 9344/10570 [00:24<00:03, 383.57it/s][1,13]<stderr>:#015 82%|████████▏ | 8678/10570 [00:24<00:05, 360.67it/s][1,6]<stderr>:#015 84%|████████▍ | 8877/10570 [00:24<00:04, 366.83it/s][1,10]<stderr>:#015 89%|████████▉ | 9457/10570 [00:24<00:02, 393.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8287/10570 [00:23<00:07, 313.43it/s]#033[A[1,7]<stderr>:#015 92%|█████████▏| 9726/10570 [00:24<00:02, 414.09it/s][1,9]<stderr>:#015 88%|████████▊ | 9282/10570 [00:24<00:03, 390.60it/s][1,5]<stderr>:#015 88%|████████▊ | 9256/10570 [00:24<00:03, 391.79it/s][1,2]<stderr>:#015 90%|████████▉ | 9509/10570 [00:24<00:02, 394.53it/s][1,11]<stderr>:#015 85%|████████▌ | 9022/10570 [00:24<00:04, 369.17it/s][1,12]<stderr>:#015 91%|█████████ | 9627/10570 [00:24<00:02, 403.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 87%|████████▋ | 9182/10570 [00:24<00:03, 380.32it/s]#033[A[1,3]<stderr>:#015 85%|████████▌ | 9008/10570 [00:24<00:04, 377.25it/s][1,15]<stderr>:#015 89%|████████▊ | 9370/10570 [00:24<00:03, 386.10it/s][1,1]<stderr>:#015 90%|████████▉ | 9468/10570 [00:24<00:02, 395.63it/s][1,4]<stderr>:#015 89%|████████▉ | 9442/10570 [00:24<00:02, 389.48it/s][1,14]<stderr>:#015 89%|████████▉ | 9383/10570 [00:24<00:03, 384.72it/s][1,13]<stderr>:#015 82%|████████▏ | 8715/10570 [00:24<00:05, 363.12it/s][1,10]<stderr>:#015 90%|████████▉ | 9497/10570 [00:24<00:02, 394.77it/s][1,6]<stderr>:#015 84%|████████▍ | 8914/10570 [00:24<00:04, 366.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▊  | 8323/10570 [00:23<00:06, 324.78it/s]#033[A[1,7]<stderr>:#015 92%|█████████▏| 9768/10570 [00:24<00:01, 413.08it/s][1,5]<stderr>:#015 88%|████████▊ | 9296/10570 [00:24<00:03, 389.60it/s][1,9]<stderr>:#015 88%|████████▊ | 9322/10570 [00:24<00:03, 384.20it/s][1,2]<stderr>:#015 90%|█████████ | 9552/10570 [00:24<00:02, 402.74it/s][1,12]<stderr>:#015 91%|█████████▏| 9668/10570 [00:24<00:02, 405.14it/s][1,11]<stderr>:#015 86%|████████▌ | 9059/10570 [00:24<00:04, 358.35it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 87%|████████▋ | 9221/10570 [00:24<00:03, 381.78it/s]#033[A[1,3]<stderr>:#015 86%|████████▌ | 9046/10570 [00:24<00:04, 372.90it/s][1,15]<stderr>:#015 89%|████████▉ | 9409/10570 [00:24<00:03, 384.14it/s][1,1]<stderr>:#015 90%|████████▉ | 9508/10570 [00:24<00:02, 393.74it/s][1,4]<stderr>:#015 90%|████████▉ | 9482/10570 [00:24<00:02, 391.96it/s][1,14]<stderr>:#015 89%|████████▉ | 9422/10570 [00:24<00:02, 385.82it/s][1,10]<stderr>:#015 90%|█████████ | 9538/10570 [00:24<00:02, 398.67it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▉  | 8359/10570 [00:23<00:06, 334.42it/s]#033[A[1,6]<stderr>:#015 85%|████████▍ | 8952/10570 [00:24<00:04, 368.41it/s][1,13]<stderr>:#015 83%|████████▎ | 8752/10570 [00:24<00:05, 351.96it/s][1,7]<stderr>:#015 93%|█████████▎| 9810/10570 [00:25<00:01, 406.54it/s][1,9]<stderr>:#015 89%|████████▊ | 9361/10570 [00:24<00:03, 384.79it/s][1,5]<stderr>:#015 88%|████████▊ | 9335/10570 [00:24<00:03, 382.97it/s][1,2]<stderr>:#015 91%|█████████ | 9593/10570 [00:24<00:02, 397.39it/s][1,12]<stderr>:#015 92%|█████████▏| 9711/10570 [00:24<00:02, 409.73it/s][1,11]<stderr>:#015 86%|████████▌ | 9097/10570 [00:24<00:04, 362.61it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9262/10570 [00:24<00:03, 387.03it/s]#033[A[1,3]<stderr>:#015 86%|████████▌ | 9084/10570 [00:24<00:03, 374.70it/s][1,15]<stderr>:#015 89%|████████▉ | 9448/10570 [00:24<00:02, 384.76it/s][1,1]<stderr>:#015 90%|█████████ | 9551/10570 [00:24<00:02, 401.85it/s][1,4]<stderr>:#015 90%|█████████ | 9523/10570 [00:24<00:02, 396.64it/s][1,14]<stderr>:#015 90%|████████▉ | 9464/10570 [00:24<00:02, 393.26it/s][1,10]<stderr>:#015 91%|█████████ | 9579/10570 [00:24<00:02, 400.55it/s][1,6]<stderr>:#015 85%|████████▌ | 8990/10570 [00:24<00:04, 371.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▉  | 8396/10570 [00:23<00:06, 342.75it/s]#033[A[1,13]<stderr>:#015 83%|████████▎ | 8788/10570 [00:24<00:05, 347.38it/s][1,9]<stderr>:#015 89%|████████▉ | 9400/10570 [00:24<00:03, 383.35it/s][1,7]<stderr>:#015 93%|█████████▎| 9851/10570 [00:25<00:01, 396.87it/s][1,5]<stderr>:#015 89%|████████▊ | 9375/10570 [00:24<00:03, 384.83it/s][1,2]<stderr>:#015 91%|█████████ | 9633/10570 [00:25<00:02, 394.99it/s][1,12]<stderr>:#015 92%|█████████▏| 9752/10570 [00:25<00:01, 409.45it/s][1,11]<stderr>:#015 86%|████████▋ | 9135/10570 [00:24<00:03, 367.12it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9301/10570 [00:24<00:03, 385.56it/s]#033[A[1,3]<stderr>:#015 86%|████████▋ | 9122/10570 [00:24<00:03, 374.18it/s][1,15]<stderr>:#015 90%|████████▉ | 9488/10570 [00:24<00:02, 387.73it/s][1,1]<stderr>:#015 91%|█████████ | 9592/10570 [00:24<00:02, 397.52it/s][1,4]<stderr>:#015 90%|█████████ | 9565/10570 [00:24<00:02, 402.34it/s][1,14]<stderr>:#015 90%|████████▉ | 9504/10570 [00:24<00:02, 391.92it/s][1,6]<stderr>:#015 85%|████████▌ | 9028/10570 [00:24<00:04, 373.64it/s][1,10]<stderr>:#015 91%|█████████ | 9620/10570 [00:24<00:02, 400.12it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 80%|███████▉  | 8434/10570 [00:23<00:06, 352.24it/s]#033[A[1,13]<stderr>:#015 83%|████████▎ | 8824/10570 [00:24<00:04, 349.61it/s][1,9]<stderr>:#015 89%|████████▉ | 9440/10570 [00:24<00:02, 386.27it/s][1,7]<stderr>:#015 94%|█████████▎| 9893/10570 [00:25<00:01, 401.75it/s][1,5]<stderr>:#015 89%|████████▉ | 9414/10570 [00:24<00:03, 384.50it/s][1,2]<stderr>:#015 92%|█████████▏| 9674/10570 [00:25<00:02, 398.72it/s][1,11]<stderr>:#015 87%|████████▋ | 9172/10570 [00:24<00:03, 366.67it/s][1,12]<stderr>:#015 93%|█████████▎| 9793/10570 [00:25<00:01, 405.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9340/10570 [00:24<00:03, 381.05it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9160/10570 [00:24<00:03, 373.19it/s][1,15]<stderr>:#015 90%|█████████ | 9530/10570 [00:24<00:02, 394.44it/s][1,1]<stderr>:#015 91%|█████████ | 9633/10570 [00:25<00:02, 400.84it/s][1,4]<stderr>:#015 91%|█████████ | 9606/10570 [00:25<00:02, 397.71it/s][1,14]<stderr>:#015 90%|█████████ | 9546/10570 [00:24<00:02, 398.47it/s][1,6]<stderr>:#015 86%|████████▌ | 9066/10570 [00:24<00:04, 372.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 80%|████████  | 8472/10570 [00:24<00:05, 358.89it/s]#033[A[1,10]<stderr>:#015 91%|█████████▏| 9661/10570 [00:25<00:02, 401.07it/s][1,13]<stderr>:#015 84%|████████▍ | 8861/10570 [00:24<00:04, 353.58it/s][1,9]<stderr>:#015 90%|████████▉ | 9480/10570 [00:24<00:02, 389.37it/s][1,7]<stderr>:#015 94%|█████████▍| 9935/10570 [00:25<00:01, 404.85it/s][1,5]<stderr>:#015 89%|████████▉ | 9455/10570 [00:24<00:02, 390.09it/s][1,2]<stderr>:#015 92%|█████████▏| 9717/10570 [00:25<00:02, 405.87it/s][1,11]<stderr>:#015 87%|████████▋ | 9211/10570 [00:24<00:03, 372.11it/s][1,12]<stderr>:#015 93%|█████████▎| 9834/10570 [00:25<00:01, 402.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▊ | 9379/10570 [00:24<00:03, 381.77it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9199/10570 [00:24<00:03, 377.82it/s][1,15]<stderr>:#015 91%|█████████ | 9571/10570 [00:25<00:02, 395.91it/s][1,1]<stderr>:#015 92%|█████████▏| 9674/10570 [00:25<00:02, 402.43it/s][1,4]<stderr>:#015 91%|█████████▏| 9647/10570 [00:25<00:02, 399.33it/s][1,14]<stderr>:#015 91%|█████████ | 9586/10570 [00:24<00:02, 397.34it/s][1,10]<stderr>:#015 92%|█████████▏| 9704/10570 [00:25<00:02, 406.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8509/10570 [00:24<00:05, 358.28it/s]#033[A[1,6]<stderr>:#015 86%|████████▌ | 9104/10570 [00:24<00:03, 369.42it/s][1,13]<stderr>:#015 84%|████████▍ | 8897/10570 [00:24<00:04, 354.09it/s][1,9]<stderr>:#015 90%|█████████ | 9521/10570 [00:25<00:02, 392.92it/s][1,5]<stderr>:#015 90%|████████▉ | 9495/10570 [00:24<00:02, 391.61it/s][1,7]<stderr>:#015 94%|█████████▍| 9976/10570 [00:25<00:01, 396.62it/s][1,2]<stderr>:#015 92%|█████████▏| 9759/10570 [00:25<00:01, 408.22it/s][1,11]<stderr>:#015 88%|████████▊ | 9251/10570 [00:24<00:03, 377.53it/s][1,12]<stderr>:#015 93%|█████████▎| 9875/10570 [00:25<00:01, 400.80it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▉ | 9418/10570 [00:24<00:03, 381.49it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9238/10570 [00:24<00:03, 380.72it/s][1,15]<stderr>:#015 91%|█████████ | 9611/10570 [00:25<00:02, 396.27it/s][1,1]<stderr>:#015 92%|█████████▏| 9717/10570 [00:25<00:02, 407.50it/s][1,4]<stderr>:#015 92%|█████████▏| 9688/10570 [00:25<00:02, 401.56it/s][1,14]<stderr>:#015 91%|█████████ | 9627/10570 [00:24<00:02, 400.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8547/10570 [00:24<00:05, 364.21it/s]#033[A[1,10]<stderr>:#015 92%|█████████▏| 9746/10570 [00:25<00:02, 407.65it/s][1,6]<stderr>:#015 86%|████████▋ | 9143/10570 [00:24<00:03, 372.57it/s][1,13]<stderr>:#015 85%|████████▍ | 8934/10570 [00:24<00:04, 357.22it/s][1,9]<stderr>:#015 90%|█████████ | 9563/10570 [00:25<00:02, 399.07it/s][1,5]<stderr>:#015 90%|█████████ | 9536/10570 [00:24<00:02, 395.40it/s][1,7]<stderr>:#015 95%|█████████▍| 10017/10570 [00:25<00:01, 398.78it/s][1,2]<stderr>:#015 93%|█████████▎| 9800/10570 [00:25<00:01, 402.37it/s][1,11]<stderr>:#015 88%|████████▊ | 9290/10570 [00:24<00:03, 379.12it/s][1,12]<stderr>:#015 94%|█████████▍| 9916/10570 [00:25<00:01, 398.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▉ | 9459/10570 [00:24<00:02, 388.18it/s]#033[A[1,3]<stderr>:#015 88%|████████▊ | 9278/10570 [00:24<00:03, 383.78it/s][1,15]<stderr>:#015 91%|█████████▏| 9651/10570 [00:25<00:02, 396.62it/s][1,1]<stderr>:#015 92%|█████████▏| 9758/10570 [00:25<00:01, 408.11it/s][1,4]<stderr>:#015 92%|█████████▏| 9731/10570 [00:25<00:02, 407.13it/s][1,14]<stderr>:#015 91%|█████████▏| 9668/10570 [00:25<00:02, 401.68it/s][1,10]<stderr>:#015 93%|█████████▎| 9787/10570 [00:25<00:01, 404.56it/s][1,6]<stderr>:#015 87%|████████▋ | 9181/10570 [00:24<00:03, 372.29it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8584/10570 [00:24<00:05, 355.77it/s]#033[A[1,13]<stderr>:#015 85%|████████▍ | 8970/10570 [00:24<00:04, 355.28it/s][1,5]<stderr>:#015 91%|█████████ | 9576/10570 [00:25<00:02, 396.54it/s][1,9]<stderr>:#015 91%|█████████ | 9603/10570 [00:25<00:02, 393.88it/s][1,7]<stderr>:#015 95%|█████████▌| 10059/10570 [00:25<00:01, 404.35it/s][1,11]<stderr>:#015 88%|████████▊ | 9328/10570 [00:25<00:03, 375.06it/s][1,2]<stderr>:#015 93%|█████████▎| 9841/10570 [00:25<00:01, 395.74it/s][1,12]<stderr>:#015 94%|█████████▍| 9958/10570 [00:25<00:01, 401.93it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|████████▉ | 9498/10570 [00:25<00:02, 388.26it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9692/10570 [00:25<00:02, 399.32it/s][1,3]<stderr>:#015 88%|████████▊ | 9317/10570 [00:25<00:03, 375.97it/s][1,1]<stderr>:#015 93%|█████████▎| 9799/10570 [00:25<00:01, 402.49it/s][1,4]<stderr>:#015 92%|█████████▏| 9772/10570 [00:25<00:01, 406.91it/s][1,14]<stderr>:#015 92%|█████████▏| 9710/10570 [00:25<00:02, 406.65it/s][1,6]<stderr>:#015 87%|████████▋ | 9219/10570 [00:24<00:03, 373.91it/s][1,10]<stderr>:#015 93%|█████████▎| 9828/10570 [00:25<00:01, 403.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8621/10570 [00:24<00:05, 358.08it/s]#033[A[1,13]<stderr>:#015 85%|████████▌ | 9009/10570 [00:24<00:04, 363.06it/s][1,5]<stderr>:#015 91%|█████████ | 9616/10570 [00:25<00:02, 397.29it/s][1,9]<stderr>:#015 91%|█████████ | 9643/10570 [00:25<00:02, 395.61it/s][1,7]<stderr>:#015 96%|█████████▌| 10100/10570 [00:25<00:01, 402.44it/s][1,11]<stderr>:#015 89%|████████▊ | 9366/10570 [00:25<00:03, 376.04it/s][1,2]<stderr>:#015 93%|█████████▎| 9882/10570 [00:25<00:01, 398.21it/s][1,12]<stderr>:#015 95%|█████████▍| 9999/10570 [00:25<00:01, 393.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|█████████ | 9539/10570 [00:25<00:02, 392.02it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9734/10570 [00:25<00:02, 404.13it/s][1,3]<stderr>:#015 89%|████████▊ | 9355/10570 [00:25<00:03, 376.27it/s][1,1]<stderr>:#015 93%|█████████▎| 9840/10570 [00:25<00:01, 396.02it/s][1,4]<stderr>:#015 93%|█████████▎| 9813/10570 [00:25<00:01, 402.60it/s][1,14]<stderr>:#015 92%|█████████▏| 9751/10570 [00:25<00:02, 405.89it/s][1,6]<stderr>:#015 88%|████████▊ | 9260/10570 [00:25<00:03, 381.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8657/10570 [00:24<00:05, 355.92it/s]#033[A[1,10]<stderr>:#015 93%|█████████▎| 9869/10570 [00:25<00:01, 395.78it/s][1,13]<stderr>:#015 86%|████████▌ | 9046/10570 [00:25<00:04, 358.57it/s][1,5]<stderr>:#015 91%|█████████▏| 9656/10570 [00:25<00:02, 397.01it/s][1,9]<stderr>:#015 92%|█████████▏| 9683/10570 [00:25<00:02, 396.86it/s][1,7]<stderr>:#015 96%|█████████▌| 10142/10570 [00:25<00:01, 406.62it/s][1,11]<stderr>:#015 89%|████████▉ | 9404/10570 [00:25<00:03, 374.82it/s][1,2]<stderr>:#015 94%|█████████▍| 9922/10570 [00:25<00:01, 398.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 95%|█████████▍| 10041/10570 [00:25<00:01, 400.29it/s][1,0]<stderr>:#015 91%|█████████ | 9579/10570 [00:25<00:02, 394.09it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9775/10570 [00:25<00:01, 404.57it/s][1,3]<stderr>:#015 89%|████████▉ | 9393/10570 [00:25<00:03, 374.47it/s][1,1]<stderr>:#015 93%|█████████▎| 9881/10570 [00:25<00:01, 397.21it/s][1,4]<stderr>:#015 93%|█████████▎| 9854/10570 [00:25<00:01, 392.78it/s][1,14]<stderr>:#015 93%|█████████▎| 9792/10570 [00:25<00:01, 402.90it/s][1,6]<stderr>:#015 88%|████████▊ | 9299/10570 [00:25<00:03, 378.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8695/10570 [00:24<00:05, 362.13it/s]#033[A[1,10]<stderr>:#015 94%|█████████▎| 9909/10570 [00:25<00:01, 394.20it/s][1,5]<stderr>:#015 92%|█████████▏| 9698/10570 [00:25<00:02, 402.16it/s][1,13]<stderr>:#015 86%|████████▌ | 9084/10570 [00:25<00:04, 362.24it/s][1,9]<stderr>:#015 92%|█████████▏| 9726/10570 [00:25<00:02, 403.45it/s][1,7]<stderr>:#015 96%|█████████▋| 10183/10570 [00:25<00:00, 394.48it/s][1,2]<stderr>:#015 94%|█████████▍| 9963/10570 [00:25<00:01, 400.33it/s][1,11]<stderr>:#015 89%|████████▉ | 9444/10570 [00:25<00:02, 379.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 91%|█████████ | 9619/10570 [00:25<00:02, 393.60it/s]#033[A[1,12]<stderr>:#015 95%|█████████▌| 10082/10570 [00:25<00:01, 398.81it/s][1,3]<stderr>:#015 89%|████████▉ | 9432/10570 [00:25<00:03, 376.69it/s][1,15]<stderr>:#015 93%|█████████▎| 9816/10570 [00:25<00:01, 395.03it/s][1,1]<stderr>:#015 94%|█████████▍| 9921/10570 [00:25<00:01, 396.09it/s][1,4]<stderr>:#015 94%|█████████▎| 9894/10570 [00:25<00:01, 393.38it/s][1,14]<stderr>:#015 93%|█████████▎| 9833/10570 [00:25<00:01, 400.10it/s][1,6]<stderr>:#015 88%|████████▊ | 9337/10570 [00:25<00:03, 373.81it/s][1,10]<stderr>:#015 94%|█████████▍| 9951/10570 [00:25<00:01, 399.24it/s][1,5]<stderr>:#015 92%|█████████▏| 9739/10570 [00:25<00:02, 404.24it/s][1,13]<stderr>:#015 86%|████████▋ | 9121/10570 [00:25<00:04, 362.08it/s][1,9]<stderr>:#015 92%|█████████▏| 9767/10570 [00:25<00:01, 404.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8732/10570 [00:24<00:05, 353.51it/s]#033[A[1,7]<stderr>:#015 97%|█████████▋| 10224/10570 [00:26<00:00, 398.17it/s][1,11]<stderr>:#015 90%|████████▉ | 9483/10570 [00:25<00:02, 381.71it/s][1,2]<stderr>:#015 95%|█████████▍| 10004/10570 [00:25<00:01, 391.52it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 91%|█████████▏| 9659/10570 [00:25<00:02, 393.77it/s]#033[A[1,12]<stderr>:#015 96%|█████████▌| 10124/10570 [00:26<00:01, 402.84it/s][1,3]<stderr>:#015 90%|████████▉ | 9472/10570 [00:25<00:02, 381.52it/s][1,15]<stderr>:#015 93%|█████████▎| 9856/10570 [00:25<00:01, 388.23it/s][1,1]<stderr>:#015 94%|█████████▍| 9962/10570 [00:25<00:01, 398.67it/s][1,4]<stderr>:#015 94%|█████████▍| 9935/10570 [00:25<00:01, 396.52it/s][1,14]<stderr>:#015 93%|█████████▎| 9874/10570 [00:25<00:01, 398.33it/s][1,6]<stderr>:#015 89%|████████▊ | 9375/10570 [00:25<00:03, 374.76it/s][1,5]<stderr>:#015 93%|█████████▎| 9780/10570 [00:25<00:01, 402.99it/s][1,10]<stderr>:#015 95%|█████████▍| 9991/10570 [00:25<00:01, 390.91it/s][1,13]<stderr>:#015 87%|████████▋ | 9158/10570 [00:25<00:03, 361.65it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8768/10570 [00:24<00:05, 350.96it/s]#033[A[1,9]<stderr>:#015 93%|█████████▎| 9808/10570 [00:25<00:01, 399.87it/s][1,7]<stderr>:#015 97%|█████████▋| 10264/10570 [00:26<00:00, 394.75it/s][1,11]<stderr>:#015 90%|█████████ | 9523/10570 [00:25<00:02, 386.42it/s][1,2]<stderr>:#015 95%|█████████▌| 10046/10570 [00:26<00:01, 397.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 92%|█████████▏| 9700/10570 [00:25<00:02, 398.46it/s]#033[A[1,3]<stderr>:#015 90%|████████▉ | 9511/10570 [00:25<00:02, 380.81it/s][1,12]<stderr>:#015 96%|█████████▌| 10165/10570 [00:26<00:01, 390.39it/s][1,15]<stderr>:#015 94%|█████████▎| 9896/10570 [00:25<00:01, 390.58it/s][1,1]<stderr>:#015 95%|█████████▍| 10002/10570 [00:25<00:01, 382.76it/s][1,4]<stderr>:#015 94%|█████████▍| 9975/10570 [00:25<00:01, 389.20it/s][1,14]<stderr>:#015 94%|█████████▍| 9914/10570 [00:25<00:01, 395.25it/s][1,6]<stderr>:#015 89%|████████▉ | 9413/10570 [00:25<00:03, 373.77it/s][1,10]<stderr>:#015 95%|█████████▍| 10033/10570 [00:26<00:01, 398.61it/s][1,13]<stderr>:#015 87%|████████▋ | 9196/10570 [00:25<00:03, 365.64it/s][1,5]<stderr>:#015 93%|█████████▎| 9821/10570 [00:25<00:01, 399.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8804/10570 [00:24<00:05, 346.63it/s]#033[A[1,9]<stderr>:#015 93%|█████████▎| 9849/10570 [00:25<00:01, 388.99it/s][1,7]<stderr>:#015 97%|█████████▋| 10305/10570 [00:26<00:00, 397.37it/s][1,11]<stderr>:#015 90%|█████████ | 9564/10570 [00:25<00:02, 391.70it/s][1,2]<stderr>:#015 95%|█████████▌| 10086/10570 [00:26<00:01, 396.09it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 92%|█████████▏| 9741/10570 [00:25<00:02, 401.84it/s]#033[A[1,3]<stderr>:#015 90%|█████████ | 9552/10570 [00:25<00:02, 388.76it/s][1,12]<stderr>:#015 97%|█████████▋| 10206/10570 [00:26<00:00, 395.57it/s][1,15]<stderr>:#015 94%|█████████▍| 9937/10570 [00:26<00:01, 394.36it/s][1,1]<stderr>:#015 95%|█████████▌| 10043/10570 [00:26<00:01, 390.31it/s][1,4]<stderr>:#015 95%|█████████▍| 10015/10570 [00:26<00:01, 391.76it/s][1,14]<stderr>:#015 94%|█████████▍| 9956/10570 [00:25<00:01, 400.86it/s][1,6]<stderr>:#015 89%|████████▉ | 9453/10570 [00:25<00:02, 378.60it/s][1,10]<stderr>:#015 95%|█████████▌| 10073/10570 [00:26<00:01, 398.60it/s][1,13]<stderr>:#015 87%|████████▋ | 9234/10570 [00:25<00:03, 368.25it/s][1,5]<stderr>:#015 93%|█████████▎| 9861/10570 [00:25<00:01, 391.42it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▎ | 8839/10570 [00:25<00:05, 345.86it/s]#033[A[1,9]<stderr>:#015 94%|█████████▎| 9890/10570 [00:26<00:01, 393.51it/s][1,7]<stderr>:#015 98%|█████████▊| 10347/10570 [00:26<00:00, 401.81it/s][1,11]<stderr>:#015 91%|█████████ | 9604/10570 [00:25<00:02, 387.19it/s][1,2]<stderr>:#015 96%|█████████▌| 10128/10570 [00:26<00:01, 400.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9782/10570 [00:25<00:01, 399.11it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 97%|█████████▋| 10247/10570 [00:26<00:00, 398.78it/s][1,3]<stderr>:#015 91%|█████████ | 9591/10570 [00:25<00:02, 384.61it/s][1,15]<stderr>:#015 94%|█████████▍| 9977/10570 [00:26<00:01, 387.72it/s][1,1]<stderr>:#015 95%|█████████▌| 10083/10570 [00:26<00:01, 391.24it/s][1,4]<stderr>:#015 95%|█████████▌| 10056/10570 [00:26<00:01, 396.82it/s][1,14]<stderr>:#015 95%|█████████▍| 9997/10570 [00:25<00:01, 390.58it/s][1,6]<stderr>:#015 90%|████████▉ | 9492/10570 [00:25<00:02, 380.93it/s][1,13]<stderr>:#015 88%|████████▊ | 9272/10570 [00:25<00:03, 371.37it/s][1,10]<stderr>:#015 96%|█████████▌| 10113/10570 [00:26<00:01, 390.01it/s][1,5]<stderr>:#015 94%|█████████▎| 9901/10570 [00:25<00:01, 392.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▍ | 8876/10570 [00:25<00:04, 350.87it/s]#033[A[1,9]<stderr>:#015 94%|█████████▍| 9930/10570 [00:26<00:01, 394.05it/s][1,7]<stderr>:#015 98%|█████████▊| 10388/10570 [00:26<00:00, 395.91it/s][1,11]<stderr>:#015 91%|█████████ | 9643/10570 [00:25<00:02, 387.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9822/10570 [00:25<00:01, 397.69it/s]#033[A[1,2]<stderr>:#015 96%|█████████▌| 10169/10570 [00:26<00:01, 389.10it/s][1,3]<stderr>:#015 91%|█████████ | 9630/10570 [00:25<00:02, 385.73it/s][1,12]<stderr>:#015 97%|█████████▋| 10287/10570 [00:26<00:00, 394.77it/s][1,15]<stderr>:#015 95%|█████████▍| 10016/10570 [00:26<00:01, 385.05it/s][1,1]<stderr>:#015 96%|█████████▌| 10124/10570 [00:26<00:01, 396.05it/s][1,4]<stderr>:#015 96%|█████████▌| 10096/10570 [00:26<00:01, 393.83it/s][1,14]<stderr>:#015 95%|█████████▍| 10038/10570 [00:26<00:01, 394.80it/s][1,6]<stderr>:#015 90%|█████████ | 9532/10570 [00:25<00:02, 385.29it/s][1,10]<stderr>:#015 96%|█████████▌| 10153/10570 [00:26<00:01, 392.48it/s][1,5]<stderr>:#015 94%|█████████▍| 9941/10570 [00:25<00:01, 392.94it/s][1,13]<stderr>:#015 88%|████████▊ | 9310/10570 [00:25<00:03, 361.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▍ | 8912/10570 [00:25<00:04, 348.08it/s]#033[A[1,9]<stderr>:#015 94%|█████████▍| 9970/10570 [00:26<00:01, 384.28it/s][1,7]<stderr>:#015 99%|█████████▊| 10428/10570 [00:26<00:00, 392.68it/s][1,11]<stderr>:#015 92%|█████████▏| 9682/10570 [00:25<00:02, 385.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9862/10570 [00:25<00:01, 389.39it/s]#033[A[1,2]<stderr>:#015 97%|█████████▋| 10210/10570 [00:26<00:00, 393.61it/s][1,3]<stderr>:#015 91%|█████████▏| 9670/10570 [00:25<00:02, 388.77it/s][1,12]<stderr>:#015 98%|█████████▊| 10327/10570 [00:26<00:00, 394.42it/s][1,15]<stderr>:#015 95%|█████████▌| 10057/10570 [00:26<00:01, 389.79it/s][1,4]<stderr>:#015 96%|█████████▌| 10137/10570 [00:26<00:01, 397.77it/s][1,1]<stderr>:#015 96%|█████████▌| 10164/10570 [00:26<00:01, 384.04it/s][1,14]<stderr>:#015 95%|█████████▌| 10078/10570 [00:26<00:01, 389.50it/s][1,6]<stderr>:#015 91%|█████████ | 9571/10570 [00:25<00:02, 386.03it/s][1,10]<stderr>:#015 96%|█████████▋| 10193/10570 [00:26<00:00, 382.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▍ | 8947/10570 [00:25<00:04, 347.76it/s]#033[A[1,5]<stderr>:#015 94%|█████████▍| 9981/10570 [00:26<00:01, 384.84it/s][1,13]<stderr>:#015 88%|████████▊ | 9347/10570 [00:25<00:03, 356.60it/s][1,9]<stderr>:#015 95%|█████████▍| 10009/10570 [00:26<00:01, 385.05it/s][1,11]<stderr>:#015 92%|█████████▏| 9721/10570 [00:26<00:02, 385.56it/s][1,7]<stderr>:#015 99%|█████████▉| 10468/10570 [00:26<00:00, 387.92it/s][1,2]<stderr>:#015 97%|█████████▋| 10250/10570 [00:26<00:00, 395.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▎| 9902/10570 [00:26<00:01, 389.40it/s]#033[A[1,3]<stderr>:#015 92%|█████████▏| 9711/10570 [00:26<00:02, 394.19it/s][1,12]<stderr>:#015 98%|█████████▊| 10367/10570 [00:26<00:00, 391.86it/s][1,15]<stderr>:#015 96%|█████████▌| 10097/10570 [00:26<00:01, 378.20it/s][1,1]<stderr>:#015 97%|█████████▋| 10205/10570 [00:26<00:00, 390.01it/s][1,4]<stderr>:#015 96%|█████████▋| 10177/10570 [00:26<00:01, 386.06it/s][1,14]<stderr>:#015 96%|█████████▌| 10118/10570 [00:26<00:01, 392.24it/s][1,6]<stderr>:#015 91%|█████████ | 9610/10570 [00:25<00:02, 384.97it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▍ | 8983/10570 [00:25<00:04, 350.74it/s]#033[A[1,10]<stderr>:#015 97%|█████████▋| 10233/10570 [00:26<00:00, 386.15it/s][1,5]<stderr>:#015 95%|█████████▍| 10021/10570 [00:26<00:01, 389.12it/s][1,13]<stderr>:#015 89%|████████▉ | 9384/10570 [00:25<00:03, 357.80it/s][1,9]<stderr>:#015 95%|█████████▌| 10049/10570 [00:26<00:01, 388.31it/s][1,11]<stderr>:#015 92%|█████████▏| 9761/10570 [00:26<00:02, 387.54it/s][1,7]<stderr>:#015 99%|█████████▉| 10509/10570 [00:26<00:00, 393.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▍| 9942/10570 [00:26<00:01, 391.36it/s]#033[A[1,2]<stderr>:#015 97%|█████████▋| 10290/10570 [00:26<00:00, 393.12it/s][1,3]<stderr>:#015 92%|█████████▏| 9751/10570 [00:26<00:02, 393.04it/s][1,12]<stderr>:#015 98%|█████████▊| 10407/10570 [00:26<00:00, 386.56it/s][1,15]<stderr>:#015 96%|█████████▌| 10138/10570 [00:26<00:01, 384.97it/s][1,1]<stderr>:#015 97%|█████████▋| 10246/10570 [00:26<00:00, 392.76it/s][1,4]<stderr>:#015 97%|█████████▋| 10218/10570 [00:26<00:00, 390.56it/s][1,14]<stderr>:#015 96%|█████████▌| 10158/10570 [00:26<00:01, 391.15it/s][1,6]<stderr>:#015 91%|█████████▏| 9649/10570 [00:26<00:02, 384.31it/s][1,10]<stderr>:#015 97%|█████████▋| 10272/10570 [00:26<00:00, 385.38it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▌ | 9020/10570 [00:25<00:04, 353.76it/s]#033[A[1,13]<stderr>:#015 89%|████████▉ | 9421/10570 [00:26<00:03, 359.35it/s][1,5]<stderr>:#015 95%|█████████▌| 10060/10570 [00:26<00:01, 382.85it/s][1,9]<stderr>:#015 95%|█████████▌| 10088/10570 [00:26<00:01, 386.43it/s][1,7]<stderr>:#015100%|█████████▉| 10550/10570 [00:26<00:00, 396.19it/s][1,11]<stderr>:#015 93%|█████████▎| 9800/10570 [00:26<00:02, 381.68it/s][1,2]<stderr>:#015 98%|█████████▊| 10330/10570 [00:26<00:00, 390.19it/s][1,3]<stderr>:#015 93%|█████████▎| 9791/10570 [00:26<00:02, 387.63it/s][1,12]<stderr>:#015 99%|█████████▉| 10446/10570 [00:26<00:00, 386.50it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▍| 9982/10570 [00:26<00:01, 376.69it/s]#033[A[1,7]<stderr>:#015100%|██████████| 10570/10570 [00:26<00:00, 392.51it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 96%|█████████▋| 10177/10570 [00:26<00:01, 375.73it/s][1,4]<stderr>:#015 97%|█████████▋| 10258/10570 [00:26<00:00, 388.52it/s][1,6]<stderr>:#015 92%|█████████▏| 9688/10570 [00:26<00:02, 383.20it/s][1,14]<stderr>:#015 96%|█████████▋| 10198/10570 [00:26<00:00, 380.98it/s][1,1]<stderr>:#015 97%|█████████▋| 10286/10570 [00:26<00:00, 358.49it/s][1,10]<stderr>:#015 98%|█████████▊| 10312/10570 [00:26<00:00, 387.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 86%|████████▌ | 9056/10570 [00:25<00:04, 351.04it/s]#033[A[1,13]<stderr>:#015 89%|████████▉ | 9460/10570 [00:26<00:03, 367.16it/s][1,5]<stderr>:#015 96%|█████████▌| 10099/10570 [00:26<00:01, 383.44it/s][1,9]<stderr>:#015 96%|█████████▌| 10129/10570 [00:26<00:01, 391.03it/s][1,11]<stderr>:#015 93%|█████████▎| 9839/10570 [00:26<00:01, 376.50it/s][1,2]<stderr>:#015 98%|█████████▊| 10370/10570 [00:26<00:00, 390.85it/s][1,3]<stderr>:#015 93%|█████████▎| 9830/10570 [00:26<00:01, 385.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 95%|█████████▍| 10022/10570 [00:26<00:01, 383.16it/s]#033[A[1,12]<stderr>:#015 99%|█████████▉| 10485/10570 [00:26<00:00, 382.54it/s][1,4]<stderr>:#015 97%|█████████▋| 10297/10570 [00:26<00:00, 388.92it/s][1,15]<stderr>:#015 97%|█████████▋| 10218/10570 [00:26<00:00, 382.99it/s][1,6]<stderr>:#015 92%|█████████▏| 9729/10570 [00:26<00:02, 389.00it/s][1,14]<stderr>:#015 97%|█████████▋| 10238/10570 [00:26<00:00, 385.33it/s][1,1]<stderr>:#015 98%|█████████▊| 10325/10570 [00:26<00:00, 366.44it/s][1,10]<stderr>:#015 98%|█████████▊| 10352/10570 [00:26<00:00, 389.85it/s][1,13]<stderr>:#015 90%|████████▉ | 9497/10570 [00:26<00:02, 366.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 96%|█████████▌| 10139/10570 [00:26<00:01, 387.69it/s][1,8]<stderr>:#015 86%|████████▌ | 9092/10570 [00:25<00:04, 351.64it/s]#033[A[1,9]<stderr>:#015 96%|█████████▌| 10169/10570 [00:26<00:01, 380.37it/s][1,11]<stderr>:#015 93%|█████████▎| 9878/10570 [00:26<00:01, 378.32it/s][1,2]<stderr>:#015 98%|█████████▊| 10410/10570 [00:26<00:00, 385.29it/s][1,12]<stderr>:#015100%|█████████▉| 10526/10570 [00:27<00:00, 389.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 95%|█████████▌| 10061/10570 [00:26<00:01, 378.94it/s]#033[A[1,3]<stderr>:#015 93%|█████████▎| 9869/10570 [00:26<00:01, 379.17it/s][1,4]<stderr>:#015 98%|█████████▊| 10336/10570 [00:26<00:00, 387.57it/s][1,15]<stderr>:#015 97%|█████████▋| 10257/10570 [00:26<00:00, 383.91it/s][1,6]<stderr>:#015 92%|█████████▏| 9769/10570 [00:26<00:02, 389.42it/s][1,14]<stderr>:#015 97%|█████████▋| 10277/10570 [00:26<00:00, 383.73it/s][1,1]<stderr>:#015 98%|█████████▊| 10364/10570 [00:26<00:00, 371.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 86%|████████▋ | 9128/10570 [00:25<00:04, 352.44it/s]#033[A[1,13]<stderr>:#015 90%|█████████ | 9536/10570 [00:26<00:02, 371.02it/s][1,10]<stderr>:#015 98%|█████████▊| 10392/10570 [00:26<00:00, 384.51it/s][1,5]<stderr>:#015 96%|█████████▋| 10178/10570 [00:26<00:01, 378.31it/s][1,9]<stderr>:#015 97%|█████████▋| 10209/10570 [00:26<00:00, 384.92it/s][1,11]<stderr>:#015 94%|█████████▍| 9916/10570 [00:26<00:01, 373.85it/s][1,2]<stderr>:#015 99%|█████████▉| 10449/10570 [00:27<00:00, 383.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▌| 10100/10570 [00:26<00:01, 381.52it/s]#033[A[1,12]<stderr>:#015100%|█████████▉| 10568/10570 [00:27<00:00, 395.87it/s][1,3]<stderr>:#015 94%|█████████▎| 9907/10570 [00:26<00:01, 377.76it/s][1,12]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 389.33it/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 98%|█████████▊| 10375/10570 [00:26<00:00, 387.34it/s][1,15]<stderr>:#015 97%|█████████▋| 10296/10570 [00:26<00:00, 385.05it/s][1,6]<stderr>:#015 93%|█████████▎| 9808/10570 [00:26<00:01, 385.52it/s][1,14]<stderr>:#015 98%|█████████▊| 10316/10570 [00:26<00:00, 382.95it/s][1,1]<stderr>:#015 98%|█████████▊| 10402/10570 [00:27<00:00, 370.05it/s][1,13]<stderr>:#015 91%|█████████ | 9574/10570 [00:26<00:02, 372.68it/s][1,10]<stderr>:#015 99%|█████████▊| 10431/10570 [00:27<00:00, 384.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9164/10570 [00:25<00:04, 350.39it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10216/10570 [00:26<00:00, 369.49it/s][1,9]<stderr>:#015 97%|█████████▋| 10249/10570 [00:26<00:00, 386.45it/s][1,11]<stderr>:#015 94%|█████████▍| 9956/10570 [00:26<00:01, 379.81it/s][1,2]<stderr>:#015 99%|█████████▉| 10488/10570 [00:27<00:00, 380.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▌| 10140/10570 [00:26<00:01, 386.32it/s]#033[A[1,3]<stderr>:#015 94%|█████████▍| 9947/10570 [00:26<00:01, 382.94it/s][1,15]<stderr>:#015 98%|█████████▊| 10335/10570 [00:27<00:00, 383.90it/s][1,4]<stderr>:#015 99%|█████████▊| 10414/10570 [00:27<00:00, 383.99it/s][1,14]<stderr>:#015 98%|█████████▊| 10355/10570 [00:26<00:00, 384.74it/s][1,6]<stderr>:#015 93%|█████████▎| 9847/10570 [00:26<00:01, 373.93it/s][1,1]<stderr>:#015 99%|█████████▉| 10441/10570 [00:27<00:00, 374.08it/s][1,13]<stderr>:#015 91%|█████████ | 9612/10570 [00:26<00:02, 372.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9201/10570 [00:26<00:03, 355.50it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10254/10570 [00:26<00:00, 367.78it/s][1,9]<stderr>:#015 97%|█████████▋| 10288/10570 [00:27<00:00, 385.25it/s][1,10]<stderr>:#015 99%|█████████▉| 10470/10570 [00:27<00:00, 365.79it/s][1,11]<stderr>:#015 95%|█████████▍| 9995/10570 [00:26<00:01, 370.05it/s][1,2]<stderr>:#015100%|█████████▉| 10529/10570 [00:27<00:00, 387.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▋| 10179/10570 [00:26<00:01, 375.88it/s]#033[A[1,3]<stderr>:#015 94%|█████████▍| 9986/10570 [00:26<00:01, 374.93it/s][1,15]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 385.34it/s][1,4]<stderr>:#015 99%|█████████▉| 10453/10570 [00:27<00:00, 381.55it/s][1,14]<stderr>:#015 98%|█████████▊| 10394/10570 [00:26<00:00, 380.07it/s][1,6]<stderr>:#015 94%|█████████▎| 9887/10570 [00:26<00:01, 379.74it/s][1,1]<stderr>:#015 99%|█████████▉| 10479/10570 [00:27<00:00, 371.69it/s][1,13]<stderr>:#015 91%|█████████▏| 9650/10570 [00:26<00:02, 372.40it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9238/10570 [00:26<00:03, 358.49it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10293/10570 [00:26<00:00, 372.88it/s][1,10]<stderr>:#015 99%|█████████▉| 10510/10570 [00:27<00:00, 375.14it/s][1,9]<stderr>:#015 98%|█████████▊| 10328/10570 [00:27<00:00, 386.56it/s][1,11]<stderr>:#015 95%|█████████▍| 10036/10570 [00:26<00:01, 378.83it/s][1,2]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 386.09it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10219/10570 [00:26<00:00, 381.68it/s]#033[A[1,3]<stderr>:#015 95%|█████████▍| 10026/10570 [00:26<00:01, 380.58it/s][1,15]<stderr>:#015 99%|█████████▊| 10414/10570 [00:27<00:00, 383.19it/s][1,4]<stderr>:#015 99%|█████████▉| 10492/10570 [00:27<00:00, 377.38it/s][1,14]<stderr>:#015 99%|█████████▊| 10433/10570 [00:27<00:00, 382.18it/s][1,6]<stderr>:#015 94%|█████████▍| 9926/10570 [00:26<00:01, 379.67it/s][1,1]<stderr>:#015100%|█████████▉| 10520/10570 [00:27<00:00, 380.51it/s][1,13]<stderr>:#015 92%|█████████▏| 9689/10570 [00:26<00:02, 375.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9275/10570 [00:26<00:03, 360.50it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10332/10570 [00:27<00:00, 375.44it/s][1,9]<stderr>:#015 98%|█████████▊| 10367/10570 [00:27<00:00, 386.20it/s][1,10]<stderr>:#015100%|█████████▉| 10551/10570 [00:27<00:00, 382.76it/s][1,11]<stderr>:#015 95%|█████████▌| 10074/10570 [00:26<00:01, 377.26it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10258/10570 [00:27<00:00, 382.01it/s]#033[A[1,3]<stderr>:#015 95%|█████████▌| 10065/10570 [00:27<00:01, 381.06it/s][1,10]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 385.40it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 99%|█████████▉| 10453/10570 [00:27<00:00, 377.94it/s][1,4]<stderr>:#015100%|█████████▉| 10533/10570 [00:27<00:00, 384.42it/s][1,14]<stderr>:#015 99%|█████████▉| 10472/10570 [00:27<00:00, 374.58it/s][1,6]<stderr>:#015 94%|█████████▍| 9965/10570 [00:26<00:01, 378.97it/s][1,1]<stderr>:#015100%|█████████▉| 10561/10570 [00:27<00:00, 387.55it/s][1,13]<stderr>:#015 92%|█████████▏| 9728/10570 [00:26<00:02, 379.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9312/10570 [00:26<00:03, 357.15it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10371/10570 [00:27<00:00, 378.61it/s][1,1]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 384.73it/s]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015 98%|█████████▊| 10406/10570 [00:27<00:00, 380.23it/s][1,11]<stderr>:#015 96%|█████████▌| 10112/10570 [00:27<00:01, 373.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10297/10570 [00:27<00:00, 383.31it/s]#033[A[1,3]<stderr>:#015 96%|█████████▌| 10104/10570 [00:27<00:01, 382.64it/s][1,4]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 384.64it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 99%|█████████▉| 10491/10570 [00:27<00:00, 373.90it/s][1,14]<stderr>:#015 99%|█████████▉| 10513/10570 [00:27<00:00, 382.70it/s][1,13]<stderr>:#015 92%|█████████▏| 9766/10570 [00:27<00:02, 379.03it/s][1,6]<stderr>:#015 95%|█████████▍| 10003/10570 [00:27<00:01, 369.44it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9348/10570 [00:26<00:03, 350.84it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10409/10570 [00:27<00:00, 376.11it/s][1,9]<stderr>:#015 99%|█████████▉| 10445/10570 [00:27<00:00, 379.44it/s][1,11]<stderr>:#015 96%|█████████▌| 10150/10570 [00:27<00:01, 364.47it/s][1,3]<stderr>:#015 96%|█████████▌| 10144/10570 [00:27<00:01, 385.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 98%|█████████▊| 10336/10570 [00:27<00:00, 378.01it/s]#033[A[1,15]<stderr>:#015100%|█████████▉| 10532/10570 [00:27<00:00, 382.12it/s][1,14]<stderr>:#015100%|█████████▉| 10554/10570 [00:27<00:00, 389.69it/s][1,13]<stderr>:#015 93%|█████████▎| 9804/10570 [00:27<00:02, 373.99it/s][1,6]<stderr>:#015 95%|█████████▌| 10043/10570 [00:27<00:01, 376.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9384/10570 [00:26<00:03, 352.95it/s]#033[A[1,5]<stderr>:#015 99%|█████████▉| 10447/10570 [00:27<00:00, 375.52it/s][1,9]<stderr>:#015 99%|█████████▉| 10483/10570 [00:27<00:00, 375.36it/s][1,14]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 385.88it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 96%|█████████▋| 10187/10570 [00:27<00:01, 360.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 379.56it/s]#033[A[1,3]<stderr>:#015 96%|█████████▋| 10183/10570 [00:27<00:01, 373.70it/s][1,15]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 382.12it/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015 95%|█████████▌| 10081/10570 [00:27<00:01, 376.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9420/10570 [00:26<00:03, 353.58it/s]#033[A[1,13]<stderr>:#015 93%|█████████▎| 9842/10570 [00:27<00:01, 364.49it/s][1,5]<stderr>:#015 99%|█████████▉| 10485/10570 [00:27<00:00, 373.12it/s][1,9]<stderr>:#015100%|█████████▉| 10524/10570 [00:27<00:00, 382.70it/s][1,11]<stderr>:#015 97%|█████████▋| 10227/10570 [00:27<00:00, 368.94it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▊| 10413/10570 [00:27<00:00, 374.81it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10222/10570 [00:27<00:00, 377.96it/s][1,6]<stderr>:#015 96%|█████████▌| 10120/10570 [00:27<00:01, 379.94it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9458/10570 [00:26<00:03, 360.79it/s]#033[A[1,13]<stderr>:#015 93%|█████████▎| 9880/10570 [00:27<00:01, 368.46it/s][1,5]<stderr>:#015100%|█████████▉| 10525/10570 [00:27<00:00, 379.40it/s][1,9]<stderr>:#015100%|█████████▉| 10565/10570 [00:27<00:00, 389.89it/s][1,9]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 380.61it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 97%|█████████▋| 10265/10570 [00:27<00:00, 368.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▉| 10451/10570 [00:27<00:00, 373.68it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10260/10570 [00:27<00:00, 375.52it/s][1,6]<stderr>:#015 96%|█████████▌| 10159/10570 [00:27<00:01, 382.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 90%|████████▉ | 9495/10570 [00:26<00:02, 361.73it/s]#033[A[1,13]<stderr>:#015 94%|█████████▍| 9917/10570 [00:27<00:01, 366.36it/s][1,5]<stderr>:#015100%|█████████▉| 10567/10570 [00:27<00:00, 388.23it/s][1,5]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 382.20it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 97%|█████████▋| 10304/10570 [00:27<00:00, 372.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▉| 10489/10570 [00:27<00:00, 371.51it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10299/10570 [00:27<00:00, 377.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 90%|█████████ | 9533/10570 [00:27<00:02, 365.73it/s]#033[A[1,6]<stderr>:#015 96%|█████████▋| 10198/10570 [00:27<00:01, 371.58it/s][1,13]<stderr>:#015 94%|█████████▍| 9956/10570 [00:27<00:01, 371.47it/s][1,11]<stderr>:#015 98%|█████████▊| 10343/10570 [00:27<00:00, 376.51it/s][1,3]<stderr>:#015 98%|█████████▊| 10337/10570 [00:27<00:00, 377.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|█████████▉| 10530/10570 [00:27<00:00, 379.41it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9570/10570 [00:27<00:02, 366.80it/s]#033[A[1,6]<stderr>:#015 97%|█████████▋| 10237/10570 [00:27<00:00, 374.88it/s][1,13]<stderr>:#015 95%|█████████▍| 9994/10570 [00:27<00:01, 360.68it/s][1,11]<stderr>:#015 98%|█████████▊| 10381/10570 [00:27<00:00, 373.71it/s][1,0]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 379.53it/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 377.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9607/10570 [00:27<00:02, 364.20it/s]#033[A[1,6]<stderr>:#015 97%|█████████▋| 10275/10570 [00:27<00:00, 373.22it/s][1,13]<stderr>:#015 95%|█████████▍| 10033/10570 [00:27<00:01, 368.91it/s][1,11]<stderr>:#015 99%|█████████▊| 10419/10570 [00:27<00:00, 371.57it/s][1,3]<stderr>:#015 99%|█████████▊| 10413/10570 [00:27<00:00, 373.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9645/10570 [00:27<00:02, 366.24it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10313/10570 [00:27<00:00, 374.75it/s][1,13]<stderr>:#015 95%|█████████▌| 10071/10570 [00:27<00:01, 368.15it/s][1,3]<stderr>:#015 99%|█████████▉| 10451/10570 [00:28<00:00, 371.71it/s][1,11]<stderr>:#015 99%|█████████▉| 10457/10570 [00:28<00:00, 360.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9682/10570 [00:27<00:02, 366.11it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10352/10570 [00:27<00:00, 375.72it/s][1,13]<stderr>:#015 96%|█████████▌| 10109/10570 [00:27<00:01, 369.02it/s][1,3]<stderr>:#015 99%|█████████▉| 10489/10570 [00:28<00:00, 367.78it/s][1,11]<stderr>:#015 99%|█████████▉| 10494/10570 [00:28<00:00, 358.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9721/10570 [00:27<00:02, 372.27it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10390/10570 [00:28<00:00, 371.39it/s][1,13]<stderr>:#015 96%|█████████▌| 10147/10570 [00:28<00:01, 370.76it/s][1,3]<stderr>:#015100%|█████████▉| 10529/10570 [00:28<00:00, 374.64it/s][1,11]<stderr>:#015100%|█████████▉| 10534/10570 [00:28<00:00, 367.98it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9759/10570 [00:27<00:02, 373.46it/s]#033[A[1,6]<stderr>:#015 99%|█████████▊| 10428/10570 [00:28<00:00, 367.94it/s][1,13]<stderr>:#015 96%|█████████▋| 10185/10570 [00:28<00:01, 361.85it/s][1,11]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 373.07it/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015100%|█████████▉| 10568/10570 [00:28<00:00, 379.04it/s][1,3]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 372.82it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9797/10570 [00:27<00:02, 366.63it/s]#033[A[1,6]<stderr>:#015 99%|█████████▉| 10465/10570 [00:28<00:00, 363.11it/s][1,13]<stderr>:#015 97%|█████████▋| 10224/10570 [00:28<00:00, 368.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9834/10570 [00:27<00:02, 358.27it/s]#033[A[1,6]<stderr>:#015 99%|█████████▉| 10503/10570 [00:28<00:00, 367.40it/s][1,13]<stderr>:#015 97%|█████████▋| 10261/10570 [00:28<00:00, 365.66it/s][1,6]<stderr>:#015100%|█████████▉| 10543/10570 [00:28<00:00, 374.88it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9870/10570 [00:27<00:01, 355.74it/s]#033[A[1,13]<stderr>:#015 97%|█████████▋| 10299/10570 [00:28<00:00, 368.74it/s][1,6]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 370.33it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▎| 9906/10570 [00:28<00:01, 354.99it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10336/10570 [00:28<00:00, 368.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▍| 9945/10570 [00:28<00:01, 362.73it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10373/10570 [00:28<00:00, 368.91it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▍| 9982/10570 [00:28<00:01, 353.77it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10410/10570 [00:28<00:00, 356.75it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 95%|█████████▍| 10020/10570 [00:28<00:01, 358.73it/s]#033[A[1,13]<stderr>:#015 99%|█████████▉| 10446/10570 [00:28<00:00, 357.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 95%|█████████▌| 10058/10570 [00:28<00:01, 363.35it/s]#033[A[1,13]<stderr>:#015 99%|█████████▉| 10482/10570 [00:28<00:00, 355.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10095/10570 [00:28<00:01, 359.48it/s]#033[A[1,13]<stderr>:#015100%|█████████▉| 10521/10570 [00:29<00:00, 363.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10133/10570 [00:28<00:01, 364.23it/s]#033[A[1,13]<stderr>:#015100%|█████████▉| 10560/10570 [00:29<00:00, 368.81it/s][1,13]<stderr>:#015100%|██████████| 10570/10570 [00:29<00:00, 361.93it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10170/10570 [00:28<00:01, 351.04it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10207/10570 [00:28<00:01, 355.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10245/10570 [00:28<00:00, 360.32it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10282/10570 [00:29<00:00, 357.03it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10319/10570 [00:29<00:00, 358.80it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10356/10570 [00:29<00:00, 359.37it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10392/10570 [00:29<00:00, 354.28it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▊| 10428/10570 [00:29<00:00, 351.92it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 10464/10570 [00:29<00:00, 346.91it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 10501/10570 [00:29<00:00, 350.89it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|█████████▉| 10539/10570 [00:29<00:00, 357.21it/s]#033[A[1,8]<stderr>:#015100%|██████████| 10570/10570 [00:29<00:00, 353.51it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 169/169 [00:59<00:00,  2.86it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:03:12,263 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   epoch        =   0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   eval_samples =  10784\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   exact_match  = 0.3217\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   f1           = 4.4442\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 169/169 [01:01<00:00,  2.74it/s]\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-05 08:03:54 Uploading - Uploading generated training model\u001b[35m2021-09-05 08:03:49,459 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-09-05 08:03:49,459 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-09-05 08:06:54 Completed - Training job completed\n",
      "ProfilerReport-1630828214: IssuesFound\n",
      "Training seconds: 1514\n",
      "Billable seconds: 1514\n"
     ]
    }
   ],
   "source": [
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9237662553787231, 'start': 68, 'end': 77, 'answer': 'sagemaker'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "\"inputs\": {\n",
    "\t\"question\": \"What is used for inference?\",\n",
    "\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2021-09-05-07-50-14-818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-05 08:06:54 Starting - Preparing the instances for training\n",
      "2021-09-05 08:06:54 Downloading - Downloading input data\n",
      "2021-09-05 08:06:54 Training - Training image download completed. Training in progress.\n",
      "2021-09-05 08:06:54 Uploading - Uploading generated training model\n",
      "2021-09-05 08:06:54 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-09-05 07:56:56,395 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-09-05 07:56:56,474 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:01,644 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:01,724 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:04,747 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:04,747 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:05,111 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.4.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.4.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,805 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,805 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,806 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:07,806 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:08,808 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:08,808 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,113 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:09,810 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:09,811 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:09,427 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:10,812 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:10,812 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.4.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.4.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:11,814 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:11,814 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.105.117\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,070 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,070 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,073 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,074 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:12,074 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,822 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,890 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:12,894 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,082 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,153 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,154 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,155 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-09-05 07:57:13,237 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"max_steps\": 100,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"squad\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"do_eval\": true,\n",
      "        \"doc_stride\": 128,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"max_seq_length\": 384,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"model_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-09-05-07-50-14-818\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-09-05-07-50-14-818\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-029438132834/huggingface-pytorch-training-2021-09-05-07-50-14-818/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"squad\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--max_steps\",\"100\",\"--model_name_or_path\",\"bert-large-uncased-whole-word-masking\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=squad\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-large-uncased-whole-word-masking\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.6 -m mpi4py run_qa.py --dataset_name squad --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path bert-large-uncased-whole-word-masking --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[35m2021-09-05 07:57:14,901 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=55, name='orted', status='sleeping', started='07:57:13')]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.105.117<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.39<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO comm 0x555d1552df40 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO comm 0x564f04a8b110 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO comm 0x560286410140 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO comm 0x564825e6e300 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO comm 0x557385d4d010 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO comm 0x5637e61c3ed0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO comm 0x55db42bbb240 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO comm 0x55672dd96570 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO comm 0x5641c1b43780 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO comm 0x5594c388b580 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO comm 0x56496b013720 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO comm 0x55e52d082260 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO comm 0x560c5b985d70 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO comm 0x55f04cbd0cd0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO comm 0x55f2cb99e360 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO comm 0x561d02b2e540 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:65:65 [5] NCCL INFO comm 0x5637e8e94900 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:66:66 [6] NCCL INFO comm 0x55db4588bd70 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:72:72 [5] NCCL INFO comm 0x55f04f8a1800 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:64:64 [1] NCCL INFO comm 0x5641c4814500 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:73:73 [6] NCCL INFO comm 0x55f2ce66ee90 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:70:70 [4] NCCL INFO comm 0x560c5e656af0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:74:74 [7] NCCL INFO comm 0x561d057ff2c0 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:57:57 [1] NCCL INFO comm 0x555d181fecc0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:59:59 [2] NCCL INFO comm 0x5602890e0ec0 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:61 [3] NCCL INFO comm 0x557388a1dd90 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:66:66 [2] NCCL INFO comm 0x56496dce4250 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:67:67 [7] NCCL INFO comm 0x556730a66fa0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:63:63 [4] NCCL INFO comm 0x564828b3f080 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:62 [0] NCCL INFO comm 0x5594c655c300 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:55 [0] NCCL INFO comm 0x564f0775be90 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:68:68 [3] NCCL INFO comm 0x55e52fd52fe0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep05_07-57-19_algo-2, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep05_07-57-18_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:09/05/2021 07:57:26 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,192 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,193 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:09/05/2021 07:57:34 - WARNING - datasets.builder -   Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,217 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,217 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,239 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmputh1vp4b\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,264 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,264 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,274 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,274 >> creating metadata file for /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,288 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,307 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvce9yq53\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,356 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,356 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,530 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:19,509 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:19,509 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:23,793 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:23,793 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:398] 2021-09-05 07:59:21,316 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:415] 2021-09-05 07:59:21,316 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:784 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:398] 2021-09-05 07:59:27,951 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:415] 2021-09-05 07:59:27,951 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,580 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,580 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,581 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,581 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,581 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,582 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,617 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,617 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,618 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,618 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,618 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,618 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.813 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.813 algo-1:59 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.813 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.813 algo-1:61 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.813 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.813 algo-1:63 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.831 algo-1:65 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.836 algo-1:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:28.870 algo-2:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:28.870 algo-2:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:28.870 algo-2:72 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:28.873 algo-2:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:28.877 algo-2:74 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:28.877 algo-2:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:28.886 algo-2:70 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:28.887 algo-2:73 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.939 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.939 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.939 algo-1:59 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.939 algo-1:57 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.939 algo-1:63 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.939 algo-1:61 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.939 algo-1:65 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.939 algo-1:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.939 algo-1:55 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.939 algo-1:61 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.940 algo-1:59 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.940 algo-1:63 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.940 algo-1:57 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.940 algo-1:66 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.940 algo-1:55 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.940 algo-1:61 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.940 algo-1:65 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.940 algo-1:59 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.940 algo-1:63 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.940 algo-1:57 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.940 algo-1:65 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.941 algo-1:55 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.941 algo-1:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.941 algo-1:61 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.941 algo-1:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:28.941 algo-1:55 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:28.941 algo-1:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:28.941 algo-1:61 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.941 algo-1:59 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:28.941 algo-1:59 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.942 algo-1:67 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.942 algo-1:63 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.942 algo-1:65 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:28.942 algo-1:63 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.942 algo-1:57 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:28.942 algo-1:65 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:28.942 algo-1:57 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.942 algo-1:67 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.943 algo-1:67 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:28.943 algo-1:67 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.012 algo-2:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.012 algo-2:72 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.012 algo-2:73 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.012 algo-2:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.012 algo-2:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.012 algo-2:74 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.012 algo-2:72 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.012 algo-2:62 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.013 algo-2:64 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.013 algo-2:68 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.013 algo-2:74 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.013 algo-2:73 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.013 algo-2:74 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.013 algo-2:62 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.013 algo-2:64 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.013 algo-2:68 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.013 algo-2:72 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.013 algo-2:73 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.014 algo-2:62 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.014 algo-2:72 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.014 algo-2:62 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.014 algo-2:74 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.014 algo-2:72 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.014 algo-2:64 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.014 algo-2:74 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.014 algo-2:68 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.014 algo-2:64 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.015 algo-2:68 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.014 algo-2:73 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.015 algo-2:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.015 algo-2:73 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.015 algo-2:70 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.015 algo-2:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.015 algo-2:70 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.016 algo-2:66 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.016 algo-2:70 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.017 algo-2:70 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.018 algo-2:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.018 algo-2:70 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.018 algo-2:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.135 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.135 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.136 algo-1:59 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.136 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.136 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.137 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.137 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.137 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.138 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.138 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.138 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.139 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.139 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.139 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.140 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.140 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.140 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.141 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.141 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.141 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.141 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.142 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.142 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.142 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.142 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.143 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.143 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.143 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.143 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.144 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.144 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.144 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.144 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.145 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.145 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.145 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.145 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.146 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.146 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.146 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.146 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.147 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.147 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.147 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.147 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.148 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.148 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.148 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.148 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.149 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.149 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.149 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.149 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.150 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.150 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.150 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.150 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.151 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.151 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.151 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.151 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.151 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.152 algo-1:65 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.152 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.152 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.152 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.152 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.153 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.153 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.153 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.153 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.153 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.154 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.154 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.154 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.154 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.154 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.155 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.155 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.155 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.155 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.155 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.156 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.156 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.156 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.156 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.156 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.157 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.157 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.157 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.157 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.157 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.158 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.158 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.158 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.158 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.158 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.159 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.159 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.159 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.159 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.159 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.160 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.160 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.160 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.160 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.160 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.161 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.161 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.161 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.161 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.161 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.162 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.162 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.162 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.162 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.162 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.162 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.163 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.163 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.163 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.163 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.163 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.163 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.164 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-09-05 07:59:29.164 algo-1:61 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.164 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.164 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.164 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.164 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.165 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.165 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.165 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.165 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.165 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.166 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.166 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.166 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.166 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.166 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.166 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.167 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.167 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.167 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.167 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.167 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.168 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.168 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.168 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.168 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.168 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.169 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.169 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.169 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.169 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.169 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.170 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.170 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.170 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.170 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.170 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.171 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.171 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.171 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.171 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.171 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.172 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.172 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.172 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.172 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.172 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.173 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.173 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.173 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.173 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.173 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.174 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.174 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.174 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.174 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.174 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.175 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.175 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.175 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.175 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.175 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.176 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.176 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.176 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.176 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.176 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.177 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.177 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.177 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.177 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.177 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.178 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.178 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.178 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.178 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.178 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.179 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.179 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.179 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.179 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.179 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.180 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.180 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.180 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.180 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.180 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.180 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.181 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.181 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.181 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.181 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.181 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.182 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.182 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.182 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.182 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.182 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.183 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.183 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.183 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.183 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.183 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.183 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.184 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.184 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.184 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.184 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.184 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.185 algo-1:63 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.185 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.185 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.185 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-09-05 07:59:29.186 algo-1:63 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.185 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.186 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.186 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.186 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.186 algo-1:59 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-09-05 07:59:29.187 algo-1:59 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.187 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.187 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.187 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.188 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.188 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.188 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.189 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.189 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.189 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.190 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.190 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.190 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.190 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.191 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.191 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.191 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.192 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.192 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.192 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.193 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.193 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.193 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.194 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.194 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.194 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.195 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.195 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.195 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.195 algo-1:55 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-09-05 07:59:29.196 algo-1:55 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.196 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.196 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.197 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.197 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.198 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.198 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.199 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.199 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.200 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.200 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.201 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.201 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.202 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.202 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.203 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.203 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.204 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.204 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.205 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.205 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.206 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.206 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.207 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.207 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.208 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.208 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.209 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.209 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.209 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.210 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-09-05 07:59:29.210 algo-1:57 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.211 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.212 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.212 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.213 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.213 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.214 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.215 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.215 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.216 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.216 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.217 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.217 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.217 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.218 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.218 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.219 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.219 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.220 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.220 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.221 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.221 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.221 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.222 algo-1:67 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.222 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.222 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.223 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.223 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.223 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.224 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.224 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.224 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.225 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.225 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.225 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.226 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.226 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.226 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.227 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.227 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.227 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.228 algo-2:74 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.228 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.228 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.228 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.229 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.229 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.229 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.229 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.229 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.230 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.230 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.230 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.230 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.230 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.231 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.231 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.231 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.231 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.231 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.232 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.232 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.232 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.232 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.232 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.233 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.233 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.233 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.233 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.233 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.234 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.234 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.234 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.234 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.234 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.235 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.235 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.235 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.235 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.235 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.236 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.236 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.236 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.236 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.236 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.237 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.237 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.237 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-09-05 07:59:29.237 algo-1:65 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.237 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.237 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.238 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.238 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.238 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.238 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.238 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.239 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.239 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.239 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.239 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.239 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.239 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.240 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.240 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.240 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.240 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.240 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.240 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.241 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.241 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.241 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.241 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.241 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.241 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.242 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.242 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.242 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.242 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.242 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.242 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.243 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.243 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.243 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.243 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.243 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.243 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.244 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.244 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.244 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.244 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.244 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.244 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.245 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.245 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.245 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.245 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.245 algo-2:73 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.245 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.245 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.246 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.246 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.246 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.246 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.246 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.246 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.246 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.247 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.247 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.247 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.247 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.247 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.247 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.247 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.248 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.248 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.249 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.248 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.248 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.248 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.249 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.248 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.248 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.250 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.250 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.249 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.249 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.249 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.249 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.249 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.251 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.251 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.250 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.250 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.250 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.250 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.250 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.252 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.252 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.253 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.251 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.253 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.251 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.251 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.251 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.251 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.254 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.254 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.252 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.252 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.252 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.252 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.252 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.255 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.255 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.253 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.253 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.253 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.253 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.253 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.256 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.256 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.254 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.254 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.254 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.254 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.254 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.257 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.257 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.255 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.255 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.258 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.255 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.255 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.255 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.258 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.259 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.259 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.256 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.256 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.256 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.256 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.256 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.260 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.260 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.257 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.257 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.257 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.257 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.257 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.261 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.261 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.258 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.258 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.258 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.258 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.258 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.262 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.262 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.259 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.259 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.259 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.259 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.259 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.263 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.263 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.260 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.260 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.260 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.260 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.260 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.264 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.264 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.261 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.261 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.261 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.261 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.261 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.265 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.265 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.262 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.262 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.262 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.262 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.262 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.266 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.266 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.263 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.263 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.263 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.263 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.263 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.267 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.267 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.264 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.264 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.264 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.264 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.264 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.268 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.268 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.265 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.265 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.265 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.265 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.265 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.269 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.269 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.266 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.266 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.266 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.266 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.266 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.270 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.270 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.267 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.267 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.267 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.267 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.267 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.271 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.271 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.268 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.268 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.268 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.268 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.268 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.272 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.272 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.269 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.269 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.269 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.269 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.269 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.273 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.273 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.270 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.270 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.270 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.270 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.270 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.274 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.274 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.271 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.271 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.271 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.271 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.271 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.275 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.275 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.276 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.272 algo-2:62 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.272 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.272 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.272 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.272 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-09-05 07:59:29.273 algo-2:62 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.276 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.273 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.273 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.273 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.273 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.277 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.277 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-09-05 07:59:29.274 algo-2:74 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.274 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.274 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.274 algo-2:72 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.278 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-09-05 07:59:29.275 algo-2:72 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.278 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.275 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.275 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.279 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.276 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.276 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.279 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.277 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.277 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.280 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.280 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.278 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.278 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.279 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.279 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.281 algo-1:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.281 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.280 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.280 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.281 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.281 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-09-05 07:59:29.282 algo-1:66 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.282 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.282 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.282 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.283 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.283 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.283 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.284 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.284 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.284 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.285 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.285 algo-2:73 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.286 algo-2:73 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-09-05 07:59:29.286 algo-2:73 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.285 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.286 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.286 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.287 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.287 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.288 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.288 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.289 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.289 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.290 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.290 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.291 algo-2:64 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.291 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.291 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.291 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.292 algo-2:68 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.292 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-09-05 07:59:29.292 algo-2:64 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.292 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.293 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.293 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.293 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.294 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.294 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.294 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.295 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.295 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.295 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.296 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.296 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.296 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.297 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.297 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.297 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.297 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.298 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.298 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.298 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.298 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.299 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.299 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.299 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.299 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.300 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.300 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.300 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.300 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.301 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.301 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.301 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.301 algo-1:67 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.302 algo-1:67 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-09-05 07:59:29.302 algo-1:67 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.302 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.302 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.302 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.303 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.303 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.303 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.304 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.304 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.304 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.305 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.305 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.305 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.306 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.306 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.306 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.307 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.307 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.307 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.308 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.308 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.308 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.309 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.309 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.309 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.310 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.310 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.310 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.311 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.311 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.311 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.312 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.312 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.312 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.313 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.313 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.313 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.314 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.314 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.314 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.315 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.315 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.315 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.316 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.316 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.316 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.317 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.317 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.317 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.318 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.318 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.318 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.319 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.319 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.319 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.320 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.320 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.320 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.321 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.321 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.321 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.322 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.322 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.322 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.323 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.323 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.323 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.324 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.324 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.324 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.325 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.325 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.325 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.326 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.326 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.326 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.327 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.327 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.327 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.328 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.328 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.328 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.329 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.329 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.329 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.330 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.330 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.330 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.331 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.331 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.331 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.332 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.332 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.332 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.333 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.333 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.333 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.334 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.334 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.334 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.335 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.335 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.335 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.336 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.336 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.336 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.337 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.337 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.337 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.338 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.338 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.338 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.339 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.339 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.339 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.340 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.340 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.340 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.341 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.341 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.341 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.342 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.342 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.342 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.343 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.343 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.343 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.344 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.344 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.344 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.345 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.345 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.345 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.346 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.346 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.346 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.347 algo-2:70 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.347 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.347 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-09-05 07:59:29.348 algo-2:70 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.348 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.348 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.349 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.349 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.350 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.350 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.351 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.351 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.352 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.352 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.353 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.353 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.354 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.354 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.355 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.355 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.356 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.356 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.357 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.357 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.358 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.358 algo-2:68 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.359 algo-2:68 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-09-05 07:59:29.359 algo-2:68 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.359 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.360 algo-2:66 INFO hook.py:591] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:591] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:593] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-09-05 07:59:29.361 algo-2:66 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:62:772 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:55:776 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,565 >> \u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,693 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'train_runtime': 161.1116, 'train_samples_per_second': 0.621, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{'train_runtime': 160.9471, 'train_samples_per_second': 0.621, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:09/05/2021 08:02:10 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:516] 2021-09-05 08:02:10,635 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2115] 2021-09-05 08:02:10,639 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2117] 2021-09-05 08:02:10,639 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[INFO|trainer.py:2120] 2021-09-05 08:02:10,639 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:1885] 2021-09-05 08:02:10,793 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|configuration_utils.py:351] 2021-09-05 08:02:10,794 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|modeling_utils.py:889] 2021-09-05 08:02:13,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1924] 2021-09-05 08:02:13,044 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|tokenization_utils_base.py:1930] 2021-09-05 08:02:13,044 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:02:13,088 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   epoch                      =       0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_alloc_delta   =     -122MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_peaked_delta  =      121MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_alloc_delta   =     1275MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_alloc_delta  =     1424MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_alloc_delta  =     5113MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_peaked_delta =     4884MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_runtime              = 0:02:41.11\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples              =      88524\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples_per_second   =      0.621\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:02:13 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:516] 2021-09-05 08:02:13,091 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2115] 2021-09-05 08:02:13,094 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2117] 2021-09-05 08:02:13,094 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer.py:2120] 2021-09-05 08:02:13,094 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:02:37 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10784 features.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:03:05 - INFO - utils_qa -   Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:09/05/2021 08:03:05 - INFO - utils_qa -   Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:03:12,263 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   epoch        =   0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   eval_samples =  10784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   exact_match  = 0.3217\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   f1           = 4.4442\u001b[0m\n",
      "\u001b[35m2021-09-05 08:03:19,443 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.74.39' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 5.03kB [00:00, 4.20MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,5]<stderr>:#015Downloading: 5.03kB [00:00, 2.88MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,6]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 5.03kB [00:00, 4.28MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading: 5.03kB [00:00, 4.05MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 5.03kB [00:00, 4.58MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,12]<stderr>:#015Downloading: 5.03kB [00:00, 3.15MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,1]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,1]<stderr>:#015Downloading: 2.19kB [00:00, 2.21MB/s]                 [1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading: 2.19kB [00:00, 1.75MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s][1,10]<stderr>:#015Downloading: 5.03kB [00:00, 4.47MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,3]<stderr>:#015Downloading: 2.19kB [00:00, 2.12MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,9]<stderr>:#015Downloading: 2.19kB [00:00, 1.16MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,6]<stderr>:#015Downloading: 2.19kB [00:00, 2.21MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 2.19kB [00:00, 2.13MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,2]<stderr>:#015Downloading: 2.19kB [00:00, 1.74MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 2.19kB [00:00, 2.14MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,4]<stderr>:#015Downloading: 2.19kB [00:00, 2.06MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,0]<stderr>:#015Downloading: 2.19kB [00:00, 1.95MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,11]<stderr>:#015Downloading: 2.19kB [00:00, 2.24MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015Downloading:   0%|          | 0.00/955 [00:00<?, ?B/s][1,14]<stderr>:#015Downloading: 2.19kB [00:00, 2.30MB/s]                 \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s][1,13]<stderr>:#015Downloading:  58%|█████▊    | 4.73M/8.12M [00:00<00:00, 47.3MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 6.96M/8.12M [00:00<00:00, 69.6MB/s][1,13]<stderr>:#015Downloading: 9.07MB [00:00, 46.0MB/s]                            [1,7]<stderr>:#015Downloading: 13.9MB [00:00, 69.6MB/s]                            [1,13]<stderr>:#015Downloading: 14.8MB [00:00, 49.0MB/s][1,7]<stderr>:#015Downloading: 20.8MB [00:00, 69.4MB/s][1,13]<stderr>:#015Downloading: 22.0MB [00:00, 54.2MB/s][1,7]<stderr>:#015Downloading: 27.8MB [00:00, 69.6MB/s][1,7]<stderr>:#015Downloading: 30.3MB [00:00, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading: 29.4MB [00:00, 58.8MB/s][1,13]<stderr>:#015Downloading: 30.3MB [00:00, 58.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 4.85MB [00:00, 51.6MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading: 4.85MB [00:00, 78.8MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#0150 examples [00:00, ? examples/s][1,7]<stderr>:#0150 examples [00:00, ? examples/s][1,13]<stderr>:#0151 examples [00:00,  1.98 examples/s][1,7]<stderr>:#0151 examples [00:00,  1.96 examples/s][1,13]<stderr>:#0152064 examples [00:00,  2.83 examples/s][1,7]<stderr>:#0152046 examples [00:00,  2.80 examples/s][1,13]<stderr>:#0154158 examples [00:00,  4.04 examples/s][1,7]<stderr>:#0154113 examples [00:00,  4.00 examples/s][1,13]<stderr>:#0156258 examples [00:00,  5.78 examples/s][1,7]<stderr>:#0156214 examples [00:00,  5.72 examples/s][1,13]<stderr>:#0158378 examples [00:00,  8.25 examples/s][1,7]<stderr>:#0158313 examples [00:00,  8.17 examples/s][1,13]<stderr>:#0159975 examples [00:01, 11.78 examples/s][1,7]<stderr>:#0159898 examples [00:01, 11.66 examples/s][1,13]<stderr>:#01511341 examples [00:01, 16.83 examples/s][1,7]<stderr>:#01511249 examples [00:01, 16.65 examples/s][1,13]<stderr>:#01513467 examples [00:01, 24.03 examples/s][1,7]<stderr>:#01513376 examples [00:01, 23.78 examples/s][1,13]<stderr>:#01515614 examples [00:01, 34.31 examples/s][1,7]<stderr>:#01515480 examples [00:01, 33.96 examples/s][1,13]<stderr>:#01517760 examples [00:01, 48.99 examples/s][1,7]<stderr>:#01517608 examples [00:01, 48.48 examples/s][1,13]<stderr>:#01519917 examples [00:01, 69.91 examples/s][1,7]<stderr>:#01519734 examples [00:01, 69.19 examples/s][1,13]<stderr>:#01521866 examples [00:01, 99.71 examples/s][1,7]<stderr>:#01521662 examples [00:01, 98.68 examples/s][1,13]<stderr>:#01524028 examples [00:01, 142.16 examples/s][1,7]<stderr>:#01523803 examples [00:01, 140.70 examples/s][1,13]<stderr>:#01526183 examples [00:01, 202.51 examples/s][1,7]<stderr>:#01525933 examples [00:01, 200.43 examples/s][1,13]<stderr>:#01528343 examples [00:02, 288.15 examples/s][1,7]<stderr>:#01528062 examples [00:02, 285.18 examples/s][1,13]<stderr>:#01530418 examples [00:02, 408.73 examples/s][1,7]<stderr>:#01530111 examples [00:02, 404.55 examples/s][1,13]<stderr>:#01532590 examples [00:02, 579.22 examples/s][1,7]<stderr>:#01532269 examples [00:02, 573.32 examples/s][1,13]<stderr>:#01534770 examples [00:02, 818.14 examples/s][1,7]<stderr>:#01534425 examples [00:02, 809.80 examples/s][1,13]<stderr>:#01536926 examples [00:02, 1150.07 examples/s][1,7]<stderr>:#01536570 examples [00:02, 1138.44 examples/s][1,13]<stderr>:#01539078 examples [00:02, 1606.16 examples/s][1,7]<stderr>:#01538710 examples [00:02, 1590.07 examples/s][1,13]<stderr>:#01541194 examples [00:02, 2132.18 examples/s][1,7]<stderr>:#01540811 examples [00:02, 2106.36 examples/s][1,13]<stderr>:#01543368 examples [00:02, 2923.07 examples/s][1,7]<stderr>:#01542942 examples [00:02, 2886.78 examples/s][1,13]<stderr>:#01545533 examples [00:02, 3947.35 examples/s][1,7]<stderr>:#01545091 examples [00:02, 3899.44 examples/s][1,13]<stderr>:#01547673 examples [00:03, 5225.93 examples/s][1,7]<stderr>:#01547229 examples [00:03, 5166.63 examples/s][1,13]<stderr>:#01549825 examples [00:03, 6761.76 examples/s][1,7]<stderr>:#01549363 examples [00:03, 6686.79 examples/s][1,13]<stderr>:#01551883 examples [00:03, 8280.88 examples/s][1,7]<stderr>:#01551403 examples [00:03, 8203.52 examples/s][1,13]<stderr>:#01554049 examples [00:03, 10164.36 examples/s][1,7]<stderr>:#01553542 examples [00:03, 10064.46 examples/s][1,13]<stderr>:#01556224 examples [00:03, 12097.23 examples/s][1,7]<stderr>:#01555691 examples [00:03, 11973.66 examples/s][1,13]<stderr>:#01558349 examples [00:03, 13891.97 examples/s][1,7]<stderr>:#01557828 examples [00:03, 13792.11 examples/s][1,13]<stderr>:#01560435 examples [00:03, 14794.61 examples/s][1,7]<stderr>:#01559929 examples [00:03, 15375.34 examples/s][1,13]<stderr>:#01562600 examples [00:03, 16346.25 examples/s][1,7]<stderr>:#01562012 examples [00:03, 16046.69 examples/s][1,13]<stderr>:#01564769 examples [00:03, 17649.27 examples/s][1,7]<stderr>:#01564144 examples [00:03, 17331.26 examples/s][1,13]<stderr>:#01566943 examples [00:04, 18703.33 examples/s][1,7]<stderr>:#01566294 examples [00:04, 18400.24 examples/s][1,13]<stderr>:#01569049 examples [00:04, 14776.56 examples/s][1,7]<stderr>:#01568370 examples [00:04, 14200.61 examples/s][1,13]<stderr>:#01570821 examples [00:04, 15512.45 examples/s][1,7]<stderr>:#01570097 examples [00:04, 14830.32 examples/s][1,13]<stderr>:#01572972 examples [00:04, 16928.11 examples/s][1,7]<stderr>:#01572232 examples [00:04, 16324.58 examples/s][1,13]<stderr>:#01575142 examples [00:04, 18121.85 examples/s][1,7]<stderr>:#01574380 examples [00:04, 17590.64 examples/s][1,13]<stderr>:#01577321 examples [00:04, 19083.70 examples/s][1,7]<stderr>:#01576532 examples [00:04, 18608.40 examples/s][1,13]<stderr>:#01579490 examples [00:04, 19796.01 examples/s][1,7]<stderr>:#01578684 examples [00:04, 19394.99 examples/s][1,13]<stderr>:#01581563 examples [00:04, 19149.35 examples/s][1,7]<stderr>:#01580726 examples [00:04, 18780.61 examples/s][1,13]<stderr>:#01583742 examples [00:04, 19869.76 examples/s][1,7]<stderr>:#01582878 examples [00:04, 19525.79 examples/s][1,13]<stderr>:#01585923 examples [00:05, 20414.12 examples/s][1,7]<stderr>:#01585028 examples [00:05, 20077.61 examples/s][1,13]<stderr>:#015                                           #015[1,13]<stderr>:#0150 examples [00:00, ? examples/s][1,7]<stderr>:#01587180 examples [00:05, 20488.84 examples/s][1,7]<stderr>:#015                                           #015[1,13]<stderr>:#015910 examples [00:00, 9097.38 examples/s][1,7]<stderr>:#0150 examples [00:00, ? examples/s][1,13]<stderr>:#0152524 examples [00:00, 10467.38 examples/s][1,7]<stderr>:#015887 examples [00:00, 8865.36 examples/s][1,13]<stderr>:#0153881 examples [00:00, 11237.43 examples/s][1,7]<stderr>:#0152678 examples [00:00, 10448.23 examples/s][1,7]<stderr>:#0154385 examples [00:00, 11822.87 examples/s][1,13]<stderr>:#0154995 examples [00:00, 10201.57 examples/s][1,7]<stderr>:#0155404 examples [00:00, 10663.20 examples/s][1,13]<stderr>:#0156785 examples [00:00, 11712.43 examples/s][1,7]<stderr>:#0157168 examples [00:00, 12098.17 examples/s][1,13]<stderr>:#0158573 examples [00:00, 13063.78 examples/s][1,7]<stderr>:#0158954 examples [00:00, 13393.62 examples/s][1,13]<stderr>:#01510000 examples [00:00, 13278.44 examples/s][1,13]<stderr>:#015                                           #015[1,7]<stderr>:#01510337 examples [00:00, 13335.11 examples/s][1,7]<stderr>:#015                                           #015[1,13]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s][1,13]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 349kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,192 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,193 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,217 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,217 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,239 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmputh1vp4b\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 364kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s][1,0]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,264 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,264 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 21.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,274 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,274 >> creating metadata file for /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:517] 2021-09-05 07:57:34,288 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:553] 2021-09-05 07:57:34,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m2021-09-05 08:03:19,438 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1532] 2021-09-05 07:57:34,307 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvce9yq53\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s][1,8]<stderr>:#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 24.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1536] 2021-09-05 07:57:34,356 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|file_utils.py:1544] 2021-09-05 07:57:34,356 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 44.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 39.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 22.2kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,507 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,530 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1717] 2021-09-05 07:57:34,531 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015Downloading:   0%|          | 0.00/1.35G [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.35G [00:00<?, ?B/s][1,12]<stderr>:#015Downloading:   0%|          | 4.19M/1.35G [00:00<00:31, 41.9MB/s][1,7]<stderr>:#015Downloading:   0%|          | 4.12M/1.35G [00:00<00:32, 41.2MB/s][1,12]<stderr>:#015Downloading:   1%|          | 8.38M/1.35G [00:00<00:33, 39.8MB/s][1,7]<stderr>:#015Downloading:   1%|          | 8.40M/1.35G [00:00<00:32, 41.7MB/s][1,12]<stderr>:#015Downloading:   1%|          | 11.4M/1.35G [00:00<00:36, 36.4MB/s][1,7]<stderr>:#015Downloading:   1%|          | 11.8M/1.35G [00:00<00:34, 38.8MB/s][1,12]<stderr>:#015Downloading:   1%|          | 13.9M/1.35G [00:00<00:41, 32.0MB/s][1,7]<stderr>:#015Downloading:   1%|          | 14.3M/1.35G [00:00<00:39, 33.6MB/s][1,12]<stderr>:#015Downloading:   1%|          | 16.8M/1.35G [00:00<00:51, 25.7MB/s][1,7]<stderr>:#015Downloading:   1%|▏         | 16.8M/1.35G [00:00<00:52, 25.3MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 22.0M/1.35G [00:00<00:43, 30.4MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 21.8M/1.35G [00:00<00:44, 29.7MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 25.2M/1.35G [00:00<00:45, 29.1MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 25.2M/1.35G [00:00<00:45, 29.3MB/s][1,12]<stderr>:#015Downloading:   2%|▏         | 30.3M/1.35G [00:00<00:39, 33.5MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 30.1M/1.35G [00:00<00:39, 33.4MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 33.7M/1.35G [00:01<00:54, 24.2MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 34.0M/1.35G [00:01<00:54, 24.1MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 38.8M/1.35G [00:01<00:45, 28.8MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 39.6M/1.35G [00:01<00:44, 29.1MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 42.4M/1.35G [00:01<00:52, 24.9MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 43.4M/1.35G [00:01<00:52, 25.0MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 45.5M/1.35G [00:01<00:50, 25.6MB/s][1,12]<stderr>:#015Downloading:   3%|▎         | 46.6M/1.35G [00:01<00:53, 24.2MB/s][1,7]<stderr>:#015Downloading:   4%|▎         | 48.6M/1.35G [00:01<01:08, 18.9MB/s][1,12]<stderr>:#015Downloading:   4%|▎         | 49.5M/1.35G [00:01<01:08, 18.9MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 51.0M/1.35G [00:01<01:08, 18.8MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 51.9M/1.35G [00:01<01:08, 18.9MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 54.2M/1.35G [00:02<01:00, 21.4MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 54.8M/1.35G [00:02<01:07, 19.0MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 56.8M/1.35G [00:02<01:03, 20.3MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 57.0M/1.35G [00:02<01:15, 17.1MB/s][1,12]<stderr>:#015Downloading:   4%|▍         | 58.9M/1.35G [00:02<01:22, 15.7MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 59.1M/1.35G [00:02<01:22, 15.5MB/s][1,12]<stderr>:#015Downloading:   5%|▍         | 63.8M/1.35G [00:02<01:05, 19.7MB/s][1,7]<stderr>:#015Downloading:   5%|▍         | 63.9M/1.35G [00:02<01:05, 19.5MB/s][1,12]<stderr>:#015Downloading:   5%|▌         | 68.6M/1.35G [00:02<00:53, 23.9MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 68.7M/1.35G [00:02<00:53, 23.7MB/s][1,12]<stderr>:#015Downloading:   5%|▌         | 73.7M/1.35G [00:02<00:45, 28.2MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 73.7M/1.35G [00:02<00:45, 28.2MB/s][1,12]<stderr>:#015Downloading:   6%|▌         | 77.5M/1.35G [00:02<00:59, 21.3MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 77.6M/1.35G [00:02<01:00, 20.9MB/s][1,12]<stderr>:#015Downloading:   6%|▌         | 83.8M/1.35G [00:03<00:47, 26.6MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 83.3M/1.35G [00:03<00:48, 25.8MB/s][1,7]<stderr>:#015Downloading:   6%|▋         | 87.2M/1.35G [00:03<00:43, 28.7MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 87.8M/1.35G [00:03<00:44, 28.0MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 92.4M/1.35G [00:03<00:37, 33.1MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 92.6M/1.35G [00:03<00:39, 32.0MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 97.6M/1.35G [00:03<00:33, 37.1MB/s][1,12]<stderr>:#015Downloading:   7%|▋         | 97.5M/1.35G [00:03<00:34, 35.7MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 104M/1.35G [00:03<00:29, 41.9MB/s] [1,12]<stderr>:#015Downloading:   8%|▊         | 103M/1.35G [00:03<00:31, 39.9MB/s] [1,7]<stderr>:#015Downloading:   8%|▊         | 110M/1.35G [00:03<00:26, 46.0MB/s][1,12]<stderr>:#015Downloading:   8%|▊         | 108M/1.35G [00:03<00:28, 43.2MB/s][1,7]<stderr>:#015Downloading:   9%|▊         | 116M/1.35G [00:03<00:24, 49.4MB/s][1,12]<stderr>:#015Downloading:   8%|▊         | 114M/1.35G [00:03<00:26, 46.2MB/s][1,12]<stderr>:#015Downloading:   9%|▉         | 119M/1.35G [00:03<00:26, 46.3MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 121M/1.35G [00:03<00:27, 44.3MB/s][1,12]<stderr>:#015Downloading:   9%|▉         | 125M/1.35G [00:03<00:24, 50.0MB/s][1,7]<stderr>:#015Downloading:   9%|▉         | 126M/1.35G [00:03<00:25, 46.9MB/s][1,12]<stderr>:#015Downloading:  10%|▉         | 130M/1.35G [00:04<00:25, 48.6MB/s][1,7]<stderr>:#015Downloading:  10%|▉         | 132M/1.35G [00:04<00:25, 47.1MB/s][1,12]<stderr>:#015Downloading:  10%|█         | 135M/1.35G [00:04<00:24, 49.2MB/s][1,7]<stderr>:#015Downloading:  10%|█         | 137M/1.35G [00:04<00:24, 48.7MB/s][1,12]<stderr>:#015Downloading:  10%|█         | 141M/1.35G [00:04<00:23, 51.3MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 143M/1.35G [00:04<00:23, 51.6MB/s][1,12]<stderr>:#015Downloading:  11%|█         | 147M/1.35G [00:04<00:23, 52.1MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 149M/1.35G [00:04<00:22, 53.6MB/s][1,12]<stderr>:#015Downloading:  11%|█▏        | 152M/1.35G [00:04<00:22, 52.8MB/s][1,7]<stderr>:#015Downloading:  11%|█▏        | 155M/1.35G [00:04<00:21, 55.0MB/s][1,12]<stderr>:#015Downloading:  12%|█▏        | 158M/1.35G [00:04<00:21, 54.9MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 160M/1.35G [00:04<00:23, 49.9MB/s][1,12]<stderr>:#015Downloading:  12%|█▏        | 164M/1.35G [00:04<00:23, 50.8MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 166M/1.35G [00:04<00:22, 51.3MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 169M/1.35G [00:04<00:30, 38.6MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 171M/1.35G [00:04<00:30, 38.5MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 175M/1.35G [00:04<00:27, 42.9MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 177M/1.35G [00:04<00:27, 42.5MB/s][1,12]<stderr>:#015Downloading:  13%|█▎        | 180M/1.35G [00:05<00:26, 44.6MB/s][1,7]<stderr>:#015Downloading:  14%|█▎        | 182M/1.35G [00:05<00:25, 45.6MB/s][1,12]<stderr>:#015Downloading:  14%|█▎        | 184M/1.35G [00:05<00:28, 40.5MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 187M/1.35G [00:05<00:32, 35.7MB/s][1,12]<stderr>:#015Downloading:  14%|█▍        | 189M/1.35G [00:05<00:30, 38.0MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 193M/1.35G [00:05<00:28, 40.5MB/s][1,12]<stderr>:#015Downloading:  14%|█▍        | 193M/1.35G [00:05<00:34, 33.7MB/s][1,12]<stderr>:#015Downloading:  15%|█▍        | 197M/1.35G [00:05<00:40, 28.6MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 198M/1.35G [00:05<00:39, 28.9MB/s][1,12]<stderr>:#015Downloading:  15%|█▍        | 201M/1.35G [00:05<00:36, 31.5MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 201M/1.35G [00:05<00:38, 29.4MB/s][1,12]<stderr>:#015Downloading:  15%|█▌        | 205M/1.35G [00:05<00:34, 33.3MB/s][1,7]<stderr>:#015Downloading:  15%|█▌        | 207M/1.35G [00:05<00:33, 33.7MB/s][1,12]<stderr>:#015Downloading:  16%|█▌        | 210M/1.35G [00:06<00:31, 35.7MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 211M/1.35G [00:06<00:32, 34.8MB/s][1,12]<stderr>:#015Downloading:  16%|█▌        | 215M/1.35G [00:06<00:28, 39.2MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 216M/1.35G [00:06<00:28, 39.4MB/s][1,12]<stderr>:#015Downloading:  16%|█▋        | 219M/1.35G [00:06<00:40, 28.1MB/s][1,7]<stderr>:#015Downloading:  16%|█▋        | 221M/1.35G [00:06<00:38, 29.0MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 225M/1.35G [00:06<00:33, 33.3MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 226M/1.35G [00:06<00:33, 33.7MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 230M/1.35G [00:06<00:29, 37.5MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 231M/1.35G [00:06<00:29, 37.7MB/s][1,12]<stderr>:#015Downloading:  17%|█▋        | 235M/1.35G [00:06<00:29, 38.0MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 236M/1.35G [00:06<00:28, 39.1MB/s][1,12]<stderr>:#015Downloading:  18%|█▊        | 240M/1.35G [00:06<00:27, 40.8MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 242M/1.35G [00:06<00:25, 43.5MB/s][1,12]<stderr>:#015Downloading:  18%|█▊        | 245M/1.35G [00:06<00:25, 43.6MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 248M/1.35G [00:06<00:23, 47.3MB/s][1,12]<stderr>:#015Downloading:  19%|█▊        | 251M/1.35G [00:06<00:22, 47.9MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 253M/1.35G [00:07<00:26, 40.6MB/s][1,12]<stderr>:#015Downloading:  19%|█▉        | 256M/1.35G [00:07<00:25, 43.4MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 259M/1.35G [00:07<00:24, 44.1MB/s][1,12]<stderr>:#015Downloading:  19%|█▉        | 262M/1.35G [00:07<00:23, 45.7MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 263M/1.35G [00:07<00:24, 44.5MB/s][1,12]<stderr>:#015Downloading:  20%|█▉        | 267M/1.35G [00:07<00:22, 47.3MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 268M/1.35G [00:07<00:23, 46.0MB/s][1,12]<stderr>:#015Downloading:  20%|██        | 272M/1.35G [00:07<00:22, 48.7MB/s][1,7]<stderr>:#015Downloading:  20%|██        | 274M/1.35G [00:07<00:22, 47.8MB/s][1,12]<stderr>:#015Downloading:  21%|██        | 278M/1.35G [00:07<00:20, 50.9MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 279M/1.35G [00:07<00:22, 48.0MB/s][1,12]<stderr>:#015Downloading:  21%|██        | 284M/1.35G [00:07<00:19, 53.4MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 284M/1.35G [00:07<00:20, 50.8MB/s][1,12]<stderr>:#015Downloading:  21%|██▏       | 289M/1.35G [00:07<00:21, 48.7MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 290M/1.35G [00:07<00:20, 51.2MB/s][1,12]<stderr>:#015Downloading:  22%|██▏       | 294M/1.35G [00:08<00:34, 30.4MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 295M/1.35G [00:08<00:33, 31.4MB/s][1,12]<stderr>:#015Downloading:  22%|██▏       | 300M/1.35G [00:08<00:29, 35.2MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 300M/1.35G [00:08<00:29, 35.7MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 304M/1.35G [00:08<00:31, 33.0MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 305M/1.35G [00:08<00:30, 34.2MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 310M/1.35G [00:08<00:27, 37.3MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 310M/1.35G [00:08<00:28, 36.1MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 315M/1.35G [00:08<00:25, 39.9MB/s][1,12]<stderr>:#015Downloading:  23%|██▎       | 315M/1.35G [00:08<00:26, 39.5MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 320M/1.35G [00:08<00:24, 42.4MB/s][1,12]<stderr>:#015Downloading:  24%|██▍       | 321M/1.35G [00:08<00:23, 42.7MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 326M/1.35G [00:08<00:21, 46.6MB/s][1,12]<stderr>:#015Downloading:  24%|██▍       | 326M/1.35G [00:08<00:26, 39.0MB/s][1,12]<stderr>:#015Downloading:  25%|██▍       | 331M/1.35G [00:09<00:31, 32.1MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 331M/1.35G [00:09<00:31, 32.1MB/s][1,12]<stderr>:#015Downloading:  25%|██▍       | 334M/1.35G [00:09<00:34, 28.9MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 335M/1.35G [00:09<00:39, 25.7MB/s][1,12]<stderr>:#015Downloading:  25%|██▌       | 337M/1.35G [00:09<00:43, 23.3MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 339M/1.35G [00:09<00:43, 23.1MB/s][1,12]<stderr>:#015Downloading:  25%|██▌       | 341M/1.35G [00:09<00:39, 25.5MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 343M/1.35G [00:09<00:37, 26.8MB/s][1,12]<stderr>:#015Downloading:  26%|██▌       | 344M/1.35G [00:09<00:39, 25.2MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 346M/1.35G [00:09<00:37, 26.4MB/s][1,12]<stderr>:#015Downloading:  26%|██▌       | 348M/1.35G [00:09<00:34, 28.8MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 352M/1.35G [00:09<00:31, 31.2MB/s][1,12]<stderr>:#015Downloading:  26%|██▋       | 354M/1.35G [00:09<00:29, 34.1MB/s][1,7]<stderr>:#015Downloading:  26%|██▋       | 356M/1.35G [00:09<00:28, 34.5MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 358M/1.35G [00:09<00:27, 36.2MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 361M/1.35G [00:10<00:30, 31.8MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 363M/1.35G [00:10<00:30, 31.9MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 364M/1.35G [00:10<00:31, 31.4MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 366M/1.35G [00:10<00:31, 31.5MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 369M/1.35G [00:10<00:30, 31.8MB/s][1,12]<stderr>:#015Downloading:  27%|██▋       | 370M/1.35G [00:10<00:32, 30.2MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 374M/1.35G [00:10<00:27, 35.6MB/s][1,12]<stderr>:#015Downloading:  28%|██▊       | 375M/1.35G [00:10<00:27, 35.1MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 378M/1.35G [00:10<00:29, 33.3MB/s][1,12]<stderr>:#015Downloading:  28%|██▊       | 379M/1.35G [00:10<00:30, 32.0MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 383M/1.35G [00:10<00:25, 37.8MB/s][1,12]<stderr>:#015Downloading:  29%|██▊       | 385M/1.35G [00:10<00:25, 37.4MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 387M/1.35G [00:10<00:25, 38.2MB/s][1,12]<stderr>:#015Downloading:  29%|██▉       | 390M/1.35G [00:10<00:25, 37.7MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 393M/1.35G [00:10<00:23, 41.2MB/s][1,12]<stderr>:#015Downloading:  29%|██▉       | 395M/1.35G [00:10<00:22, 41.4MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 397M/1.35G [00:10<00:21, 43.5MB/s][1,12]<stderr>:#015Downloading:  30%|██▉       | 400M/1.35G [00:10<00:21, 44.4MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 402M/1.35G [00:11<00:20, 45.1MB/s][1,12]<stderr>:#015Downloading:  30%|███       | 406M/1.35G [00:11<00:20, 46.4MB/s][1,7]<stderr>:#015Downloading:  30%|███       | 407M/1.35G [00:11<00:20, 46.3MB/s][1,12]<stderr>:#015Downloading:  31%|███       | 411M/1.35G [00:11<00:19, 48.1MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 412M/1.35G [00:11<00:19, 47.4MB/s][1,12]<stderr>:#015Downloading:  31%|███       | 416M/1.35G [00:11<00:18, 49.4MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 418M/1.35G [00:11<00:18, 49.2MB/s][1,12]<stderr>:#015Downloading:  31%|███▏      | 421M/1.35G [00:11<00:18, 50.3MB/s][1,7]<stderr>:#015Downloading:  31%|███▏      | 423M/1.35G [00:11<00:18, 51.0MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 427M/1.35G [00:11<00:18, 50.8MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 429M/1.35G [00:11<00:17, 52.3MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 432M/1.35G [00:11<00:17, 51.4MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 435M/1.35G [00:11<00:16, 53.6MB/s][1,12]<stderr>:#015Downloading:  32%|███▏      | 437M/1.35G [00:11<00:17, 51.7MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 440M/1.35G [00:11<00:16, 54.9MB/s][1,12]<stderr>:#015Downloading:  33%|███▎      | 442M/1.35G [00:11<00:17, 52.0MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 446M/1.35G [00:11<00:16, 55.7MB/s][1,12]<stderr>:#015Downloading:  33%|███▎      | 448M/1.35G [00:11<00:17, 52.3MB/s][1,7]<stderr>:#015Downloading:  34%|███▎      | 452M/1.35G [00:11<00:15, 57.5MB/s][1,12]<stderr>:#015Downloading:  34%|███▎      | 454M/1.35G [00:11<00:16, 54.2MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 458M/1.35G [00:12<00:16, 54.3MB/s][1,12]<stderr>:#015Downloading:  34%|███▍      | 460M/1.35G [00:12<00:15, 55.7MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 464M/1.35G [00:12<00:23, 37.8MB/s][1,12]<stderr>:#015Downloading:  35%|███▍     \n",
      " | 465M/1.35G [00:12<00:23, 38.1MB/s][1,7]<stderr>:#015Downloading:  35%|███▍      | 469M/1.35G [00:12<00:21, 41.6MB/s][1,12]<stderr>:#015Downloading:  35%|███▌      | 471M/1.35G [00:12<00:20, 42.9MB/s][1,7]<stderr>:#015Downloading:  35%|███▌      | 475M/1.35G [00:12<00:19, 45.0MB/s][1,12]<stderr>:#015Downloading:  35%|███▌      | 476M/1.35G [00:12<00:19, 45.2MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 480M/1.35G [00:12<00:19, 44.4MB/s][1,12]<stderr>:#015Downloading:  36%|███▌      | 481M/1.35G [00:12<00:19, 44.7MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 485M/1.35G [00:12<00:19, 44.9MB/s][1,12]<stderr>:#015Downloading:  36%|███▌      | 486M/1.35G [00:12<00:20, 42.9MB/s][1,7]<stderr>:#015Downloading:  36%|███▋      | 490M/1.35G [00:12<00:22, 38.6MB/s][1,12]<stderr>:#015Downloading:  36%|███▋      | 491M/1.35G [00:12<00:21, 39.3MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 495M/1.35G [00:12<00:20, 41.7MB/s][1,12]<stderr>:#015Downloading:  37%|███▋      | 496M/1.35G [00:13<00:19, 43.0MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 500M/1.35G [00:13<00:18, 44.8MB/s][1,12]<stderr>:#015Downloading:  37%|███▋      | 502M/1.35G [00:13<00:17, 46.9MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 505M/1.35G [00:13<00:18, 45.7MB/s][1,12]<stderr>:#015Downloading:  38%|███▊      | 507M/1.35G [00:13<00:17, 47.5MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 511M/1.35G [00:13<00:16, 49.1MB/s][1,12]<stderr>:#015Downloading:  38%|███▊      | 512M/1.35G [00:13<00:17, 47.8MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 517M/1.35G [00:13<00:16, 51.5MB/s][1,12]<stderr>:#015Downloading:  39%|███▊      | 518M/1.35G [00:13<00:16, 50.4MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 523M/1.35G [00:13<00:15, 53.5MB/s][1,12]<stderr>:#015Downloading:  39%|███▉      | 523M/1.35G [00:13<00:16, 50.6MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 529M/1.35G [00:13<00:14, 55.8MB/s][1,12]<stderr>:#015Downloading:  39%|███▉      | 529M/1.35G [00:13<00:15, 51.8MB/s][1,7]<stderr>:#015Downloading:  40%|███▉      | 535M/1.35G [00:13<00:14, 56.9MB/s][1,12]<stderr>:#015Downloading:  40%|███▉      | 534M/1.35G [00:13<00:15, 52.7MB/s][1,7]<stderr>:#015Downloading:  40%|████      | 541M/1.35G [00:13<00:14, 56.7MB/s][1,12]<stderr>:#015Downloading:  40%|████      | 540M/1.35G [00:13<00:15, 52.1MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 546M/1.35G [00:13<00:14, 57.0MB/s][1,12]<stderr>:#015Downloading:  41%|████      | 545M/1.35G [00:13<00:15, 52.0MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 552M/1.35G [00:13<00:13, 57.6MB/s][1,12]<stderr>:#015Downloading:  41%|████      | 551M/1.35G [00:14<00:14, 53.6MB/s][1,12]<stderr>:#015Downloading:  41%|████▏     | 556M/1.35G [00:14<00:14, 53.1MB/s][1,7]<stderr>:#015Downloading:  41%|████▏     | 558M/1.35G [00:14<00:16, 49.0MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 561M/1.35G [00:14<00:20, 37.9MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 563M/1.35G [00:14<00:24, 31.8MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 566M/1.35G [00:14<00:21, 36.1MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 569M/1.35G [00:14<00:21, 36.5MB/s][1,12]<stderr>:#015Downloading:  42%|████▏     | 570M/1.35G [00:14<00:21, 36.5MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 573M/1.35G [00:14<00:21, 36.0MB/s][1,12]<stderr>:#015Downloading:  43%|████▎     | 575M/1.35G [00:14<00:19, 39.6MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 579M/1.35G [00:14<00:19, 40.0MB/s][1,12]<stderr>:#015Downloading:  43%|████▎     | 581M/1.35G [00:14<00:17, 42.6MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 584M/1.35G [00:14<00:18, 41.3MB/s][1,12]<stderr>:#015Downloading:  44%|████▎     | 585M/1.35G [00:14<00:17, 44.2MB/s][1,7]<stderr>:#015Downloading:  44%|████▎     | 588M/1.35G [00:15<00:22, 33.3MB/s][1,12]<stderr>:#015Downloading:  44%|████▍     | 590M/1.35G [00:15<00:22, 33.8MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 593M/1.35G [00:15<00:20, 36.7MB/s][1,12]<stderr>:#015Downloading:  44%|████▍     | 595M/1.35G [00:15<00:19, 38.0MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 598M/1.35G [00:15<00:18, 39.6MB/s][1,12]<stderr>:#015Downloading:  45%|████▍     | 601M/1.35G [00:15<00:17, 41.8MB/s][1,7]<stderr>:#015Downloading:  45%|████▍     | 603M/1.35G [00:15<00:17, 43.3MB/s][1,12]<stderr>:#015Downloading:  45%|████▌     | 606M/1.35G [00:15<00:16, 44.2MB/s][1,7]<stderr>:#015Downloading:  45%|████▌     | 609M/1.35G [00:15<00:15, 46.8MB/s][1,12]<stderr>:#015Downloading:  45%|████▌     | 612M/1.35G [00:15<00:15, 47.8MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 615M/1.35G [00:15<00:15, 48.5MB/s][1,12]<stderr>:#015Downloading:  46%|████▌     | 617M/1.35G [00:15<00:15, 47.8MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 620M/1.35G [00:15<00:14, 49.8MB/s][1,12]<stderr>:#015Downloading:  46%|████▌     | 622M/1.35G [00:15<00:19, 37.0MB/s][1,7]<stderr>:#015Downloading:  46%|████▋     | 625M/1.35G [00:15<00:18, 38.4MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 628M/1.35G [00:15<00:17, 41.9MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 629M/1.35G [00:15<00:18, 39.6MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 633M/1.35G [00:16<00:17, 41.2MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 635M/1.35G [00:16<00:16, 43.1MB/s][1,12]<stderr>:#015Downloading:  47%|████▋     | 638M/1.35G [00:16<00:17, 40.8MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 640M/1.35G [00:16<00:17, 40.6MB/s][1,12]<stderr>:#015Downloading:  48%|████▊     | 643M/1.35G [00:16<00:16, 43.6MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 646M/1.35G [00:16<00:15, 45.0MB/s][1,12]<stderr>:#015Downloading:  48%|████▊     | 648M/1.35G [00:16<00:15, 45.7MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 651M/1.35G [00:16<00:15, 46.2MB/s][1,12]<stderr>:#015Downloading:  49%|████▊     | 654M/1.35G [00:16<00:14, 49.1MB/s][1,7]<stderr>:#015Downloading:  49%|████▊     | 655M/1.35G [00:16<00:15, 45.3MB/s][1,12]<stderr>:#015Downloading:  49%|████▉     | 659M/1.35G [00:16<00:14, 47.6MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 661M/1.35G [00:16<00:17, 39.7MB/s][1,12]<stderr>:#015Downloading:  49%|████▉     | 664M/1.35G [00:16<00:20, 33.6MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 665M/1.35G [00:16<00:19, 35.4MB/s][1,12]<stderr>:#015Downloading:  50%|████▉     | 669M/1.35G [00:16<00:17, 38.2MB/s][1,7]<stderr>:#015Downloading:  50%|████▉     | 671M/1.35G [00:16<00:17, 39.6MB/s][1,12]<stderr>:#015Downloading:  50%|█████     | 674M/1.35G [00:17<00:17, 38.7MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 675M/1.35G [00:17<00:16, 40.7MB/s][1,12]<stderr>:#015Downloading:  51%|█████     | 679M/1.35G [00:17<00:15, 42.1MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 681M/1.35G [00:17<00:14, 44.5MB/s][1,12]<stderr>:#015Downloading:  51%|█████     | 685M/1.35G [00:17<00:14, 45.1MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 686M/1.35G [00:17<00:14, 45.9MB/s][1,12]<stderr>:#015Downloading:  51%|█████▏    | 690M/1.35G [00:17<00:13, 47.7MB/s][1,12]<stderr>:#015Downloading:  52%|█████▏    | 695M/1.35G [00:17<00:13, 48.7MB/s][1,7]<stderr>:#015Downloading:  51%|█████▏    | 691M/1.35G [00:17<00:17, 37.6MB/s][1,12]<stderr>:#015Downloading:  52%|█████▏    | 701M/1.35G [00:17<00:12, 51.4MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 696M/1.35G [00:17<00:15, 41.5MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 707M/1.35G [00:17<00:12, 51.5MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 701M/1.35G [00:17<00:14, 43.0MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 712M/1.35G [00:17<00:11, 53.7MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 706M/1.35G [00:17<00:14, 44.9MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 711M/1.35G [00:17<00:13, 46.1MB/s][1,12]<stderr>:#015Downloading:  53%|█████▎    | 718M/1.35G [00:17<00:13, 47.8MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 716M/1.35G [00:17<00:13, 47.1MB/s][1,12]<stderr>:#015Downloading:  54%|█████▍    | 724M/1.35G [00:18<00:12, 49.5MB/s][1,7]<stderr>:#015Downloading:  54%|█████▎    | 721M/1.35G [00:18<00:13, 47.8MB/s][1,12]<stderr>:#015Downloading:  54%|█████▍    | 729M/1.35G [00:18<00:12, 51.2MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 726M/1.35G [00:18<00:12, 48.6MB/s][1,12]<stderr>:#015Downloading:  55%|█████▍    | 735M/1.35G [00:18<00:11, 52.6MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 731M/1.35G [00:18<00:12, 48.6MB/s][1,12]<stderr>:#015Downloading:  55%|█████▌    | 740M/1.35G [00:18<00:12, 47.0MB/s][1,7]<stderr>:#015Downloading:  55%|█████▍    | 736M/1.35G [00:18<00:12, 50.3MB/s][1,12]<stderr>:#015Downloading:  55%|█████▌    | 746M/1.35G [00:18<00:11, 50.6MB/s][1,7]<stderr>:#015Downloading:  55%|█████▌    | 742M/1.35G [00:18<00:11, 52.2MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 747M/1.35G [00:18<00:11, 50.2MB/s][1,12]<stderr>:#015Downloading:  56%|█████▌    | 752M/1.35G [00:18<00:15, 38.5MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 752M/1.35G [00:18<00:12, 48.9MB/s][1,12]<stderr>:#015Downloading:  56%|█████▋    | 757M/1.35G [00:18<00:14, 41.8MB/s][1,7]<stderr>:#015Downloading:  56%|█████▋    | 757M/1.35G [00:18<00:11, 49.2MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 763M/1.35G [00:18<00:12, 46.1MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 763M/1.35G [00:18<00:11, 50.2MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 768M/1.35G [00:18<00:12, 47.2MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 768M/1.35G [00:18<00:11, 50.9MB/s][1,12]<stderr>:#015Downloading:  57%|█████▋    | 773M/1.35G [00:19<00:12, 45.4MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 773M/1.35G [00:19<00:11, 51.1MB/s][1,12]<stderr>:#015Downloading:  58%|█████▊    | 779M/1.35G [00:19<00:11, 49.3MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 779M/1.35G [00:19<00:10, 53.2MB/s][1,12]<stderr>:#015Downloading:  58%|█████▊    | 784M/1.35G [00:19<00:12, 45.0MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 784M/1.35G [00:19<00:11, 47.8MB/s][1,12]<stderr>:#015Downloading:  59%|█████▊    | 789M/1.35G [00:19<00:15, 35.8MB/s][1,7]<stderr>:#015Downloading:  59%|█████▊    | 789M/1.35G [00:19<00:14, 37.5MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 794M/1.35G [00:19<00:14, 38.9MB/s][1,12]<stderr>:#015Downloading:  59%|█████▉    | 794M/1.35G [00:19<00:14, 37.6MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 798M/1.35G [00:19<00:13, 39.1MB/s][1,12]<stderr>:#015Downloading:  59%|█████▉    | 798M/1.35G [00:19<00:14, 37.7MB/s][1,7]<stderr>:#015Downloading:  60%|█████▉    | 803M/1.35G [00:19<00:12, 41.8MB/s][1,12]<stderr>:#015Downloading:  60%|█████▉    | 803M/1.35G [00:19<00:13, 41.3MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 807M/1.35G [00:19<00:15, 35.2MB/s][1,12]<stderr>:#015Downloading:  60%|██████    | 807M/1.35G [00:20<00:15, 35.0MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 811M/1.35G [00:20<00:16, 32.2MB/s][1,12]<stderr>:#015Downloading:  60%|██████    | 811M/1.35G [00:20<00:16, 32.1MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 815M/1.35G [00:20<00:15, 34.4MB/s][1,12]<stderr>:#015Downloading:  61%|██████    | 815M/1.35G [00:20<00:15, 33.9MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 821M/1.35G [00:20<00:13, 38.9MB/s][1,12]<stderr>:#015Downloading:  61%|██████    | 821M/1.35G [00:20<00:13, 39.0MB/s][1,7]<stderr>:#015Downloading:  61%|██████▏   | 825M/1.35G [00:20<00:12, 40.5MB/s][1,12]<stderr>:#015Downloading:  61%|██████▏   | 826M/1.35G [00:20<00:12, 41.6MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 830M/1.35G [00:20<00:11, 43.3MB/s][1,12]<stderr>:#015Downloading:  62%|██████▏   | 831M/1.35G [00:20<00:11, 44.4MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 836M/1.35G [00:20<00:10, 46.4MB/s][1,12]<stderr>:#015Downloading:  62%|██████▏   | 837M/1.35G [00:20<00:10, 48.1MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 841M/1.35G [00:20<00:12, 40.7MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 843M/1.35G [00:20<00:12, 40.4MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 847M/1.35G [00:20<00:11, 45.1MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 847M/1.35G [00:20<00:11, 42.7MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 852M/1.35G [00:20<00:11, 44.5MB/s][1,12]<stderr>:#015Downloading:  63%|██████▎   | 853M/1.35G [00:21<00:10, 45.1MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 858M/1.35G [00:21<00:10, 47.8MB/s][1,12]<stderr>:#015Downloading:  64%|██████▍   | 858M/1.35G [00:21<00:10, 46.9MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 863M/1.35G [00:21<00:09, 48.9MB/s][1,12]<stderr>:#015Downloading:  64%|██████▍   | 863M/1.35G [00:21<00:10, 44.8MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 868M/1.35G [00:21<00:12, 38.6MB/s][1,12]<stderr>:#015Downloading:  65%|██████▍   | 868M/1.35G [00:21<00:12, 38.9MB/s][1,12]<stderr>:#015Downloading:  65%|██████▍   | 872M/1.35G [00:21<00:16, 28.6MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 872M/1.35G [00:21<00:16, 28.9MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 875M/1.35G [00:21<00:20, 23.4MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 876M/1.35G [00:21<00:19, 23.8MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 878M/1.35G [00:22<00:21, 21.6MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 879M/1.35G [00:21<00:21, 22.1MB/s][1,12]<stderr>:#015Downloading:  65%|██████▌   | 881M/1.35G [00:22<00:20, 22.1MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 881M/1.35G [00:22<00:22, 20.4MB/s][1,12]<stderr>:#015Downloading:  66%|██████▌   | 883M/1.35G [00:22<00:20, 23.0MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 886M/1.35G [00:22<00:18, 24.5MB/s][1,12]<stderr>:#015Downloading:  66%|██████▌   | 889M/1.35G [00:22<00:16, 27.8MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 891M/1.35G [00:22<00:15, 28.7MB/s][1,12]<stderr>:#015Downloading:  66%|██████▋   | 894M/1.35G [00:22<00:13, 32.5MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 897M/1.35G [00:22<00:13, 33.8MB/s][1,12]<stderr>:#015Downloading:  67%|██████▋   | 899M/1.35G [00:22<00:12, 35.6MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 903M/1.35G [00:22<00:11, 39.0MB/s][1,12]<stderr>:#015Downloading:  67%|██████▋   | 904M/1.35G [00:22<00:12, 34.8MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 907M/1.35G [00:22<00:16, 25.9MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 908M/1.35G [00:22<00:16, 26.6MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 913M/1.35G [00:22<00:13, 30.9MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 914M/1.35G [00:23<00:13, 31.7MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 918M/1.35G [00:23<00:12, 34.5MB/s][1,12]<stderr>:#015Downloading:  68%|██████▊   | 919M/1.35G [00:23<00:11, 35.7MB/s][1,7]<stderr>:#015Downloading:  69%|██████▊   | 923M/1.35G [00:23<00:11, 37.9MB/s][1,12]<stderr>:#015Downloading:  69%|██████▊   | 924M/1.35G [00:23<00:10, 39.0MB/s][1,7]<stderr>:#015Download\u001b[0m\n",
      "\u001b[34ming:  69%|██████▉   | 928M/1.35G [00:23<00:10, 41.3MB/s][1,12]<stderr>:#015Downloading:  69%|██████▉   | 929M/1.35G [00:23<00:09, 42.0MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 933M/1.35G [00:23<00:09, 44.5MB/s][1,12]<stderr>:#015Downloading:  69%|██████▉   | 934M/1.35G [00:23<00:09, 44.3MB/s][1,7]<stderr>:#015Downloading:  70%|██████▉   | 939M/1.35G [00:23<00:08, 47.1MB/s][1,12]<stderr>:#015Downloading:  70%|██████▉   | 939M/1.35G [00:23<00:08, 46.3MB/s][1,7]<stderr>:#015Downloading:  70%|███████   | 945M/1.35G [00:23<00:07, 50.1MB/s][1,12]<stderr>:#015Downloading:  70%|███████   | 945M/1.35G [00:23<00:08, 49.9MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 950M/1.35G [00:23<00:09, 43.8MB/s][1,12]<stderr>:#015Downloading:  71%|███████   | 951M/1.35G [00:23<00:08, 44.1MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 955M/1.35G [00:23<00:09, 40.2MB/s][1,12]<stderr>:#015Downloading:  71%|███████   | 955M/1.35G [00:23<00:09, 39.0MB/s][1,12]<stderr>:#015Downloading:  71%|███████▏  | 960M/1.35G [00:24<00:10, 35.4MB/s][1,7]<stderr>:#015Downloading:  71%|███████▏  | 960M/1.35G [00:24<00:10, 36.0MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 965M/1.35G [00:24<00:09, 38.5MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 965M/1.35G [00:24<00:10, 37.8MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 969M/1.35G [00:24<00:09, 37.9MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 969M/1.35G [00:24<00:09, 37.8MB/s][1,12]<stderr>:#015Downloading:  72%|███████▏  | 974M/1.35G [00:24<00:08, 41.5MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 974M/1.35G [00:24<00:09, 40.9MB/s][1,12]<stderr>:#015Downloading:  73%|███████▎  | 980M/1.35G [00:24<00:08, 45.2MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 979M/1.35G [00:24<00:08, 43.5MB/s][1,12]<stderr>:#015Downloading:  73%|███████▎  | 985M/1.35G [00:24<00:07, 46.4MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 984M/1.35G [00:24<00:07, 46.0MB/s][1,12]<stderr>:#015Downloading:  74%|███████▎  | 990M/1.35G [00:24<00:07, 49.2MB/s][1,7]<stderr>:#015Downloading:  74%|███████▎  | 990M/1.35G [00:24<00:07, 48.9MB/s][1,12]<stderr>:#015Downloading:  74%|███████▍  | 996M/1.35G [00:24<00:06, 51.5MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 996M/1.35G [00:24<00:06, 51.6MB/s][1,12]<stderr>:#015Downloading:  74%|███████▍  | 1.00G/1.35G [00:24<00:06, 51.0MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 1.00G/1.35G [00:24<00:06, 52.9MB/s][1,12]<stderr>:#015Downloading:  75%|███████▍  | 1.01G/1.35G [00:24<00:06, 51.9MB/s][1,7]<stderr>:#015Downloading:  75%|███████▍  | 1.01G/1.35G [00:24<00:06, 54.2MB/s][1,12]<stderr>:#015Downloading:  75%|███████▌  | 1.01G/1.35G [00:25<00:06, 54.4MB/s][1,7]<stderr>:#015Downloading:  75%|███████▌  | 1.01G/1.35G [00:25<00:05, 56.2MB/s][1,12]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 39.6MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 40.1MB/s][1,12]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:08, 40.1MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 1.02G/1.35G [00:25<00:09, 33.8MB/s][1,12]<stderr>:#015Downloading:  76%|███████▋  | 1.03G/1.35G [00:25<00:09, 33.7MB/s][1,7]<stderr>:#015Downloading:  76%|███████▋  | 1.03G/1.35G [00:25<00:08, 35.8MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.03G/1.35G [00:25<00:09, 34.3MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.03G/1.35G [00:25<00:08, 35.7MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 38.4MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:07, 39.3MB/s][1,12]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 36.1MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 1.04G/1.35G [00:25<00:08, 36.7MB/s][1,12]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 41.0MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 41.5MB/s][1,12]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 38.7MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 1.05G/1.35G [00:26<00:07, 39.2MB/s][1,12]<stderr>:#015Downloading:  79%|███████▊  | 1.06G/1.35G [00:26<00:08, 35.4MB/s][1,7]<stderr>:#015Downloading:  79%|███████▊  | 1.06G/1.35G [00:26<00:08, 36.1MB/s][1,12]<stderr>:#015Downloading:  79%|███████▉  | 1.06G/1.35G [00:26<00:08, 33.0MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.06G/1.35G [00:26<00:08, 33.4MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 1.07G/1.35G [00:26<00:08, 33.1MB/s][1,12]<stderr>:#015Downloading:  79%|███████▉  | 1.07G/1.35G [00:26<00:08, 33.0MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:07, 35.9MB/s][1,12]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:07, 36.7MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 1.07G/1.35G [00:26<00:06, 39.2MB/s][1,12]<stderr>:#015Downloading:  80%|███████▉  | 1.08G/1.35G [00:26<00:06, 40.4MB/s][1,12]<stderr>:#015Downloading:  80%|████████  | 1.08G/1.35G [00:26<00:06, 43.8MB/s][1,7]<stderr>:#015Downloading:  80%|████████  | 1.08G/1.35G [00:26<00:06, 42.0MB/s][1,12]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 46.1MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.08G/1.35G [00:27<00:05, 43.8MB/s][1,12]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 47.7MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 1.09G/1.35G [00:27<00:05, 45.4MB/s][1,7]<stderr>:#015Downloading:  81%|████████▏ | 1.09G/1.35G [00:27<00:05, 46.6MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 48.7MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 47.5MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:04, 50.0MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.10G/1.35G [00:27<00:05, 48.1MB/s][1,12]<stderr>:#015Downloading:  82%|████████▏ | 1.11G/1.35G [00:27<00:04, 51.5MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.11G/1.35G [00:27<00:04, 48.5MB/s][1,12]<stderr>:#015Downloading:  83%|████████▎ | 1.11G/1.35G [00:27<00:04, 53.8MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 50.8MB/s][1,12]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 45.7MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.12G/1.35G [00:27<00:04, 49.9MB/s][1,12]<stderr>:#015Downloading:  84%|████████▎ | 1.12G/1.35G [00:27<00:04, 47.8MB/s][1,7]<stderr>:#015Downloading:  84%|████████▎ | 1.13G/1.35G [00:27<00:04, 50.1MB/s][1,12]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:27<00:04, 49.5MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:27<00:04, 52.1MB/s][1,12]<stderr>:#015Downloading:  84%|████████▍ | 1.13G/1.35G [00:28<00:04, 46.6MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.14G/1.35G [00:28<00:04, 50.7MB/s][1,12]<stderr>:#015Downloading:  85%|████████▍ | 1.14G/1.35G [00:28<00:04, 49.4MB/s][1,7]<stderr>:#015Downloading:  85%|████████▍ | 1.14G/1.35G [00:28<00:03, 52.9MB/s][1,12]<stderr>:#015Downloading:  85%|████████▌ | 1.15G/1.35G [00:28<00:03, 52.1MB/s][1,7]<stderr>:#015Downloading:  85%|████████▌ | 1.15G/1.35G [00:28<00:04, 40.5MB/s][1,12]<stderr>:#015Downloading:  86%|████████▌ | 1.15G/1.35G [00:28<00:05, 33.7MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.15G/1.35G [00:28<00:05, 35.7MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:05, 31.6MB/s][1,12]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:06, 30.8MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.16G/1.35G [00:28<00:08, 22.0MB/s][1,12]<stderr>:#015Downloading:  86%|████████▋ | 1.16G/1.35G [00:28<00:08, 22.3MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.16G/1.35G [00:29<00:06, 26.6MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:06, 27.0MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:05, 31.7MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:06, 28.8MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.17G/1.35G [00:29<00:05, 29.4MB/s][1,12]<stderr>:#015Downloading:  87%|████████▋ | 1.18G/1.35G [00:29<00:05, 31.4MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:05, 32.9MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:04, 34.6MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:05, 32.2MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.18G/1.35G [00:29<00:04, 33.4MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.19G/1.35G [00:29<00:04, 32.0MB/s][1,12]<stderr>:#015Downloading:  88%|████████▊ | 1.19G/1.35G [00:29<00:05, 29.1MB/s][1,12]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:29<00:07, 21.1MB/s][1,7]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:29<00:07, 20.9MB/s][1,12]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:30<00:07, 20.7MB/s][1,7]<stderr>:#015Downloading:  89%|████████▊ | 1.19G/1.35G [00:30<00:07, 20.4MB/s][1,12]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.5MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.3MB/s][1,12]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.3MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.20G/1.35G [00:30<00:05, 25.5MB/s][1,12]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 27.1MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 27.1MB/s][1,12]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 26.0MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.21G/1.35G [00:30<00:05, 25.1MB/s][1,12]<stderr>:#015Downloading:  90%|█████████ | 1.21G/1.35G [00:30<00:05, 26.1MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.21G/1.35G [00:30<00:04, 27.7MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.22G/1.35G [00:30<00:04, 28.2MB/s][1,12]<stderr>:#015Downloading:  90%|█████████ | 1.22G/1.35G [00:30<00:04, 28.2MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:30<00:03, 32.2MB/s][1,12]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:30<00:03, 32.5MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.22G/1.35G [00:31<00:04, 28.0MB/s][1,12]<stderr>:#015Downloading:  91%|█████████ | 1.23G/1.35G [00:31<00:04, 25.6MB/s][1,7]<stderr>:#015Downloading:  91%|█████████▏| 1.23G/1.35G [00:31<00:04, 28.1MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.23G/1.35G [00:31<00:03, 31.0MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.23G/1.35G [00:31<00:04, 24.0MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:04, 25.1MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 28.9MB/s][1,12]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 30.4MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.24G/1.35G [00:31<00:03, 33.9MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 34.1MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 37.7MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 35.0MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 36.7MB/s][1,12]<stderr>:#015Downloading:  93%|█████████▎| 1.25G/1.35G [00:31<00:02, 38.1MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▎| 1.26G/1.35G [00:31<00:02, 41.2MB/s][1,12]<stderr>:#015Downloading:  94%|█████████▎| 1.26G/1.35G [00:31<00:02, 41.4MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.26G/1.35G [00:32<00:01, 42.1MB/s][1,12]<stderr>:#015Downloading:  94%|█████████▍| 1.27G/1.35G [00:32<00:01, 45.5MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.27G/1.35G [00:32<00:01, 43.8MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▍| 1.27G/1.35G [00:32<00:01, 47.0MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▍| 1.27G/1.35G [00:32<00:01, 46.8MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▍| 1.28G/1.35G [00:32<00:01, 44.2MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 45.3MB/s][1,12]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 44.8MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.28G/1.35G [00:32<00:01, 41.4MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 39.0MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 42.4MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 42.8MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.29G/1.35G [00:32<00:01, 45.3MB/s][1,12]<stderr>:#015Downloading:  96%|█████████▋| 1.30G/1.35G [00:32<00:01, 46.3MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:00, 47.8MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:01, 39.5MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.30G/1.35G [00:32<00:01, 37.7MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 31.9MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 33.6MB/s][1,12]<stderr>:#015Downloading:  97%|█████████▋| 1.31G/1.35G [00:33<00:01, 31.7MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.31G/1.35G [00:33<00:01, 31.5MB/s][1,12]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 36.7MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 34.9MB/s][1,12]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 40.9MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.32G/1.35G [00:33<00:00, 37.5MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▊| 1.33G/1.35G [00:33<00:00, 31.3MB/s][1,12]<stderr>:#015Downloading:  99%|█████████▊| 1.33G/1.35G [00:33<00:00, 25.6MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.33G/1.35G [00:33<00:00, 27.4MB/s][1,12]<stderr>:#015Downloading:  99%|████████�\u001b[0m\n",
      "\u001b[34m��▉| 1.33G/1.35G [00:33<00:00, 30.3MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.33G/1.35G [00:33<00:00, 30.7MB/s][1,12]<stderr>:#015Downloading:  99%|█████████▉| 1.34G/1.35G [00:34<00:00, 25.6MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.34G/1.35G [00:34<00:00, 25.7MB/s][1,12]<stderr>:#015Downloading: 100%|█████████▉| 1.34G/1.35G [00:34<00:00, 30.3MB/s][1,7]<stderr>:#015Downloading: 100%|█████████▉| 1.34G/1.35G [00:34<00:00, 30.4MB/s][1,12]<stderr>:#015Downloading: 100%|██████████| 1.35G/1.35G [00:34<00:00, 39.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading: 100%|██████████| 1.35G/1.35G [00:34<00:00, 39.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:1155] 2021-09-05 07:58:09,228 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,0]<stderr>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:19,509 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:19,509 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,2]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,5]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,6]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,1]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,3]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,4]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,7]<stderr>:#015  1%|          | 1/88 [00:00<00:54,  1.60ba/s][1,1]<stderr>:#015  1%|          | 1/88 [00:00<01:01,  1.42ba/s][1,0]<stderr>:#015  1%|          | 1/88 [00:00<01:04,  1.35ba/s][1,2]<stderr>:#015  1%|          | 1/88 [00:00<01:10,  1.23ba/s][1,6]<stderr>:#015  1%|          | 1/88 [00:00<01:11,  1.21ba/s][1,5]<stderr>:#015  1%|          | 1/88 [00:00<01:15,  1.15ba/s][1,3]<stderr>:#015  1%|          | 1/88 [00:00<01:09,  1.25ba/s][1,7]<stderr>:#015  2%|▏         | 2/88 [00:00<00:46,  1.84ba/s][1,4]<stderr>:#015  1%|          | 1/88 [00:00<01:08,  1.27ba/s][1,1]<stderr>:#015  2%|▏         | 2/88 [00:01<00:54,  1.58ba/s][1,2]<stderr>:#015  2%|▏         | 2/88 [00:01<01:00,  1.42ba/s][1,0]<stderr>:#015  2%|▏         | 2/88 [00:01<00:58,  1.47ba/s][1,6]<stderr>:#015  2%|▏         | 2/88 [00:01<01:01,  1.40ba/s][1,5]<stderr>:#015  2%|▏         | 2/88 [00:01<01:04,  1.34ba/s][1,3]<stderr>:#015  2%|▏         | 2/88 [00:01<01:02,  1.38ba/s][1,4]<stderr>:#015  2%|▏         | 2/88 [00:01<00:59,  1.45ba/s][1,7]<stderr>:#015  3%|▎         | 3/88 [00:01<00:45,  1.88ba/s][1,1]<stderr>:#015  3%|▎         | 3/88 [00:01<00:49,  1.71ba/s][1,2]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.60ba/s][1,6]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.60ba/s][1,0]<stderr>:#015  3%|▎         | 3/88 [00:01<00:51,  1.64ba/s][1,5]<stderr>:#015  3%|▎         | 3/88 [00:01<00:54,  1.55ba/s][1,3]<stderr>:#015  3%|▎         | 3/88 [00:01<00:53,  1.59ba/s][1,4]<stderr>:#015  3%|▎         | 3/88 [00:01<00:51,  1.66ba/s][1,7]<stderr>:#015  5%|▍         | 4/88 [00:01<00:40,  2.06ba/s][1,1]<stderr>:#015  5%|▍         | 4/88 [00:02<00:46,  1.82ba/s][1,7]<stderr>:#015  6%|▌         | 5/88 [00:02<00:37,  2.24ba/s][1,6]<stderr>:#015  5%|▍         | 4/88 [00:02<00:49,  1.71ba/s][1,5]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.68ba/s][1,0]<stderr>:#015  5%|▍         | 4/88 [00:02<00:48,  1.72ba/s][1,2]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.65ba/s][1,4]<stderr>:#015  5%|▍         | 4/88 [00:02<00:47,  1.77ba/s][1,3]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.67ba/s][1,1]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.85ba/s][1,5]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.81ba/s][1,2]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.82ba/s][1,0]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.85ba/s][1,4]<stderr>:#015  6%|▌         | 5/88 [00:02<00:42,  1.96ba/s][1,6]<stderr>:#015  6%|▌         | 5/88 [00:02<00:46,  1.80ba/s][1,3]<stderr>:#015  6%|▌         | 5/88 [00:02<00:44,  1.86ba/s][1,7]<stderr>:#015  7%|▋         | 6/88 [00:02<00:40,  2.03ba/s][1,2]<stderr>:#015  7%|▋         | 6/88 [00:03<00:42,  1.94ba/s][1,0]<stderr>:#015  7%|▋         | 6/88 [00:03<00:41,  1.97ba/s][1,6]<stderr>:#015  7%|▋         | 6/88 [00:03<00:42,  1.92ba/s][1,4]<stderr>:#015  7%|▋         | 6/88 [00:02<00:40,  2.05ba/s][1,3]<stderr>:#015  7%|▋         | 6/88 [00:03<00:40,  2.02ba/s][1,1]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.89ba/s][1,7]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.18ba/s][1,5]<stderr>:#015  7%|▋         | 6/88 [00:03<00:46,  1.78ba/s][1,0]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.07ba/s][1,2]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.04ba/s][1,6]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.04ba/s][1,4]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.13ba/s][1,7]<stderr>:#015  9%|▉         | 8/88 [00:03<00:35,  2.26ba/s][1,5]<stderr>:#015  8%|▊         | 7/88 [00:03<00:40,  2.01ba/s][1,1]<stderr>:#015  8%|▊         | 7/88 [00:03<00:42,  1.92ba/s][1,3]<stderr>:#015  8%|▊         | 7/88 [00:03<00:41,  1.96ba/s][1,0]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.08ba/s][1,7]<stderr>:#015 10%|█         | 9/88 [00:04<00:35,  2.25ba/s][1,4]<stderr>:#015  9%|▉         | 8/88 [00:03<00:38,  2.08ba/s][1,6]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.98ba/s][1,2]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.97ba/s][1,1]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  2.00ba/s][1,3]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.05ba/s][1,5]<stderr>:#015  9%|▉         | 8/88 [00:04<00:40,  1.99ba/s][1,15]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[WARNING|modeling_utils.py:1331] 2021-09-05 07:58:23,793 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[WARNING|modeling_utils.py:1342] 2021-09-05 07:58:23,793 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,11]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,8]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,10]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,12]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,11]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,9]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,13]<stderr>:Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.08ba/s][1,13]<stderr>:#015  0%|          | 0/88 [00:00<?, ?ba/s][1,2]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.08ba/s][1,1]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.09ba/s][1,3]<stderr>:#015 10%|█         | 9/88 [00:04<00:36,  2.16ba/s][1,4]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.08ba/s][1,5]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.08ba/s][1,7]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:37,  2.09ba/s][1,6]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.93ba/s][1,0]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.13ba/s][1,15]<stderr>:#015  1%|          | 1/88 [00:00<00:58,  1.49ba/s][1,4]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.17ba/s][1,12]<stderr>:#015  1%|          | 1/88 [00:00<00:58,  1.48ba/s][1,1]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.14ba/s][1,2]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:37,  2.09ba/s][1,5]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.13ba/s][1,7]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:35,  2.16ba/s][1,3]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.14ba/s][1,13]<stderr>:#015  1%|          | 1/88 [00:00<00:46,  1.87ba/s][1,6]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.06ba/s][1,10]<stderr>:#015  1%|          | 1/88 [00:00<01:11,  1.22ba/s][1,8]<stderr>:#015  1%|          | 1/88 [00:00<01:17,  1.12ba/s][1,11]<stderr>:#015  1%|          | 1/88 [00:00<01:16,  1.14ba/s][1,14]<stderr>:#015  1%|          | 1/88 [00:00<01:24,  1.03ba/s][1,9]<stderr>:#015  1%|          | 1/88 [00:00<01:23,  1.05ba/s][1,12]<stderr>:#015  2%|▏         | 2/88 [00:01<00:51,  1.66ba/s][1,0]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:36,  2.10ba/s][1,15]<stderr>:#015  2%|▏         | 2/88 [00:01<00:53,  1.61ba/s][1,7]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.18ba/s][1,13]<stderr>:#015  2%|▏         | 2/88 [00:00<00:43,  1.97ba/s][1,10]<stderr>:#015  2%|▏         | 2/88 [00:01<00:59,  1.45ba/s][1,5]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.06ba/s][1,4]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s][1,3]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.07ba/s][1,2]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  2.00ba/s][1,6]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.05ba/s][1,1]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  2.00ba/s][1,11]<stderr>:#015  2%|▏         | 2/88 [00:01<01:03,  1.35ba/s][1,8]<stderr>:#015  2%|▏         | 2/88 [00:01<01:05,  1.32ba/s][1,14]<stderr>:#015  2%|▏         | 2/88 [00:01<01:09,  1.24ba/s][1,12]<stderr>:#015  3%|▎         | 3/88 [00:01<00:45,  1.85ba/s][1,0]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.19ba/s][1,9]<stderr>:#015  2%|▏         | 2/88 [00:01<01:10,  1.21ba/s][1,7]<stderr>:#015 15%|█▍        | 13/88 [00:05<00:34,  2.19ba/s][1,10]<stderr>:#015  3%|▎         | 3/88 [00:01<00:52,  1.63ba/s][1,5]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.16ba/s][1,4]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.14ba/s][1,3]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.17ba/s][1,15]<stderr>:#015  3%|▎         | 3/88 [00:01<00:49,  1.72ba/s][1,2]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.12ba/s][1,6]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:35,  2.13ba/s][1,1]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:36,  2.09ba/s][1,11]<stderr>:#015  3%|▎         | 3/88 [00:01<00:55,  1.53ba/s][1,8]<stderr>:#015  3%|▎         | 3/88 [00:01<00:56,  1.51ba/s][1,14]<stderr>:#015  3%|▎         | 3/88 [00:01<00:58,  1.46ba/s][1,13]<stderr>:#015  3%|▎         | 3/88 [00:01<00:48,  1.74ba/s][1,12]<stderr>:#015  5%|▍         | 4/88 [00:01<00:42,  1.96ba/s][1,9]<stderr>:#015  3%|▎         | 3/88 [00:01<00:58,  1.45ba/s][1,7]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:31,  2.32ba/s][1,0]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.22ba/s][1,5]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.26ba/s][1,2]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.22ba/s][1,6]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:33,  2.21ba/s][1,3]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.19ba/s][1,4]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.15ba/s][1,1]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.14ba/s][1,11]<stderr>:#015  5%|▍         | 4/88 [00:02<00:50,  1.65ba/s][1,15]<stderr>:#015  5%|▍         | 4/88 [00:02<00:49,  1.70ba/s][1,8]<stderr>:#015  5%|▍         | 4/88 [00:02<00:51,  1.64ba/s][1,10]<stderr>:#015  5%|▍         | 4/88 [00:02<00:52,  1.61ba/s][1,12]<stderr>:#015  6%|▌         | 5/88 [00:02<00:38,  2.16ba/s][1,13]<stderr>:#015  5%|▍         | 4/88 [00:02<00:43,  1.93ba/s][1,14]<stderr>:#015  5%|▍         | 4/88 [00:02<00:53,  1.56ba/s][1,7]<stderr>:#015 17%|█▋        | 15/88 [00:06<00:30,  2.37ba/s][1,0]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.29ba/s][1,5]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:31,  2.33ba/s][1,2]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.28ba/s][1,9]<stderr>:#015  5%|▍         | 4/88 [00:02<00:53,  1.58ba/s][1,1]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.27ba/s][1,4]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:33,  2.24ba/s][1,6]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.26ba/s][1,3]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:34,  2.16ba/s][1,10]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.76ba/s][1,12]<stderr>:#015  7%|▋         | 6/88 [00:02<00:37,  2.21ba/s][1,8]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.76ba/s][1,14]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.74ba/s][1,7]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:29,  2.43ba/s][1,11]<stderr>:#015  6%|▌         | 5/88 [00:02<00:48,  1.72ba/s][1,15]<stderr>:#015  6%|▌         | 5/88 [00:02<00:47,  1.74ba/s][1,9]<stderr>:#015  6%|▌         | 5/88 [00:02<00:45,  1.84ba/s][1,13]<stderr>:#015  6%|▌         | 5/88 [00:02<00:42,  1.95ba/s][1,5]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.17ba/s][1,0]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.08ba/s][1,2]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.16ba/s][1,3]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.20ba/s][1,6]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:33,  2.18ba/s][1,4]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:34,  2.14ba/s][1,1]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:34,  2.13ba/s][1,10]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.87ba/s][1,7]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:29,  2.43ba/s][1,14]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.85ba/s][1,12]<stderr>:#015  8%|▊         | 7/88 [00:03<00:37,  2.16ba/s][1,11]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.86ba/s][1,8]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.84ba/s][1,15]<stderr>:#015  7%|▋         | 6/88 [00:03<00:43,  1.89ba/s][1,9]<stderr>:#015  7%|▋         | 6/88 [00:03<00:41,  1.97ba/s][1,5]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.25ba/s][1,6]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.29ba/s][1,3]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.30ba/s][1,1]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.29ba/s][1,2]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.22ba/s][1,13]<stderr>:#015  7%|▋         | 6/88 [00:03<00:44,  1.82ba/s][1,0]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:34,  2.07ba/s][1,7]<stderr>:#015 20%|██        | 18/88 [00:07<00:26,  2.67ba/s][1,4]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.15ba/s][1,10]<stderr>:#015  8%|▊         | 7/88 [00:03<00:41,  1.93ba/s][1,15]<stderr>:#015  8%|▊         | 7/88 [00:03<00:40,  1.99ba/s][1,9]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.03ba/s][1,14]<stderr>:#015  8%|▊         | 7/88 [00:03<00:43,  1.84ba/s][1,11]<stderr>:#015  8%|▊         | 7/88 [00:03<00:43,  1.85ba/s][1,13]<stderr>:#015  8%|▊         | 7/88 [00:03<00:39,  2.07ba/s][1,8]<stderr>:#015  8%|▊         | 7/88 [00:03<00:44,  1.82ba/s][1,1]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:30,  2.34ba/s][1,12]<stderr>:#015  9%|▉         | 8/88 [00:03<00:39,  2.01ba/s][1,6]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:30,  2.33ba/s][1,5]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.27ba/s][1,0]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.22ba/s][1,4]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:30,  2.30ba/s][1,7]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:25,  2.66ba/s][1,2]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:32,  2.17ba/s][1,3]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:33,  2.14ba/s][1,9]<stderr>:#015  9%|▉         | 8/88 [00:04<00:38,  2.09ba/s][1,15]<stderr>:#015  9%|▉         | 8/88 [00:04<00:39,  2.01ba/s][1,10]<stderr>:#015  9%|▉         | 8/88 [00:04<00:41,  1.92ba/s][1,13]<stderr>:#015  9%|▉         | 8/88 [00:03<00:37,  2.14ba/s][1,12]<stderr>:#015 10%|█         | 9/88 [00:04<00:37,  2.09ba/s][1,14]<stderr>:#015  9%|▉         | 8/88 [00:04<00:42,  1.89ba/s][1,5]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.20ba/s][1,0]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,6]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,7]<stderr>:#015 23%|██▎       | 20/88 [00:08<00:27,  2.45ba/s][1,11]<stderr>:#015  9%|▉         | 8/88 [00:04<00:44,  1.80ba/s][1,1]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.15ba/s][1,8]<stderr>:#015  9%|▉         | 8/88 [00:04<00:44,  1.79ba/s][1,2]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.16ba/s][1,3]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.20ba/s][1,4]<stderr>:#015 20%|██        | 18/88 [00:08<00:33,  2.10ba/s][1,9]<stderr>:#015 10%|█         | 9/88 [00:04<00:38,  2.07ba/s][1,10]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  1.98ba/s][1,14]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  2.03ba/s][1,13]<stderr>:#015 10%|█         | 9/88 [00:04<00:36,  2.16ba/s][1,15]<stderr>:#015 10%|█         | 9/88 [00:04<00:39,  1.98ba/s][1,12]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:36,  2.12ba/s][1,5]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.23ba/s][1,6]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.26ba/s][1,11]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.97ba/s][1,7]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:27,  2.44ba/s][1,0]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.21ba/s][1,1]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.20ba/s][1,8]<stderr>:#015 10%|█         | 9/88 [00:04<00:40,  1.95ba/s][1,4]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:31,  2.22ba/s][1,3]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.14ba/s][1,2]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:33,  2.09ba/s][1,9]<stderr>:#015 11%|█▏        | 10/88 [00:04<00:34,  2.23ba/s][1,14]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.09ba/s][1,15]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:37,  2.07ba/s][1,11]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:36,  2.15ba/s][1,5]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.31ba/s][1,6]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.34ba/s][1,8]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:36,  2.14ba/s][1,0]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.31ba/s][1,10]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:39,  1.96ba/s][1,7]<stderr>:#015 25%|██▌       | 22/88 [00:09<00:27,  2.40ba/s][1,4]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.33ba/s][1,12]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.04ba/s][1,13]<stderr>:#015 11%|█▏        | 10/88 [00:05<00:38,  2.02ba/s][1,3]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.28ba/s][1,2]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.26ba/s][1,1]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.16ba/s][1,9]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:35,  2.16ba/s][1,6]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.36ba/s][1,12]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.21ba/s][1,5]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.29ba/s][1,13]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:34,  2.21ba/s][1,0]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.29ba/s][1,7]<stderr>:#015 26%|██▌       | 23/88 [00:09<00:26,  2.42ba/s][1,4]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.35ba/s][1,3]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.38ba/s][1,14]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s][1,1]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.30ba/s][1,2]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:29,  2.31ba/s][1,15]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,11]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,8]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s][1,10]<stderr>:#015 12%|█▎        | 11/88 [00:05<00:41,  1.85ba/s][1,9]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:34,  2.20ba/s][1,6]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:28,  2.31ba/s][1,12]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.15ba/s][1,14]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.08ba/s][1,5]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:30,  2.18ba/s][1,4]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.27ba/s][1,15]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.07ba/s][1,7]<stderr>:#015 27%|██▋       | 24/88 [00:10<00:28,  2.28ba/s][1,3]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.23ba/s][1,11]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.09ba/s][1,2]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.22ba/s][1,0]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.12ba/s][1,13]<stderr>:#015 14%|█▎        | 12/88 [00:05<00:36,  2.06ba/s][1,1]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:30,  2.18ba/s][1,8]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:36,  2.07ba/s][1,10]<stderr>:#015 14%|█▎        | 12/88 [00:06<00:37,  2.03ba/s][1,9]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:34,  2.20ba/s][1,6]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.31ba/s][1,5]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.30ba/s][1,7]<stderr>:#015 28%|██▊       | 25/88 [00:10<00:26,  2.37ba/s][1,4]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:27,  2.33ba/s][1,1]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.32ba/s][1,0]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.25ba/s][1,14]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.12ba/s][1,12]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:34,  2.12ba/s][1,3]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.25ba/s][1,15]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:36,  2.08ba/s][1,8]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:35,  2.14ba/s][1,2]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.22ba/s][1,11]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:36,  2.08ba/s][1,10]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:37,  2.01ba/s][1,13]<stderr>:#015 15%|█▍        | 13/88 [00:06<00:39,  1.90ba/s][1,9]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:32,  2.28ba/s][1,7]<stderr>:#015 30%|██▉       | 26/88 [00:11<00:25,  2.40ba/s][1,6]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.27ba/s][1,5]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.20ba/s][1,1]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.28ba/s][1,3]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:27,  2.29ba/s][1,14]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:34,  2.12ba/s][1,8]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:33,  2.22ba/s][1,15]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:35,  2.11ba/s][1,11]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:34,  2.18ba/s][1,10]<stderr>:#015 16%|█▌        | 14/88 [00:07<00:33,  2.20ba/s][1,4]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.13ba/s][1,2]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.18ba/s][1,0]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.10ba/s][1,13]<stderr>:#015 16%|█▌        | 14/88 [00:06<00:35,  2.08ba/s][1,12]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:38,  1.88ba/s][1,9]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,5]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.20ba/s][1,3]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:27,  2.29ba/s][1,6]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:29,  2.14ba/s][1,7]<stderr>:#015 31%|███       | 27/88 [00:11<00:28,  2.16ba/s][1,1]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.22ba/s][1,13]<stderr>:#015 17%\u001b[0m\n",
      "\u001b[34m|█▋        | 15/88 [00:07<00:32,  2.25ba/s][1,15]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.08ba/s][1,14]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.05ba/s][1,12]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.16ba/s][1,4]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:29,  2.12ba/s][1,0]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.09ba/s][1,11]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,10]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.07ba/s][1,8]<stderr>:#015 17%|█▋        | 15/88 [00:07<00:35,  2.03ba/s][1,2]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.06ba/s][1,9]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.20ba/s][1,7]<stderr>:#015 32%|███▏      | 28/88 [00:12<00:26,  2.22ba/s][1,5]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.17ba/s][1,13]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:31,  2.27ba/s][1,3]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.16ba/s][1,4]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.19ba/s][1,1]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.15ba/s][1,6]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.05ba/s][1,0]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.20ba/s][1,11]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.19ba/s][1,15]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.13ba/s][1,10]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:32,  2.21ba/s][1,8]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:33,  2.18ba/s][1,14]<stderr>:#015 18%|█▊        | 16/88 [00:07<00:34,  2.07ba/s][1,12]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:33,  2.15ba/s][1,2]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:28,  2.17ba/s][1,9]<stderr>:#015 19%|█▉        | 17/88 [00:07<00:30,  2.34ba/s][1,15]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.22ba/s][1,5]<stderr>:#015 31%|███       | 27/88 [00:12<00:28,  2.14ba/s][1,1]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.21ba/s][1,11]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.25ba/s][1,10]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.27ba/s][1,8]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:31,  2.25ba/s][1,14]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:32,  2.20ba/s][1,3]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.18ba/s][1,4]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.21ba/s][1,2]<stderr>:#015 31%|███       | 27/88 [00:12<00:26,  2.29ba/s][1,0]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.22ba/s][1,6]<stderr>:#015 31%|███       | 27/88 [00:12<00:29,  2.09ba/s][1,7]<stderr>:#015 33%|███▎      | 29/88 [00:12<00:28,  2.09ba/s][1,13]<stderr>:#015 19%|█▉        | 17/88 [00:08<00:34,  2.08ba/s][1,12]<stderr>:#015 20%|██        | 18/88 [00:08<00:35,  1.95ba/s][1,9]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.21ba/s][1,5]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.18ba/s][1,13]<stderr>:#015 20%|██        | 18/88 [00:08<00:30,  2.29ba/s][1,1]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.19ba/s][1,3]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.19ba/s][1,0]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.21ba/s][1,15]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.17ba/s][1,4]<stderr>:#015 32%|███▏      | 28/88 [00:12<00:27,  2.19ba/s][1,14]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.16ba/s][1,2]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:26,  2.23ba/s][1,10]<stderr>:#015 20%|██        | 18/88 [00:08<00:31,  2.19ba/s][1,6]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.11ba/s][1,7]<stderr>:#015 34%|███▍      | 30/88 [00:13<00:27,  2.09ba/s][1,8]<stderr>:#015 20%|██        | 18/88 [00:08<00:32,  2.13ba/s][1,11]<stderr>:#015 20%|██        | 18/88 [00:08<00:33,  2.10ba/s][1,12]<stderr>:#015 22%|██▏       | 19/88 [00:08<00:31,  2.20ba/s][1,9]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.12ba/s][1,13]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:30,  2.29ba/s][1,14]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.21ba/s][1,10]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.23ba/s][1,11]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.22ba/s][1,8]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:31,  2.19ba/s][1,12]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:29,  2.27ba/s][1,15]<stderr>:#015 22%|██▏       | 19/88 [00:09<00:32,  2.12ba/s][1,5]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:28,  2.06ba/s][1,7]<stderr>:#015 35%|███▌      | 31/88 [00:13<00:28,  2.03ba/s][1,1]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.00ba/s][1,3]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  1.99ba/s][1,4]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  1.98ba/s][1,2]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.01ba/s][1,0]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.96ba/s][1,6]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:31,  1.85ba/s][1,9]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:32,  2.11ba/s][1,14]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.26ba/s][1,10]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:30,  2.25ba/s][1,12]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:28,  2.32ba/s][1,8]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.18ba/s][1,11]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:31,  2.18ba/s][1,7]<stderr>:#015 36%|███▋      | 32/88 [00:14<00:26,  2.08ba/s][1,15]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:33,  2.02ba/s][1,13]<stderr>:#015 23%|██▎       | 20/88 [00:09<00:33,  2.03ba/s][1,1]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.04ba/s][1,3]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.02ba/s][1,0]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.05ba/s][1,4]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.03ba/s][1,5]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.94ba/s][1,6]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.02ba/s][1,9]<stderr>:#015 24%|██▍       | 21/88 [00:09<00:30,  2.17ba/s][1,2]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:31,  1.85ba/s][1,14]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:29,  2.29ba/s][1,8]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:29,  2.25ba/s][1,10]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:30,  2.18ba/s][1,13]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:30,  2.17ba/s][1,15]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:31,  2.15ba/s][1,7]<stderr>:#015 38%|███▊      | 33/88 [00:14<00:26,  2.11ba/s][1,11]<stderr>:#015 24%|██▍       | 21/88 [00:10<00:32,  2.07ba/s][1,12]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.08ba/s][1,1]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,0]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,3]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.02ba/s][1,4]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.03ba/s][1,6]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.01ba/s][1,2]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.05ba/s][1,9]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:31,  2.11ba/s][1,14]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.26ba/s][1,5]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:33,  1.73ba/s][1,13]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:29,  2.26ba/s][1,12]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.23ba/s][1,8]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.06ba/s][1,10]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.02ba/s][1,11]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.05ba/s][1,15]<stderr>:#015 25%|██▌       | 22/88 [00:10<00:32,  2.04ba/s][1,7]<stderr>:#015 39%|███▊      | 34/88 [00:15<00:26,  2.05ba/s][1,1]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.95ba/s][1,9]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:29,  2.19ba/s][1,14]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:27,  2.33ba/s][1,4]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.94ba/s][1,0]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.89ba/s][1,6]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.89ba/s][1,2]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.90ba/s][1,13]<stderr>:#015 26%|██▌       | 23/88 [00:10<00:28,  2.24ba/s][1,15]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:29,  2.17ba/s][1,5]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:32,  1.74ba/s][1,7]<stderr>:#015 40%|███▉      | 35/88 [00:15<00:24,  2.16ba/s][1,11]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:30,  2.14ba/s][1,10]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:32,  2.02ba/s][1,12]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.05ba/s][1,3]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:34,  1.63ba/s][1,8]<stderr>:#015 26%|██▌       | 23/88 [00:11<00:32,  1.97ba/s][1,1]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:26,  2.06ba/s][1,14]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:27,  2.36ba/s][1,9]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:29,  2.14ba/s][1,4]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.93ba/s][1,0]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.92ba/s][1,6]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:28,  1.92ba/s][1,15]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:28,  2.22ba/s][1,5]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:29,  1.89ba/s][1,7]<stderr>:#015 41%|████      | 36/88 [00:16<00:23,  2.17ba/s][1,2]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:30,  1.81ba/s][1,10]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.08ba/s][1,8]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:30,  2.08ba/s][1,13]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.00ba/s][1,3]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:31,  1.76ba/s][1,11]<stderr>:#015 27%|██▋       | 24/88 [00:11<00:31,  2.04ba/s][1,1]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:25,  2.14ba/s][1,12]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:32,  1.96ba/s][1,14]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:28,  2.18ba/s][1,4]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.98ba/s][1,9]<stderr>:#015 28%|██▊       | 25/88 [00:11<00:30,  2.06ba/s][1,6]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.98ba/s][1,5]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.99ba/s][1,0]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:29,  1.86ba/s][1,15]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:28,  2.19ba/s][1,13]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.00ba/s][1,8]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:30,  2.05ba/s][1,7]<stderr>:#015 42%|████▏     | 37/88 [00:16<00:25,  2.01ba/s][1,10]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.01ba/s][1,1]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:25,  2.11ba/s][1,11]<stderr>:#015 28%|██▊       | 25/88 [00:12<00:31,  2.03ba/s][1,12]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.05ba/s][1,3]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:30,  1.80ba/s][1,2]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:30,  1.77ba/s][1,14]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.28ba/s][1,9]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.24ba/s][1,4]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:24,  2.13ba/s][1,6]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.01ba/s][1,15]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:27,  2.25ba/s][1,5]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.04ba/s][1,0]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:28,  1.84ba/s][1,11]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:29,  2.08ba/s][1,8]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.06ba/s][1,9]<stderr>:#015 31%|███       | 27/88 [00:12<00:26,  2.29ba/s][1,10]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:30,  2.03ba/s][1,14]<stderr>:#015 31%|███       | 27/88 [00:12<00:27,  2.19ba/s][1,13]<stderr>:#015 30%|██▉       | 26/88 [00:12<00:32,  1.91ba/s][1,7]<stderr>:#015 43%|████▎     | 38/88 [00:17<00:25,  1.93ba/s][1,3]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:29,  1.80ba/s][1,15]<stderr>:#015 31%|███       | 27/88 [00:12<00:25,  2.40ba/s][1,2]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:30,  1.73ba/s][1,12]<stderr>:#015 31%|███       | 27/88 [00:12<00:32,  1.86ba/s][1,4]<stderr>:#015 41%|████      | 36/88 [00:17<00:26,  1.93ba/s][1,6]<stderr>:#015 41%|████      | 36/88 [00:17<00:26,  1.98ba/s][1,1]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.79ba/s][1,0]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,9]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:26,  2.22ba/s][1,15]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:24,  2.50ba/s][1,13]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.02ba/s][1,14]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:27,  2.22ba/s][1,10]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.02ba/s][1,8]<stderr>:#015 31%|███       | 27/88 [00:13<00:30,  2.01ba/s][1,5]<stderr>:#015 41%|████      | 36/88 [00:17<00:29,  1.78ba/s][1,7]<stderr>:#015 44%|████▍     | 39/88 [00:17<00:24,  2.01ba/s][1,11]<stderr>:#015 31%|███       | 27/88 [00:13<00:31,  1.95ba/s][1,12]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:29,  2.04ba/s][1,3]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.84ba/s][1,1]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:26,  1.92ba/s][1,4]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.96ba/s][1,6]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,2]<stderr>:#015 41%|████      | 36/88 [00:17<00:31,  1.63ba/s][1,0]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,11]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.12ba/s][1,10]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:28,  2.08ba/s][1,8]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:29,  2.07ba/s][1,7]<stderr>:#015 45%|████▌     | 40/88 [00:18<00:23,  2.06ba/s][1,5]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:27,  1.85ba/s][1,15]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:26,  2.23ba/s][1,3]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.93ba/s][1,13]<stderr>:#015 32%|███▏      | 28/88 [00:13<00:30,  1.94ba/s][1,9]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:29,  2.00ba/s][1,14]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.96ba/s][1,12]<stderr>:#015 33%|███▎      | 29/88 [00:13<00:30,  1.92ba/s][1,4]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.96ba/s][1,1]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:26,  1.90ba/s][1,6]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.98ba/s][1,0]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.08ba/s][1,2]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:30,  1.68ba/s][1,5]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.02ba/s][1,8]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:29,  1.99ba/s][1,3]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.01ba/s][1,12]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.12ba/s][1,10]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:30,  1.97ba/s][1,14]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.12ba/s][1,9]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.08ba/s][1,13]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:29,  1.97ba/s][1,11]<stderr>:#015 33%|███▎      | 29/88 [00:14<00:30,  1.93ba/s][1,15]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:27,  2.13ba/s][1,7]<stderr>:#015 47%|████▋     | 41/88 [00:18<00:24,  1.93ba/s][1,2]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:26,  1.86ba/s][1,6]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:25,  1.91ba/s][1,4]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:26,  1.86ba/s][1,0]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:24,  1.98ba/s][1,5]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:24,  1.98ba/s][1,3]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:23,  2.07ba/s][1,1]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:28,  1.72ba/s][1,7]<stderr>:#015 48%|████▊     \u001b[0m\n",
      "\u001b[34m| 42/88 [00:19<00:22,  2.05ba/s][1,10]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.98ba/s][1,11]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.01ba/s][1,9]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:27,  2.06ba/s][1,15]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:26,  2.14ba/s][1,13]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:28,  2.01ba/s][1,8]<stderr>:#015 34%|███▍      | 30/88 [00:14<00:29,  1.95ba/s][1,14]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  2.02ba/s][1,12]<stderr>:#015 35%|███▌      | 31/88 [00:14<00:28,  1.98ba/s][1,6]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.00ba/s][1,2]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:26,  1.83ba/s][1,4]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.86ba/s][1,1]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.87ba/s][1,7]<stderr>:#015 49%|████▉     | 43/88 [00:19<00:20,  2.18ba/s][1,3]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:22,  2.14ba/s][1,5]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.03ba/s][1,11]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:27,  2.05ba/s][1,10]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:28,  2.02ba/s][1,0]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:26,  1.83ba/s][1,8]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:28,  2.00ba/s][1,14]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.91ba/s][1,13]<stderr>:#015 35%|███▌      | 31/88 [00:15<00:30,  1.84ba/s][1,9]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:30,  1.86ba/s][1,6]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.07ba/s][1,15]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:30,  1.85ba/s][1,4]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  1.97ba/s][1,2]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.91ba/s][1,12]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:32,  1.72ba/s][1,5]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.05ba/s][1,3]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.12ba/s][1,1]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:24,  1.92ba/s][1,7]<stderr>:#015 50%|█████     | 44/88 [00:19<00:20,  2.14ba/s][1,0]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  1.99ba/s][1,8]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:28,  1.93ba/s][1,10]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.92ba/s][1,9]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.97ba/s][1,14]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.99ba/s][1,15]<stderr>:#015 38%|███▊      | 33/88 [00:15<00:27,  1.99ba/s][1,13]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:29,  1.88ba/s][1,6]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:21,  2.09ba/s][1,11]<stderr>:#015 36%|███▋      | 32/88 [00:15<00:31,  1.78ba/s][1,12]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.90ba/s][1,4]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,5]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.08ba/s][1,0]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.06ba/s][1,1]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:23,  1.98ba/s][1,2]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:25,  1.88ba/s][1,3]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.05ba/s][1,7]<stderr>:#015 51%|█████     | 45/88 [00:20<00:22,  1.91ba/s][1,8]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.96ba/s][1,10]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:28,  1.93ba/s][1,9]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.02ba/s][1,14]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.02ba/s][1,15]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.03ba/s][1,12]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.05ba/s][1,11]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:29,  1.89ba/s][1,6]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.09ba/s][1,13]<stderr>:#015 38%|███▊      | 33/88 [00:16<00:29,  1.87ba/s][1,4]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.05ba/s][1,2]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.05ba/s][1,3]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:21,  2.08ba/s][1,5]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  1.99ba/s][1,7]<stderr>:#015 52%|█████▏    | 46/88 [00:20<00:20,  2.09ba/s][1,0]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:23,  1.90ba/s][1,8]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.00ba/s][1,1]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:25,  1.75ba/s][1,10]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:27,  1.95ba/s][1,12]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:25,  2.12ba/s][1,13]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:26,  2.06ba/s][1,15]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:26,  2.02ba/s][1,6]<stderr>:#015 50%|█████     | 44/88 [00:21<00:20,  2.12ba/s][1,9]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:27,  1.94ba/s][1,11]<stderr>:#015 39%|███▊      | 34/88 [00:16<00:28,  1.92ba/s][1,14]<stderr>:#015 40%|███▉      | 35/88 [00:16<00:27,  1.90ba/s][1,4]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.09ba/s][1,2]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  2.03ba/s][1,3]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.04ba/s][1,5]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.98ba/s][1,0]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.99ba/s][1,7]<stderr>:#015 53%|█████▎    | 47/88 [00:21<00:20,  2.05ba/s][1,8]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.99ba/s][1,10]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.99ba/s][1,1]<stderr>:#015 50%|█████     | 44/88 [00:21<00:24,  1.81ba/s][1,6]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.14ba/s][1,11]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:26,  1.97ba/s][1,13]<stderr>:#015 40%|███▉      | 35/88 [00:17<00:27,  1.90ba/s][1,2]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,15]<stderr>:#015 41%|████      | 36/88 [00:17<00:27,  1.88ba/s][1,9]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,4]<stderr>:#015 51%|█████     | 45/88 [00:21<00:21,  1.99ba/s][1,12]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,14]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.81ba/s][1,0]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.05ba/s][1,5]<stderr>:#015 51%|█████     | 45/88 [00:21<00:21,  2.01ba/s][1,3]<stderr>:#015 51%|█████     | 45/88 [00:21<00:22,  1.92ba/s][1,1]<stderr>:#015 51%|█████     | 45/88 [00:22<00:22,  1.93ba/s][1,7]<stderr>:#015 55%|█████▍    | 48/88 [00:22<00:21,  1.90ba/s][1,8]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,2]<stderr>:#015 51%|█████     | 45/88 [00:22<00:20,  2.13ba/s][1,15]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.99ba/s][1,9]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.97ba/s][1,10]<stderr>:#015 41%|████      | 36/88 [00:17<00:28,  1.85ba/s][1,14]<stderr>:#015 42%|████▏     | 37/88 [00:17<00:25,  1.98ba/s][1,13]<stderr>:#015 41%|████      | 36/88 [00:17<00:27,  1.92ba/s][1,6]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.93ba/s][1,11]<stderr>:#015 41%|████      | 36/88 [00:18<00:27,  1.88ba/s][1,12]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.90ba/s][1,4]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:23,  1.76ba/s][1,0]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.88ba/s][1,7]<stderr>:#015 56%|█████▌    | 49/88 [00:22<00:19,  1.95ba/s][1,5]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.86ba/s][1,3]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.87ba/s][1,1]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.91ba/s][1,6]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:19,  2.14ba/s][1,8]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:26,  1.92ba/s][1,15]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.03ba/s][1,9]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.01ba/s][1,13]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:25,  2.02ba/s][1,11]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:25,  2.00ba/s][1,14]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.04ba/s][1,12]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.07ba/s][1,10]<stderr>:#015 42%|████▏     | 37/88 [00:18<00:27,  1.86ba/s][1,2]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:20,  2.03ba/s][1,4]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:22,  1.85ba/s][1,5]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.94ba/s][1,7]<stderr>:#015 57%|█████▋    | 50/88 [00:23<00:19,  2.00ba/s][1,0]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.92ba/s][1,3]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:21,  1.95ba/s][1,6]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:18,  2.16ba/s][1,2]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:18,  2.19ba/s][1,8]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.99ba/s][1,11]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.06ba/s][1,10]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:24,  2.02ba/s][1,1]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.85ba/s][1,13]<stderr>:#015 43%|████▎     | 38/88 [00:18<00:25,  1.94ba/s][1,12]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.96ba/s][1,14]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.92ba/s][1,9]<stderr>:#015 44%|████▍     | 39/88 [00:18<00:27,  1.81ba/s][1,15]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:27,  1.81ba/s][1,4]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.95ba/s][1,7]<stderr>:#015 58%|█████▊    | 51/88 [00:23<00:17,  2.06ba/s][1,5]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.00ba/s][1,3]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.03ba/s][1,0]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.98ba/s][1,1]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.04ba/s][1,6]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.14ba/s][1,2]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.09ba/s][1,14]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.08ba/s][1,12]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.06ba/s][1,10]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.95ba/s][1,8]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.90ba/s][1,13]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:24,  1.97ba/s][1,15]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:24,  2.00ba/s][1,11]<stderr>:#015 44%|████▍     | 39/88 [00:19<00:25,  1.90ba/s][1,9]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:26,  1.84ba/s][1,4]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.02ba/s][1,5]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.04ba/s][1,3]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.04ba/s][1,0]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:19,  2.01ba/s][1,1]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.11ba/s][1,2]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:17,  2.24ba/s][1,7]<stderr>:#015 59%|█████▉    | 52/88 [00:24<00:18,  1.91ba/s][1,12]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.11ba/s][1,6]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.92ba/s][1,13]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:23,  2.02ba/s][1,11]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:24,  1.98ba/s][1,15]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:23,  2.01ba/s][1,9]<stderr>:#015 47%|████▋     | 41/88 [00:19<00:22,  2.05ba/s][1,8]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.90ba/s][1,10]<stderr>:#015 45%|████▌     | 40/88 [00:19<00:25,  1.92ba/s][1,14]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:24,  1.92ba/s][1,7]<stderr>:#015 60%|██████    | 53/88 [00:24<00:16,  2.09ba/s][1,4]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,3]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,6]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:17,  2.15ba/s][1,0]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.90ba/s][1,1]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.96ba/s][1,5]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.88ba/s][1,2]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:18,  2.08ba/s][1,12]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:21,  2.09ba/s][1,11]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:22,  2.06ba/s][1,13]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:22,  2.05ba/s][1,10]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:23,  1.98ba/s][1,15]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.01ba/s][1,9]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.04ba/s][1,8]<stderr>:#015 47%|████▋     | 41/88 [00:20<00:24,  1.90ba/s][1,14]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:25,  1.83ba/s][1,4]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:18,  2.04ba/s][1,7]<stderr>:#015 61%|██████▏   | 54/88 [00:24<00:16,  2.11ba/s][1,3]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:18,  2.01ba/s][1,6]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.10ba/s][1,2]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:17,  2.12ba/s][1,5]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.94ba/s][1,1]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.99ba/s][1,11]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.04ba/s][1,12]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  2.01ba/s][1,10]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,0]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:20,  1.80ba/s][1,8]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s][1,9]<stderr>:#015 49%|████▉     | 43/88 [00:20<00:22,  2.01ba/s][1,13]<stderr>:#015 48%|████▊     | 42/88 [00:20<00:24,  1.88ba/s][1,15]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:23,  1.91ba/s][1,4]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.05ba/s][1,14]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:24,  1.86ba/s][1,3]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.05ba/s][1,1]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.08ba/s][1,2]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:16,  2.14ba/s][1,5]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  2.00ba/s][1,0]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.01ba/s][1,7]<stderr>:#015 62%|██████▎   | 55/88 [00:25<00:17,  1.90ba/s][1,12]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,11]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  1.99ba/s][1,9]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.08ba/s][1,13]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:22,  2.03ba/s][1,6]<stderr>:#015 60%|██████    | 53/88 [00:25<00:19,  1.84ba/s][1,15]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.06ba/s][1,14]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.04ba/s][1,8]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:24,  1.81ba/s][1,7]<stderr>:#015 64%|██████▎   | 56/88 [00:25<00:15,  2.09ba/s][1,10]<stderr>:#015 49%|████▉     | 43/88 [00:21<00:26,  1.73ba/s][1,4]<stderr>:#015 60%|██████    | 53/88 [00:25<00:18,  1.87ba/s][1,2]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  2.00ba/s][1,6]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.06ba/s][1,5]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.93ba/s][1,1]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.92ba/s][1,0]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.93ba/s][1,12]<stderr>:#015 51%|███\u001b[0m\n",
      "\u001b[34m██     | 45/88 [00:21<00:21,  2.04ba/s][1,3]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.83ba/s][1,9]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.08ba/s][1,13]<stderr>:#015 50%|█████     | 44/88 [00:21<00:21,  2.07ba/s][1,11]<stderr>:#015 50%|█████     | 44/88 [00:21<00:22,  1.99ba/s][1,14]<stderr>:#015 51%|█████     | 45/88 [00:21<00:20,  2.08ba/s][1,8]<stderr>:#015 50%|█████     | 44/88 [00:22<00:22,  1.99ba/s][1,15]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  1.97ba/s][1,4]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.02ba/s][1,10]<stderr>:#015 50%|█████     | 44/88 [00:22<00:24,  1.80ba/s][1,7]<stderr>:#015 65%|██████▍   | 57/88 [00:26<00:15,  1.97ba/s][1,1]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,0]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,2]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.97ba/s][1,6]<stderr>:#015 62%|██████▎   | 55/88 [00:26<00:16,  2.00ba/s][1,5]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.89ba/s][1,3]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.87ba/s][1,12]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.95ba/s][1,8]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  2.03ba/s][1,11]<stderr>:#015 51%|█████     | 45/88 [00:22<00:22,  1.92ba/s][1,9]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.93ba/s][1,14]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:20,  2.00ba/s][1,10]<stderr>:#015 51%|█████     | 45/88 [00:22<00:21,  2.02ba/s][1,15]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:22,  1.87ba/s][1,13]<stderr>:#015 51%|█████     | 45/88 [00:22<00:23,  1.79ba/s][1,4]<stderr>:#015 62%|██████▎   | 55/88 [00:26<00:16,  1.97ba/s][1,7]<stderr>:#015 66%|██████▌   | 58/88 [00:27<00:15,  1.95ba/s][1,6]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,2]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.04ba/s][1,1]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,3]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.01ba/s][1,5]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,0]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.03ba/s][1,12]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:20,  1.97ba/s][1,14]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:20,  2.02ba/s][1,9]<stderr>:#015 53%|█████▎    | 47/88 [00:22<00:20,  1.95ba/s][1,13]<stderr>:#015 52%|█████▏    | 46/88 [00:22<00:21,  1.98ba/s][1,15]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:20,  1.96ba/s][1,11]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.82ba/s][1,4]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.05ba/s][1,8]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.80ba/s][1,3]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.12ba/s][1,7]<stderr>:#015 67%|██████▋   | 59/88 [00:27<00:14,  2.02ba/s][1,5]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,0]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.10ba/s][1,10]<stderr>:#015 52%|█████▏    | 46/88 [00:23<00:23,  1.78ba/s][1,1]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.97ba/s][1,2]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.97ba/s][1,12]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:19,  2.04ba/s][1,9]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.97ba/s][1,6]<stderr>:#015 65%|██████▍   | 57/88 [00:27<00:17,  1.78ba/s][1,14]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.99ba/s][1,15]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.98ba/s][1,8]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:21,  1.94ba/s][1,13]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.85ba/s][1,7]<stderr>:#015 68%|██████▊   | 60/88 [00:27<00:13,  2.07ba/s][1,11]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:23,  1.77ba/s][1,4]<stderr>:#015 65%|██████▍   | 57/88 [00:27<00:16,  1.90ba/s][1,12]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:18,  2.14ba/s][1,10]<stderr>:#015 53%|█████▎    | 47/88 [00:23<00:22,  1.82ba/s][1,0]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:15,  1.97ba/s][1,2]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:15,  1.98ba/s][1,3]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.85ba/s][1,1]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.84ba/s][1,5]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:17,  1.82ba/s][1,6]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.92ba/s][1,15]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.03ba/s][1,9]<stderr>:#015 56%|█████▌    | 49/88 [00:23<00:20,  1.92ba/s][1,13]<stderr>:#015 55%|█████▍    | 48/88 [00:23<00:20,  1.97ba/s][1,14]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:20,  1.92ba/s][1,8]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:20,  1.95ba/s][1,7]<stderr>:#015 69%|██████▉   | 61/88 [00:28<00:12,  2.15ba/s][1,4]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.96ba/s][1,2]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.09ba/s][1,10]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:20,  1.92ba/s][1,11]<stderr>:#015 55%|█████▍    | 48/88 [00:24<00:22,  1.79ba/s][1,0]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.02ba/s][1,3]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:15,  1.97ba/s][1,6]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:14,  1.98ba/s][1,12]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.93ba/s][1,5]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.87ba/s][1,1]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.85ba/s][1,15]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.99ba/s][1,8]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.01ba/s][1,14]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.86ba/s][1,11]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:20,  1.94ba/s][1,10]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:19,  2.02ba/s][1,9]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:20,  1.81ba/s][1,4]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:14,  2.01ba/s][1,2]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:13,  2.10ba/s][1,7]<stderr>:#015 70%|███████   | 62/88 [00:29<00:13,  1.95ba/s][1,3]<stderr>:#015 67%|██████▋   | 59/88 [00:28<00:13,  2.10ba/s][1,13]<stderr>:#015 56%|█████▌    | 49/88 [00:24<00:22,  1.73ba/s][1,0]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.90ba/s][1,5]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  1.95ba/s][1,12]<stderr>:#015 58%|█████▊    | 51/88 [00:24<00:19,  1.90ba/s][1,1]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.88ba/s][1,6]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.84ba/s][1,14]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.97ba/s][1,9]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.94ba/s][1,8]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:19,  1.93ba/s][1,15]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.89ba/s][1,7]<stderr>:#015 72%|███████▏  | 63/88 [00:29<00:12,  1.99ba/s][1,13]<stderr>:#015 57%|█████▋    | 50/88 [00:24<00:19,  1.95ba/s][1,2]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:13,  2.00ba/s][1,4]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.91ba/s][1,3]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.98ba/s][1,11]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:21,  1.81ba/s][1,10]<stderr>:#015 57%|█████▋    | 50/88 [00:25<00:20,  1.85ba/s][1,0]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.89ba/s][1,12]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:17,  2.00ba/s][1,5]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.90ba/s][1,6]<stderr>:#015 69%|██████▉   | 61/88 [00:29<00:13,  1.97ba/s][1,1]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.85ba/s][1,7]<stderr>:#015 73%|███████▎  | 64/88 [00:29<00:11,  2.11ba/s][1,15]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.95ba/s][1,8]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  1.97ba/s][1,9]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.93ba/s][1,14]<stderr>:#015 59%|█████▉    | 52/88 [00:25<00:18,  1.90ba/s][1,13]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s][1,10]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:18,  2.00ba/s][1,11]<stderr>:#015 58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s][1,2]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.00ba/s][1,3]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.01ba/s][1,4]<stderr>:#015 69%|██████▉   | 61/88 [00:29<00:14,  1.91ba/s][1,0]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.96ba/s][1,5]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.91ba/s][1,1]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.93ba/s][1,8]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.05ba/s][1,12]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.84ba/s][1,6]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.87ba/s][1,15]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  1.95ba/s][1,11]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.07ba/s][1,10]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:17,  2.07ba/s][1,7]<stderr>:#015 74%|███████▍  | 65/88 [00:30<00:11,  1.96ba/s][1,4]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.00ba/s][1,2]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.02ba/s][1,3]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.03ba/s][1,14]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.77ba/s][1,13]<stderr>:#015 59%|█████▉    | 52/88 [00:26<00:19,  1.82ba/s][1,0]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.00ba/s][1,9]<stderr>:#015 60%|██████    | 53/88 [00:26<00:20,  1.74ba/s][1,5]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.96ba/s][1,1]<stderr>:#015 70%|███████   | 62/88 [00:30<00:13,  1.93ba/s][1,12]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.89ba/s][1,15]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:16,  2.02ba/s][1,6]<stderr>:#015 72%|███████▏  | 63/88 [00:30<00:13,  1.88ba/s][1,7]<stderr>:#015 75%|███████▌  | 66/88 [00:30<00:10,  2.05ba/s][1,4]<stderr>:#015 72%|███████▏  | 63/88 [00:30<00:12,  2.03ba/s][1,8]<stderr>:#015 60%|██████    | 53/88 [00:26<00:19,  1.82ba/s][1,9]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:17,  1.91ba/s][1,11]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.92ba/s][1,13]<stderr>:#015 60%|██████    | 53/88 [00:26<00:17,  1.96ba/s][1,2]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.00ba/s][1,0]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:11,  2.10ba/s][1,10]<stderr>:#015 60%|██████    | 53/88 [00:26<00:18,  1.87ba/s][1,14]<stderr>:#015 61%|██████▏   | 54/88 [00:26<00:18,  1.84ba/s][1,3]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  1.93ba/s][1,5]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.04ba/s][1,1]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  1.99ba/s][1,12]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.92ba/s][1,15]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  1.95ba/s][1,7]<stderr>:#015 76%|███████▌  | 67/88 [00:31<00:10,  2.03ba/s][1,6]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.86ba/s][1,11]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:16,  2.01ba/s][1,13]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:16,  2.01ba/s][1,9]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  1.96ba/s][1,8]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:18,  1.88ba/s][1,10]<stderr>:#015 61%|██████▏   | 54/88 [00:27<00:17,  1.99ba/s][1,0]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:11,  2.03ba/s][1,2]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.92ba/s][1,5]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:11,  2.08ba/s][1,4]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.85ba/s][1,14]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:18,  1.82ba/s][1,3]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.90ba/s][1,12]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:16,  1.99ba/s][1,1]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.96ba/s][1,15]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.08ba/s][1,6]<stderr>:#015 74%|███████▍  | 65/88 [00:31<00:11,  1.98ba/s][1,7]<stderr>:#015 77%|███████▋  | 68/88 [00:31<00:09,  2.06ba/s][1,9]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:15,  2.01ba/s][1,11]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,13]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:16,  2.00ba/s][1,8]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.91ba/s][1,0]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  2.07ba/s][1,2]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.93ba/s][1,3]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.98ba/s][1,10]<stderr>:#015 62%|██████▎   | 55/88 [00:27<00:17,  1.89ba/s][1,4]<stderr>:#015 74%|███████▍  | 65/88 [00:31<00:12,  1.92ba/s][1,14]<stderr>:#015 64%|██████▎   | 56/88 [00:27<00:17,  1.79ba/s][1,5]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:12,  1.88ba/s][1,1]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.98ba/s][1,6]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.04ba/s][1,0]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:09,  2.20ba/s][1,15]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.90ba/s][1,8]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.01ba/s][1,11]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.01ba/s][1,10]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:15,  2.07ba/s][1,7]<stderr>:#015 78%|███████▊  | 69/88 [00:32<00:10,  1.89ba/s][1,12]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:18,  1.71ba/s][1,3]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.05ba/s][1,2]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.99ba/s][1,4]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.98ba/s][1,13]<stderr>:#015 64%|██████▎   | 56/88 [00:28<00:17,  1.87ba/s][1,9]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,5]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.88ba/s][1,1]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.96ba/s][1,6]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:10,  1.95ba/s][1,0]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:09,  2.19ba/s][1,7]<stderr>:#015 80%|███████▉  | 70/88 [00:32<00:08,  2.09ba/s][1,14]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:18,  1.70ba/s][1,15]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.87ba/s][1,9]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:14,  2.02ba/s][1,12]<stderr>:#015 66%|██████▌   | 58/88 [00:28<00:16,  1.84ba/s][1,4]<stderr>:#015 76%|███████▌  | 67/88 [00:32<00:10,  1.93ba/s][1,8]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:17,  1.78ba/s][1,10]<stderr>:#015 6\u001b[0m\n",
      "\u001b[34m5%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,13]<stderr>:#015 65%|██████▍   | 57/88 [00:28<00:16,  1.83ba/s][1,2]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.86ba/s][1,3]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.84ba/s][1,11]<stderr>:#015 65%|██████▍   | 57/88 [00:29<00:18,  1.69ba/s][1,0]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:08,  2.26ba/s][1,6]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.05ba/s][1,7]<stderr>:#015 81%|████████  | 71/88 [00:33<00:07,  2.14ba/s][1,5]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.78ba/s][1,14]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:17,  1.73ba/s][1,9]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  2.03ba/s][1,15]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.89ba/s][1,12]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.86ba/s][1,1]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:12,  1.72ba/s][1,4]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.01ba/s][1,2]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.01ba/s][1,3]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  2.00ba/s][1,10]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:15,  1.92ba/s][1,13]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:15,  1.92ba/s][1,8]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:16,  1.83ba/s][1,0]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:08,  2.21ba/s][1,11]<stderr>:#015 66%|██████▌   | 58/88 [00:29<00:17,  1.76ba/s][1,6]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:09,  1.95ba/s][1,5]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  1.86ba/s][1,14]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.82ba/s][1,7]<stderr>:#015 82%|████████▏ | 72/88 [00:33<00:08,  1.93ba/s][1,9]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:14,  1.94ba/s][1,1]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:11,  1.78ba/s][1,10]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:14,  2.03ba/s][1,3]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.07ba/s][1,2]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.02ba/s][1,4]<stderr>:#015 78%|███████▊  | 69/88 [00:33<00:09,  2.00ba/s][1,12]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:15,  1.80ba/s][1,0]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:07,  2.29ba/s][1,8]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.87ba/s][1,15]<stderr>:#015 68%|██████▊   | 60/88 [00:29<00:16,  1.73ba/s][1,11]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.91ba/s][1,13]<stderr>:#015 67%|██████▋   | 59/88 [00:29<00:15,  1.85ba/s][1,6]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.93ba/s][1,5]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.90ba/s][1,7]<stderr>:#015 83%|████████▎ | 73/88 [00:34<00:07,  1.90ba/s][1,14]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:15,  1.77ba/s][1,2]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.08ba/s][1,9]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  1.98ba/s][1,1]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.86ba/s][1,4]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.02ba/s][1,3]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.04ba/s][1,15]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.88ba/s][1,13]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:13,  2.01ba/s][1,10]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:14,  1.90ba/s][1,0]<stderr>:#015 81%|████████  | 71/88 [00:34<00:07,  2.16ba/s][1,12]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:15,  1.70ba/s][1,8]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:15,  1.79ba/s][1,11]<stderr>:#015 68%|██████▊   | 60/88 [00:30<00:16,  1.74ba/s][1,5]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.91ba/s][1,7]<stderr>:#015 84%|████████▍ | 74/88 [00:34<00:07,  2.00ba/s][1,6]<stderr>:#015 81%|████████  | 71/88 [00:34<00:09,  1.87ba/s][1,9]<stderr>:#015 70%|███████   | 62/88 [00:30<00:12,  2.05ba/s][1,14]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:14,  1.84ba/s][1,1]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.93ba/s][1,0]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:06,  2.33ba/s][1,4]<stderr>:#015 81%|████████  | 71/88 [00:34<00:08,  1.98ba/s][1,3]<stderr>:#015 81%|████████  | 71/88 [00:35<00:08,  1.99ba/s][1,13]<stderr>:#015 69%|██████▉   | 61/88 [00:30<00:13,  2.00ba/s][1,15]<stderr>:#015 70%|███████   | 62/88 [00:30<00:14,  1.85ba/s][1,2]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.84ba/s][1,12]<stderr>:#015 70%|███████   | 62/88 [00:30<00:14,  1.83ba/s][1,10]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.84ba/s][1,8]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.87ba/s][1,11]<stderr>:#015 69%|██████▉   | 61/88 [00:31<00:14,  1.89ba/s][1,9]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:11,  2.09ba/s][1,6]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.92ba/s][1,3]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:07,  2.16ba/s][1,4]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:07,  2.14ba/s][1,7]<stderr>:#015 85%|████████▌ | 75/88 [00:35<00:06,  1.93ba/s][1,14]<stderr>:#015 70%|███████   | 62/88 [00:31<00:13,  1.92ba/s][1,5]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.79ba/s][1,13]<stderr>:#015 70%|███████   | 62/88 [00:31<00:12,  2.06ba/s][1,0]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.05ba/s][1,2]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.96ba/s][1,15]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.90ba/s][1,12]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.87ba/s][1,10]<stderr>:#015 70%|███████   | 62/88 [00:31<00:13,  1.87ba/s][1,1]<stderr>:#015 81%|████████  | 71/88 [00:35<00:10,  1.66ba/s][1,8]<stderr>:#015 70%|███████   | 62/88 [00:31<00:14,  1.82ba/s][1,11]<stderr>:#015 70%|███████   | 62/88 [00:31<00:14,  1.85ba/s][1,14]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:12,  2.05ba/s][1,4]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.13ba/s][1,5]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.96ba/s][1,3]<stderr>:#015 83%|████████▎ | 73/88 [00:35<00:07,  2.12ba/s][1,9]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.94ba/s][1,2]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,6]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:08,  1.75ba/s][1,7]<stderr>:#015 86%|████████▋ | 76/88 [00:36<00:06,  1.77ba/s][1,15]<stderr>:#015 73%|███████▎  | 64/88 [00:31<00:12,  1.90ba/s][1,1]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.84ba/s][1,0]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.94ba/s][1,13]<stderr>:#015 72%|███████▏  | 63/88 [00:31<00:13,  1.88ba/s][1,10]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.91ba/s][1,8]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.87ba/s][1,12]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.75ba/s][1,5]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,11]<stderr>:#015 72%|███████▏  | 63/88 [00:32<00:13,  1.84ba/s][1,14]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:12,  1.97ba/s][1,9]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  2.07ba/s][1,3]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:06,  2.05ba/s][1,4]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:06,  2.03ba/s][1,1]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  2.00ba/s][1,0]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.08ba/s][1,7]<stderr>:#015 88%|████████▊ | 77/88 [00:36<00:05,  1.84ba/s][1,2]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.91ba/s][1,6]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:08,  1.74ba/s][1,13]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:12,  1.91ba/s][1,15]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:12,  1.87ba/s][1,10]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.81ba/s][1,9]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:10,  2.10ba/s][1,8]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.82ba/s][1,4]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.10ba/s][1,14]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.97ba/s][1,12]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:13,  1.76ba/s][1,5]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.95ba/s][1,3]<stderr>:#015 85%|████████▌ | 75/88 [00:36<00:06,  2.01ba/s][1,0]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.20ba/s][1,11]<stderr>:#015 73%|███████▎  | 64/88 [00:32<00:13,  1.79ba/s][1,6]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.92ba/s][1,2]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  2.03ba/s][1,7]<stderr>:#015 89%|████████▊ | 78/88 [00:37<00:05,  1.87ba/s][1,13]<stderr>:#015 74%|███████▍  | 65/88 [00:32<00:11,  1.94ba/s][1,15]<stderr>:#015 75%|███████▌  | 66/88 [00:32<00:11,  1.93ba/s][1,1]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.80ba/s][1,10]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:12,  1.87ba/s][1,4]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.07ba/s][1,11]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:11,  1.94ba/s][1,5]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  2.00ba/s][1,14]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.96ba/s][1,12]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:12,  1.81ba/s][1,8]<stderr>:#015 74%|███████▍  | 65/88 [00:33<00:13,  1.76ba/s][1,0]<stderr>:#015 88%|████████▊ | 77/88 [00:37<00:05,  2.10ba/s][1,2]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.12ba/s][1,3]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:06,  1.94ba/s][1,6]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.00ba/s][1,9]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.79ba/s][1,15]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:10,  1.93ba/s][1,1]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.87ba/s][1,10]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.97ba/s][1,7]<stderr>:#015 90%|████████▉ | 79/88 [00:37<00:05,  1.74ba/s][1,13]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.84ba/s][1,5]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:05,  2.08ba/s][1,4]<stderr>:#015 88%|████████▊ | 77/88 [00:37<00:05,  2.05ba/s][1,11]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:11,  1.93ba/s][1,2]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.12ba/s][1,9]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:10,  1.99ba/s][1,3]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.99ba/s][1,8]<stderr>:#015 75%|███████▌  | 66/88 [00:33<00:12,  1.83ba/s][1,6]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.99ba/s][1,14]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.83ba/s][1,15]<stderr>:#015 77%|███████▋  | 68/88 [00:33<00:09,  2.10ba/s][1,12]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:12,  1.68ba/s][1,0]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.91ba/s][1,1]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.91ba/s][1,13]<stderr>:#015 76%|███████▌  | 67/88 [00:33<00:11,  1.90ba/s][1,5]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.12ba/s][1,7]<stderr>:#015 91%|█████████ | 80/88 [00:38<00:04,  1.77ba/s][1,10]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.88ba/s][1,4]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.97ba/s][1,11]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.90ba/s][1,9]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.96ba/s][1,14]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,15]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  2.11ba/s][1,2]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.99ba/s][1,12]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:11,  1.82ba/s][1,0]<stderr>:#015 90%|████████▉ | 79/88 [00:38<00:04,  2.01ba/s][1,8]<stderr>:#015 76%|███████▌  | 67/88 [00:34<00:11,  1.76ba/s][1,6]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.90ba/s][1,1]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.07ba/s][1,3]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.84ba/s][1,13]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,7]<stderr>:#015 92%|█████████▏| 81/88 [00:38<00:03,  1.87ba/s][1,10]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.96ba/s][1,5]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.98ba/s][1,4]<stderr>:#015 90%|████████▉ | 79/88 [00:38<00:04,  2.03ba/s][1,2]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  2.10ba/s][1,11]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.92ba/s][1,9]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:09,  1.98ba/s][1,14]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:09,  1.94ba/s][1,15]<stderr>:#015 80%|███████▉  | 70/88 [00:34<00:08,  2.09ba/s][1,8]<stderr>:#015 77%|███████▋  | 68/88 [00:34<00:10,  1.88ba/s][1,12]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.87ba/s][1,0]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.99ba/s][1,6]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.93ba/s][1,7]<stderr>:#015 93%|█████████▎| 82/88 [00:39<00:02,  2.05ba/s][1,3]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:05,  1.79ba/s][1,4]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.11ba/s][1,1]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.82ba/s][1,10]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.96ba/s][1,5]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  2.02ba/s][1,13]<stderr>:#015 78%|███████▊  | 69/88 [00:34<00:10,  1.87ba/s][1,2]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.21ba/s][1,11]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.99ba/s][1,14]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.94ba/s][1,8]<stderr>:#015 78%|███████▊  | 69/88 [00:35<00:09,  1.91ba/s][1,12]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.86ba/s][1,9]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.85ba/s][1,6]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.91ba/s][1,3]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:04,  1.90ba/s][1,15]<stderr>:#015 81%|████████  | 71/88 [00:35<00:08,  1.90ba/s][1,7]<stderr>:#015 94%|█████████▍| 83/88 [00:39<00:02,  2.01ba/s][1,0]<stderr>:#015 92%|█████████▏| 81/88 [00:39<00:03,  1.85ba/s][1,1]<stderr>:#015 90%|██�\u001b[0m\n",
      "\u001b[34m��█████▉ | 79/88 [00:39<00:04,  1.89ba/s][1,5]<stderr>:#015 91%|█████████ | 80/88 [00:39<00:03,  2.03ba/s][1,10]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.97ba/s][1,11]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:08,  2.03ba/s][1,13]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:10,  1.79ba/s][1,2]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  2.00ba/s][1,4]<stderr>:#015 92%|█████████▏| 81/88 [00:39<00:03,  1.88ba/s][1,9]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.88ba/s][1,7]<stderr>:#015 95%|█████████▌| 84/88 [00:40<00:01,  2.04ba/s][1,14]<stderr>:#015 81%|████████  | 71/88 [00:35<00:09,  1.82ba/s][1,0]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.93ba/s][1,15]<stderr>:#015 82%|████████▏ | 72/88 [00:35<00:08,  1.94ba/s][1,8]<stderr>:#015 80%|███████▉  | 70/88 [00:35<00:09,  1.82ba/s][1,1]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.05ba/s][1,6]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.87ba/s][1,3]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.88ba/s][1,12]<stderr>:#015 81%|████████  | 71/88 [00:36<00:09,  1.78ba/s][1,10]<stderr>:#015 81%|████████  | 71/88 [00:36<00:08,  1.93ba/s][1,2]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:02,  2.07ba/s][1,4]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.94ba/s][1,5]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.85ba/s][1,11]<stderr>:#015 81%|████████  | 71/88 [00:36<00:08,  1.91ba/s][1,13]<stderr>:#015 81%|████████  | 71/88 [00:36<00:09,  1.80ba/s][1,7]<stderr>:#015 97%|█████████▋| 85/88 [00:40<00:01,  2.09ba/s][1,15]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.97ba/s][1,6]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.96ba/s][1,9]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.88ba/s][1,12]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.89ba/s][1,3]<stderr>:#015 93%|█████████▎| 82/88 [00:40<00:03,  1.94ba/s][1,0]<stderr>:#015 94%|█████████▍| 83/88 [00:40<00:02,  1.92ba/s][1,1]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.94ba/s][1,14]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:09,  1.70ba/s][1,10]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.96ba/s][1,8]<stderr>:#015 81%|████████  | 71/88 [00:36<00:10,  1.67ba/s][1,5]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.95ba/s][1,2]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.02ba/s][1,11]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.97ba/s][1,13]<stderr>:#015 82%|████████▏ | 72/88 [00:36<00:08,  1.88ba/s][1,4]<stderr>:#015 94%|█████████▍| 83/88 [00:40<00:02,  1.92ba/s][1,12]<stderr>:#015 83%|████████▎ | 73/88 [00:36<00:07,  1.97ba/s][1,6]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.96ba/s][1,0]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.96ba/s][1,1]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:02,  2.06ba/s][1,9]<stderr>:#015 84%|████████▍ | 74/88 [00:36<00:07,  1.87ba/s][1,3]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.92ba/s][1,7]<stderr>:#015 98%|█████████▊| 86/88 [00:41<00:01,  1.91ba/s][1,15]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.81ba/s][1,8]<stderr>:#015 82%|████████▏ | 72/88 [00:37<00:08,  1.82ba/s][1,5]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.07ba/s][1,14]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.76ba/s][1,11]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:07,  2.07ba/s][1,4]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:01,  2.01ba/s][1,2]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.95ba/s][1,10]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.75ba/s][1,13]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.82ba/s][1,9]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.99ba/s][1,1]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  2.10ba/s][1,12]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.92ba/s][1,3]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.99ba/s][1,6]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:02,  1.95ba/s][1,7]<stderr>:#015 99%|█████████▉| 87/88 [00:41<00:00,  1.97ba/s][1,5]<stderr>:#015 95%|█████████▌| 84/88 [00:41<00:01,  2.08ba/s][1,15]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:07,  1.84ba/s][1,8]<stderr>:#015 83%|████████▎ | 73/88 [00:37<00:08,  1.86ba/s][1,0]<stderr>:#015 97%|█████████▋| 85/88 [00:41<00:01,  1.81ba/s][1,4]<stderr>:#015 97%|█████████▋| 85/88 [00:41<00:01,  2.01ba/s][1,14]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:08,  1.70ba/s][1,11]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.91ba/s][1,7]<stderr>:#015100%|██████████| 88/88 [00:42<00:00,  2.18ba/s][1,7]<stderr>:#015100%|██████████| 88/88 [00:42<00:00,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.88ba/s][1,13]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:07,  1.87ba/s][1,7]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,1]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:01,  2.13ba/s][1,10]<stderr>:#015 84%|████████▍ | 74/88 [00:37<00:08,  1.74ba/s][1,12]<stderr>:#015 85%|████████▌ | 75/88 [00:37<00:06,  1.98ba/s][1,9]<stderr>:#015 86%|████████▋ | 76/88 [00:37<00:06,  1.92ba/s][1,6]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.93ba/s][1,3]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.93ba/s][1,0]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:01,  1.96ba/s][1,15]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.92ba/s][1,4]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.09ba/s][1,5]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.92ba/s][1,8]<stderr>:#015 84%|████████▍ | 74/88 [00:38<00:07,  1.78ba/s][1,11]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.95ba/s][1,13]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.97ba/s][1,14]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:07,  1.74ba/s][1,2]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:01,  1.89ba/s][1,9]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.05ba/s][1,12]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:05,  2.04ba/s][1,10]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.86ba/s][1,6]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.02ba/s][1,1]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  2.03ba/s][1,3]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.05ba/s][1,0]<stderr>:#015 99%|█████████▉| 87/88 [00:42<00:00,  2.08ba/s][1,4]<stderr>:#015 99%|█████████▉| 87/88 [00:42<00:00,  2.27ba/s][1,15]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  1.97ba/s][1,5]<stderr>:#015 98%|█████████▊| 86/88 [00:42<00:00,  2.05ba/s][1,8]<stderr>:#015 85%|████████▌ | 75/88 [00:38<00:06,  1.90ba/s][1,2]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.03ba/s][1,11]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.97ba/s][1,13]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.99ba/s][1,6]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.14ba/s][1,3]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.16ba/s][1,0]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.17ba/s][1,0]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.94ba/s][1,14]<stderr>:#015 86%|████████▋ | 76/88 [00:38<00:06,  1.77ba/s][1,4]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.39ba/s][1,4]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.05ba/s][1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:00,  2.10ba/s][1,12]<stderr>:#015 88%|████████▊ | 77/88 [00:38<00:05,  2.03ba/s][1,0]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,4]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,5]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.24ba/s][1,9]<stderr>:#015 89%|████████▊ | 78/88 [00:38<00:05,  1.91ba/s][1,2]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.21ba/s][1,2]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,15]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.84ba/s][1,8]<stderr>:#015 86%|████████▋ | 76/88 [00:39<00:06,  1.90ba/s][1,3]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.28ba/s][1,3]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  2.02ba/s][1,6]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.20ba/s][1,6]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,14]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.93ba/s][1,10]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  2.05ba/s][1,6]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,1]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  2.17ba/s][1,5]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.41ba/s][1,5]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.02ba/s][1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.89ba/s][1,5]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,9]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.88ba/s][1,12]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.84ba/s][1,1]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.30ba/s][1,1]<stderr>:#015100%|██████████| 88/88 [00:43<00:00,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 90%|████████▉ | 79/88 [00:39<00:04,  1.89ba/s][1,1]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,8]<stderr>:#015 88%|████████▊ | 77/88 [00:39<00:05,  1.90ba/s][1,13]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:04,  2.01ba/s][1,14]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.87ba/s][1,11]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.89ba/s][1,7]<stderr>:#015  9%|▉         | 1/11 [00:02<00:20,  2.02s/ba][1,10]<stderr>:#015 89%|████████▊ | 78/88 [00:39<00:05,  1.89ba/s][1,12]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.91ba/s][1,15]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.97ba/s][1,9]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.82ba/s][1,13]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  2.02ba/s][1,11]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.95ba/s][1,10]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  2.00ba/s][1,8]<stderr>:#015 89%|████████▊ | 78/88 [00:40<00:05,  1.80ba/s][1,12]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.97ba/s][1,14]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:05,  1.74ba/s][1,0]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.79s/ba][1,15]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.89ba/s][1,4]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.80s/ba][1,9]<stderr>:#015 92%|█████████▏| 81/88 [00:40<00:03,  1.78ba/s][1,11]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.03ba/s][1,10]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:03,  2.08ba/s][1,8]<stderr>:#015 90%|████████▉ | 79/88 [00:40<00:04,  1.90ba/s][1,2]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.71s/ba][1,3]<stderr>:#015  9%|▉         | 1/11 [00:01<00:16,  1.66s/ba][1,14]<stderr>:#015 91%|█████████ | 80/88 [00:41<00:04,  1.84ba/s][1,5]<stderr>:#015  9%|▉         | 1/11 [00:01<00:16,  1.61s/ba][1,6]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.74s/ba][1,13]<stderr>:#015 91%|█████████ | 80/88 [00:40<00:04,  1.74ba/s][1,12]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.83ba/s][1,15]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.94ba/s][1,9]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.86ba/s][1,8]<stderr>:#015 91%|█████████ | 80/88 [00:41<00:04,  1.93ba/s][1,7]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:16,  1.85s/ba][1,10]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.94ba/s][1,11]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.89ba/s][1,14]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.84ba/s][1,13]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.83ba/s][1,1]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.87s/ba][1,12]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.89ba/s][1,15]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.93ba/s][1,9]<stderr>:#015 94%|█████████▍| 83/88 [00:41<00:02,  1.88ba/s][1,8]<stderr>:#015 92%|█████████▏| 81/88 [00:41<00:03,  1.85ba/s][1,11]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.89ba/s][1,10]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.84ba/s][1,14]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.82ba/s][1,12]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.94ba/s][1,13]<stderr>:#015 93%|█████████▎| 82/88 [00:41<00:03,  1.77ba/s][1,4]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.70s/ba][1,9]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.95ba/s][1,15]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.96ba/s][1,0]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,2]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.62s/ba][1,3]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.57s/ba][1,5]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:13,  1.54s/ba][1,8]<stderr>:#015 93%|█████████▎| 82/88 [00:42<00:03,  1.87ba/s][1,10]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.91ba/s][1,11]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.84ba/s][1,6]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.67s/ba][1,12]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:01,  2.01ba/s][1,14]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.86ba/s][1,13]<stderr>:#015 94%|█████████▍| 83/88 [00:42<00:02,  1.77ba/s][1,7]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.72s/ba][1,9]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.84ba/s][1,15]<stderr>:#015 97%|█████████▋| 85/88 [00:42<00:01,  1.76ba/s][1,10]<stderr>:#015 95%|█████████▌| 84/88 [00:42<00:02,  1.97ba/s][1,11]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.88ba/s][1,14]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.99ba/s][1,8]<stderr>:#015 94%|█████████▍| 83/88 [00:43<00:02,  1.78ba/s][1,1]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.78s/ba][1,13]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.81ba/s][1,12]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.73ba/s][1,15]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.88ba/s][1,9]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.83ba/s][1,8]<stderr>:#015 95%|█████████▌| 84/88 [00:43<00:02,  1.96ba/s][1,14]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.96ba/s][1,10]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.84ba/s][1,11]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.84ba/s][1,13]<stderr>:#015 97%|█████████▋| 85/88 [00:43<00:01,  1.97ba/s][1,9]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  1.92ba/s][1,15]<stderr>:#015 99%|█████████▉| 87/88 [00:43<00:00,  1.90ba/s][1,12]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:01,  1.76ba/s][1,4]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.72s/ba][1,3]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:12,  1.60s/ba][1,0]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.72s/ba][1,2]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.65s/ba][1,11]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.94ba/s][1,8]<stderr>:#015 97%|█████████▋| 85/88 [00:44<00:01,  1.88ba/s][1,13]<stderr>:#015 98%|█████████▊| 86/88 [00:43<00:00,  2.10ba/s][1,5]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:12,  1.57s/ba][1,10]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.88ba/s][1,14]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.86ba/s][1,9]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.11ba/s][1,9]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.99ba/s][1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,6]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.70s/ba][1,15]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.93ba/s][1,15]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.70s/ba][1,15]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,12]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.77ba/s][1,8]<stderr>:#015 98%|█████████▊| 86/88 [00:44<00:01,  1.98ba/s][1,14]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  2.04ba/s][1,13]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  2.00ba/s][1,11]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.85ba/s][1,10]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.84ba/s][1,13]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.39ba/s][1,13]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,12]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.91ba/s][1,12]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,12]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.21ba/s][1,14]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,14]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,14]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,11]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  2.06ba/s][1,11]<stderr>:#015100%|██████████| 88/88 [00:44<00:00,  1.96ba/s][1,11]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 87/88 [00:44<00:00,  1.97ba/s][1,1]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:14,  1.80s/ba][1,11]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,10]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.95ba/s][1,10]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.95ba/s][1,10]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,8]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  2.14ba/s][1,8]<stderr>:#015100%|██████████| 88/88 [00:45<00:00,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/11 [00:00<?, ?ba/s][1,0]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,3]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.59s/ba][1,4]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.68s/ba][1,5]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:10,  1.57s/ba][1,2]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.64s/ba][1,6]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,9]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.83s/ba][1,15]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.81s/ba][1,7]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.78s/ba][1,13]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.74s/ba][1,12]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.72s/ba][1,1]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:12,  1.77s/ba][1,14]<stderr>:#015  9%|▉         | 1/11 [00:01<00:18,  1.80s/ba][1,10]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.75s/ba][1,11]<stderr>:#015  9%|▉         | 1/11 [00:01<00:19,  1.99s/ba][1,8]<stderr>:#015  9%|▉         | 1/11 [00:01<00:17,  1.71s/ba][1,0]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,9]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,3]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.72s/ba][1,4]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.82s/ba][1,15]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,2]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,5]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.75s/ba][1,6]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.78s/ba][1,13]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,12]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,14]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:15,  1.71s/ba][1,7]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.81s/ba][1,10]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.66s/ba][1,11]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:16,  1.83s/ba][1,8]<stderr>:#015 18%|█▊        | 2/11 [00:03<00:14,  1.64s/ba][1,1]<stderr>:#015 45%|████▌     | 5/11 [00:09<00:11,  1.90s/ba][1,0]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,9]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.73s/ba][1,3]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,15]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:13,  1.71s/ba][1,4]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.83s/ba][1,2]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.79s/ba][1,5]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.76s/ba][1,6]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,13]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.69s/ba][1,12]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,14]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.69s/ba][1,7]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.80s/ba][1,10]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,11]<stderr>:#015 27%|██▋       | 3/11 [00:05<00:14,  1.79s/ba][1,8]<stderr>:#015 27%|██▋       | 3/11 [00:04<00:13,  1.67s/ba][1,1]<stderr>:#015 55%|█████▍    | 6/11 [00:11<00:09,  1.90s/ba][1,9]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.69s/ba][1,15]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.66s/ba][1,0]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.75s/ba][1,3]<stderr>:#015 64%|██████▎   | 7/11 [00:11<00:07,  1.77s/ba][1,4]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.83s/ba][1,2]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.80s/ba][1,5]<stderr>:#015 64%|██████▎   | 7/11 [00:11<00:07,  1.78s/ba][1,14]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,13]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.67s/ba][1,12]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.66s/ba][1,6]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.79s/ba][1,10]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,11]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.71s/ba][1,7]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.81s/ba][1,8]<stderr>:#015 36%|███▋      | 4/11 [00:06<00:11,  1.65s/ba][1,1]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.87s/ba][1,0]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.71s/ba][1,3]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,15]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,9]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.81s/ba][1,2]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.76s/ba][1,4]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.79s/ba][1,5]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.77s/ba][1,6]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.74s/ba][1,13]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.80s/ba][1,7]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,14]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,12]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.79s/ba][1,10]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.75s/ba][1,11]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:11,  1.85s/ba][1,8]<stderr>:#015 45%|████▌     | 5/11 [00:08<00:10,  1.77s/ba][1,0]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.69s/ba][1,1]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.83s/ba][1,3]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,2]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.73s/ba][1,4]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,9]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,15]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.78s/ba][1,6]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.71s/ba][1,5]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,7]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.76s/ba][1,13]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,14]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.80s/ba][1,12]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.81s/ba][1,10]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.77s/ba][1,11]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:09,  1.86s/ba][1,8]<stderr>:#015 55%|█████▍    | 6/11 [00:10<00:08,  1.79s/ba][1,0]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.69s/ba][1,1]<stderr>:#015 82%|████████▏ | 9/11 [00:16<00:03,  1.80s/ba][1,7]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.52s/ba][1,7]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.72s/ba][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 4.51kB [00:00, 4.06MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.76s/ba][1,15]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.77s/ba][1,9]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.79s/ba][1,4]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.78s/ba][1,5]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,6]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.73s/ba][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 3.31kB [00:00, 1.61MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.46s/ba][1,0]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.63s/ba]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.78s/ba][1,10]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.77s/ba][1,14]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.82s/ba][1,2]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,2]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.65s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.83s/ba][1,3]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.51s/ba][1,3]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:398] 2021-09-05 07:59:21,316 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:415] 2021-09-05 07:59:21,316 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,5]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.52s/ba][1,4]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.69s/ba]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,6]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.85s/ba][1,8]<stderr>:#015 64%|██████▎   | 7/11 [00:12<00:07,  1.81s/ba][1,1]<stderr>:#015 91%|█████████ | 10/11 [00:18<00:01,  1.79s/ba][1,15]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,9]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.76s/ba][1,13]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.75s/ba][1,1]<stderr>:#015100%|██████████| 11/11 [00:19<00:00,  1.54s/ba][1,1]<stderr>:#015100%|██████████| 11/11 [00:19<00:00,  1.73s/ba]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.77s/ba][1,14]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.80s/ba][1,12]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.81s/ba][1,11]<stderr>:#015 73%|███████▎  | 8/11 [00:14<00:05,  1.80s/ba][1,8]<stderr>:#015 73%|███████▎  | 8/11 [00:13<00:05,  1.78s/ba][1,15]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.72s/ba][1,9]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.73s/ba][1,13]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.72s/ba][1,10]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.74s/ba][1,14]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,12]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.76s/ba][1,11]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.77s/ba][1,8]<stderr>:#015 82%|████████▏ | 9/11 [00:15<00:03,  1.75s/ba][1,15]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.71s/ba][1,9]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.74s/ba][1,13]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.70s/ba][1,10]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.72s/ba][1,15]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.45s/ba][1,15]<stderr>:#015100%|██████████| 11/11 [00:17<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,14]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.77s/ba][1,9]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,9]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba][1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 4.51kB [00:00, 2.08MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s][1,15]<stderr>:#015Downloading: 3.31kB [00:00, 2.70MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.75s/ba][1,8]<stderr>:#015 91%|█████████ | 10/11 [00:17<00:01,  1.73s/ba][1,13]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.46s/ba][1,13]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.65s/ba]\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.47s/ba][1,10]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.49s/ba][1,12]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.51s/ba][1,14]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.50s/ba][1,11]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.69s/ba]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.48s/ba][1,8]<stderr>:#015100%|██████████| 11/11 [00:18<00:00,  1.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:398] 2021-09-05 07:59:27,951 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:415] 2021-09-05 07:59:27,951 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,580 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,580 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,581 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,581 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,581 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,582 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s][1,8]<stderr>:[INFO|trainer.py:1156] 2021-09-05 07:59:28,617 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1157] 2021-09-05 07:59:28,617 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1158] 2021-09-05 07:59:28,618 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1159] 2021-09-05 07:59:28,618 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1160] 2021-09-05 07:59:28,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1161] 2021-09-05 07:59:28,618 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:1162] 2021-09-05 07:59:28,618 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 1/100 [00:03<06:31,  3.96s/it][1,8]<stderr>:#015  1%|          | 1/100 [00:03<06:28,  3.92s/it][1,8]<stderr>:#015  2%|▏         | 2/100 [00:05<05:21,  3.28s/it][1,0]<stderr>:#015  2%|▏         | 2/100 [00:05<05:24,  3.31s/it][1,8]<stderr>:#015  3%|▎         | 3/100 [00:07<04:21,  2.70s/it][1,0]<stderr>:#015  3%|▎         | 3/100 [00:07<04:24,  2.73s/it][1,8]<stderr>:#015  4%|▍         | 4/100 [00:08<03:56,  2.47s/it][1,0]<stderr>:#015  4%|▍         | 4/100 [00:09<03:57,  2.48s/it][1,0]<stderr>:#015  5%|▌         | 5/100 [00:10<03:29,  2.21s/it][1,8]<stderr>:#015  5%|▌         | 5/100 [00:10<03:30,  2.21s/it][1,8]<stderr>:#015  6%|▌         | 6/100 [00:11<03:05,  1.97s/it][1,0]<stderr>:#015  6%|▌         | 6/100 [00:12<03:06,  1.98s/it][1,8]<stderr>:#015  7%|▋         | 7/100 [00:13<02:51,  1.85s/it][1,0]<stderr>:#015  7%|▋         | 7/100 [00:13<02:53,  1.86s/it][1,0]<stderr>:#015  8%|▊         | 8/100 [00:15<02:40,  1.74s/it][1,8]<stderr>:#015  8%|▊         | 8/100 [00:15<02:42,  1.76s/it][1,0]<stderr>:#015  9%|▉         | 9/100 [00:16<02:40,  1.76s/it][1,8]<stderr>:#015  9%|▉         | 9/100 [00:16<02:41,  1.78s/it][1,8]<stderr>:#015 10%|█         | 10/100 [00:18<02:31,  1.68s/it][1,0]<stderr>:#015 10%|█         | 10/100 [00:18<02:32,  1.69s/it][1,8]<stderr>:#015 11%|█         | 11/100 [00:19<02:18,  1.56s/it][1,0]<stderr>:#015 11%|█         | 11/100 [00:19<02:20,  1.58s/it][1,0]<stderr>:#015 12%|█▏        | 12/100 [00:21<02:19,  1.58s/it][1,8]<stderr>:#015 12%|█▏        | 12/100 [00:21<02:20,  1.59s/it][1,8]<stderr>:#015 13%|█▎        | 13/100 [00:22<02:06,  1.46s/it][1,0]<stderr>:#015 13%|█▎        | 13/100 [00:22<02:10,  1.50s/it][1,8]<stderr>:#015 14%|█▍        | 14/100 [00:24<02:11,  1.53s/it][1,0]<stderr>:#015 14%|█▍        | 14/100 [00:24<02:10,  1.52s/it][1,8]<stderr>:#015 15%|█▌        | 15/100 [00:25<02:11,  1.55s/it][1,0]<stderr>:#015 15%|█▌        | 15/100 [00:25<02:12,  1.55s/it][1,0]<stderr>:#015 16%|█▌        | 16/100 [00:27<02:02,  1.45s/it][1,8]<stderr>:#015 16%|█▌        | 16/100 [00:27<02:08,  1.53s/it][1,8]<stderr>:#015 17%|█▋        | 17/100 [00:28<02:03,  1.49s/it][1,0]<stderr>:#015 17%|█▋        | 17/100 [00:28<02:08,  1.55s/it][1,0]<stderr>:#015 18%|█▊        | 18/100 [00:30<02:00,  1.47s/it][1,8]<stderr>:#015 18%|█▊        | 18/100 [00:30<02:01,  1.49s/it][1,0]<stderr>:#015 19%|█▉        | 19/100 [00:31<02:06,  1.56s/it][1,8]<stderr>:#015 19%|█▉        | 19/100 [00:31<02:07,  1.58s/it][1,0]<stderr>:#015 20%|██        | 20/100 [00:33<02:06,  1.59s/it][1,8]<stderr>:#015 20%|██        | 20/100 [00:33<02:06,  1.58s/it][1,0]<stderr>:#015 21%|██        | 21/100 [00:34<02:00,  1.53s/it][1,8]<stderr>:#015 21%|██        | 21/100 [00:34<02:01,  1.53s/it][1,8]<stderr>:#015 22%|██▏       | 22/100 [00:36<01:57,  1.51s/it][1,0]<stderr>:#015 22%|██▏       | 22/100 [00:36<02:02,  1.57s/it][1,8]<stderr>:#015 23%|██▎       | 23/100 [00:38<02:01,  1.58s/it][1,0]<stderr>:#015 23%|██▎       | 23/100 [00:38<02:00,  1.57s/it][1,8]<stderr>:#015 24%|██▍       | 24/100 [00:40<02:07,  1.68s/it][1,0]<stderr>:#015 24%|██▍       | 24/100 [00:40<02:08,  1.69s/it][1,8]<stderr>:#015 25%|██▌       | 25/100 [00:41<02:01,  1.62s/it][1,0]<stderr>:#015 25%|██▌       | 25/100 [00:41<02:00,  1.60s/it][1,8]<stderr>:#015 26%|██▌       | 26/100 [00:43<01:58,  1.61s/it][1,0]<stderr>:#015 26%|██▌       | 26/100 [00:43<01:58,  1.60s/it][1,8]<stderr>:#015 27%|██▋       | 27/100 [00:44<01:59,  1.64s/it][1,0]<stderr>:#015 27%|██▋       | 27/100 [00:44<02:00,  1.66s/it][1,0]<stderr>:#015 28%|██▊       | 28/100 [00:46<01:57,  1.63s/it][1,8]<stderr>:#015 28%|██▊       | 28/100 [00:46<01:58,  1.64s/it][1,8]<stderr>:#015 29%|██▉       | 29/100 [00:47<01:53,  1.59s/it][1,0]<stderr>:#015 29%|██▉       | 29/100 [00:48<01:54,  1.61s/it][1,8]<stderr>:#015 30%|███       | 30/100 [00:49<01:52,  1.61s/it][1,0]<stderr>:#015 30%|███       | 30/100 [00:49<01:53,  1.62s/it][1,8]<stderr>:#015 31%|███       | 31/100 [00:51<01:47,  1.56s/it][1,0]<stderr>:#015 31%|███       | 31/100 [00:51<01:47,  1.55s/it][1,8]<stderr>:#015 32%|███▏      | 32/100 [00:52<01:40,  1.48s/it][1,0]<stderr>:#015 32%|███▏      | 32/100 [00:52<01:39,  1.47s/it][1,8]<stderr>:#015 33%|███▎      | 33/100 [00:53<01:37,  1.46s/it][1,0]<stderr>:#015 33%|███▎      | 33/100 [00:53<01:36,  1.45s/it][1,8]<stderr>:#015 34%|███▍      | 34/100 [00:55<01:36,  1.46s/it][1,0]<stderr>:#015 34%|███▍      | 34/100 [00:55<01:36,  1.46s/it][1,8]<stderr>:#015 35%|███▌      | 35/100 [00:56<01:40,  1.55s/it][1,0]<stderr>:#015 35%|███▌      | 35/100 [00:57<01:41,  1.55s/it][1,8]<stderr>:#015 36%|███▌      | 36/100 [00:58<01:39,  1.55s/it][1,0]<stderr>:#015 36%|███▌      | 36/100 [00:58<01:39,  1.55s/it][1,8]<stderr>:#015 37%|███▋      | 37/100 [01:00<01:38,  1.57s/it][1,0]<stderr>:#015 37%|███▋      | 37/100 [01:00<01:38,  1.56s/it][1,8]<stderr>:#015 38%|███▊      | 38/100 [01:01<01:34,  1.53s/it][1,0]<stderr>:#015 38%|███▊      | 38/100 [01:01<01:36,  1.55s/it][1,8]<stderr>:#015 39%|███▉      | 39/100 [01:03<01:34,  1.55s/it][1,0]<stderr>:#015 39%|███▉      | 39/100 [01:03<01:33,  1.54s/it][1,0]<stderr>:#015 40%|████      | 40/100 [01:04<01:32,  1.54s/it][1,8]<stderr>:#015 40%|████      | 40/100 [01:04<01:34,  1.57s/it][1,8]<stderr>:#015 41%|████      | 41/100 [01:06<01:33,  1.59s/it][1,0]<stderr>:#015 41%|████      | 41/100 [01:06<01:34,  1.60s/it][1,0]<stderr>:#015 42%|████▏     | 42/100 [01:08<01:34,  1.63s/it][1,8]<stderr>:#015 42%|████▏     | 42/100 [01:08<01:34,  1.63s/it][1,8]<stderr>:#015 43%|████▎     | 43/100 [01:09<01:32,  1.63s/it][1,0]<stderr>:#015 43%|████▎     | 43/100 [01:09<01:32,  1.62s/it][1,0]<stderr>:#015 44%|████▍     | 44/100 [01:11<01:28,  1.58s/it][1,8]<stderr>:#015 44%|████▍     | 44/100 [01:11<01:31,  1.64s/it][1,8]<stderr>:#015 45%|████▌     | 45/100 [01:12<01:25,  1.55s/it][1,0]<stderr>:#015 45%|████▌     | 45/100 [01:12<01:28,  1.61s/it][1,0]<stderr>:#015 46%|████▌     | 46/100 [01:14<01:26,  1.59s/it][1,8]<stderr>:#015 46%|████▌     | 46/100 [01:14<01:29,  1.65s/it][1,8]<stderr>:#015 47%|████▋     | 47/100 [01:16<01:25,  1.61s/it][1,0]<stderr>:#015 47%|████▋     | 47/100 [01:16<01:27,  1.65s/it][1,8]<stderr>:#015 48%|████▊     | 48/100 [01:17<01:26,  1.66s/it][1,0]<stderr>:#015 48%|████▊     | 48/100 [01:18<01:27,  1.69s/it][1,8]<stderr>:#015 49%|████▉     | 49/100 [01:19<01:20,  1.58s/it][1,0]<stderr>:#015 49%|████▉     | 49/100 [01:19<01:21,  1.59s/it][1,0]<stderr>:#015 50%|█████     | 50/100 [01:20<01:19,  1.58s/it][1,8]<stderr>:#015 50%|█████     | 50/100 [01:20<01:19,  1.59s/it][1,8]<stderr>:#015 51%|█████     | 51/100 [01:22<01:16,  1.55s/it][1,0]<stderr>:#015 51%|█████     | 51/100 [01:22<01:16,  1.57s/it][1,8]<stderr>:#015 52%|█████▏    | 52/100 [01:23<01:13,  1.53s/it][1,0]<stderr>:#015 52%|█████▏    | 52/100 [01:23<01:13,  1.54s/it][1,0]<stderr>:#015 53%|█████▎    | 53/100 [01:25<01:09,  1.48s/it][1,8]<stderr>:#015 53%|█████▎    | 53/100 [01:25<01:10,  1.50s/it][1,8]<stderr>:#015 54%|█████▍    | 54/100 [01:27<01:11,  1.56s/it][1,0]<stderr>:#015 54%|█████▍    | 54/100 [01:27<01:12,  1.58s/it][1,0]<stderr>:#015 55%|█████▌    | 55/100 [01:28<01:08,  1.52s/it][1,8]<stderr>:#015 55%|█████▌    | 55/100 [01:28<01:10,  1.56s/it][1,8]<stderr>:#015 56%|█████▌    | 56/100 [01:29<01:05,  1.49s/it][1,0]<stderr>:#015 56%|█████▌    | 56/100 [01:29<01:06,  1.50s/it][1,8]<stderr>:#015 57%|█████▋    | 57/100 [01:31<01:05,  1.51s/it][1,0]<stderr>:#015 57%|█████▋    | 57/100 [01:31<01:06,  1.55s/it][1,8]<stderr>:#015 58%|█████▊    | 58/100 [01:33<01:04,  1.53s/it][1,0]<stderr>:#015 58%|█████▊    | 58/100 [01:33<01:04,  1.53s/it][1,0]<stderr>:#015 59%|█████▉    | 59/100 [01:34<01:04,  1.57s/it][1,8]<stderr>:#015 59%|█████▉    | 59/100 [01:34<01:05,  1.61s/it][1,8]<stderr>:#015 60%|██████    | 60/100 [01:36<01:04,  1.61s/it][1,0]<stderr>:#015 60%|██████    | 60/100 [01:36<01:04,  1.61s/it][1,0]<stderr>:#015 61%|██████    | 61/100 [01:38<01:05,  1.69s/it][1,8]<stderr>:#015 61%|██████    | 61/100 [01:38<01:06,  1.72s/it][1,0]<stderr>:#015 62%|██████▏   | 62/100 [01:39<01:02,  1.63s/it][1,8]<stderr>:#015 62%|██████▏   | 62/100 [01:39<01:02,  1.63s/it][1,0]<stderr>:#015 63%|██████▎   | 63/100 [01:41<01:00,  1.64s/it][1,8]<stderr>:#015 63%|██████▎   | 63/100 [01:41<01:01,  1.65s/it][1,8]<stderr>:#015 64%|██████▍   | 64/100 [01:43<01:01,  1.72s/it][1,0]<stderr>:#015 64%|██████▍   | 64/100 [01:43<01:02,  1.73s/it][1,8]<stderr>:#015 65%|██████▌   | 65/100 [01:45<00:59,  1.71s/it][1,0]<stderr>:#015 65%|██████▌   | 65/100 [01:45<01:00,  1.72s/it][1,8]<stderr>:#015 66%|██████▌   | 66/100 [01:46<00:57,  1.68s/it][1,0]<stderr>:#015 66%|██████▌   | 66/100 [01:46<00:57,  1.69s/it][1,8]<stderr>:#015 67%|██████▋   | 67/100 [01:48<00:53,  1.64s/it][1,0]<stderr>:#015 67%|██████▋   | 67/100 [01:48<00:54,  1.66s/it][1,0]<stderr>:#015 68%|██████▊   | 68/100 [01:50<00:54,  1.70s/it][1,8]<stderr>:#015 68%|██████▊   | 68/100 [01:50<00:55,  1.74s/it][1,0]<stderr>:#015 69%|██████▉   | 69/100 [01:51<00:53,  1.72s/it][1,8]<stderr>:#015 69%|██████▉   | 69/100 [01:51<00:53,  1.72s/it][1,8]<stderr>:#015 70%|███████   | 70/100 [01:53<00:48,  1.63s/it][1,0]<stderr>:#015 70%|███████   | 70/100 [01:53<00:49,  1.66s/it][1,0]<stderr>:#015 71%|███████   | 71/100 [01:55<00:47,  1.63s/it][1,8]<stderr>:#015 71%|███████   | 71/100 [01:55<00:49,  1.71s/it][1,8]<stderr>:#015 72%|███████▏  | 72/100 [01:56<00:47,  1.69s/it][1,0]<stderr>:#015 72%|███████▏  | 72/100 [01:56<00:47,  1.71s/it][1,8]<stderr>:#015 73%|███████▎  | 73/100 [01:58<00:45,  1.69s/it][1,0]<stderr>:#015 73%|███████▎  | 73/100 [01:58<00:47,  1.76s/it][1,0]<stderr>:#015 74%|███████▍  | 74/100 [02:00<00:43,  1.66s/it][1,8]<stderr>:#015 74%|███████▍  | 74/100 [02:00<00:43,  1.67s/it][1,8]<stderr>:#015 75%|███████▌  | 75/100 [02:01<00:41,  1.66s/it][1,0]<stderr>:#015 75%|███████▌  | 75/100 [02:01<00:41,  1.68s/it][1,8]<stderr>:#015 76%|███████▌  | 76/100 [02:03<00:39,  1.63s/it][1,0]<stderr>:#015 76%|███████▌  | 76/100 [02:03<00:39,  1.66s/it][1,8]<stderr>:#015 77%|███████▋  | 77/100 [02:04<00:37,  1.62s/it][1,0]<stderr>:#015 77%|███████▋  | 77/100 [02:05<00:37,  1.64s/it][1,0]<stderr>:#015 78%|███████▊  | 78/100 [02:06<00:34,  1.57s/it][1,8]<stderr>:#015 78%|███████▊  | 78/100 [02:06<00:35,  1.60s/it][1,8]<stderr>:#015 79%|███████▉  | 79/100 [02:08<00:33,  1.59s/it][1,0]<stderr>:#015 79%|███████▉  | 79/100 [02:08<00:33,  1.59s/it][1,0]<stderr>:#015 80%|████████  | 80/100 [02:09<00:31,  1.56s/it][1,8]<stderr>:#015 80%|████████  | 80/100 [02:09<00:31,  1.58s/it][1,0]<stderr>:#015 81%|████████  | 81/100 [02:11<00:29,  1.57s/it][1,8]<stderr>:#015 81%|████████  | 81/100 [02:11<00:30,  1.59s/it][1,8]<stderr>:#015 82%|████████▏ | 82/100 [02:12<00:28,  1.60s/it][1,0]<stderr>:#015 82%|████████▏ | 82/100 [02:12<00:28,  1.61s/it][1,0]<stderr>:#015 83%|████████▎ | 83/100 [02:14<00:27,  1.61s/it][1,8]<stderr>:#015 83%|████████▎ | 83/100 [02:14<00:27,  1.62s/it][1,0]<stderr>:#015 84%|████████▍ | 84/100 [02:16<00:25,  1.58s/it][1,8]<stderr>:#015 84%|████████▍ | 84/100 [02:16<00:25,  1.59s/it][1,8]<stderr>:#015 85%|████████▌ | 85/100 [02:17<00:23,  1.56s/it][1,0]<stderr>:#015 85%|████████▌ | 85/100 [02:17<00:23,  1.58s/it][1,8]<stderr>:#015 86%|████████▌ | 86/100 [02:19<00:22,  1.58s/it][1,0]<stderr>:#015 86%|████████▌ | 86/100 [02:19<00:22,  1.59s/it][1,8]<stderr>:#015 87%|████████▋ | 87/100 [02:20<00:20,  1.55s/it][1,0]<stderr>:#015 87%|████████▋ | 87/100 [02:20<00:20,  1.56s/it][1,0]<stderr>:#015 88%|████████▊ | 88/100 [02:22<00:19,  1.58s/it][1,8]<stderr>:#015 88%|████████▊ | 88/100 [02:22<00:19,  1.60s/it][1,0]<stderr>:#015 89%|████████▉ | 89/100 [02:23<00:16,  1.54s/it][1,8]<stderr>:#015 89%|████████▉ | 89/100 [02:23<00:17,  1.57s/it][1,8]<stderr>:#015 90%|█████████ | 90/100 [02:25<00:15,  1.56s/it][1,0]<stderr>:#015 90%|█████████ | 90/100 [02:25<00:15,  1.58s/it][1,8]<stderr>:#015 91%|█████████ | 91/100 [02:27<00:14,  1.59s/it][1,0]<stderr>:#015 91%|█████████ | 91/100 [02:27<00:14,  1.60s/it][1,8]<stderr>:#015 92%|█████████▏| 92/100 [02:28<00:12,  1.53s/it][1,0]<stderr>:#015 92%|█████████▏| 92/100 [02:28<00:12,  1.53s/it][1,8]<stderr>:#015 93%|█████████▎| 93/100 [02:30<00:10,  1.54s/it][1,0]<stderr>:#015 93%|█████████▎| 93/100 [02:30<00:10,  1.54s/it][1,8]<stderr>:#015 94%|█████████▍| 94/100 [02:31<00:09,  1.60s/it][1,0]<stderr>:#015 94%|█████████▍| 94/100 [02:31<00:09,  1.59s/it][1,0]<stderr>:#015 95%|█████████▌| 95/100 [02:33<00:07,  1.59s/it][1,8]<stderr>:#015 95%|█████████▌| 95/100 [02:33<00:07,  1.59s/it][1,0]<stderr>:#015 96%|█████████▌| 96/100 [02:34<00:06,  1.55s/it][1,8]<stderr>:#015 96%|█████████▌| 96/100 [02:34<00:06,  1.55s/it][1,0]<stderr>:#015 97%|█████████▋| 97/100 [02:36<00:04,  1.54s/it][1,8]<stderr>:#015 97%|█████████▋| 97/100 [02:36<00:04,  1.54s/it][1,8]<stderr>:#015 98%|█████████▊| 98/100 [02:37<00:03,  1.56s/it][1,0]<stderr>:#015 98%|█████████▊| 98/100 [02:37<00:03,  1.57s/it][1,0]<stderr>:#015 99%|█████████▉| 99/100 [02:39<00:01,  1.49s/it][1,8]<stderr>:#015 99%|█████████▉| 99/100 [02:39<00:01,  1.52s/it][1,8]<stderr>:#015100%|██████████| 100/100 [02:40<00:00,  1.54s/it][1,8]<stderr>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,565 >> \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.59s/it][1,0]<stderr>:[INFO|trainer.py:1352] 2021-09-05 08:02:09,693 >> \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015                                                 [1,0]<stderr>:#015[1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.59s/it][1,8]<stderr>:#015                                                 #015[1,8]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.54s/it][1,0]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.62s/it][1,8]<stderr>:#015100%|██████████| 100/100 [02:41<00:00,  1.62s/it][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:516] 2021-09-05 08:02:10,635 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2115] 2021-09-05 08:02:10,639 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2117] 2021-09-05 08:02:10,639 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:[INFO|trainer.py:2120] 2021-09-05 08:02:10,639 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:1885] 2021-09-05 08:02:10,793 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|configuration_utils.py:351] 2021-09-05 08:02:10,794 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|modeling_utils.py:889] 2021-09-05 08:02:13,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1924] 2021-09-05 08:02:13,044 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|tokenization_utils_base.py:1930] 2021-09-05 08:02:13,044 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:02:13,088 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   epoch                      =       0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_alloc_delta   =     -122MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,088 >>   init_mem_cpu_peaked_delta  =      121MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_alloc_delta   =     1275MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_alloc_delta  =     1424MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_alloc_delta  =     5113MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_mem_gpu_peaked_delta =     4884MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_runtime              = 0:02:41.11\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples              =      88524\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:02:13,089 >>   train_samples_per_second   =      0.621\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:516] 2021-09-05 08:02:13,091 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2115] 2021-09-05 08:02:13,094 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2117] 2021-09-05 08:02:13,094 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer.py:2120] 2021-09-05 08:02:13,094 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s][1,8]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s][1,0]<stderr>:#015  1%|          | 2/169 [00:00<00:09, 18.30it/s][1,8]<stderr>:#015  1%|          | 2/169 [00:00<00:09, 18.29it/s][1,0]<stderr>:#015  2%|▏         | 4/169 [00:00<00:10, 16.32it/s][1,8]<stderr>:#015  2%|▏         | 4/169 [00:00<00:10, 16.31it/s][1,0]<stderr>:#015  4%|▎         | 6/169 [00:00<00:10, 15.10it/s][1,8]<stderr>:#015  4%|▎         | 6/169 [00:00<00:10, 15.09it/s][1,0]<stderr>:#015  5%|▍         | 8/169 [00:00<00:11, 14.41it/s][1,8]<stderr>:#015  5%|▍         | 8/169 [00:00<00:11, 14.41it/s][1,0]<stderr>:#015  6%|▌         | 10/169 [00:00<00:11, 13.95it/s][1,8]<stderr>:#015  6%|▌         | 10/169 [00:00<00:11, 13.95it/s][1,0]<stderr>:#015  7%|▋         | 12/169 [00:00<00:11, 13.17it/s][1,8]<stderr>:#015  7%|▋         | 12/169 [00:00<00:11, 13.17it/s][1,0]<stderr>:#015  8%|▊         | 14/169 [00:01<00:11, 13.02it/s][1,8]<stderr>:#015  8%|▊         | 14/169 [00:01<00:11, 13.02it/s][1,0]<stderr>:#015  9%|▉         | 16/169 [00:01<00:11, 13.04it/s][1,8]<stderr>:#015  9%|▉         | 16/169 [00:01<00:11, 13.04it/s][1,0]<stderr>:#015 11%|█         | 18/169 [00:01<00:11, 13.04it/s][1,8]<stderr>:#015 11%|█         | 18/169 [00:01<00:11, 13.04it/s][1,0]<stderr>:#015 12%|█▏        | 20/169 [00:01<00:11, 13.03it/s][1,8]<stderr>:#015 12%|█▏        | 20/169 [00:01<00:11, 13.03it/s][1,0]<stderr>:#015 13%|█▎        | 22/169 [00:01<00:11, 13.03it/s][1,8]<stderr>:#015 13%|█▎        | 22/169 [00:01<00:11, 13.03it/s][1,0]<stderr>:#015 14%|█▍        | 24/169 [00:01<00:11, 13.00it/s][1,8]<stderr>:#015 14%|█▍        | 24/169 [00:01<00:11, 13.00it/s][1,0]<stderr>:#015 15%|█▌        | 26/169 [00:01<00:10, 13.01it/s][1,8]<stderr>:#015 15%|█▌        | 26/169 [00:01<00:10, 13.01it/s][1,0]<stderr>:#015 17%|█▋        | 28/169 [00:02<00:10, 13.02it/s][1,8]<stderr>:#015 17%|█▋        | 28/169 [00:02<00:10, 13.02it/s][1,0]<stderr>:#015 18%|█▊        | 30/169 [00:02<00:10, 13.04it/s][1,8]<stderr>:#015 18%|█▊        | 30/169 [00:02<00:10, 13.04it/s][1,0]<stderr>:#015 19%|█▉        | 32/169 [00:02<00:10, 13.06it/s][1,8]<stderr>:#015 19%|█▉        | 32/169 [00:02<00:10, 13.06it/s][1,0]<stderr>:#015 20%|██        | 34/169 [00:02<00:10, 13.06it/s][1,8]<stderr>:#015 20%|██        | 34/169 [00:02<00:10, 13.06it/s][1,0]<stderr>:#015 21%|██▏       | 36/169 [00:02<00:10, 13.07it/s][1,8]<stderr>:#015 21%|██▏       | 36/169 [00:02<00:10, 13.07it/s][1,0]<stderr>:#015 22%|██▏       | 38/169 [00:02<00:10, 13.05it/s][1,8]<stderr>:#015 22%|██▏       | 38/169 [00:02<00:10, 13.05it/s][1,0]<stderr>:#015 24%|██▎       | 40/169 [00:03<00:09, 13.06it/s][1,8]<stderr>:#015 24%|██▎       | 40/169 [00:03<00:09, 13.06it/s][1,0]<stderr>:#015 25%|██▍       | 42/169 [00:03<00:09, 13.07it/s][1,8]<stderr>:#015 25%|██▍       | 42/169 [00:03<00:09, 13.07it/s][1,0]<stderr>:#015 26%|██▌       | 44/169 [00:03<00:09, 13.07it/s][1,8]<stderr>:#015 26%|██▌       | 44/169 [00:03<00:09, 13.07it/s][1,0]<stderr>:#015 27%|██▋       | 46/169 [00:03<00:09, 13.05it/s][1,8]<stderr>:#015 27%|██▋       | 46/169 [00:03<00:09, 13.04it/s][1,0]<stderr>:#015 28%|██▊       | 48/169 [00:03<00:09, 13.04it/s][1,8]<stderr>:#015 28%|██▊       | 48/169 [00:03<00:09, 13.04it/s][1,0]<stderr>:#015 30%|██▉       | 50/169 [00:03<00:09, 13.02it/s][1,8]<stderr>:#015 30%|██▉       | 50/169 [00:03<00:09, 13.02it/s][1,0]<stderr>:#015 31%|███       | 52/169 [00:03<00:08, 13.01it/s][1,8]<stderr>:#015 31%|███       | 52/169 [00:03<00:08, 13.02it/s][1,0]<stderr>:#015 32%|███▏      | 54/169 [00:04<00:08, 13.01it/s][1,8]<stderr>:#015 32%|███▏      | 54/169 [00:04<00:08, 13.01it/s][1,0]<stderr>:#015 33%|███▎      | 56/169 [00:04<00:08, 13.01it/s][1,8]<stderr>:#015 33%|███▎      | 56/169 [00:04<00:08, 13.01it/s][1,0]<stderr>:#015 34%|███▍      | 58/169 [00:04<00:08, 13.02it/s][1,8]<stderr>:#015 34%|███▍      | 58/169 [00:04<00:08, 13.02it/s][1,0]<stderr>:#015 36%|███▌      | 60/169 [00:04<00:08, 12.94it/s][1,8]<stderr>:#015 36%|███▌      | 60/169 [00:04<00:08, 12.94it/s][1,0]<stderr>:#015 37%|███▋      | 62/169 [00:04<00:08, 12.96it/s][1,8]<stderr>:#015 37%|███▋      | 62/169 [00:04<00:08, 12.96it/s][1,0]<stderr>:#015 38%|███▊      | 64/169 [00:04<00:08, 12.98it/s][1,8]<stderr>:#015 38%|███▊      | 64/169 [00:04<00:08, 12.98it/s][1,0]<stderr>:#015 39%|███▉      | 66/169 [00:05<00:07, 12.99it/s][1,8]<stderr>:#015 39%|███▉      | 66/169 [00:05<00:07, 12.99it/s][1,0]<stderr>:#015 40%|████      | 68/169 [00:05<00:07, 13.01it/s][1,8]<stderr>:#015 40%|████      | 68/169 [00:05<00:07, 13.01it/s][1,0]<stderr>:#015 41%|████▏     | 70/169 [00:05<00:07, 13.02it/s][1,8]<stderr>:#015 41%|████▏     | 70/169 [00:05<00:07, 13.02it/s][1,0]<stderr>:#015 43%|████▎     | 72/169 [00:05<00:08, 11.58it/s][1,8]<stderr>:#015 43%|████▎     | 72/169 [00:05<00:08, 11.58it/s][1,0]<stderr>:#015 44%|████▍     | 74/169 [00:05<00:09,  9.82it/s][1,8]<stderr>:#015 44%|████▍     | 74/169 [00:05<00:09,  9.82it/s][1,0]<stderr>:#015 45%|████▍     | 76/169 [00:06<00:08, 10.60it/s][1,8]<stderr>:#015 45%|████▍     | 76/169 [00:06<00:08, 10.60it/s][1,0]<stderr>:#015 46%|████▌     | 78/169 [00:06<00:08, 11.22it/s][1,8]<stderr>:#015 46%|████▌     | 78/169 [00:06<00:08, 11.22it/s][1,0]<stderr>:#015 47%|████▋     | 80/169 [00:06<00:07, 11.70it/s][1,8]<stderr>:#015 47%|████▋     | 80/169 [00:06<00:07, 11.70it/s][1,0]<stderr>:#015 49%|████▊     | 82/169 [00:06<00:07, 12.06it/s][1,8]<stderr>:#015 49%|████▊     | 82/169 [00:06<00:07, 12.06it/s][1,0]<stderr>:#015 50%|████▉     | 84/169 [00:06<00:06, 12.33it/s][1,8]<stderr>:#015 50%|████▉     | 84/169 [00:06<00:06, 12.33it/s][1,0]<stderr>:#015 51%|█████     | 86/169 [00:06<00:06, 12.51it/s][1,8]<stderr>:#015 51%|█████     | 86/169 [00:06<00:06, 12.51it/s][1,0]<stderr>:#015 52%|█████▏    | 88/169 [00:06<00:06, 12.65it/s][1,8]<stderr>:#015 52%|█████▏    | 88/169 [00:06<00:06, 12.65it/s][1,0]<stderr>:#015 53%|█████▎    | 90/169 [00:07<00:06, 12.70it/s][1,8]<stderr>:#015 53%|█████▎    | 90/169 [00:07<00:06, 12.70it/s][1,0]<stderr>:#015 54%|█████▍    | 92/169 [00:07<00:06, 12.75it/s][1,8]<stderr>:#015 54%|█████▍    | 92/169 [00:07<00:06, 12.75it/s][1,0]<stderr>:#015 56%|█████▌    | 94/169 [00:07<00:05, 12.81it/s][1,8]<stderr>:#015 56%|█████▌    | 94/169 [00:07<00:05, 12.81it/s][1,0]<stderr>:#015 57%|█████▋    | 96/169 [00:07<00:05, 12.86it/s][1,8]<stderr>:#015 57%|█████▋    | 96/169 [00:07<00:05, 12.86it/s][1,0]<stderr>:#015 58%|█████▊    | 98/169 [00:07<00:05, 12.87it/s][1,8]<stderr>:#015 58%|█████▊    | 98/169 [00:07<00:05, 12.88it/s][1,0]<stderr>:#015 59%|█████▉    | 100/169 [00:07<00:05, 12.83it/s][1,8]<stderr>:#015 59%|█████▉    | 100/169 [00:07<00:05, 12.83it/s][1,0]<stderr>:#015 60%|██████    | 102/169 [00:08<00:05, 12.78it/s][1,8]<stderr>:#015 60%|██████    | 102/169 [00:08<00:05, 12.78it/s][1,0]<stderr>:#015 62%|██████▏   | 104/169 [00:08<00:05, 12.82it/s][1,8]<stderr>:#015 62%|██████▏   | 104/169 [00:08<00:05, 12.82it/s][1,0]<stderr>:#015 63%|██████▎   | 106/169 [00:08<00:04, 12.83it/s][1,8]<stderr>:#015 63%|██████▎   | 106/169 [00:08<00:04, 12.83it/s][1,0]<stderr>:#015 64%|██████▍   | 108/169 [00:08<00:04, 12.81it/s][1,8]<stderr>:#015 64%|██████▍   | 108/169 [00:08<00:04, 12.81it/s][1,0]<stderr>:#015 65%|██████▌   | 110/169 [00:08<00:04, 12.82it/s][1,8]<stderr>:#015 65%|██████▌   | 110/169 [00:08<00:04, 12.81it/s][1,0]<stderr>:#015 66%|██████▋   | 112/169 [00:08<00:04, 12.85it/s][1,8]<stderr>:#015 66%|██████▋   | 112/169 [00:08<00:04, 12.85it/s][1,0]<stderr>:#015 67%|██████▋   | 114/169 [00:08<00:04, 12.85it/s][1,8]<stderr>:#015 67%|██████▋   | 114/169 [00:08<00:04, 12.86it/s][1,0]<stderr>:#015 69%|██████▊   | 116/169 [00:09<00:04, 12.89it/s][1,8]<stderr>:#015 69%|██████▊   | 116/169 [00:09<00:04, 12.89it/s][1,0]<stderr>:#015 70%|██████▉   | 118/169 [00:09<00:03, 12.85it/s][1,8]<stderr>:#015 70%|██████▉   | 118/169 [00:09<00:03, 12.85it/s][1,0]<stderr>:#015 71%|███████   | 120/169 [00:09<00:03, 12.87it/s][1,8]<stderr>:#015 71%|███████   | 120/169 [00:09<00:03, 12.87it/s][1,0]<stderr>:#015 72%|███████▏  | 122/169 [00:09<00:03, 12.89it/s][1,8]<stderr>:#015 72%|███████▏  | 122/169 [00:09<00:03, 12.89it/s][1,0]<stderr>:#015 73%|███████▎  | 124/169 [00:09<00:03, 12.80it/s][1,8]<stderr>:#015 73%|███████▎  | 124/169 [00:09<00:03, 12.80it/s][1,0]<stderr>:#015 75%|███████▍  | 126/169 [00:09<00:03, 12.81it/s][1,8]<stderr>:#015 75%|███████▍  | 126/169 [00:09<00:03, 12.81it/s][1,0]<stderr>:#015 76%|███████▌  | 128/169 [00:10<00:03, 12.78it/s][1,8]<stderr>:#015 76%|███████▌  | 128/169 [00:10<00:03, 12.79it/s][1,0]<stderr>:#015 77%|███████▋  | 130/169 [00:10<00:03, 12.84it/s][1,8]<stderr>:#015 77%|███████▋  | 130/169 [00:10<00:03, 12.84it/s][1,0]<stderr>:#015 78%|███████▊  | 132/169 [00:10<00:02, 12.87it/s][1,8]<stderr>:#015 78%|███████▊  | 132/169 [00:10<00:02, 12.87it/s][1,0]<stderr>:#015 79%|███████▉  | 134/169 [00:10<00:02, 12.90it/s][1,8]<stderr>:#015 79%|███████▉  | 134/169 [00:10<00:02, 12.90it/s][1,0]<stderr>:#015 80%|████████  | 136/169 [00:10<00:02, 12.89it/s][1,8]<stderr>:#015 80%|████████  | 136/169 [00:10<00:02, 12.89it/s][1,0]<stderr>:#015 82%|████████▏ | 138/169 [00:10<00:02, 12.91it/s][1,8]<stderr>:#015 82%|████████▏ | 138/169 [00:10<00:02, 12.91it/s][1,0]<stderr>:#015 83%|████████▎ | 140/169 [00:10<00:02, 12.91it/s][1,8]<stderr>:#015 83%|████████▎ | 140/169 [00:10<00:02, 12.91it/s][1,0]<stderr>:#015 84%|████████▍ | 142/169 [00:11<00:02, 12.93it/s][1,8]<stderr>:#015 84%|████████▍ | 142/169 [00:11<00:02, 12.93it/s][1,0]<stderr>:#015 85%|████████▌ | 144/169 [00:11<00:01, 12.94it/s][1,8]<stderr>:#015 85%|████████▌ | 144/169 [00:11<00:01, 12.94it/s][1,0]<stderr>:#015 86%|████████▋ | 146/169 [00:11<00:01, 12.93it/s][1,8]<stderr>:#015 86%|████████▋ | 146/169 [00:11<00:01, 12.93it/s][1,0]<stderr>:#015 88%|████████▊ | 148/169 [00:11<00:01, 12.93it/s][1,8]<stderr>:#015 88%|████████▊ | 148/169 [00:11<00:01, 12.93it/s][1,0]<stderr>:#015 89%|████████▉ | 150/169 [00:11<00:01, 11.36it/s][1,8]<stderr>:#015 89%|████████▉ | 150/169 [00:11<00:01, 11.36it/s][1,0]<stderr>:#015 90%|████████▉ | 152/169 [00:11<00:01, 11.80it/s][1,8]<stderr>:#015 90%|████████▉ | 152/169 [00:11<00:01, 11.80it/s][1,0]<stderr>:#015 91%|█████████ | 154/169 [00:12<00:01,  9.77it/s][1,8]<stderr>:#015 91%|█████████ | 154/169 [00:12<00:01,  9.77it/s][1,8]<stderr>:#015 92%|█████████▏| 156/169 [00:12<00:01, 10.56it/s][1,0]<stderr>:#015 92%|█████████▏| 156/169 [00:12<00:01, 10.56it/s][1,0]<stderr>:#015 93%|█████████▎| 158/169 [00:12<00:00, 11.19it/s][1,8]<stderr>:#015 93%|█████████▎| 158/169 [00:12<00:00, 11.19it/s][1,0]<stderr>:#015 95%|█████████▍| 160/169 [00:12<00:00, 11.65it/s][1,8]<stderr>:#015 95%|█████████▍| 160/169 [00:12<00:00, 11.65it/s][1,0]<stderr>:#015 96%|█████████▌| 162/169 [00:12<00:00, 12.03it/s][1,8]<stderr>:#015 96%|█████████▌| 162/169 [00:12<00:00, 12.03it/s][1,0]<stderr>:#015 97%|█████████▋| 164/169 [00:13<00:00, 12.30it/s][1,8]<stderr>:#015 97%|█████████▋| 164/169 [00:13<00:00, 12.30it/s][1,0]<stderr>:#015 98%|█████████▊| 166/169 [00:13<00:00, 12.50it/s][1,8]<stderr>:#015 98%|█████████▊| 166/169 [00:13<00:00, 12.50it/s][1,0]<stderr>:#015 99%|█████████▉| 168/169 [00:13<00:00, 12.65it/s][1,8]<stderr>:#015 99%|█████████▉| 168/169 [00:13<00:00, 12.65it/s][1,7]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,7]<stderr>:#015  0%|          | 41/10570 [00:00<00:25, 405.70it/s][1,2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 402.26it/s][1,7]<stderr>:#015  1%|          | 87/10570 [00:00<00:24, 420.49it/s][1,10]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,2]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 402.94it/s][1,4]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,12]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 416.71it/s][1,15]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,7]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 441.38it/s][1,10]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 397.04it/s][1,1]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 398.73it/s][1,2]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 417.46it/s][1,9]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,4]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 399.20it/s][1,12]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 437.66it/s][1,15]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 394.55it/s][1,7]<stderr>:#015  2%|▏         | 182/10570 [00:00<00:23, 441.25it/s][1,10]<stderr>:#015  1%|          | 86/10570 [00:00<00:25, 412.05it/s][1,1]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 412.18it/s][1,2]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:23, 437.24it/s][1,9]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 397.42it/s][1,4]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 412.99it/s][1,12]<stderr>:#015  2%|▏         | 182/10570 [00:00<00:23, 438.19it/s][1,15]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 409.15it/s][1,7]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:25, 400.75it/s][1,14]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,10]<stderr>:#015  1%|▏         | 136/10570 [00:00<00:24, 432.73it/s][1,1]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 431.56it/s][1,2]<stderr>:#015  2%|▏         | 181/10570 [00:00<00:23, 437.17it/s][1,9]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 410.68it/s][1,5]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,4]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 433.59it/s][1,12]<stderr>:#015  2%|▏         | 220/10570 [00:00<00:24, 418.49it/s][1,15]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 430.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A[1,7]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:28, 367.16it/s][1,3]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,11]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,14]<stderr>:#015  0%|          | 41/10570 [00:00<00:26, 401.33it/s][1,10]<stderr>:#015  2%|▏         | 180/10570 [00:00<00:23, 433.75it/s][1,1]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 430.00it/s][1,2]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:24, 415.71it/s][1,9]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 431.21it/s][1,5]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 394.37it/s][1,4]<stderr>:#015  2%|▏         | 178/10570 [00:00<00:24, 432.13it/s][1,15]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 429.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 40/10570 [00:00<00:26, 393.51it/s]#033[A[1,12]<stderr>:#015  2%|▏         | 256/10570 [00:00<00:28, 366.48it/s][1,3]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 387.59it/s][1,11]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 387.93it/s][1,14]<stderr>:#015  1%|          | 87/10570 [00:00<00:25, 415.20it/s][1,1]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:24, 416.21it/s][1,10]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:24, 415.45it/s][1,6]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,9]<stderr>:#015  2%|▏         | 178/10570 [00:00<00:24, 430.76it/s][1,5]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 408.22it/s][1,4]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:24, 415.30it/s][1,7]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 310.25it/s][1,2]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 366.53it/s][1,13]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s][1,15]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:24, 415.22it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  1%|          | 85/10570 [00:00<00:25, 406.67it/s]#033[A[1,11]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 401.54it/s][1,3]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 399.57it/s][1,14]<stderr>:#015  1%|▏         | 137/10570 [00:00<00:24, 434.70it/s][1,12]<stderr>:#015  3%|▎         | 290/10570 [00:00<00:32, 320.16it/s][1,1]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 375.30it/s][1,6]<stderr>:#015  0%|          | 39/10570 [00:00<00:27, 384.12it/s][1,10]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 367.05it/s][1,9]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:25, 410.82it/s][1,5]<stderr>:#015  1%|▏         | 135/10570 [00:00<00:24, 429.42it/s][1,7]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 339.85it/s][1,13]<stderr>:#015  0%|          | 37/10570 [00:00<00:28, 369.49it/s][1,4]<stderr>:#015  2%|▏         | 253/10570 [00:00<00:27, 371.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  1%|▏         | 134/10570 [00:00<00:24, 427.09it/s]#033[A[1,15]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 375.20it/s][1,2]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 316.02it/s][1,11]<stderr>:#015  1%|          | 132/10570 [00:00<00:24, 422.53it/s][1,3]<stderr>:#015  1%|          | 132/10570 [00:00<00:24, 420.94it/s][1,14]<stderr>:#015  2%|▏         | 181/10570 [00:00<00:23, 434.57it/s][1,12]<stderr>:#015  3%|▎         | 335/10570 [00:00<00:29, 349.94it/s][1,6]<stderr>:#015  1%|          | 83/10570 [00:00<00:26, 397.88it/s][1,5]<stderr>:#015  2%|▏         | 179/10570 [00:00<00:24, 430.12it/s][1,7]<stderr>:#015  4%|▎         | 377/10570 [00:00<00:27, 370.43it/s][1,9]<stderr>:#015  2%|▏         | 251/10570 [00:00<00:27, 372.65it/s][1,13]<stderr>:#015  1%|          | 79/10570 [00:00<00:27, 381.15it/s][1,10]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 315.69it/s][1,1]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:32, 312.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 177/10570 [00:00<00:24, 426.41it/s]#033[A[1,2]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 345.85it/s][1,11]<stderr>:#015  2%|▏         | 174/10570 [00:00<00:24, 421.24it/s][1,3]<stderr>:#015  2%|▏         | 174/10570 [00:00<00:24, 420.61it/s][1,14]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:25, 413.93it/s][1,4]<stderr>:#015  3%|▎         | 287/10570 [00:00<00:32, 312.28it/s][1,12]<stderr>:#015  4%|▎         | 382/10570 [00:00<00:26, 378.03it/s][1,6]<stderr>:#015  1%|          | 131/10570 [00:00<00:24, 418.93it/s][1,15]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 310.73it/s][1,5]<stderr>:#015  2%|▏         | 217/10570 [00:00<00:25, 412.84it/s][1,7]<stderr>:#015  4%|▍         | 417/10570 [00:01<00:26, 376.94it/s][1,13]<stderr>:#015  1%|          | 124/10570 [00:00<00:26, 399.00it/s][1,10]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 345.26it/s][1,1]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 340.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 216/10570 [00:00<00:25, 412.07it/s]#033[A[1,2]<stderr>:#015  4%|▎         | 380/10570 [00:00<00:27, 372.58it/s][1,11]<stderr>:#015  2%|▏         | 213/10570 [00:00<00:25, 408.91it/s][1,3]<stderr>:#015  2%|▏         | 213/10570 [00:00<00:25, 408.14it/s][1,9]<stderr>:#015  3%|▎         | 285/10570 [00:00<00:33, 306.86it/s][1,4]<stderr>:#015  3%|▎         | 331/10570 [00:00<00:29, 341.40it/s][1,12]<stderr>:#015  4%|▍         | 422/10570 [00:01<00:26, 383.27it/s][1,6]<stderr>:#015  2%|▏         | 173/10570 [00:00<00:24, 418.76it/s][1,14]<stderr>:#015  2%|▏         | 255/10570 [00:00<00:28, 366.78it/s][1,15]<stderr>:#015  3%|▎         | 329/10570 [00:00<00:30, 338.92it/s][1,7]<stderr>:#015  4%|▍         | 462/10570 [00:01<00:25, 395.39it/s][1,13]<stderr>:#015  2%|▏         | 165/10570 [00:00<00:25, 400.93it/s][1,5]<stderr>:#015  2%|▏         | 252/10570 [00:00<00:27, 372.88it/s][1,10]<stderr>:#015  4%|▎         | 381/10570 [00:00<00:27, 374.22it/s][1,1]<stderr>:#015  4%|▎         | 376/10570 [00:00<00:27, 368.98it/s][1,2]<stderr>:#015  4%|▍         | 419/10570 [00:01<00:26, 377.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  2%|▏         | 251/10570 [00:00<00:27, 374.17it/s]#033[A[1,9]<stderr>:#015  3%|▎         | 328/10570 [00:00<00:30, 335.30it/s][1,3]<stderr>:#015  2%|▏         | 248/10570 [00:00<00:27, 374.29it/s][1,11]<stderr>:#015  2%|▏         | 248/10570 [00:00<00:27, 373.69it/s][1,4]<stderr>:#015  4%|▎         | 378/10570 [00:00<00:27, 370.38it/s][1,12]<stderr>:#015  4%|▍         | 465/10570 [00:01<00:25, 395.81it/s][1,6]<stderr>:#015  2%|▏         | 212/10570 [00:00<00:25, 406.98it/s][1,15]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:27, 365.04it/s][1,7]<stderr>:#015  5%|▍         | 508/10570 [00:01<00:24, 411.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A[1,13]<stderr>:#015  2%|▏         | 204/10570 [00:00<00:26, 395.97it/s][1,14]<stderr>:#015  3%|▎         | 289/10570 [00:00<00:32, 316.69it/s][1,10]<stderr>:#015  4%|▍         | 420/10570 [00:01<00:26, 377.67it/s][1,1]<stderr>:#015  4%|▍         | 415/10570 [00:01<00:27, 374.39it/s][1,2]<stderr>:#015  4%|▍         | 463/10570 [00:01<00:25, 393.91it/s][1,9]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:28, 363.91it/s][1,5]<stderr>:#015  3%|▎         | 286/10570 [00:00<00:33, 309.86it/s][1,4]<stderr>:#015  4%|▍         | 417/10570 [00:01<00:27, 374.92it/s][1,12]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:24, 412.69it/s][1,6]<stderr>:#015  2%|▏         | 247/10570 [00:00<00:27, 372.17it/s][1,15]<stderr>:#015  4%|▍         | 414/10570 [00:01<00:27, 371.99it/s][1,7]<stderr>:#015  5%|▌         | 553/10570 [00:01<00:23, 421.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  3%|▎         | 285/10570 [00:00<00:33, 307.55it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 37/10570 [00:00<00:28, 367.44it/s]#033[A[1,13]<stderr>:#015  2%|▏         | 238/10570 [00:00<00:28, 364.92it/s][1,14]<stderr>:#015  3%|▎         | 334/10570 [00:00<00:29, 346.14it/s][1,1]<stderr>:#015  4%|▍         | 459/10570 [00:01<00:25, 391.11it/s][1,3]<stderr>:#015  3%|▎         | 282/10570 [00:00<00:34, 301.87it/s][1,10]<stderr>:#015  4%|▍         | 464/10570 [00:01<00:25, 392.24it/s][1,11]<stderr>:#015  3%|▎         | 282/10570 [00:00<00:34, 301.23it/s][1,2]<stderr>:#015  5%|▍         | 509/10570 [00:01<00:24, 409.94it/s][1,5]<stderr>:#015  3%|▎         | 330/10570 [00:00<00:30, 338.56it/s][1,9]<stderr>:#015  4%|▍         | 413/10570 [00:01<00:27, 365.87it/s][1,4]<stderr>:#015  4%|▍         | 461/10570 [00:01<00:25, 392.12it/s][1,12]<stderr>:#015  5%|▌         | 556/10570 [00:01<00:23, 420.53it/s][1,15]<stderr>:#015  4%|▍         | 458/10570 [00:01<00:26, 388.74it/s][1,7]<stderr>:#015  6%|▌         | 596/10570 [00:01<00:23, 420.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  3%|▎         | 328/10570 [00:00<00:30, 335.13it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  1%|          | 79/10570 [00:00<00:27, 380.26it/s]#033[A[1,14]<stderr>:#015  4%|▎         | 381/10570 [00:00<00:27, 374.75it/s][1,10]<stderr>:#015  5%|▍         | 509/10570 [00:01<00:24, 407.78it/s][1,1]<stderr>:#015  5%|▍         | 504/10570 [00:01<00:24, 405.21it/s][1,3]<stderr>:#015  3%|▎         | 324/10570 [00:00<00:31, 328.41it/s][1,11]<stderr>:#015  3%|▎         | 324/10570 [00:00<00:31, 327.71it/s][1,2]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:23, 418.92it/s][1,6]<stderr>:#015  3%|▎         | 281/10570 [00:00<00:34, 298.71it/s][1,5]<stderr>:#015  4%|▎         | 376/10570 [00:00<00:27, 367.19it/s][1,9]<stderr>:#015  4%|▍         | 456/10570 [00:01<00:26, 382.09it/s][1,4]<stderr>:#015  5%|▍         | 506/10570 [00:01<00:24, 406.98it/s][1,12]<stderr>:#015  6%|▌         | 599/10570 [00:01<00:23, 419.95it/s][1,13]<stderr>:#015  3%|▎         | 271/10570 [00:00<00:35, 289.55it/s][1,15]<stderr>:#015  5%|▍         | 502/10570 [00:01<00:25, 401.92it/s][1,7]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 421.28it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▎         | 374/10570 [00:00<00:28, 363.60it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  1%|          | 125/10570 [00:00<00:26, 399.43it/s]#033[A[1,14]<stderr>:#015  4%|▍         | 421/10570 [00:01<00:26, 380.05it/s][1,10]<stderr>:#015  5%|▌         | 552/10570 [00:01<00:24, 413.94it/s][1,1]<stderr>:#015  5%|▌         | 549/10570 [00:01<00:24, 416.97it/s][1,11]<stderr>:#015  3%|▎         | 369/10570 [00:00<00:28, 356.54it/s][1,3]<stderr>:#015  4%|▎         | 370/10570 [00:00<00:28, 357.53it/s][1,2]<stderr>:#015  6%|▌         | 597/10570 [00:01<00:23, 417.08it/s][1,6]<stderr>:#015  3%|▎         | 322/10570 [00:00<00:31, 324.25it/s][1,5]<stderr>:#015  4%|▍         | 415/10570 [00:01<00:27, 372.94it/s][1,9]<stderr>:#015  5%|▍         | 500/10570 [00:01<00:25, 395.75it/s][1,4]<stderr>:#015  5%|▌         | 551/10570 [00:01<00:23, 417.59it/s][1,12]<stderr>:#015  6%|▌         | 644/10570 [00:01<00:23, 426.04it/s][1,13]<stderr>:#015  3%|▎         | 307/10570 [00:00<00:33, 306.43it/s][1,15]<stderr>:#015  5%|▌         | 548/10570 [00:01<00:24, 415.38it/s][1,7]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 429.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▍         | 413/10570 [00:01<00:27, 370.01it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 166/10570 [00:00<00:25, 401.51it/s]#033[A[1,14]<stderr>:#015  4%|▍         | 465/10570 [00:01<00:25, 393.77it/s][1,10]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:24, 413.89it/s][1,11]<stderr>:#015  4%|▍         | 409/10570 [00:01<00:27, 366.65it/s][1,1]<stderr>:#015  6%|▌         | 592/10570 [00:01<00:23, 415.82it/s][1,3]<stderr>:#015  4%|▍         | 410/10570 [00:01<00:27, 366.43it/s][1,2]<stderr>:#015  6%|▌         | 642/10570 [00:01<00:23, 425.30it/s][1,6]<stderr>:#015  3%|▎         | 367/10570 [00:00<00:28, 353.82it/s][1,5]<stderr>:#015  4%|▍         | 459/10570 [00:01<00:25, 390.04it/s][1,9]<stderr>:#015  5%|▌         | 546/10570 [00:01<00:24, 411.66it/s][1,4]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:24, 415.26it/s][1,12]<stderr>:#015  7%|▋         | 690/10570 [00:01<00:22, 435.05it/s][1,13]<stderr>:#015  3%|▎         | 351/10570 [00:00<00:30, 336.27it/s][1,15]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:24, 414.25it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  4%|▍         | 456/10570 [00:01<00:26, 385.62it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 205/10570 [00:00<00:26, 395.75it/s]#033[A[1,7]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 419.65it/s][1,14]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:24, 409.27it/s][1,10]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 422.96it/s][1,11]<stderr>:#015  4%|▍         | 451/10570 [00:01<00:26, 379.54it/s][1,1]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:23, 423.51it/s][1,3]<stderr>:#015  4%|▍         | 452/10570 [00:01<00:26, 380.07it/s][1,2]<stderr>:#015  7%|▋         | 688/10570 [00:01<00:22, 433.55it/s][1,6]<stderr>:#015  4%|▍         | 407/10570 [00:01<00:27, 364.58it/s][1,5]<stderr>:#015  5%|▍         | 503/10570 [00:01<00:24, 403.30it/s][1,9]<stderr>:#015  6%|▌         | 588/10570 [00:01<00:24, 411.77it/s][1,4]<stderr>:#015  6%|▌         | 639/10570 [00:01<00:23, 423.70it/s][1,13]<stderr>:#015  4%|▎         | 393/10570 [00:01<00:28, 356.20it/s][1,12]<stderr>:#015  7%|▋         | 734/10570 [00:01<00:23, 421.73it/s][1,15]<stderr>:#015  6%|▌         | 636/10570 [00:01<00:23, 422.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  5%|▍         | 500/10570 [00:01<00:25, 398.42it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  2%|▏         | 239/10570 [00:00<00:28, 361.68it/s]#033[A[1,7]<stderr>:#015  7%|▋         | 771/10570 [00:01<00:23, 412.09it/s][1,14]<stderr>:#015  5%|▌         | 555/10570 [00:01<00:23, 417.39it/s][1,10]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 428.62it/s][1,11]<stderr>:#015  5%|▍         | 493/10570 [00:01<00:25, 390.69it/s][1,1]<stderr>:#015  6%|▋         | 681/10570 [00:01<00:23, 428.16it/s][1,3]<stderr>:#015  5%|▍         | 494/10570 [00:01<00:25, 390.53it/s][1,6]<stderr>:#015  4%|▍         | 448/10570 [00:01<00:26, 376.10it/s][1,2]<stderr>:#015  7%|▋         | 732/10570 [00:01<00:23, 420.03it/s][1,5]<stderr>:#015  5%|▌         | 549/10570 [00:01<00:24, 415.93it/s][1,9]<stderr>:#015  6%|▌         | 632/10570 [00:01<00:23, 419.33it/s][1,4]<stderr>:#015  6%|▋         | 684/10570 [00:01<00:23, 429.21it/s][1,13]<stderr>:#015  4%|▍         | 432/10570 [00:01<00:27, 363.91it/s][1,12]<stderr>:#015  7%|▋         | 777/10570 [00:01<00:23, 410.28it/s][1,15]<stderr>:#015  6%|▋         | 680/10570 [00:01<00:23, 421.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  5%|▌         | 545/10570 [00:01<00:24, 412.56it/s]#033[A[1,11]<stderr>:#015  5%|▌         | 539/10570 [00:01<00:24, 408.22it/s][1,3]<stderr>:#015  5%|▌         | 540/10570 [00:01<00:24, 407.40it/s][1,14]<stderr>:#015  6%|▌         | 598/10570 [00:01<00:24, 412.98it/s][1,10]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 417.36it/s][1,1]<stderr>:#015  7%|▋         | 725/10570 [00:01<00:23, 416.76it/s][1,7]<stderr>:#015  8%|▊         | 813/10570 [00:02<00:25, 386.81it/s][1,6]<stderr>:#015  5%|▍         | 490/10570 [00:01<00:26, 386.50it/s][1,9]<stderr>:#015  6%|▋         | 676/10570 [00:01<00:23, 423.02it/s][1,5]<stderr>:#015  6%|▌         | 592/10570 [00:01<00:24, 411.43it/s][1,2]<stderr>:#015  7%|▋         | 775/10570 [00:01<00:23, 408.75it/s][1,4]<stderr>:#015  7%|▋         | 728/10570 [00:01<00:23, 416.54it/s][1,13]<stderr>:#015  4%|▍         | 470/10570 [00:01<00:27, 366.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 272/10570 [00:00<00:36, 284.79it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▌         | 587/10570 [00:01<00:24, 411.80it/s]#033[A[1,15]<stderr>:#015  7%|▋         | 723/10570 [00:01<00:23, 411.80it/s][1,12]<stderr>:#015  8%|▊         | 819/10570 [00:02<00:25, 382.78it/s][1,11]<stderr>:#015  5%|▌         | 581/10570 [00:01<00:24, 407.17it/s][1,14]<stderr>:#015  6%|▌         | 643/10570 [00:01<00:23, 421.09it/s][1,3]<stderr>:#015  6%|▌         | 582/10570 [00:01<00:24, 407.47it/s][1,7]<stderr>:#015  8%|▊         | 853/10570 [00:02<00:25, 388.45it/s][1,10]<stderr>:#015  7%|▋         | 770/10570 [00:01<00:23, 409.12it/s][1,1]<stderr>:#015  7%|▋         | 767/10570 [00:01<00:23, 408.85it/s][1,6]<stderr>:#015  5%|▌         | 537/10570 [00:01<00:24, 405.67it/s][1,5]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:23, 420.56it/s][1,9]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:23, 414.77it/s][1,13]<stderr>:#015  5%|▍         | 513/10570 [00:01<00:26, 382.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 308/10570 [00:00<00:33, 302.68it/s]#033[A[1,2]<stderr>:#015  8%|▊         | 817/10570 [00:02<00:25, 382.24it/s][1,4]<stderr>:#015  7%|▋         | 770/10570 [00:01<00:24, 400.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▌         | 631/10570 [00:01<00:23, 418.26it/s]#033[A[1,15]<stderr>:#015  7%|▋         | 765/10570 [00:01<00:24, 404.77it/s][1,12]<stderr>:#015  8%|▊         | 860/10570 [00:02<00:24, 389.22it/s][1,11]<stderr>:#015  6%|▌         | 623/10570 [00:01<00:24, 408.87it/s][1,14]<stderr>:#015  7%|▋         | 689/10570 [00:01<00:22, 429.87it/s][1,3]<stderr>:#015  6%|▌         | 624/10570 [00:01<00:24, 399.66it/s][1,7]<stderr>:#015  8%|▊         | 896/10570 [00:02<00:24, 398.72it/s][1,6]<stderr>:#015  5%|▌         | 579/10570 [00:01<00:24, 403.58it/s][1,5]<stderr>:#015  6%|▋         | 681/10570 [00:01<00:23, 425.39it/s][1,1]<stderr>:#015  8%|▊         | 809/10570 [00:02<00:25, 382.22it/s][1,10]<stderr>:#015  8%|▊         | 812/10570 [00:02<00:25, 376.23it/s][1,13]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:25, 388.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  3%|▎         | 352/10570 [00:00<00:30, 333.16it/s]#033[A[1,9]<stderr>:#015  7%|▋         | 761/10570 [00:01<00:24, 393.51it/s][1,2]<stderr>:#015  8%|▊         | 857/10570 [00:02<00:25, 385.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  6%|▋         | 674/10570 [00:01<00:23, 421.40it/s]#033[A[1,4]<stderr>:#015  8%|▊         | 811/10570 [00:02<00:25, 376.62it/s][1,12]<stderr>:#015  9%|▊         | 902/10570 [00:02<00:24, 395.08it/s][1,11]<stderr>:#015  6%|▋         | 665/10570 [00:01<00:24, 411.60it/s][1,15]<stderr>:#015  8%|▊         | 806/10570 [00:02<00:25, 380.11it/s][1,3]<stderr>:#015  6%|▋         | 667/10570 [00:01<00:24, 406.10it/s][1,14]<stderr>:#015  7%|▋         | 733/10570 [00:01<00:23, 416.98it/s][1,7]<stderr>:#015  9%|▉         | 937/10570 [00:02<00:24, 393.31it/s][1,6]<stderr>:#015  6%|▌         | 620/10570 [00:01<00:24, 405.12it/s][1,1]<stderr>:#015  8%|▊         | 848/10570 [00:02<00:25, 381.29it/s][1,5]<stderr>:#015  7%|▋         | 724/10570 [00:01<00:23, 414.92it/s][1,10]<stderr>:#015  8%|▊         | 851/10570 [00:02<00:25, 376.46it/s][1,13]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:25, 386.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▎         | 394/10570 [00:01<00:28, 353.23it/s]#033[A[1,2]<stderr>:#015  9%|▊         | 899/10570 [00:02<00:24, 394.35it/s][1,9]<stderr>:#015  8%|▊         | 801/10570 [00:02<00:25, 375.74it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  7%|▋         | 717/10570 [00:01<00:23, 414.17it/s]#033[A[1,4]<stderr>:#015  8%|▊         | 850/10570 [00:02<00:25, 378.38it/s][1,12]<stderr>:#015  9%|▉         | 942/10570 [00:02<00:24, 389.66it/s][1,11]<stderr>:#015  7%|▋         | 707/10570 [00:01<00:23, 412.27it/s][1,3]<stderr>:#015  7%|▋         | 709/10570 [00:01<00:24, 407.38it/s][1,15]<stderr>:#015  8%|▊         | 845/10570 [00:02<00:25, 376.86it/s][1,7]<stderr>:#015  9%|▉         | 977/10570 [00:02<00:24, 394.67it/s][1,14]<stderr>:#015  7%|▋         | 775/10570 [00:01<00:24, 406.17it/s][1,6]<stderr>:#015  6%|▋         | 662/10570 [00:01<00:24, 408.57it/s][1,1]<stderr>:#015  8%|▊         | 890/10570 [00:02<00:24, 391.43it/s][1,10]<stderr>:#015  8%|▊         | 893/10570 [00:02<00:24, 387.99it/s][1,5]<stderr>:#015  7%|▋         | 766/10570 [00:01<00:24, 406.54it/s][1,13]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:25, 395.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▍         | 433/10570 [00:01<00:28, 361.19it/s]#033[A[1,2]<stderr>:#015  9%|▉         | 939/10570 [00:02<00:24, 387.30it/s][1,9]<stderr>:#015  8%|▊         | 839/10570 [00:02<00:26, 365.17it/s][1,4]<stderr>:#015  8%|▊         | 892/10570 [00:02<00:24, 389.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  7%|▋         | 759/10570 [00:01<00:24, 402.37it/s]#033[A[1,12]<stderr>:#015  9%|▉         | 982/10570 [00:02<00:25, 377.79it/s][1,15]<stderr>:#015  8%|▊         | 886/10570 [00:02<00:25, 385.74it/s][1,11]<stderr>:#015  7%|▋         | 749/10570 [00:01<00:24, 398.94it/s][1,7]<stderr>:#015 10%|▉         | 1017/10570 [00:02<00:24, 392.06it/s][1,3]<stderr>:#015  7%|▋         | 750/10570 [00:01<00:24, 395.28it/s][1,6]<stderr>:#015  7%|▋         | 704/10570 [00:01<00:24, 410.93it/s][1,14]<stderr>:#015  8%|▊         | 816/10570 [00:02<00:25, 381.52it/s][1,1]<stderr>:#015  9%|▉         | 930/10570 [00:02<00:25, 383.10it/s][1,13]<stderr>:#015  6%|▋         | 678/10570 [00:01<00:24, 399.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  4%|▍         | 470/10570 [00:01<00:27, 363.50it/s]#033[A[1,10]<stderr>:#015  9%|▉         | 933/10570 [00:02<00:25, 373.95it/s][1,2]<stderr>:#015  9%|▉         | 978/10570 [00:02<00:24, 386.70it/s][1,5]<stderr>:#015  8%|▊         | 807/10570 [00:02<00:25, 380.35it/s][1,9]<stderr>:#015  8%|▊         | 878/10570 [00:02<00:26, 370.46it/s][1,4]<stderr>:#015  9%|▉         | 932/10570 [00:02<00:25, 379.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 800/10570 [00:02<00:25, 382.97it/s]#033[A[1,12]<stderr>:#015 10%|▉         | 1021/10570 [00:02<00:25, 377.36it/s][1,15]<stderr>:#015  9%|▉         | 925/10570 [00:02<00:25, 378.91it/s][1,7]<stderr>:#015 10%|█         | 1057/10570 [00:02<00:24, 391.37it/s][1,11]<stderr>:#015  7%|▋         | 790/10570 [00:02<00:25, 385.04it/s][1,3]<stderr>:#015  7%|▋         | 790/10570 [00:02<00:25, 382.26it/s][1,6]<stderr>:#015  7%|▋         | 746/10570 [00:01<00:24, 398.25it/s][1,14]<stderr>:#015  8%|▊         | 855/10570 [00:02<00:25, 383.78it/s][1,1]<stderr>:#015  9%|▉         | 969/10570 [00:02<00:24, 384.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  5%|▍         | 513/10570 [00:01<00:26, 380.48it/s]#033[A[1,13]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:25, 389.94it/s][1,10]<stderr>:#015  9%|▉         | 972/10570 [00:02<00:26, 368.61it/s][1,2]<stderr>:#015 10%|▉         | 1017/10570 [00:02<00:24, 383.67it/s][1,5]<stderr>:#015  8%|▊         | 846/10570 [00:02<00:25, 376.18it/s][1,9]<stderr>:#015  9%|▊         | 916/10570 [00:02<00:26, 361.69it/s][1,4]<stderr>:#015  9%|▉         | 971/10570 [00:02<00:25, 380.35it/s][1,12]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:24, 381.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 839/10570 [00:02<00:26, 371.72it/s]#033[A[1,7]<stderr>:#015 10%|█         | 1097/10570 [00:02<00:24, 383.17it/s][1,15]<stderr>:#015  9%|▉         | 964/10570 [00:02<00:26, 367.57it/s][1,14]<stderr>:#015  8%|▊         | 897/10570 [00:02<00:24, 392.32it/s][1,11]<stderr>:#015  8%|▊         | 829/10570 [00:02<00:26, 364.15it/s][1,1]<stderr>:#015 10%|▉         | 1008/10570 [00:02<00:25, 382.23it/s][1,3]<stderr>:#015  8%|▊         | 829/10570 [00:02<00:26, 362.86it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:25, 386.93it/s]#033[A[1,6]<stderr>:#015  7%|▋         | 787/10570 [00:02<00:25, 380.63it/s][1,10]<stderr>:#015 10%|▉         | 1010/10570 [00:02<00:25, 371.15it/s][1,2]<stderr>:#015 10%|▉         | 1056/10570 [00:02<00:24, 384.51it/s][1,5]<stderr>:#015  8%|▊         | 888/10570 [00:02<00:25, 386.74it/s][1,13]<stderr>:#015  7%|▋         | 759/10570 [00:02<00:25, 379.28it/s][1,9]<stderr>:#015  9%|▉         | 955/10570 [00:02<00:26, 369.19it/s][1,4]<stderr>:#015 10%|▉         | 1010/10570 [00:02<00:25, 380.80it/s][1,12]<stderr>:#015 10%|█         | 1100/10570 [00:02<00:25, 375.87it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  8%|▊         | 881/10570 [00:02<00:25, 382.45it/s]#033[A[1,7]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:23, 393.69it/s][1,15]<stderr>:#015  9%|▉         | 1002/10570 [00:02<00:25, 369.86it/s][1,11]<stderr>:#015  8%|▊         | 869/10570 [00:02<00:26, 372.60it/s][1,14]<stderr>:#015  9%|▉         | 937/10570 [00:02<00:25, 385.10it/s][1,3]<stderr>:#015  8%|▊         | 869/10570 [00:02<00:26, 372.18it/s][1,1]<stderr>:#015 10%|▉         | 1047/10570 [00:02<00:24, 381.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:25, 385.45it/s]#033[A[1,10]<stderr>:#015 10%|▉         | 1049/10570 [00:02<00:25, 374.39it/s][1,2]<stderr>:#015 10%|█         | 1095/10570 [00:02<00:24, 380.25it/s][1,6]<stderr>:#015  8%|▊         | 826/10570 [00:02<00:27, 359.03it/s][1,5]<stderr>:#015  9%|▉         | 927/10570 [00:02<00:25, 379.00it/s][1,9]<stderr>:#015  9%|▉         | 993/10570 [00:02<00:26, 368.25it/s][1,13]<stderr>:#015  8%|▊         | 798/10570 [00:02<00:26, 362.38it/s][1,4]<stderr>:#015 10%|▉         | 1049/10570 [00:02<00:25, 380.63it/s][1,12]<stderr>:#015 11%|█         | 1142/10570 [00:02<00:24, 386.88it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▊         | 920/10570 [00:02<00:25, 376.56it/s]#033[A[1,7]<stderr>:#015 11%|█         | 1186/10570 [00:02<00:22, 411.44it/s][1,15]<stderr>:#015 10%|▉         | 1040/10570 [00:02<00:25, 368.86it/s][1,11]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:25, 374.47it/s][1,14]<stderr>:#015  9%|▉         | 976/10570 [00:02<00:24, 384.35it/s][1,1]<stderr>:#015 10%|█         | 1086/10570 [00:02<00:24, 383.17it/s][1,3]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:25, 374.43it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▌         | 637/10570 [00:01<00:25, 395.19it/s][1,8]<stderr>:#033[A[1,10]<stderr>:#015 10%|█         | 1089/10570 [00:02<00:24, 380.41it/s][1,2]<stderr>:#015 11%|█         | 1136/10570 [00:02<00:24, 387.37it/s][1,6]<stderr>:#015  8%|▊         | 865/10570 [00:02<00:26, 367.14it/s][1,5]<stderr>:#015  9%|▉         | 966/10570 [00:02<00:25, 381.92it/s][1,9]<stderr>:#015 10%|▉         | 1031/10570 [00:02<00:25, 370.44it/s][1,4]<stderr>:#015 10%|█         | 1088/10570 [00:02<00:24, 383.35it/s][1,13]<stderr>:#015  8%|▊         | 835/10570 [00:02<00:27, 349.17it/s][1,12]<stderr>:#015 11%|█         | 1188/10570 [00:02<00:23, 404.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▉         | 960/10570 [00:02<00:25, 380.76it/s]#033[A[1,7]<stderr>:#015 12%|█▏        | 1230/10570 [00:03<00:22, 418.08it/s][1,15]<stderr>:#015 10%|█         | 1080/10570 [00:02<00:25, 375.11it/s][1,11]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:25, 373.19it/s][1,14]<stderr>:#015 10%|▉         | 1015/10570 [00:02<00:25, 381.57it/s][1,3]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:25, 374.12it/s][1,1]<stderr>:#015 11%|█         | 1125/10570 [00:02<00:24, 378.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  6%|▋         | 678/10570 [00:01<00:24, 399.24it/s]#033[A[1,10]<stderr>:#015 11%|█         | 1128/10570 [00:02<00:24, 378.87it/s][1,2]<stderr>:#015 11%|█         | 1181/10570 [00:02<00:23, 404.23it/s][1,6]<stderr>:#015  9%|▊         | 905/10570 [00:02<00:25, 373.38it/s][1,5]<stderr>:#015 10%|▉         | 1005/10570 [00:02<00:25, 378.91it/s][1,9]<stderr>:#015 10%|█         | 1069/10570 [00:02<00:25, 371.22it/s][1,4]<stderr>:#015 11%|█         | 1127/10570 [00:02<00:24, 379.02it/s][1,13]<stderr>:#015  8%|▊         | 873/10570 [00:02<00:27, 357.78it/s][1,12]<stderr>:#015 12%|█▏        | 1232/10570 [00:03<00:22, 413.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  9%|▉         | 999/10570 [00:02<00:25, 377.23it/s]#033[A[1,7]<stderr>:#015 12%|█▏        | 1273/10570 [00:03<00:22, 417.91it/s][1,15]<stderr>:#015 11%|█         | 1118/10570 [00:02<00:25, 371.99it/s][1,11]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:25, 370.60it/s][1,14]<stderr>:#015 10%|▉         | 1054/10570 [00:02<00:24, 381.33it/s][1,3]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:25, 372.28it/s][1,1]<stderr>:#015 11%|█         | 1169/10570 [00:02<00:23, 395.14it/s][1,10]<stderr>:#015 11%|█         | 1173/10570 [00:02<00:23, 396.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  7%|▋         | 719/10570 [00:01<00:25, 389.65it/s]#033[A[1,2]<stderr>:#015 12%|█▏        | 1224/10570 [00:03<00:22, 411.21it/s][1,6]<stderr>:#015  9%|▉         | 943/10570 [00:02<00:26, 369.63it/s][1,5]<stderr>:#015 10%|▉         | 1044/10570 [00:02<00:25, 376.95it/s][1,9]<stderr>:#015 10%|█         | 1107/10570 [00:02<00:25, 368.04it/s][1,4]<stderr>:#015 11%|█         | 1172/10570 [00:02<00:23, 395.87it/s][1,13]<stderr>:#015  9%|▊         | 910/10570 [00:02<00:27, 357.10it/s][1,12]<stderr>:#015 12%|█▏        | 1274/10570 [00:03<00:22, 412.70it/s][1,7]<stderr>:#015 12%|█▏        | 1319/10570 [00:03<00:21, 429.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|▉         | 1037/10570 [00:02<00:25, 372.92it/s]#033[A[1,15]<stderr>:#015 11%|█         | 1161/10570 [00:02<00:24, 385.66it/s][1,14]<stderr>:#015 10%|█         | 1093/10570 [00:02<00:24, 383.37it/s][1,1]<stderr>:#015 11%|█▏        | 1212/10570 [00:03<00:23, 404.63it/s][1,11]<stderr>:#015 10%|▉         | 1022/10570 [00:02<00:26, 367.07it/s][1,3]<stderr>:#015 10%|▉         | 1022/10570 [00:02<00:25, 369.44it/s][1,10]<stderr>:#015 12%|█▏        | 1216/10570 [00:03<00:23, 403.82it/s][1,2]<stderr>:#015 12%|█▏        | 1266/10570 [00:03<00:22, 410.45it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  7%|▋         | 759/10570 [00:02<00:26, 373.36it/s]#033[A[1,6]<stderr>:#015  9%|▉         | 981/10570 [00:02<00:26, 368.52it/s][1,5]<stderr>:#015 10%|█         | 1084/10570 [00:02<00:24, 381.21it/s][1,9]<stderr>:#015 11%|█         | 1148/10570 [00:02<00:24, 378.59it/s][1,4]<stderr>:#015 11%|█▏        | 1215/10570 [00:03<00:23, 404.66it/s][1,13]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:27, 356.02it/s][1,12]<stderr>:#015 12%|█▏        | 1320/10570 [00:03<00:21, 424.55it/s][1,7]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:21, 429.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 10%|█         | 1076/10570 [00:02<00:25, 377.46it/s]#033[A[1,15]<stderr>:#015 11%|█▏        | 1205/10570 [00:03<00:23, 398.30it/s][1,1]<stderr>:#015 12%|█▏        | 1255/10570 [00:03<00:22, 409.11it/s][1,14]<stderr>:#015 11%|█         | 1132/10570 [00:02<00:24, 379.12it/s][1,11]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:25, 371.17it/s][1,3]<stderr>:#015 10%|█         | 1061/10570 [00:02<00:25, 372.68it/s][1,10]<stderr>:#015 12%|█▏        | 1257/10570 [00:03<00:22, 405.64it/s][1,2]<stderr>:#015 12%|█▏        | 1311/10570 [00:03<00:21, 421.44it/s][1,6]<stderr>:#015 10%|▉         | 1018/10570 [00:02<00:26, 365.79it/s][1,5]<stderr>:#015 11%|█         | 1123/10570 [00:02<00:25, 375.40it/s][1,9]<stderr>:#015 11%|█▏        | 1192/10570 [00:03<00:23, 394.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 797/10570 [00:02<00:27, 356.95it/s]#033[A[1,4]<stderr>:#015 12%|█▏        | 1257/10570 [00:03<00:22, 406.59it/s][1,13]<stderr>:#015  9%|▉         | 982/10570 [00:02<00:26, 355.56it/s][1,12]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:21, 424.47it/s][1,15]<stderr>:#015 12%|█▏        | 1250/10570 [00:03<00:22, 408.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█         | 1114/10570 [00:02<00:25, 367.88it/s]#033[A[1,7]<stderr>:#015 13%|█▎        | 1407/10570 [00:03<00:22, 415.01it/s][1,1]<stderr>:#015 12%|█▏        | 1299/10570 [00:03<00:22, 416.49it/s][1,14]<stderr>:#015 11%|█         | 1177/10570 [00:02<00:23, 396.01it/s][1,11]<stderr>:#015 10%|█         | 1099/10570 [00:02<00:26, 363.35it/s][1,3]<stderr>:#015 10%|█         | 1099/10570 [00:02<00:26, 363.65it/s][1,10]<stderr>:#015 12%|█▏        | 1302/10570 [00:03<00:22, 417.61it/s][1,2]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:21, 423.90it/s][1,6]<stderr>:#015 10%|▉         | 1055/10570 [00:02<00:25, 366.36it/s][1,5]<stderr>:#015 11%|█         | 1167/10570 [00:02<00:24, 391.72it/s][1,9]<stderr>:#015 12%|█▏        | 1235/10570 [00:03<00:23, 403.12it/s][1,4]<stderr>:#015 12%|█▏        | 1302/10570 [00:03<00:22, 417.51it/s][1,13]<stderr>:#015 10%|▉         | 1018/10570 [00:02<00:27, 351.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 834/10570 [00:02<00:28, 339.01it/s]#033[A[1,12]<stderr>:#015 13%|█▎        | 1406/10570 [00:03<00:22, 409.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█         | 1156/10570 [00:02<00:24, 381.73it/s]#033[A[1,15]<stderr>:#015 12%|█▏        | 1292/10570 [00:03<00:22, 408.64it/s][1,7]<stderr>:#015 14%|█▎        | 1451/10570 [00:03<00:21, 421.56it/s][1,1]<stderr>:#015 13%|█▎        | 1343/10570 [00:03<00:21, 422.35it/s][1,14]<stderr>:#015 12%|█▏        | 1219/10570 [00:03<00:23, 402.61it/s][1,11]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:25, 374.31it/s][1,10]<stderr>:#015 13%|█▎        | 1346/10570 [00:03<00:21, 422.54it/s][1,3]<stderr>:#015 11%|█         | 1140/10570 [00:02<00:25, 373.99it/s][1,2]<stderr>:#015 13%|█▎        | 1397/10570 [00:03<00:22, 414.10it/s][1,6]<stderr>:#015 10%|█         | 1093/10570 [00:02<00:25, 369.00it/s][1,5]<stderr>:#015 11%|█▏        | 1210/10570 [00:03<00:23, 401.97it/s][1,9]<stderr>:#015 12%|█▏        | 1276/10570 [00:03<00:23, 401.68it/s][1,4]<stderr>:#015 13%|█▎        | 1346/10570 [00:03<00:21, 421.45it/s][1,13]<stderr>:#015 10%|▉         | 1054/10570 [00:02<00:27, 352.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  8%|▊         | 872/10570 [00:02<00:27, 349.16it/s]#033[A[1,12]<stderr>:#015 14%|█▎        | 1450/10570 [00:03<00:21, 416.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 11%|█▏        | 1199/10570 [00:03<00:23, 394.15it/s]#033[A[1,15]<stderr>:#015 13%|█▎        | 1336/10570 [00:03<00:22, 417.13it/s][1,7]<stderr>:#015 14%|█▍        | 1494/10570 [00:03<00:21, 423.52it/s][1,14]<stderr>:#015 12%|█▏        | 1260/10570 [00:03<00:23, 403.38it/s][1,1]<stderr>:#015 13%|█▎        | 1386/10570 [00:03<00:22, 414.05it/s][1,11]<stderr>:#015 11%|█         | 1185/10570 [00:03<00:23, 391.82it/s][1,3]<stderr>:#015 11%|█         | 1185/10570 [00:03<00:23, 391.45it/s][1,10]<stderr>:#015 13%|█▎        | 1389/10570 [00:03<00:22, 415.19it/s][1,2]<stderr>:#015 14%|█▎        | 1439/10570 [00:03<00:22, 406.10it/s][1,6]<stderr>:#015 11%|█         | 1130/10570 [00:02<00:25, 365.40it/s][1,5]<stderr>:#015 12%|█▏        | 1253/10570 [00:03<00:22, 408.23it/s][1,9]<stderr>:#015 12%|█▏        | 1321/10570 [00:03<00:22, 412.67it/s][1,4]<stderr>:#015 13%|█▎        | 1389/10570 [00:03<00:22, 413.54it/s][1,13]<stderr>:#015 10%|█         | 1091/10570 [00:02<00:26, 355.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:27, 351.62it/s][1,8]<stderr>:#033[A[1,12]<stderr>:#015 14%|█▍        | 1492/10570 [00:03<00:21, 417.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 12%|█▏        | 1243/10570 [00:03<00:22, 406.12it/s]#033[A[1,7]<stderr>:#015 15%|█▍        | 1538/10570 [00:03<00:21, 427.39it/s][1,15]<stderr>:#015 13%|█▎        | 1378/10570 [00:03<00:22, 410.97it/s][1,14]<stderr>:#015 12%|█▏        | 1305/10570 [00:03<00:22, 415.87it/s][1,11]<stderr>:#015 12%|█▏        | 1227/10570 [00:03<00:23, 397.88it/s][1,3]<stderr>:#015 12%|█▏        | 1227/10570 [00:03<00:23, 398.13it/s][1,1]<stderr>:#015 14%|█▎        | 1428/10570 [00:03<00:23, 394.65it/s][1,10]<stderr>:#015 14%|█▎        | 1431/10570 [00:03<00:22, 409.00it/s][1,2]<stderr>:#015 14%|█▍        | 1482/10570 [00:03<00:22, 411.65it/s][1,6]<stderr>:#015 11%|█         | 1173/10570 [00:03<00:24, 382.04it/s][1,5]<stderr>:#015 12%|█▏        | 1296/10570 [00:03<00:22, 411.98it/s][1,9]<stderr>:#015 13%|█▎        | 1363/10570 [00:03<00:22, 411.51it/s][1,4]<stderr>:#015 14%|█▎        | 1431/10570 [00:03<00:22, 404.58it/s][1,13]<stderr>:#015 11%|█         | 1127/10570 [00:03<00:26, 351.61it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▉         | 944/10570 [00:02<00:27, 351.23it/s]#033[A[1,12]<stderr>:#015 15%|█▍        | 1536/10570 [00:03<00:21, 422.87it/s][1,7]<stderr>:#015 15%|█▍        | 1582/10570 [00:03<00:20, 430.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 12%|█▏        | 1284/10570 [00:03<00:23, 403.09it/s]#033[A[1,15]<stderr>:#015 13%|█▎        | 1420/10570 [00:03<00:22, 407.16it/s][1,14]<stderr>:#015 13%|█▎        | 1348/10570 [00:03<00:22, 418.40it/s][1,11]<stderr>:#015 12%|█▏        | 1268/10570 [00:03<00:23, 395.99it/s][1,3]<stderr>:#015 12%|█▏        | 1268/10570 [00:03<00:23, 397.01it/s][1,1]<stderr>:#015 14%|█▍        | 1470/10570 [00:03<00:22, 400.44it/s][1,10]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:21, 413.64it/s][1,2]<stderr>:#015 14%|█▍        | 1526/10570 [00:03<00:21, 417.29it/s][1,6]<stderr>:#015 11%|█▏        | 1215/10570 [00:03<00:23, 390.17it/s][1,5]<stderr>:#015 13%|█▎        | 1340/10570 [00:03<00:22, 419.08it/s][1,9]<stderr>:#015 13%|█▎        | 1405/10570 [00:03<00:23, 396.11it/s][1,4]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:22, 409.87it/s][1,13]<stderr>:#015 11%|█         | 1168/10570 [00:03<00:25, 365.75it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  9%|▉         | 980/10570 [00:02<00:27, 351.87it/s]#033[A[1,12]<stderr>:#015 15%|█▍        | 1580/10570 [00:03<00:21, 426.60it/s][1,7]<stderr>:#015 15%|█▌        | 1626/10570 [00:03<00:20, 432.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1328/10570 [00:03<00:22, 412.24it/s]#033[A[1,15]<stderr>:#015 14%|█▍        | 1462/10570 [00:03<00:22, 408.62it/s][1,14]<stderr>:#015 13%|█▎        | 1390/10570 [00:03<00:22, 410.40it/s][1,11]<stderr>:#015 12%|█▏        | 1312/10570 [00:03<00:22, 406.95it/s][1,3]<stderr>:#015 12%|█▏        | 1312/10570 [00:03<00:22, 407.37it/s][1,10]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 417.81it/s][1,1]<stderr>:#015 14%|█▍        | 1514/10570 [00:03<00:22, 408.80it/s][1,2]<stderr>:#015 15%|█▍        | 1571/10570 [00:03<00:21, 424.42it/s][1,6]<stderr>:#015 12%|█▏        | 1256/10570 [00:03<00:23, 392.95it/s][1,5]<stderr>:#015 13%|█▎        | 1383/10570 [00:03<00:22, 411.14it/s][1,9]<stderr>:#015 14%|█▎        | 1448/10570 [00:03<00:22, 404.01it/s][1,4]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 414.89it/s][1,13]<stderr>:#015 11%|█▏        | 1209/10570 [00:03<00:24, 375.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 15%|█▌        | 1624/10570 [00:03<00:20, 428.97it/s][1,8]<stderr>:#015 10%|▉         | 1016/10570 [00:02<00:27, 349.40it/s]#033[A[1,7]<stderr>:#015 16%|█▌        | 1674/10570 [00:04<00:19, 445.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1370/10570 [00:03<00:22, 411.45it/s]#033[A[1,15]<stderr>:#015 14%|█▍        | 1505/10570 [00:03<00:21, 413.58it/s][1,14]<stderr>:#015 14%|█▎        | 1432/10570 [00:03<00:22, 405.18it/s][1,11]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:22, 409.47it/s][1,3]<stderr>:#015 13%|█▎        | 1354/10570 [00:03<00:22, 409.03it/s][1,1]<stderr>:#015 15%|█▍        | 1558/10570 [00:03<00:21, 416.43it/s][1,10]<stderr>:#015 15%|█▍        | 1562/10570 [00:03<00:21, 424.81it/s][1,2]<stderr>:#015 15%|█▌        | 1616/10570 [00:03<00:20, 428.48it/s][1,6]<stderr>:#015 12%|█▏        | 1299/10570 [00:03<00:23, 402.35it/s][1,5]<stderr>:#015 13%|█▎        | 1425/10570 [00:03<00:22, 404.81it/s][1,4]<stderr>:#015 15%|█▍        | 1561/10570 [00:03<00:21, 421.71it/s][1,9]<stderr>:#015 14%|█▍        | 1489/10570 [00:03<00:22, 402.17it/s][1,13]<stderr>:#015 12%|█▏        | 1250/10570 [00:03<00:24, 384.87it/s][1,12]<stderr>:#015 16%|█▌        | 1671/10570 [00:04<00:20, 440.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 10%|▉         | 1052/10570 [00:02<00:27, 349.66it/s]#033[A[1,7]<stderr>:#015 16%|█▋        | 1719/10570 [00:04<00:19, 445.42it/s][1,15]<stderr>:#015 15%|█▍        | 1548/10570 [00:03<00:21, 418.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 13%|█▎        | 1412/10570 [00:03<00:22, 400.52it/s]#033[A[1,14]<stderr>:#015 14%|█▍        | 1474/10570 [00:03<00:22, 409.24it/s][1,11]<stderr>:#015 13%|█▎        | 1396/10570 [00:03<00:22, 400.59it/s][1,10]<stderr>:#015 15%|█▌        | 1606/10570 [00:03<00:20, 428.46it/s][1,1]<stderr>:#015 15%|█▌        | 1602/10570 [00:03<00:21, 421.30it/s][1,3]<stderr>:#015 13%|█▎        | 1396/10570 [00:03<00:22, 400.01it/s][1,2]<stderr>:#015 16%|█▌        | 1661/10570 [00:04<00:20, 432.61it/s][1,6]<stderr>:#015 13%|█▎        | 1342/10570 [00:03<00:22, 408.32it/s][1,5]<stderr>:#015 14%|█▍        | 1467/10570 [00:03<00:22, 406.67it/s][1,4]<stderr>:#015 15%|█▌        | 1605/10570 [00:03<00:21, 425.61it/s][1,9]<stderr>:#015 14%|█▍        | 1532/10570 [00:03<00:22, 409.14it/s][1,13]<stderr>:#015 12%|█▏        | 1289/10570 [00:03<00:24, 383.76it/s][1,12]<stderr>:#015 16%|█▌        | 1716/10570 [00:04<00:20, 440.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 10%|█         | 1088/10570 [00:02<00:26, 352.46it/s]#033[A[1,7]<stderr>:#015 17%|█▋        | 1764/10570 [00:04<00:20, 436.67it/s][1,15]<stderr>:#015 15%|█▌        | 1591/10570 [00:03<00:21, 419.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 14%|█▍        | 1454/10570 [00:03<00:22, 404.91it/s]#033[A[1,14]<stderr>:#015 14%|█▍        | 1517/10570 [00:03<00:21, 413.18it/s][1,10]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:20, 428.52it/s][1,1]<stderr>:#015 16%|█▌        | 1645/10570 [00:04<00:21, 423.06it/s][1,11]<stderr>:#015 14%|█▎        | 1437/10570 [00:03<00:22, 398.34it/s][1,3]<stderr>:#015 14%|█▎        | 1437/10570 [00:03<00:22, 397.70it/s][1,2]<stderr>:#015 16%|█▌        | 1707/10570 [00:04<00:20, 438.63it/s][1,6]<stderr>:#015 13%|█▎        | 1383/10570 [00:03<00:22, 400.10it/s][1,5]<stderr>:#015 14%|█▍        | 1511/10570 [00:03<00:21, 413.89it/s][1,4]<stderr>:#015 16%|█▌        | 1648/10570 [00:04<00:20, 426.56it/s][1,9]<stderr>:#015 15%|█▍        | 1575/10570 [00:04<00:21, 413.95it/s][1,13]<stderr>:#015 13%|█▎        | 1331/10570 [00:03<00:23, 391.91it/s][1,12]<stderr>:#015 17%|█▋        | 1761/10570 [00:04<00:20, 433.78it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█         | 1124/10570 [00:03<00:27, 342.60it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 17%|█▋        | 1809/10570 [00:04<00:19, 438.97it/s][1,15]<stderr>:#015 15%|█▌        | 1634/10570 [00:04<00:21, 420.48it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 14%|█▍        | 1496/10570 [00:03<00:22, 407.87it/s]#033[A[1,14]<stderr>:#015 15%|█▍        | 1561/10570 [00:03<00:21, 419.34it/s][1,10]<stderr>:#015 16%|█▌        | 1696/10570 [00:04<00:20, 439.38it/s][1,1]<stderr>:#015 16%|█▌        | 1692/10570 [00:04<00:20, 435.66it/s][1,11]<stderr>:#015 14%|█▍        | 1479/10570 [00:03<00:22, 402.45it/s][1,3]<stderr>:#015 14%|█▍        | 1479/10570 [00:03<00:22, 401.62it/s][1,2]<stderr>:#015 17%|█▋        | 1751/10570 [00:04<00:20, 433.74it/s][1,6]<stderr>:#015 13%|█▎        | 1424/10570 [00:03<00:23, 393.14it/s][1,5]<stderr>:#015 15%|█▍        | 1554/10570 [00:03<00:21, 416.28it/s][1,4]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 438.16it/s][1,9]<stderr>:#015 15%|█▌        | 1618/10570 [00:04<00:21, 417.47it/s][1,13]<stderr>:#015 13%|█▎        | 1371/10570 [00:03<00:23, 389.51it/s][1,12]<stderr>:#015 17%|█▋        | 1806/10570 [00:04<00:20, 436.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█         | 1165/10570 [00:03<00:26, 358.81it/s]#033[A[1,7]<stderr>:#015 18%|█▊        | 1853/10570 [00:04<00:20, 431.49it/s][1,15]<stderr>:#015 16%|█▌        | 1683/10570 [00:04<00:20, 437.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▍        | 1538/10570 [00:03<00:22, 410.25it/s]#033[A[1,14]<stderr>:#015 15%|█▌        | 1605/10570 [00:03<00:21, 423.13it/s][1,11]<stderr>:#015 14%|█▍        | 1521/10570 [00:03<00:22, 407.32it/s][1,1]<stderr>:#015 16%|█▋        | 1736/10570 [00:04<00:20, 427.83it/s][1,10]<stderr>:#015 16%|█▋        | 1741/10570 [00:04<00:20, 430.69it/s][1,3]<stderr>:#015 14%|█▍        | 1521/10570 [00:03<00:22, 406.26it/s][1,2]<stderr>:#015 17%|█▋        | 1796/10570 [00:04<00:20, 435.46it/s][1,6]<stderr>:#015 14%|█▍        | 1465/10570 [00:03<00:23, 395.42it/s][1,5]<stderr>:#015 15%|█▌        | 1597/10570 [00:03<00:21, 419.87it/s][1,9]<stderr>:#015 16%|█▌        | 1662/10570 [00:04<00:21, 422.96it/s][1,4]<stderr>:#015 16%|█▋        | 1739/10570 [00:04<00:20, 429.19it/s][1,13]<stderr>:#015 13%|█▎        | 1411/10570 [00:03<00:24, 378.28it/s][1,12]<stderr>:#015 18%|█▊        | 1850/10570 [00:04<00:20, 431.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 11%|█▏        | 1206/10570 [00:03<00:25, 371.14it/s]#033[A[1,7]<stderr>:#015 18%|█▊        | 1901/10570 [00:04<00:19, 443.90it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▍        | 1581/10570 [00:03<00:21, 413.80it/s]#033[A[1,15]<stderr>:#015 16%|█▋        | 1727/10570 [00:04<00:20, 430.78it/s][1,14]<stderr>:#015 16%|█▌        | 1648/10570 [00:04<00:21, 423.26it/s][1,11]<stderr>:#015 15%|█▍        | 1564/10570 [00:04<00:21, 413.19it/s][1,1]<stderr>:#015 17%|█▋        | 1780/10570 [00:04<00:20, 428.97it/s][1,10]<stderr>:#015 17%|█▋        | 1786/10570 [00:04<00:20, 434.35it/s][1,3]<stderr>:#015 15%|█▍        | 1564/10570 [00:04<00:21, 411.76it/s][1,2]<stderr>:#015 17%|█▋        | 1840/10570 [00:04<00:20, 428.81it/s][1,6]<stderr>:#015 14%|█▍        | 1508/10570 [00:03<00:22, 403.45it/s][1,5]<stderr>:#015 16%|█▌        | 1640/10570 [00:04<00:21, 421.95it/s][1,9]<stderr>:#015 16%|█▌        | 1707/10570 [00:04<00:20, 427.58it/s][1,4]<stderr>:#015 17%|█▋        | 1783/10570 [00:04<00:20, 430.59it/s][1,13]<stderr>:#015 14%|█▎        | 1451/10570 [00:03<00:23, 382.58it/s][1,12]<stderr>:#015 18%|█▊        | 1896/10570 [00:04<00:19, 439.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 12%|█▏        | 1248/10570 [00:03<00:24, 384.13it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 18%|█▊        | 1946/10570 [00:04<00:19, 443.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 15%|█▌        | 1623/10570 [00:04<00:21, 415.51it/s]#033[A[1,15]<stderr>:#015 17%|█▋        | 1771/10570 [00:04<00:20, 424.00it/s][1,14]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 434.48it/s][1,11]<stderr>:#015 15%|█▌        | 1607/10570 [00:04<00:21, 417.36it/s][1,1]<stderr>:#015 17%|█▋        | 1823/10570 [00:04<00:20, 428.30it/s][1,3]<stderr>:#015 15%|█▌        | 1607/10570 [00:04<00:21, 415.06it/s][1,10]<stderr>:#015 17%|█▋        | 1830/10570 [00:04<00:20, 431.32it/s][1,2]<stderr>:#015 18%|█▊        | 1885/10570 [00:04<00:20, 434.13it/s][1,6]<stderr>:#015 15%|█▍        | 1550/10570 [00:04<00:22, 405.01it/s][1,5]<stderr>:#015 16%|█▌        | 1688/10570 [00:04<00:20, 436.44it/s][1,9]<stderr>:#015 17%|█▋        | 1750/10570 [00:04<00:20, 422.15it/s][1,4]<stderr>:#015 17%|█▋        | 1827/10570 [00:04<00:20, 429.57it/s][1,12]<stderr>:#015 18%|█▊        | 1941/10570 [00:04<00:19, 441.94it/s][1,13]<stderr>:#015 14%|█▍        | 1490/10570 [00:04<00:23, 382.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 12%|█▏        | 1287/10570 [00:03<00:24, 379.32it/s]#033[A[1,7]<stderr>:#015 19%|█▉        | 1993/10570 [00:04<00:19, 449.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 16%|█▌        | 1668/10570 [00:04<00:20, 424.58it/s]#033[A[1,15]<stderr>:#015 17%|█▋        | 1815/10570 [00:04<00:20, 428.06it/s][1,14]<stderr>:#015 16%|█▋        | 1739/10570 [00:04<00:20, 425.87it/s][1,11]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:21, 416.71it/s][1,1]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:20, 426.22it/s][1,3]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:21, 414.08it/s][1,10]<stderr>:#015 18%|█▊        | 1874/10570 [00:04<00:20, 430.91it/s][1,2]<stderr>:#015 18%|█▊        | 1932/10570 [00:04<00:19, 443.79it/s][1,6]<stderr>:#015 15%|█▌        | 1592/10570 [00:04<00:21, 408.90it/s][1,5]<stderr>:#015 16%|█▋        | 1732/10570 [00:04<00:20, 429.96it/s][1,9]<stderr>:#015 17%|█▋        | 1793/10570 [00:04<00:20, 422.90it/s][1,4]<stderr>:#015 18%|█▊        | 1871/10570 [00:04<00:20, 427.63it/s][1,12]<stderr>:#015 19%|█▉        | 1988/10570 [00:04<00:19, 449.44it/s][1,13]<stderr>:#015 14%|█▍        | 1530/10570 [00:04<00:23, 386.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1329/10570 [00:03<00:23, 388.21it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 16%|█▌        | 1711/10570 [00:04<00:20, 425.91it/s]#033[A[1,7]<stderr>:#015 19%|█▉        | 2039/10570 [00:04<00:19, 443.55it/s][1,15]<stderr>:#015 18%|█▊        | 1858/10570 [00:04<00:20, 420.95it/s][1,14]<stderr>:#015 17%|█▋        | 1783/10570 [00:04<00:20, 428.20it/s][1,11]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 427.86it/s][1,1]<stderr>:#015 18%|█▊        | 1913/10570 [00:04<00:19, 438.16it/s][1,3]<stderr>:#015 16%|█▌        | 1695/10570 [00:04<00:20, 425.50it/s][1,10]<stderr>:#015 18%|█▊        | 1921/10570 [00:04<00:19, 441.34it/s][1,2]<stderr>:#015 19%|█▊        | 1978/10570 [00:04<00:19, 448.35it/s][1,6]<stderr>:#015 15%|█▌        | 1633/10570 [00:04<00:21, 408.55it/s][1,5]<stderr>:#015 17%|█▋        | 1776/10570 [00:04<00:20, 427.36it/s][1,4]<stderr>:#015 18%|█▊        | 1919/10570 [00:04<00:19, 440.22it/s][1,9]<stderr>:#015 17%|█▋        | 1836/10570 [00:04<00:20, 416.78it/s][1,13]<stderr>:#015 15%|█▍        | 1569/10570 [00:04<00:23, 384.02it/s][1,12]<stderr>:#015 19%|█▉        | 2034/10570 [00:04<00:19, 440.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1369/10570 [00:03<00:23, 388.46it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1754/10570 [00:04<00:20, 420.95it/s]#033[A[1,7]<stderr>:#015 20%|█▉        | 2087/10570 [00:05<00:18, 451.99it/s][1,15]<stderr>:#015 18%|█▊        | 1906/10570 [00:04<00:19, 435.11it/s][1,14]<stderr>:#015 17%|█▋        | 1826/10570 [00:04<00:20, 427.24it/s][1,1]<stderr>:#015 19%|█▊        | 1958/10570 [00:04<00:19, 440.45it/s][1,11]<stderr>:#015 16%|█▋        | 1738/10570 [00:04<00:21, 418.58it/s][1,10]<stderr>:#015 19%|█▊        | 1966/10570 [00:04<00:19, 443.74it/s][1,3]<stderr>:#015 16%|█▋        | 1738/10570 [00:04<00:21, 417.62it/s][1,2]<stderr>:#015 19%|█▉        | 2023/10570 [00:04<00:19, 441.15it/s][1,6]<stderr>:#015 16%|█▌        | 1681/10570 [00:04<00:20, 425.26it/s][1,5]<stderr>:#015 17%|█▋        | 1819/10570 [00:04<00:20, 427.17it/s][1,9]<stderr>:#015 18%|█▊        | 1879/10570 [00:04<00:20, 419.73it/s][1,4]<stderr>:#015 19%|█▊        | 1964/10570 [00:04<00:19, 440.98it/s][1,13]<stderr>:#015 15%|█▌        | 1611/10570 [00:04<00:22, 392.77it/s][1,12]<stderr>:#015 20%|█▉        | 2080/10570 [00:05<00:19, 443.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1798/10570 [00:04<00:20, 426.08it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 13%|█▎        | 1408/10570 [00:03<00:24, 372.48it/s]#033[A[1,15]<stderr>:#015 18%|█▊        | 1950/10570 [00:04<00:19, 436.03it/s][1,14]<stderr>:#015 18%|█▊        | 1869/10570 [00:04<00:20, 425.90it/s][1,1]<stderr>:#015 19%|█▉        | 2003/10570 [00:04<00:19, 439.20it/s][1,11]<stderr>:#015 17%|█▋        | 1781/10570 [00:04<00:20, 419.86it/s][1,7]<stderr>:#015 20%|██        | 2133/10570 [00:05<00:20, 409.87it/s][1,3]<stderr>:#015 17%|█▋        | 1781/10570 [00:04<00:20, 419.67it/s][1,10]<stderr>:#015 19%|█▉        | 2011/10570 [00:04<00:19, 434.35it/s][1,2]<stderr>:#015 20%|█▉        | 2069/10570 [00:05<00:19, 445.02it/s][1,6]<stderr>:#015 16%|█▋        | 1724/10570 [00:04<00:21, 419.94it/s][1,5]<stderr>:#015 18%|█▊        | 1862/10570 [00:04<00:20, 423.23it/s][1,9]<stderr>:#015 18%|█▊        | 1926/10570 [00:04<00:19, 432.40it/s][1,4]<stderr>:#015 19%|█▉        | 2009/10570 [00:04<00:19, 437.70it/s][1,13]<stderr>:#015 16%|█▌        | 1651/10570 [00:04<00:22, 389.49it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 17%|█▋        | 1841/10570 [00:04<00:20, 418.48it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▎        | 1448/10570 [00:03<00:24, 379.57it/s]#033[A[1,15]<stderr>:#015 19%|█▉        | 1995/10570 [00:04<00:19, 438.51it/s][1,12]<stderr>:#015 20%|██        | 2125/10570 [00:05<00:20, 408.35it/s][1,14]<stderr>:#015 18%|█▊        | 1917/10570 [00:04<00:19, 438.83it/s][1,1]<stderr>:#015 19%|█▉        | 2047/10570 [00:05<00:19, 437.37it/s][1,11]<stderr>:#015 17%|█▋        | 1824/10570 [00:04<00:20, 418.66it/s][1,7]<stderr>:#015 21%|██        | 2175/10570 [00:05<00:20, 410.83it/s][1,10]<stderr>:#015 19%|█▉        | 2056/10570 [00:05<00:19, 436.77it/s][1,3]<stderr>:#015 17%|█▋        | 1824/10570 [00:04<00:20, 418.11it/s][1,2]<stderr>:#015 20%|██        | 2115/10570 [00:05<00:18, 449.33it/s][1,6]<stderr>:#015 17%|█▋        | 1767/10570 [00:04<00:21, 414.90it/s][1,5]<stderr>:#015 18%|█▊        | 1910/10570 [00:04<00:19, 436.91it/s][1,9]<stderr>:#015 19%|█▊        | 1970/10570 [00:04<00:19, 433.88it/s][1,4]<stderr>:#015 19%|█▉        | 2054/10570 [00:05<00:19, 439.54it/s][1,13]<stderr>:#015 16%|█▌        | 1694/10570 [00:04<00:22, 400.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 18%|█▊        | 1885/10570 [00:04<00:20, 423.21it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▍        | 1487/10570 [00:04<00:24, 372.92it/s]#033[A[1,15]<stderr>:#015 19%|█▉        | 2039/10570 [00:05<00:19, 429.84it/s][1,14]<stderr>:#015 19%|█▊        | 1962/10570 [00:04<00:19, 437.48it/s][1,12]<stderr>:#015 21%|██        | 2167/10570 [00:05<00:21, 398.77it/s][1,7]<stderr>:#015 21%|██        | 2220/10570 [00:05<00:19, 420.67it/s][1,1]<stderr>:#015 20%|█▉        | 2091/10570 [00:05<00:19, 431.81it/s][1,11]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:21, 413.51it/s][1,10]<stderr>:#015 20%|█▉        | 2101/10570 [00:05<00:19, 439.29it/s][1,3]<stderr>:#015 18%|█▊        | 1866/10570 [00:04<00:21, 413.70it/s][1,5]<stderr>:#015 18%|█▊        | 1954/10570 [00:04<00:19, 435.64it/s][1,4]<stderr>:#015 20%|█▉        | 2100/10570 [00:05<00:19, 442.34it/s][1,9]<stderr>:#015 19%|█▉        | 2014/10570 [00:05<00:20, 425.01it/s][1,2]<stderr>:#015 20%|██        | 2160/10570 [00:05<00:21, 399.74it/s][1,13]<stderr>:#015 16%|█▋        | 1735/10570 [00:04<00:22, 390.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 18%|█▊        | 1931/10570 [00:04<00:20, 431.30it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 14%|█▍        | 1527/10570 [00:04<00:23, 380.28it/s]#033[A[1,15]<stderr>:#015 20%|█▉        | 2084/10570 [00:05<00:19, 433.45it/s][1,6]<stderr>:#015 17%|█▋        | 1809/10570 [00:04<00:24, 354.16it/s][1,14]<stderr>:#015 19%|█▉        | 2006/10570 [00:04<00:19, 436.00it/s][1,12]<stderr>:#015 21%|██        | 2210/10570 [00:05<00:20, 406.41it/s][1,11]<stderr>:#015 18%|█▊        | 1912/10570 [00:04<00:20, 426.39it/s][1,7]<stderr>:#015 21%|██▏       | 2263/10570 [00:05<00:19, 416.17it/s][1,3]<stderr>:#015 18%|█▊        | 1912/10570 [00:04<00:20, 425.05it/s][1,5]<stderr>:#015 19%|█▉        | 1998/10570 [00:04<00:19, 433.98it/s][1,10]<stderr>:#015 20%|██        | 2145/10570 [00:05<00:21, 397.22it/s][1,1]<stderr>:#015 20%|██        | 2135/10570 [00:05<00:21, 385.69it/s][1,9]<stderr>:#015 19%|█▉        | 2058/10570 [00:05<00:19, 426.59it/s][1,2]<stderr>:#015 21%|██        | 2202/10570 [00:05<00:21, 391.93it/s][1,13]<stderr>:#015 17%|█▋        | 1775/10570 [00:04<00:22, 390.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 19%|█▊        | 1976/10570 [00:04<00:19, 434.13it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 15%|█▍        | 1568/10570 [00:04<00:23, 388.20it/s]#033[A[1,4]<stderr>:#015 20%|██        | 2145/10570 [00:05<00:21, 398.37it/s][1,6]<stderr>:#015 17%|█▋        | 1848/10570 [00:04<00:24, 363.11it/s][1,14]<stderr>:#015 19%|█▉        | 2051/10570 [00:05<00:19, 437.84it/s][1,12]<stderr>:#015 21%|██▏       | 2254/10570 [00:05<00:20, 415.13it/s][1,11]<stderr>:#015 19%|█▊        | 1956/10570 [00:04<00:20, 428.98it/s][1,7]<stderr>:#015 22%|██▏       | 2307/10570 [00:05<00:19, 422.12it/s][1,3]<stderr>:#015 19%|█▊        | 1956/10570 [00:04<00:20, 426.97it/s][1,15]<stderr>:#015 20%|██        | 2128/10570 [00:05<00:21, 396.41it/s][1,5]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:19, 431.02it/s][1,10]<stderr>:#015 21%|██        | 2186/10570 [00:05<00:21, 396.23it/s][1,1]<stderr>:#015 21%|██        | 2176/10570 [00:05<00:21, 389.64it/s][1,9]<stderr>:#015 20%|█▉        | 2103/10570 [00:05<00:19, 432.81it/s][1,13]<stderr>:#015 17%|█▋        | 1816/10570 [00:04<00:22, 396.34it/s][1,2]<stderr>:#015 21%|██▏       | 2247/10570 [00:05<00:20, 405.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 19%|█▉        | 2020/10570 [00:05<00:20, 427.17it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 15%|█▌        | 1609/10570 [00:04<00:22, 393.84it/s]#033[A[1,4]<stderr>:#015 21%|██        | 2186/10570 [00:05<00:21, 396.26it/s][1,6]<stderr>:#015 18%|█▊        | 1892/10570 [00:04<00:22, 381.86it/s][1,14]<stderr>:#015 20%|█▉        | 2098/10570 [00:05<00:19, 444.16it/s][1,12]<stderr>:#015 22%|██▏       | 2296/10570 [00:05<00:20, 409.81it/s][1,11]<stderr>:#015 19%|█▉        | 1999/10570 [00:05<00:20, 423.75it/s][1,7]<stderr>:#015 22%|██▏       | 2350/10570 [00:05<00:19, 422.91it/s][1,3]<stderr>:#015 19%|█▉        | 1999/10570 [00:05<00:20, 426.74it/s][1,15]<stderr>:#015 21%|██        | 2169/10570 [00:05<00:21, 399.60it/s][1,5]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 437.42it/s][1,1]<stderr>:#015 21%|██        | 2221/10570 [00:05<00:20, 404.41it/s][1,10]<stderr>:#015 21%|██        | 2233/10570 [00:05<00:20, 413.57it/s][1,2]<stderr>:#015 22%|██▏       | 2289/10570 [00:05<00:20, 409.86it/s][1,13]<stderr>:#015 18%|█▊        | 1856/10570 [00:04<00:22, 392.97it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|█▉        | 2065/10570 [00:05<00:19, 431.28it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▌        | 1649/10570 [00:04<00:22, 392.93it/s]#033[A[1,9]<stderr>:#015 20%|██        | 2147/10570 [00:05<00:21, 389.99it/s][1,4]<stderr>:#015 21%|██        | 2232/10570 [00:05<00:20, 413.13it/s][1,6]<stderr>:#015 18%|█▊        | 1936/10570 [00:04<00:21, 396.17it/s][1,12]<stderr>:#015 22%|██▏       | 2339/10570 [00:05<00:19, 413.66it/s][1,11]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:20, 421.54it/s][1,15]<stderr>:#015 21%|██        | 2211/10570 [00:05<00:20, 405.25it/s][1,7]<stderr>:#015 23%|██▎       | 2393/10570 [00:05<00:19, 415.00it/s][1,3]<stderr>:#015 19%|█▉        | 2042/10570 [00:05<00:20, 422.99it/s][1,14]<stderr>:#015 20%|██        | 2143/10570 [00:05<00:20, 402.06it/s][1,1]<stderr>:#015 21%|██▏       | 2263/10570 [00:05<00:20, 404.01it/s][1,10]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:20, 408.28it/s][1,2]<stderr>:#015 22%|██▏       | 2332/10570 [00:05<00:19, 413.14it/s][1,13]<stderr>:#015 18%|█▊        | 1901/10570 [00:05<00:21, 407.14it/s][1,5]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 395.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|█▉        | 2110/10570 [00:05<00:19, 436.40it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▌        | 1694/10570 [00:04<00:21, 405.52it/s][1,8]<stderr>:#033[A[1,9]<stderr>:#015 21%|██        | 2187/10570 [00:05<00:21, 390.20it/s][1,6]<stderr>:#015 19%|█▊        | 1981/10570 [00:05<00:20, 410.23it/s][1,4]<stderr>:#015 22%|██▏       | 2274/10570 [00:05<00:20, 408.29it/s][1,12]<stderr>:#015 23%|██▎       | 2381/10570 [00:05<00:19, 409.88it/s][1,11]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 430.59it/s][1,15]<stderr>:#015 21%|██▏       | 2254/10570 [00:05<00:20, 411.43it/s][1,3]<stderr>:#015 20%|█▉        | 2088/10570 [00:05<00:19, 431.60it/s][1,7]<stderr>:#015 23%|██▎       | 2435/10570 [00:05<00:19, 413.35it/s][1,14]<stderr>:#015 21%|██        | 2185/10570 [00:05<00:21, 398.97it/s][1,1]<stderr>:#015 22%|██▏       | 2306/10570 [00:05<00:20, 411.24it/s][1,10]<stderr>:#015 22%|██▏       | 2318/10570 [00:05<00:20, 412.58it/s][1,2]<stderr>:#015 22%|██▏       | 2374/10570 [00:05<00:19, 412.82it/s][1,13]<stderr>:#015 18%|█▊        | 1943/10570 [00:05<00:21, 409.58it/s][1,5]<stderr>:#015 21%|██        | 2173/10570 [00:05<00:21, 398.60it/s][1,9]<stderr>:#015 21%|██        | 2233/10570 [00:05<00:20, 407.72it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 16%|█▋        | 1735/10570 [00:04<00:22, 399.12it/s]#033[A[1,6]<stderr>:#015 19%|█▉        | 2023/10570 [00:05<00:20, 408.56it/s][1,4]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:19, 412.66it/s][1,12]<stderr>:#015 23%|██▎       | 2423/10570 [00:05<00:20, 401.20it/s][1,15]<stderr>:#015 22%|██▏       | 2296/10570 [00:05<00:20, 412.09it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 20%|██        | 2154/10570 [00:05<00:21, 390.92it/s]#033[A[1,7]<stderr>:#015 23%|██▎       | 2477/10570 [00:05<00:19, 409.96it/s][1,14]<stderr>:#015 21%|██        | 2231/10570 [00:05<00:20, 415.26it/s][1,11]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 387.68it/s][1,1]<stderr>:#015 22%|██▏       | 2348/10570 [00:05<00:19, 412.12it/s][1,10]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 417.09it/s][1,3]<stderr>:#015 20%|██        | 2132/10570 [00:05<00:21, 387.81it/s][1,13]<stderr>:#015 19%|█▉        | 1987/10570 [00:05<00:20, 417.24it/s][1,2]<stderr>:#015 23%|██▎       | 2416/10570 [00:05<00:20, 404.86it/s][1,5]<stderr>:#015 21%|██        | 2217/10570 [00:05<00:20, 407.84it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 17%|█▋        | 1776/10570 [00:04<00:22, 398.74it/s]#033[A[1,6]<stderr>:#015 20%|█▉        | 2067/10570 [00:05<00:20, 416.25it/s][1,9]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:20, 402.96it/s][1,4]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 417.71it/s][1,15]<stderr>:#015 22%|██▏       | 2338/10570 [00:05<00:19, 412.32it/s][1,12]<stderr>:#015 23%|██▎       | 2464/10570 [00:05<00:20, 399.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 21%|██        | 2195/10570 [00:05<00:21, 393.72it/s]#033[A[1,14]<stderr>:#015 22%|██▏       | 2274/10570 [00:05<00:20, 410.24it/s][1,7]<stderr>:#015 24%|██▍       | 2519/10570 [00:06<00:20, 388.74it/s][1,11]<stderr>:#015 21%|██        | 2172/10570 [00:05<00:21, 391.28it/s][1,1]<stderr>:#015 23%|██▎       | 2390/10570 [00:05<00:20, 406.90it/s][1,3]<stderr>:#015 21%|██        | 2172/10570 [00:05<00:21, 384.28it/s][1,10]<stderr>:#015 23%|██▎       | 2403/10570 [00:05<00:20, 405.24it/s][1,13]<stderr>:#015 19%|█▉        | 2029/10570 [00:05<00:20, 408.25it/s][1,2]<stderr>:#015 23%|██▎       | 2457/10570 [00:05<00:20, 400.96it/s][1,5]<stderr>:#015 21%|██▏       | 2259/10570 [00:05<00:20, 407.69it/s][1,8]<stderr>:#015100%|██████████| 169/169 [00:30<00:00, 12.65it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 17%|█▋        | 1816/10570 [00:04<00:21, 399.05it/s]#033[A[1,6]<stderr>:#015 20%|█▉        | 2110/10570 [00:05<00:20, 418.65it/s][1,9]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:20, 406.73it/s][1,4]<stderr>:#015 23%|██▎       | 2404/10570 [00:05<00:20, 404.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 21%|██        | 2239/10570 [00:05<00:20, 404.68it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2380/10570 [00:05<00:20, 407.40it/s][1,12]<stderr>:#015 24%|██▎       | 2505/10570 [00:06<00:20, 384.68it/s][1,14]<stderr>:#015 22%|██▏       | 2317/10570 [00:05<00:19, 414.44it/s][1,11]<stderr>:#015 21%|██        | 2214/10570 [00:05<00:20, 399.02it/s][1,7]<stderr>:#015 24%|██▍       | 2559/10570 [00:06<00:20, 382.03it/s][1,1]<stderr>:#015 23%|██▎       | 2431/10570 [00:05<00:20, 403.01it/s][1,3]<stderr>:#015 21%|██        | 2214/10570 [00:05<00:21, 393.30it/s][1,10]<stderr>:#015 23%|██▎       | 2444/10570 [00:05<00:20, 404.26it/s][1,13]<stderr>:#015 20%|█▉        | 2072/10570 [00:05<00:20, 413.17it/s][1,5]<stderr>:#015 22%|██▏       | 2302/10570 [00:05<00:20, 412.36it/s][1,2]<stderr>:#015 24%|██▎       | 2498/10570 [00:06<00:20, 391.29it/s][1,9]<stderr>:#015 22%|██▏       | 2360/10570 [00:05<00:19, 412.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1856/10570 [00:04<00:22, 392.28it/s]#033[A[1,4]<stderr>:#015 23%|██▎       | 2445/10570 [00:05<00:20, 402.99it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2281/10570 [00:05<00:20, 404.16it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2421/10570 [00:05<00:20, 402.61it/s][1,6]<stderr>:#015 20%|██        | 2153/10570 [00:05<00:22, 376.21it/s][1,0]<stderr>:#015100%|██████████| 169/169 [00:30<00:00, 12.65it/s][1,12]<stderr>:#015 24%|██▍       | 2544/10570 [00:06<00:21, 378.56it/s][1,14]<stderr>:#015 22%|██▏       | 2361/10570 [00:05<00:19, 418.72it/s][1,7]<stderr>:#015 25%|██▍       | 2600/10570 [00:06<00:20, 388.54it/s][1,11]<stderr>:#015 21%|██▏       | 2256/10570 [00:05<00:20, 402.18it/s][1,1]<stderr>:#015 23%|██▎       | 2472/10570 [00:06<00:20, 399.08it/s][1,3]<stderr>:#015 21%|██▏       | 2256/10570 [00:05<00:20, 398.16it/s][1,10]<stderr>:#015 24%|██▎       | 2485/10570 [00:06<00:20, 403.99it/s][1,13]<stderr>:#015 20%|██        | 2115/10570 [00:05<00:20, 417.47it/s][1,5]<stderr>:#015 22%|██▏       | 2344/10570 [00:05<00:19, 412.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1900/10570 [00:05<00:21, 405.14it/s]#033[A[1,2]<stderr>:#015 24%|██▍       | 2538/10570 [00:06<00:20, 382.71it/s][1,9]<stderr>:#015 23%|██▎       | 2402/10570 [00:06<00:20, 400.59it/s][1,4]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:20, 402.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2322/10570 [00:05<00:20, 405.25it/s]#033[A[1,15]<stderr>:#015 23%|██▎       | 2462/10570 [00:06<00:20, 395.44it/s][1,6]<stderr>:#015 21%|██        | 2192/10570 [00:05<00:22, 379.27it/s][1,12]<stderr>:#015 24%|██▍       | 2583/10570 [00:06<00:21, 376.19it/s][1,7]<stderr>:#015 25%|██▌       | 2645/10570 [00:06<00:19, 403.88it/s][1,14]<stderr>:#015 23%|██▎       | 2404/10570 [00:05<00:20, 406.34it/s][1,11]<stderr>:#015 22%|██▏       | 2297/10570 [00:05<00:21, 393.93it/s][1,3]<stderr>:#015 22%|██▏       | 2298/10570 [00:05<00:20, 401.59it/s][1,1]<stderr>:#015 24%|██▍       | 2513/10570 [00:06<00:21, 381.62it/s][1,10]<stderr>:#015 24%|██▍       | 2526/10570 [00:06<00:21, 380.67it/s][1,5]<stderr>:#015 23%|██▎       | 2386/10570 [00:05<00:20, 405.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 18%|█▊        | 1941/10570 [00:05<00:21, 406.41it/s]#033[A[1,2]<stderr>:#015 24%|██▍       | 2577/10570 [00:06<00:21, 375.23it/s][1,9]<stderr>:#015 23%|██▎       | 2443/10570 [00:06<00:20, 398.13it/s][1,13]<stderr>:#015 20%|██        | 2157/10570 [00:05<00:22, 372.70it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 22%|██▏       | 2364/10570 [00:05<00:20, 409.30it/s]#033[A[1,4]<stderr>:#015 24%|██▍       | 2527/10570 [00:06<00:21, 379.68it/s][1,6]<stderr>:#015 21%|██        | 2236/10570 [00:05<00:21, 394.64it/s][1,12]<stderr>:#015 25%|██▍       | 2627/10570 [00:06<00:20, 392.36it/s][1,15]<stderr>:#015 24%|██▎       | 2502/10570 [00:06<00:21, 380.65it/s][1,7]<stderr>:#015 25%|██▌       | 2689/10570 [00:06<00:19, 413.60it/s][1,14]<stderr>:#015 23%|██▎       | 2445/10570 [00:05<00:20, 405.30it/s][1,11]<stderr>:#015 22%|██▏       | 2338/10570 [00:05<00:20, 398.01it/s][1,3]<stderr>:#015 22%|██▏       | 2339/10570 [00:05<00:20, 402.72it/s][1,1]<stderr>:#015 24%|██▍       | 2552/10570 [00:06<00:21, 375.75it/s][1,5]<stderr>:#015 23%|██▎       | 2427/10570 [00:05<00:20, 403.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 19%|█▉        | 1985/10570 [00:05<00:20, 413.94it/s]#033[A[1,10]<stderr>:#015 24%|██▍       | 2565/10570 [00:06<00:21, 372.32it/s][1,2]<stderr>:#015 25%|██▍       | 2620/10570 [00:06<00:20, 388.53it/s][1,9]<stderr>:#015 23%|██▎       | 2483/10570 [00:06<00:20, 394.68it/s][1,13]<stderr>:#015 21%|██        | 2196/10570 [00:05<00:22, 376.25it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 23%|██▎       | 2406/10570 [00:05<00:20, 397.23it/s]#033[A[1,6]<stderr>:#015 22%|██▏       | 2277/10570 [00:05<00:21, 391.76it/s][1,12]<stderr>:#015 25%|██▌       | 2672/10570 [00:06<00:19, 406.34it/s][1,4]<stderr>:#015 24%|██▍       | 2566/10570 [00:06<00:21, 366.99it/s][1,7]<stderr>:#015 26%|██▌       | 2732/10570 [00:06<00:18, 416.79it/s][1,15]<stderr>:#015 24%|██▍       | 2541/10570 [00:06<00:21, 376.44it/s][1,14]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:19, 404.22it/s][1,3]<stderr>:#015 23%|██▎       | 2380/10570 [00:06<00:20, 397.45it/s][1,11]<stderr>:#015 23%|██▎       | 2379/10570 [00:06<00:21, 385.86it/s][1,1]<stderr>:#015 25%|██▍       | 2591/10570 [00:06<00:21, 378.53it/s][1,5]<stderr>:#015 23%|██▎       | 2468/10570 [00:06<00:20, 397.66it/s][1,10]<stderr>:#015 25%|██▍       | 2603/10570 [00:06<00:21, 374.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 19%|█▉        | 2027/10570 [00:05<00:21, 405.22it/s]#033[A[1,2]<stderr>:#015 25%|██▌       | 2666/10570 [00:06<00:19, 405.69it/s][1,13]<stderr>:#015 21%|██        | 2236/10570 [00:05<00:21, 381.22it/s][1,9]<stderr>:#015 24%|██▍       | 2523/10570 [00:06<00:21, 371.93it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 23%|██▎       | 2446/10570 [00:06<00:20, 394.73it/s]#033[A[1,6]<stderr>:#015 22%|██▏       | 2318/10570 [00:05<00:20, 396.17it/s][1,12]<stderr>:#015 26%|██▌       | 2715/10570 [00:06<00:19, 412.17it/s][1,4]<stderr>:#015 25%|██▍       | 2607/10570 [00:06<00:21, 378.73it/s][1,7]<stderr>:#015 26%|██▋       | 2780/10570 [00:06<00:18, 432.51it/s][1,15]<stderr>:#015 24%|██▍       | 2579/10570 [00:06<00:21, 370.90it/s][1,11]<stderr>:#015 23%|██▎       | 2418/10570 [00:06<00:21, 384.79it/s][1,3]<stderr>:#015 23%|██▎       | 2420/10570 [00:06<00:20, 390.95it/s][1,14]<stderr>:#015 24%|██▍       | 2527/10570 [00:06<00:21, 380.73it/s][1,1]<stderr>:#015 25%|██▍       | 2635/10570 [00:06<00:20, 393.16it/s][1,10]<stderr>:#015 25%|██▌       | 2647/10570 [00:06<00:20, 390.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|█▉        | 2069/10570 [00:05<00:20, 407.74it/s]#033[A[1,2]<stderr>:#015 26%|██▌       | 2707/10570 [00:06<00:19, 406.94it/s][1,5]<stderr>:#015 24%|██▎       | 2508/10570 [00:06<00:21, 377.47it/s][1,13]<stderr>:#015 22%|██▏       | 2275/10570 [00:05<00:21, 379.22it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▎       | 2486/10570 [00:06<00:20, 395.85it/s]#033[A[1,9]<stderr>:#015 24%|██▍       | 2561/10570 [00:06<00:21, 367.37it/s][1,6]<stderr>:#015 22%|██▏       | 2360/10570 [00:06<00:20, 402.48it/s][1,12]<stderr>:#015 26%|██▌       | 2763/10570 [00:06<00:18, 429.22it/s][1,4]<stderr>:#015 25%|██▌       | 2652/10570 [00:06<00:19, 397.59it/s][1,7]<stderr>:#015 27%|██▋       | 2825/10570 [00:06<00:17, 436.84it/s][1,15]<stderr>:#015 25%|██▍       | 2621/10570 [00:06<00:20, 383.98it/s][1,11]<stderr>:#015 23%|██▎       | 2457/10570 [00:06<00:21, 382.32it/s][1,3]<stderr>:#015 23%|██▎       | 2460/10570 [00:06<00:20, 387.52it/s][1,1]<stderr>:#015 25%|██▌       | 2678/10570 [00:06<00:19, 403.51it/s][1,14]<stderr>:#015 24%|██▍       | 2566/10570 [00:06<00:21, 371.70it/s][1,10]<stderr>:#015 25%|██▌       | 2690/10570 [00:06<00:19, 401.09it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|█▉        | 2112/10570 [00:05<00:20, 413.71it/s]#033[A[1,2]<stderr>:#015 26%|██▌       | 2754/10570 [00:06<00:18, 423.06it/s][1,5]<stderr>:#015 24%|██▍       | 2547/10570 [00:06<00:21, 373.39it/s][1,13]<stderr>:#015 22%|██▏       | 2316/10570 [00:06<00:21, 385.25it/s][1,9]<stderr>:#015 25%|██▍       | 2601/10570 [00:06<00:21, 375.48it/s][1,4]<stderr>:#015 25%|██▌       | 2694/10570 [00:06<00:19, 402.62it/s][1,12]<stderr>:#015 27%|██▋       | 2811/10570 [00:06<00:17, 440.83it/s][1,6]<stderr>:#015 23%|██▎       | 2401/10570 [00:06<00:20, 391.04it/s][1,15]<stderr>:#015 25%|██▌       | 2666/10570 [00:06<00:19, 400.27it/s][1,7]<stderr>:#015 27%|██▋       | 2869/10570 [00:06<00:17, 429.75it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▍       | 2526/10570 [00:06<00:21, 373.18it/s]#033[A[1,11]<stderr>:#015 24%|██▎       | 2496/10570 [00:06<00:21, 381.40it/s][1,1]<stderr>:#015 26%|██▌       | 2721/10570 [00:06<00:19, 411.11it/s][1,14]<stderr>:#015 25%|██▍       | 2608/10570 [00:06<00:20, 383.00it/s][1,3]<stderr>:#015 24%|██▎       | 2499/10570 [00:06<00:21, 372.79it/s][1,10]<stderr>:#015 26%|██▌       | 2734/10570 [00:06<00:19, 410.58it/s][1,2]<stderr>:#015 27%|██▋       | 2802/10570 [00:06<00:17, 438.35it/s][1,5]<stderr>:#015 24%|██▍       | 2585/10570 [00:06<00:21, 374.23it/s][1,13]<stderr>:#015 22%|██▏       | 2357/10570 [00:06<00:21, 390.57it/s][1,9]<stderr>:#015 25%|██▌       | 2645/10570 [00:06<00:20, 390.67it/s][1,4]<stderr>:#015 26%|██▌       | 2739/10570 [00:06<00:18, 413.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 20%|██        | 2154/10570 [00:05<00:23, 365.85it/s]#033[A[1,6]<stderr>:#015 23%|██▎       | 2441/10570 [00:06<00:20, 389.00it/s][1,12]<stderr>:#015 27%|██▋       | 2856/10570 [00:06<00:17, 429.92it/s][1,15]<stderr>:#015 26%|██▌       | 2707/10570 [00:06<00:19, 402.16it/s][1,7]<stderr>:#015 28%|██▊       | 2913/10570 [00:07<00:18, 422.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 24%|██▍       | 2564/10570 [00:06<00:21, 365.77it/s]#033[A[1,1]<stderr>:#015 26%|██▌       | 2768/10570 [00:06<00:18, 426.09it/s][1,14]<stderr>:#015 25%|██▌       | 2654/10570 [00:06<00:19, 401.95it/s][1,10]<stderr>:#015 26%|██▋       | 2781/10570 [00:06<00:18, 426.48it/s][1,3]<stderr>:#015 24%|██▍       | 2537/10570 [00:06<00:21, 369.14it/s][1,11]<stderr>:#015 24%|██▍       | 2535/10570 [00:06<00:22, 356.27it/s][1,2]<stderr>:#015 27%|██▋       | 2847/10570 [00:06<00:17, 430.06it/s][1,5]<stderr>:#015 25%|██▍       | 2628/10570 [00:06<00:20, 388.76it/s][1,13]<stderr>:#015 23%|██▎       | 2397/10570 [00:06<00:21, 378.19it/s][1,9]<stderr>:#015 25%|██▌       | 2688/10570 [00:06<00:19, 401.57it/s][1,4]<stderr>:#015 26%|██▋       | 2786/10570 [00:06<00:18, 428.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 21%|██        | 2192/10570 [00:05<00:22, 366.84it/s]#033[A[1,15]<stderr>:#015 26%|██▌       | 2753/10570 [00:06<00:18, 417.46it/s][1,6]<stderr>:#015 23%|██▎       | 2481/10570 [00:06<00:20, 386.75it/s][1,12]<stderr>:#015 27%|██▋       | 2900/10570 [00:07<00:18, 420.49it/s][1,7]<stderr>:#015 28%|██▊       | 2956/10570 [00:07<00:18, 420.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▍       | 2605/10570 [00:06<00:21, 375.45it/s]#033[A[1,1]<stderr>:#015 27%|██▋       | 2814/10570 [00:06<00:17, 434.30it/s][1,14]<stderr>:#015 26%|██▌       | 2696/10570 [00:06<00:19, 405.20it/s][1,10]<stderr>:#015 27%|██▋       | 2825/10570 [00:06<00:18, 429.06it/s][1,3]<stderr>:#015 24%|██▍       | 2575/10570 [00:06<00:22, 360.43it/s][1,11]<stderr>:#015 24%|██▍       | 2572/10570 [00:06<00:22, 349.53it/s][1,5]<stderr>:#015 25%|██▌       | 2672/10570 [00:06<00:19, 401.73it/s][1,2]<stderr>:#015 27%|██▋       | 2891/10570 [00:07<00:18, 419.49it/s][1,13]<stderr>:#015 23%|██▎       | 2436/10570 [00:06<00:21, 378.36it/s][1,9]<stderr>:#015 26%|██▌       | 2732/10570 [00:06<00:19, 410.68it/s][1,4]<stderr>:#015 27%|██▋       | 2831/10570 [00:06<00:17, 432.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 21%|██        | 2235/10570 [00:05<00:21, 381.07it/s]#033[A[1,15]<stderr>:#015 26%|██▋       | 2800/10570 [00:06<00:17, 431.94it/s][1,12]<stderr>:#015 28%|██▊       | 2943/10570 [00:07<00:18, 420.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▌       | 2649/10570 [00:06<00:20, 391.11it/s]#033[A[1,7]<stderr>:#015 28%|██▊       | 2999/10570 [00:07<00:18, 418.94it/s][1,6]<stderr>:#015 24%|██▍       | 2520/10570 [00:06<00:22, 364.07it/s][1,14]<stderr>:#015 26%|██▌       | 2741/10570 [00:06<00:18, 416.81it/s][1,1]<stderr>:#015 27%|██▋       | 2858/10570 [00:07<00:18, 424.15it/s][1,10]<stderr>:#015 27%|██▋       | 2869/10570 [00:07<00:18, 420.65it/s][1,3]<stderr>:#015 25%|██▍       | 2617/10570 [00:06<00:21, 374.20it/s][1,11]<stderr>:#015 25%|██▍       | 2614/10570 [00:06<00:21, 367.26it/s][1,5]<stderr>:#015 26%|██▌       | 2714/10570 [00:06<00:19, 405.99it/s][1,2]<stderr>:#015 28%|██▊       | 2934/10570 [00:07<00:18, 419.20it/s][1,13]<stderr>:#015 23%|██▎       | 2474/10570 [00:06<00:21, 375.94it/s][1,9]<stderr>:#015 26%|██▋       | 2779/10570 [00:06<00:18, 425.19it/s][1,4]<stderr>:#015 27%|██▋       | 2875/10570 [00:07<00:18, 421.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2274/10570 [00:06<00:22, 375.51it/s]#033[A[1,15]<stderr>:#015 27%|██▋       | 2844/10570 [00:07<00:18, 423.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 25%|██▌       | 2691/10570 [00:06<00:19, 398.46it/s]#033[A[1,7]<stderr>:#015 29%|██▉       | 3042/10570 [00:07<00:17, 420.73it/s][1,12]<stderr>:#015 28%|██▊       | 2986/10570 [00:07<00:18, 413.82it/s][1,6]<stderr>:#015 24%|██▍       | 2557/10570 [00:06<00:22, 357.38it/s][1,14]<stderr>:#015 26%|██▋       | 2788/10570 [00:06<00:18, 430.15it/s][1,1]<stderr>:#015 27%|██▋       | 2901/10570 [00:07<00:18, 415.06it/s][1,3]<stderr>:#015 25%|██▌       | 2661/10570 [00:06<00:20, 390.99it/s][1,10]<stderr>:#015 28%|██▊       | 2912/10570 [00:07<00:18, 411.73it/s][1,11]<stderr>:#015 25%|██▌       | 2657/10570 [00:06<00:20, 382.98it/s][1,5]<stderr>:#015 26%|██▌       | 2761/10570 [00:06<00:18, 422.65it/s][1,2]<stderr>:#015 28%|██▊       | 2977/10570 [00:07<00:18, 413.39it/s][1,9]<stderr>:#015 27%|██▋       | 2823/10570 [00:07<00:18, 427.48it/s][1,13]<stderr>:#015 24%|██▍       | 2512/10570 [00:06<00:22, 355.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2314/10570 [00:06<00:21, 379.53it/s]#033[A[1,4]<stderr>:#015 28%|██▊       | 2918/10570 [00:07<00:18, 413.72it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 26%|██▌       | 2735/10570 [00:06<00:19, 407.98it/s]#033[A[1,15]<stderr>:#015 27%|██▋       | 2887/10570 [00:07<00:18, 414.32it/s][1,12]<stderr>:#015 29%|██▊       | 3029/10570 [00:07<00:18, 416.45it/s][1,7]<stderr>:#015 29%|██▉       | 3085/10570 [00:07<00:18, 414.68it/s][1,6]<stderr>:#015 25%|██▍       | 2595/10570 [00:06<00:22, 362.40it/s][1,14]<stderr>:#015 27%|██▋       | 2833/10570 [00:06<00:17, 434.14it/s][1,1]<stderr>:#015 28%|██▊       | 2943/10570 [00:07<00:18, 415.29it/s][1,3]<stderr>:#015 26%|██▌       | 2701/10570 [00:06<00:20, 392.78it/s][1,11]<stderr>:#015 26%|██▌       | 2697/10570 [00:06<00:20, 387.23it/s][1,10]<stderr>:#015 28%|██▊       | 2954/10570 [00:07<00:18, 408.57it/s][1,5]<stderr>:#015 27%|██▋       | 2808/10570 [00:06<00:17, 434.58it/s][1,2]<stderr>:#015 29%|██▊       | 3019/10570 [00:07<00:18, 414.94it/s][1,9]<stderr>:#015 27%|██▋       | 2866/10570 [00:07<00:18, 419.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 22%|██▏       | 2354/10570 [00:06<00:21, 383.11it/s]#033[A[1,13]<stderr>:#015 24%|██▍       | 2548/10570 [00:06<00:22, 351.20it/s][1,4]<stderr>:#015 28%|██▊       | 2960/10570 [00:07<00:18, 412.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 26%|██▋       | 2782/10570 [00:06<00:18, 424.50it/s]#033[A[1,15]<stderr>:#015 28%|██▊       | 2929/10570 [00:07<00:18, 413.25it/s][1,7]<stderr>:#015 30%|██▉       | 3127/10570 [00:07<00:17, 413.76it/s][1,12]<stderr>:#015 29%|██▉       | 3071/10570 [00:07<00:18, 405.33it/s][1,6]<stderr>:#015 25%|██▍       | 2637/10570 [00:06<00:21, 377.61it/s][1,14]<stderr>:#015 27%|██▋       | 2877/10570 [00:07<00:18, 422.76it/s][1,1]<stderr>:#015 28%|██▊       | 2985/10570 [00:07<00:18, 410.19it/s][1,3]<stderr>:#015 26%|██▌       | 2746/10570 [00:06<00:19, 406.32it/s][1,11]<stderr>:#015 26%|██▌       | 2741/10570 [00:06<00:19, 400.19it/s][1,10]<stderr>:#015 28%|██▊       | 2995/10570 [00:07<00:18, 406.83it/s][1,5]<stderr>:#015 27%|██▋       | 2852/10570 [00:07<00:18, 421.90it/s][1,2]<stderr>:#015 29%|██▉       | 3061/10570 [00:07<00:18, 413.84it/s][1,9]<stderr>:#015 28%|██▊       | 2909/10570 [00:07<00:18, 411.21it/s][1,13]<stderr>:#015 24%|██▍       | 2584/10570 [00:06<00:22, 352.59it/s][1,4]<stderr>:#015 28%|██▊       | 3002/10570 [00:07<00:18, 410.34it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 27%|██▋       | 2826/10570 [00:07<00:18, 426.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2393/10570 [00:06<00:21, 374.35it/s][1,8]<stderr>:#033[A[1,15]<stderr>:#015 28%|██▊       | 2971/10570 [00:07<00:18, 410.38it/s][1,7]<stderr>:#015 30%|██▉       | 3169/10570 [00:07<00:18, 409.66it/s][1,12]<stderr>:#015 29%|██▉       | 3113/10570 [00:07<00:18, 408.80it/s][1,6]<stderr>:#015 25%|██▌       | 2680/10570 [00:06<00:20, 388.97it/s][1,14]<stderr>:#015 28%|██▊       | 2920/10570 [00:07<00:18, 416.45it/s][1,1]<stderr>:#015 29%|██▊       | 3027/10570 [00:07<00:18, 412.63it/s][1,3]<stderr>:#015 26%|██▋       | 2793/10570 [00:07<00:18, 421.53it/s][1,11]<stderr>:#015 26%|██▋       | 2787/10570 [00:07<00:18, 415.63it/s][1,10]<stderr>:#015 29%|██▊       | 3038/10570 [00:07<00:18, 411.44it/s][1,2]<stderr>:#015 29%|██▉       | 3103/10570 [00:07<00:18, 411.46it/s][1,5]<stderr>:#015 27%|██▋       | 2895/10570 [00:07<00:18, 411.01it/s][1,9]<stderr>:#015 28%|██▊       | 2951/10570 [00:07<00:18, 409.09it/s][1,13]<stderr>:#015 25%|██▍       | 2625/10570 [00:06<00:21, 366.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2431/10570 [00:06<00:21, 372.88it/s]#033[A[1,4]<stderr>:#015 29%|██▉       | 3044/10570 [00:07<00:18, 407.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 27%|██▋       | 2869/10570 [00:07<00:18, 416.70it/s]#033[A[1,15]<stderr>:#015 29%|██▊       | 3013/10570 [00:07<00:18, 407.35it/s][1,12]<stderr>:#015 30%|██▉       | 3154/10570 [00:07<00:18, 404.36it/s][1,7]<stderr>:#015 30%|███       | 3211/10570 [00:07<00:18, 405.63it/s][1,6]<stderr>:#015 26%|██▌       | 2721/10570 [00:06<00:19, 394.10it/s][1,14]<stderr>:#015 28%|██▊       | 2962/10570 [00:07<00:18, 414.91it/s][1,11]<stderr>:#015 27%|██▋       | 2831/10570 [00:07<00:18, 421.03it/s][1,3]<stderr>:#015 27%|██▋       | 2836/10570 [00:07<00:18, 420.37it/s][1,1]<stderr>:#015 29%|██▉       | 3069/10570 [00:07<00:18, 405.30it/s][1,10]<stderr>:#015 29%|██▉       | 3080/10570 [00:07<00:18, 405.64it/s][1,5]<stderr>:#015 28%|██▊       | 2937/10570 [00:07<00:18, 411.75it/s][1,2]<stderr>:#015 30%|██▉       | 3145/10570 [00:07<00:18, 403.01it/s][1,9]<stderr>:#015 28%|██▊       | 2993/10570 [00:07<00:18, 408.25it/s][1,13]<stderr>:#015 25%|██▌       | 2667/10570 [00:07<00:20, 380.62it/s][1,4]<stderr>:#015 29%|██▉       | 3085/10570 [00:07<00:18, 405.52it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 23%|██▎       | 2469/10570 [00:06<00:22, 365.83it/s][1,8]<stderr>:#033[A[1,15]<stderr>:#015 29%|██▉       | 3054/10570 [00:07<00:18, 406.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2911/10570 [00:07<00:18, 408.35it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 26%|██▌       | 2765/10570 [00:07<00:19, 406.56it/s][1,12]<stderr>:#015 30%|███       | 3195/10570 [00:07<00:18, 401.43it/s][1,7]<stderr>:#015 31%|███       | 3252/10570 [00:07<00:18, 398.96it/s][1,14]<stderr>:#015 28%|██▊       | 3004/10570 [00:07<00:18, 412.15it/s][1,1]<stderr>:#015 29%|██▉       | 3110/10570 [00:07<00:18, 406.47it/s][1,11]<stderr>:#015 27%|██▋       | 2874/10570 [00:07<00:18, 411.47it/s][1,10]<stderr>:#015 30%|██▉       | 3121/10570 [00:07<00:18, 406.00it/s][1,3]<stderr>:#015 27%|██▋       | 2879/10570 [00:07<00:18, 406.78it/s][1,5]<stderr>:#015 28%|██▊       | 2979/10570 [00:07<00:18, 406.73it/s][1,2]<stderr>:#015 30%|███       | 3186/10570 [00:07<00:18, 394.29it/s][1,9]<stderr>:#015 29%|██▊       | 3035/10570 [00:07<00:18, 410.00it/s][1,13]<stderr>:#015 26%|██▌       | 2706/10570 [00:07<00:20, 381.44it/s][1,4]<stderr>:#015 30%|██▉       | 3126/10570 [00:07<00:18, 404.31it/s][1,15]<stderr>:#015 29%|██▉       | 3095/10570 [00:07<00:18, 405.09it/s][1,6]<stderr>:#015 27%|██▋       | 2811/10570 [00:07<00:18, 418.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2953/10570 [00:07<00:18, 405.78it/s]#033[A[1,12]<stderr>:#015 31%|███       | 3236/10570 [00:07<00:18, 398.06it/s][1,7]<stderr>:#015 31%|███       | 3294/10570 [00:07<00:18, 402.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▎       | 2506/10570 [00:06<00:23, 349.28it/s]#033[A[1,14]<stderr>:#015 29%|██▉       | 3046/10570 [00:07<00:18, 410.49it/s][1,1]<stderr>:#015 30%|██▉       | 3151/10570 [00:07<00:18, 400.31it/s][1,10]<stderr>:#015 30%|██▉       | 3162/10570 [00:07<00:18, 402.95it/s][1,11]<stderr>:#015 28%|██▊       | 2916/10570 [00:07<00:18, 404.13it/s][1,3]<stderr>:#015 28%|██▊       | 2920/10570 [00:07<00:19, 402.41it/s][1,5]<stderr>:#015 29%|██▊       | 3021/10570 [00:07<00:18, 409.42it/s][1,2]<stderr>:#015 31%|███       | 3226/10570 [00:07<00:18, 395.19it/s][1,13]<stderr>:#015 26%|██▌       | 2750/10570 [00:07<00:19, 395.15it/s][1,9]<stderr>:#015 29%|██▉       | 3077/10570 [00:07<00:18, 402.76it/s][1,4]<stderr>:#015 30%|██▉       | 3167/10570 [00:07<00:18, 399.91it/s][1,15]<stderr>:#015 30%|██▉       | 3136/10570 [00:07<00:18, 403.41it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 28%|██▊       | 2994/10570 [00:07<00:18, 404.18it/s]#033[A[1,7]<stderr>:#015 32%|███▏      | 3337/10570 [00:08<00:17, 408.11it/s][1,12]<stderr>:#015 31%|███       | 3276/10570 [00:07<00:18, 393.82it/s][1,6]<stderr>:#015 27%|██▋       | 2854/10570 [00:07<00:18, 407.72it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▍       | 2542/10570 [00:06<00:23, 346.29it/s]#033[A[1,14]<stderr>:#015 29%|██▉       | 3088/10570 [00:07<00:18, 409.95it/s][1,1]<stderr>:#015 30%|███       | 3192/10570 [00:07<00:18, 397.33it/s][1,10]<stderr>:#015 30%|███       | 3203/10570 [00:07<00:18, 400.11it/s][1,11]<stderr>:#015 28%|██▊       | 2957/10570 [00:07<00:18, 403.39it/s][1,3]<stderr>:#015 28%|██▊       | 2961/10570 [00:07<00:18, 401.47it/s][1,5]<stderr>:#015 29%|██▉       | 3063/10570 [00:07<00:18, 406.46it/s][1,2]<stderr>:#015 31%|███       | 3266/10570 [00:07<00:18, 394.29it/s][1,13]<stderr>:#015 26%|██▋       | 2795/10570 [00:07<00:19, 408.67it/s][1,9]<stderr>:#015 29%|██▉       | 3118/10570 [00:07<00:18, 403.59it/s][1,4]<stderr>:#015 30%|███       | 3208/10570 [00:07<00:18, 394.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▊       | 3036/10570 [00:07<00:18, 406.90it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 32%|███▏      | 3379/10570 [00:08<00:17, 411.42it/s][1,12]<stderr>:#015 31%|███▏      | 3317/10570 [00:08<00:18, 397.35it/s][1,15]<stderr>:#015 30%|███       | 3177/10570 [00:07<00:19, 388.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 24%|██▍       | 2577/10570 [00:06<00:23, 340.41it/s]#033[A[1,6]<stderr>:#015 27%|██▋       | 2896/10570 [00:07<00:19, 398.95it/s][1,14]<stderr>:#015 30%|██▉       | 3130/10570 [00:07<00:18, 408.32it/s][1,1]<stderr>:#015 31%|███       | 3232/10570 [00:07<00:18, 394.84it/s][1,11]<stderr>:#015 28%|██▊       | 2998/10570 [00:07<00:18, 400.46it/s][1,3]<stderr>:#015 28%|██▊       | 3002/10570 [00:07<00:18, 399.33it/s][1,10]<stderr>:#015 31%|███       | 3244/10570 [00:07<00:18, 394.56it/s][1,5]<stderr>:#015 29%|██▉       | 3104/10570 [00:07<00:18, 405.86it/s][1,2]<stderr>:#015 31%|███▏      | 3307/10570 [00:08<00:18, 396.53it/s][1,13]<stderr>:#015 27%|██▋       | 2837/10570 [00:07<00:18, 409.02it/s][1,9]<stderr>:#015 30%|██▉       | 3159/10570 [00:07<00:18, 398.99it/s][1,4]<stderr>:#015 31%|███       | 3248/10570 [00:07<00:18, 386.98it/s][1,12]<stderr>:#015 32%|███▏      | 3359/10570 [00:08<00:17, 403.13it/s][1,7]<stderr>:#015 32%|███▏      | 3421/10570 [00:08<00:17, 409.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▉       | 3077/10570 [00:07<00:18, 398.54it/s]#033[A[1,15]<stderr>:#015 30%|███       | 3217/10570 [00:07<00:18, 389.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 25%|██▍       | 2616/10570 [00:06<00:22, 353.88it/s]#033[A[1,6]<stderr>:#015 28%|██▊       | 2937/10570 [00:07<00:19, 400.35it/s][1,1]<stderr>:#015 31%|███       | 3272/10570 [00:08<00:18, 395.71it/s][1,14]<stderr>:#015 30%|███       | 3171/10570 [00:07<00:18, 396.44it/s][1,11]<stderr>:#015 29%|██▉       | 3040/10570 [00:07<00:18, 404.04it/s][1,3]<stderr>:#015 29%|██▉       | 3043/10570 [00:07<00:18, 400.15it/s][1,10]<stderr>:#015 31%|███       | 3284/10570 [00:08<00:18, 394.49it/s][1,2]<stderr>:#015 32%|███▏      | 3347/10570 [00:08<00:18, 397.40it/s][1,5]<stderr>:#015 30%|██▉       | 3145/10570 [00:07<00:18, 397.57it/s][1,9]<stderr>:#015 30%|███       | 3199/10570 [00:07<00:18, 390.40it/s][1,13]<stderr>:#015 27%|██▋       | 2879/10570 [00:07<00:19, 396.35it/s][1,4]<stderr>:#015 31%|███       | 3289/10570 [00:08<00:18, 391.75it/s][1,12]<stderr>:#015 32%|███▏      | 3400/10570 [00:08<00:17, 404.10it/s][1,7]<stderr>:#015 33%|███▎      | 3464/10570 [00:08<00:17, 413.54it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 29%|██▉       | 3118/10570 [00:07<00:18, 399.35it/s]#033[A[1,15]<stderr>:#015 31%|███       | 3257/10570 [00:08<00:18, 386.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 25%|██▌       | 2659/10570 [00:07<00:21, 370.96it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 28%|██▊       | 2978/10570 [00:07<00:19, 394.73it/s][1,1]<stderr>:#015 31%|███▏      | 3312/10570 [00:08<00:18, 396.86it/s][1,14]<stderr>:#015 30%|███       | 3211/10570 [00:07<00:18, 394.21it/s][1,10]<stderr>:#015 31%|███▏      | 3324/10570 [00:08<00:18, 391.94it/s][1,3]<stderr>:#015 29%|██▉       | 3084/10570 [00:07<00:18, 395.80it/s][1,11]<stderr>:#015 29%|██▉       | 3081/10570 [00:07<00:18, 395.66it/s][1,2]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:17, 400.55it/s][1,5]<stderr>:#015 30%|███       | 3185/10570 [00:07<00:19, 388.00it/s][1,9]<stderr>:#015 31%|███       | 3239/10570 [00:08<00:18, 387.66it/s][1,13]<stderr>:#015 28%|██▊       | 2919/10570 [00:07<00:19, 391.15it/s][1,4]<stderr>:#015 32%|███▏      | 3330/10570 [00:08<00:18, 396.54it/s][1,12]<stderr>:#015 33%|███▎      | 3441/10570 [00:08<00:17, 405.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 30%|██▉       | 3158/10570 [00:07<00:18, 395.62it/s]#033[A[1,15]<stderr>:#015 31%|███       | 3297/10570 [00:08<00:18, 389.16it/s][1,7]<stderr>:#015 33%|███▎      | 3506/10570 [00:08<00:17, 404.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▌       | 2697/10570 [00:07<00:21, 372.49it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 29%|██▊       | 3018/10570 [00:07<00:19, 395.89it/s][1,1]<stderr>:#015 32%|███▏      | 3354/10570 [00:08<00:17, 401.19it/s][1,14]<stderr>:#015 31%|███       | 3251/10570 [00:07<00:18, 388.54it/s][1,10]<stderr>:#015 32%|███▏      | 3366/10570 [00:08<00:18, 397.61it/s][1,3]<stderr>:#015 30%|██▉       | 3124/10570 [00:07<00:18, 394.43it/s][1,11]<stderr>:#015 30%|██▉       | 3121/10570 [00:07<00:18, 395.14it/s][1,2]<stderr>:#015 32%|███▏      | 3429/10570 [00:08<00:17, 399.92it/s][1,5]<stderr>:#015 31%|███       | 3225/10570 [00:07<00:18, 391.05it/s][1,9]<stderr>:#015 31%|███       | 3278/10570 [00:08<00:18, 388.31it/s][1,13]<stderr>:#015 28%|██▊       | 2959/10570 [00:07<00:19, 389.77it/s][1,4]<stderr>:#015 32%|███▏      | 3372/10570 [00:08<00:17, 401.17it/s][1,15]<stderr>:#015 32%|███▏      | 3339/10570 [00:08<00:18, 395.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 30%|███       | 3198/10570 [00:07<00:18, 391.38it/s]#033[A[1,12]<stderr>:#015 33%|███▎      | 3482/10570 [00:08<00:17, 394.13it/s][1,7]<stderr>:#015 34%|███▎      | 3549/10570 [00:08<00:17, 410.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▌       | 2739/10570 [00:07<00:20, 383.90it/s]#033[A[1,6]<stderr>:#015 29%|██▉       | 3058/10570 [00:07<00:18, 395.67it/s][1,1]<stderr>:#015 32%|███▏      | 3395/10570 [00:08<00:17, 400.63it/s][1,14]<stderr>:#015 31%|███       | 3292/10570 [00:08<00:18, 393.44it/s][1,10]<stderr>:#015 32%|███▏      | 3408/10570 [00:08<00:17, 401.46it/s][1,11]<stderr>:#015 30%|██▉       | 3161/10570 [00:07<00:18, 391.07it/s][1,3]<stderr>:#015 30%|██▉       | 3164/10570 [00:07<00:19, 389.34it/s][1,2]<stderr>:#015 33%|███▎      | 3470/10570 [00:08<00:17, 402.04it/s][1,5]<stderr>:#015 31%|███       | 3265/10570 [00:08<00:18, 389.59it/s][1,9]<stderr>:#015 31%|███▏      | 3319/10570 [00:08<00:18, 392.85it/s][1,13]<stderr>:#015 28%|██▊       | 2999/10570 [00:07<00:19, 387.63it/s][1,4]<stderr>:#015 32%|███▏      | 3413/10570 [00:08<00:17, 400.82it/s][1,15]<stderr>:#015 32%|███▏      | 3380/10570 [00:08<00:17, 399.46it/s][1,12]<stderr>:#015 33%|███▎      | 3522/10570 [00:08<00:17, 394.91it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 26%|██▋       | 2784/10570 [00:07<00:19, 399.69it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███       | 3238/10570 [00:08<00:18, 387.29it/s]#033[A[1,7]<stderr>:#015 34%|███▍      | 3591/10570 [00:08<00:17, 402.66it/s][1,6]<stderr>:#015 29%|██▉       | 3098/10570 [00:07<00:18, 393.41it/s][1,1]<stderr>:#015 33%|███▎      | 3436/10570 [00:08<00:17, 402.59it/s][1,14]<stderr>:#015 32%|███▏      | 3334/10570 [00:08<00:18, 398.78it/s][1,10]<stderr>:#015 33%|███▎      | 3449/10570 [00:08<00:17, 401.94it/s][1,3]<stderr>:#015 30%|███       | 3203/10570 [00:08<00:19, 384.46it/s][1,11]<stderr>:#015 30%|███       | 3201/10570 [00:08<00:19, 387.60it/s][1,2]<stderr>:#015 33%|███▎      | 3511/10570 [00:08<00:17, 393.83it/s][1,5]<stderr>:#015 31%|███▏      | 3305/10570 [00:08<00:18, 388.72it/s][1,9]<stderr>:#015 32%|███▏      | 3359/10570 [00:08<00:18, 392.09it/s][1,13]<stderr>:#015 29%|██▉       | 3039/10570 [00:07<00:19, 390.46it/s][1,4]<stderr>:#015 33%|███▎      | 3454/10570 [00:08<00:18, 394.25it/s][1,15]<stderr>:#015 32%|███▏      | 3420/10570 [00:08<00:17, 397.60it/s][1,12]<stderr>:#015 34%|███▎      | 3563/10570 [00:08<00:17, 397.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2825/10570 [00:07<00:19, 402.44it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███       | 3277/10570 [00:08<00:18, 385.31it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 34%|███▍      | 3632/10570 [00:08<00:17, 402.67it/s][1,6]<stderr>:#015 30%|██▉       | 3138/10570 [00:08<00:19, 387.99it/s][1,1]<stderr>:#015 33%|███▎      | 3477/10570 [00:08<00:17, 401.32it/s][1,14]<stderr>:#015 32%|███▏      | 3376/10570 [00:08<00:17, 402.34it/s][1,10]<stderr>:#015 33%|███▎      | 3490/10570 [00:08<00:17, 396.41it/s][1,11]<stderr>:#015 31%|███       | 3240/10570 [00:08<00:19, 382.64it/s][1,3]<stderr>:#015 31%|███       | 3242/10570 [00:08<00:19, 379.68it/s][1,2]<stderr>:#015 34%|███▎      | 3553/10570 [00:08<00:17, 399.96it/s][1,5]<stderr>:#015 32%|███▏      | 3347/10570 [00:08<00:18, 396.84it/s][1,9]<stderr>:#015 32%|███▏      | 3399/10570 [00:08<00:18, 394.25it/s][1,13]<stderr>:#015 29%|██▉       | 3079/10570 [00:08<00:19, 383.58it/s][1,15]<stderr>:#015 33%|███▎      | 3462/10570 [00:08<00:17, 402.67it/s][1,4]<stderr>:#015 33%|███▎      | 3494/10570 [00:08<00:18, 388.85it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 31%|███▏      | 3317/10570 [00:08<00:18, 389.02it/s]#033[A[1,12]<stderr>:#015 34%|███▍      | 3603/10570 [00:08<00:17, 390.71it/s][1,7]<stderr>:#015 35%|███▍      | 3673/10570 [00:08<00:17, 404.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2866/10570 [00:07<00:19, 393.72it/s]#033[A[1,6]<stderr>:#015 30%|███       | 3177/10570 [00:08<00:19, 381.28it/s][1,1]<stderr>:#015 33%|███▎      | 3518/10570 [00:08<00:17, 397.24it/s][1,14]<stderr>:#015 32%|███▏      | 3417/10570 [00:08<00:17, 399.75it/s][1,10]<stderr>:#015 33%|███▎      | 3532/10570 [00:08<00:17, 401.28it/s][1,11]<stderr>:#015 31%|███       | 3279/10570 [00:08<00:19, 382.48it/s][1,3]<stderr>:#015 31%|███       | 3281/10570 [00:08<00:19, 379.46it/s][1,2]<stderr>:#015 34%|███▍      | 3594/10570 [00:08<00:17, 393.57it/s][1,5]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:18, 398.72it/s][1,9]<stderr>:#015 33%|███▎      | 3439/10570 [00:08<00:18, 395.87it/s][1,13]<stderr>:#015 29%|██▉       | 3118/10570 [00:08<00:19, 383.74it/s][1,4]<stderr>:#015 33%|███▎      | 3536/10570 [00:08<00:17, 395.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 32%|███▏      | 3358/10570 [00:08<00:18, 394.34it/s]#033[A[1,15]<stderr>:#015 33%|███▎      | 3503/10570 [00:08<00:17, 394.76it/s][1,12]<stderr>:#015 34%|███▍      | 3645/10570 [00:08<00:17, 396.57it/s][1,7]<stderr>:#015 35%|███▌      | 3714/10570 [00:08<00:16, 405.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 27%|██▋       | 2906/10570 [00:07<00:19, 383.81it/s]#033[A[1,6]<stderr>:#015 30%|███       | 3216/10570 [00:08<00:19, 379.83it/s][1,1]<stderr>:#015 34%|███▎      | 3559/10570 [00:08<00:17, 399.30it/s][1,14]<stderr>:#015 33%|███▎      | 3460/10570 [00:08<00:17, 406.19it/s][1,10]<stderr>:#015 34%|███▍      | 3573/10570 [00:08<00:17, 397.92it/s][1,11]<stderr>:#015 31%|███▏      | 3319/10570 [00:08<00:18, 386.75it/s][1,3]<stderr>:#015 31%|███▏      | 3321/10570 [00:08<00:18, 385.20it/s][1,2]<stderr>:#015 34%|███▍      | 3635/10570 [00:08<00:17, 396.84it/s][1,5]<stderr>:#015 32%|███▏      | 3428/10570 [00:08<00:17, 397.37it/s][1,9]<stderr>:#015 33%|███▎      | 3479/10570 [00:08<00:17, 394.91it/s][1,13]<stderr>:#015 30%|██▉       | 3157/10570 [00:08<00:19, 379.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 32%|███▏      | 3398/10570 [00:08<00:18, 394.88it/s]#033[A[1,15]<stderr>:#015 34%|███▎      | 3544/10570 [00:08<00:17, 398.98it/s][1,4]<stderr>:#015 34%|███▍      | 3576/10570 [00:08<00:17, 391.32it/s][1,12]<stderr>:#015 35%|███▍      | 3686/10570 [00:09<00:17, 400.09it/s][1,7]<stderr>:#015 36%|███▌      | 3755/10570 [00:09<00:16, 405.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 28%|██▊       | 2945/10570 [00:07<00:19, 383.29it/s]#033[A[1,6]<stderr>:#015 31%|███       | 3255/10570 [00:08<00:19, 375.75it/s][1,1]<stderr>:#015 34%|███▍      | 3599/10570 [00:08<00:17, 392.50it/s][1,14]<stderr>:#015 33%|███▎      | 3501/10570 [00:08<00:17, 397.19it/s][1,10]<stderr>:#015 34%|███▍      | 3613/10570 [00:08<00:17, 395.22it/s][1,11]<stderr>:#015 32%|███▏      | 3360/10570 [00:08<00:18, 391.12it/s][1,3]<stderr>:#015 32%|███▏      | 3362/10570 [00:08<00:18, 389.86it/s][1,2]<stderr>:#015 35%|███▍      | 3676/10570 [00:09<00:17, 400.22it/s][1,5]<stderr>:#015 33%|███▎      | 3469/10570 [00:08<00:17, 399.20it/s][1,9]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:17, 392.03it/s][1,13]<stderr>:#015 30%|███       | 3195/10570 [00:08<00:19, 375.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3439/10570 [00:08<00:17, 396.51it/s]#033[A[1,4]<stderr>:#015 34%|███▍      | 3616/10570 [00:08<00:17, 390.35it/s][1,12]<stderr>:#015 35%|███▌      | 3727/10570 [00:09<00:17, 399.16it/s][1,7]<stderr>:#015 36%|███▌      | 3797/10570 [00:09<00:16, 406.11it/s][1,15]<stderr>:#015 34%|███▍      | 3584/10570 [00:08<00:17, 392.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 28%|██▊       | 2984/10570 [00:07<00:20, 378.82it/s]#033[A[1,6]<stderr>:#015 31%|███       | 3294/10570 [00:08<00:19, 378.45it/s][1,1]<stderr>:#015 34%|███▍      | 3640/10570 [00:08<00:17, 396.20it/s][1,14]<stderr>:#015 34%|███▎      | 3543/10570 [00:08<00:17, 401.69it/s][1,10]<stderr>:#015 35%|███▍      | 3655/10570 [00:09<00:17, 399.81it/s][1,11]<stderr>:#015 32%|███▏      | 3400/10570 [00:08<00:18, 391.64it/s][1,3]<stderr>:#015 32%|███▏      | 3402/10570 [00:08<00:18, 391.40it/s][1,2]<stderr>:#015 35%|███▌      | 3717/10570 [00:09<00:17, 401.32it/s][1,5]<stderr>:#015 33%|███▎      | 3509/10570 [00:08<00:17, 392.81it/s][1,9]<stderr>:#015 34%|███▎      | 3560/10570 [00:08<00:17, 394.38it/s][1,13]<stderr>:#015 31%|███       | 3233/10570 [00:08<00:19, 372.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3479/10570 [00:08<00:17, 393.96it/s]#033[A[1,12]<stderr>:#015 36%|███▌      | 3767/10570 [00:09<00:17, 398.08it/s][1,4]<stderr>:#015 35%|███▍      | 3658/10570 [00:09<00:17, 395.99it/s][1,7]<stderr>:#015 36%|███▋      | 3839/10570 [00:09<00:16, 408.38it/s][1,15]<stderr>:#015 34%|███▍      | 3624/10570 [00:08<00:17, 393.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▊       | 3023/10570 [00:08<00:19, 381.56it/s]#033[A[1,6]<stderr>:#015 32%|███▏      | 3334/10570 [00:08<00:18, 384.38it/s][1,1]<stderr>:#015 35%|███▍      | 3681/10570 [00:09<00:17, 399.29it/s][1,14]<stderr>:#015 34%|███▍      | 3584/10570 [00:08<00:17, 394.35it/s][1,11]<stderr>:#015 33%|███▎      | 3440/10570 [00:08<00:18, 392.63it/s][1,10]<stderr>:#015 35%|███▍      | 3696/10570 [00:09<00:17, 398.25it/s][1,3]<stderr>:#015 33%|███▎      | 3442/10570 [00:08<00:18, 390.51it/s][1,2]<stderr>:#015 36%|███▌      | 3758/10570 [00:09<00:16, 401.69it/s][1,5]<stderr>:#015 34%|███▎      | 3551/10570 [00:08<00:17, 398.39it/s][1,9]<stderr>:#015 34%|███▍      | 3600/10570 [00:09<00:17, 388.63it/s][1,13]<stderr>:#015 31%|███       | 3271/10570 [00:08<00:19, 372.70it/s][1,12]<stderr>:#015 36%|███▌      | 3807/10570 [00:09<00:16, 398.52it/s][1,4]<stderr>:#015 35%|███▍      | 3698/10570 [00:09<00:17, 395.28it/s][1,7]<stderr>:#015 37%|███▋      | 3880/10570 [00:09<00:16, 406.57it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:18, 390.56it/s]#033[A[1,15]<stderr>:#015 35%|███▍      | 3664/10570 [00:09<00:17, 392.54it/s][1,6]<stderr>:#015 32%|███▏      | 3374/10570 [00:08<00:18, 387.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▉       | 3062/10570 [00:08<00:19, 378.30it/s]#033[A[1,1]<stderr>:#015 35%|███▌      | 3722/10570 [00:09<00:17, 399.78it/s][1,14]<stderr>:#015 34%|███▍      | 3624/10570 [00:08<00:17, 395.05it/s][1,10]<stderr>:#015 35%|███▌      | 3737/10570 [00:09<00:17, 398.98it/s][1,3]<stderr>:#015 33%|███▎      | 3482/10570 [00:08<00:18, 388.06it/s][1,11]<stderr>:#015 33%|███▎      | 3480/10570 [00:08<00:18, 385.23it/s][1,2]<stderr>:#015 36%|███▌      | 3799/10570 [00:09<00:16, 401.47it/s][1,5]<stderr>:#015 34%|███▍      | 3591/10570 [00:08<00:17, 391.68it/s][1,9]<stderr>:#015 34%|███▍      | 3641/10570 [00:09<00:17, 392.75it/s][1,13]<stderr>:#015 31%|███▏      | 3309/10570 [00:08<00:19, 373.39it/s][1,12]<stderr>:#015 36%|███▋      | 3848/10570 [00:09<00:16, 399.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▎      | 3559/10570 [00:08<00:17, 392.65it/s]#033[A[1,15]<stderr>:#015 35%|███▌      | 3704/10570 [00:09<00:17, 394.29it/s][1,4]<stderr>:#015 35%|███▌      | 3739/10570 [00:09<00:17, 396.34it/s][1,7]<stderr>:#015 37%|███▋      | 3921/10570 [00:09<00:16, 403.61it/s][1,6]<stderr>:#015 32%|███▏      | 3413/10570 [00:08<00:18, 388.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 29%|██▉       | 3100/10570 [00:08<00:19, 377.03it/s]#033[A[1,1]<stderr>:#015 36%|███▌      | 3763/10570 [00:09<00:16, 400.52it/s][1,14]<stderr>:#015 35%|███▍      | 3665/10570 [00:09<00:17, 397.06it/s][1,10]<stderr>:#015 36%|███▌      | 3777/10570 [00:09<00:17, 397.23it/s][1,3]<stderr>:#015 33%|███▎      | 3521/10570 [00:08<00:18, 385.65it/s][1,11]<stderr>:#015 33%|███▎      | 3519/10570 [00:08<00:18, 383.32it/s][1,2]<stderr>:#015 36%|███▋      | 3840/10570 [00:09<00:16, 402.38it/s][1,5]<stderr>:#015 34%|███▍      | 3631/10570 [00:09<00:17, 393.69it/s][1,9]<stderr>:#015 35%|███▍      | 3682/10570 [00:09<00:17, 395.93it/s][1,13]<stderr>:#015 32%|███▏      | 3349/10570 [00:08<00:19, 379.35it/s][1,12]<stderr>:#015 37%|███▋      | 3889/10570 [00:09<00:16, 401.60it/s][1,4]<stderr>:#015 36%|███▌      | 3782/10570 [00:09<00:16, 404.95it/s][1,7]<stderr>:#015 37%|███▋      | 3962/10570 [00:09<00:16, 404.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▍      | 3599/10570 [00:09<00:18, 386.65it/s]#033[A[1,15]<stderr>:#015 35%|███▌      | 3744/10570 [00:09<00:17, 383.89it/s][1,6]<stderr>:#015 33%|███▎      | 3452/10570 [00:08<00:18, 385.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|██▉       | 3138/10570 [00:08<00:19, 373.51it/s]#033[A[1,1]<stderr>:#015 36%|███▌      | 3804/10570 [00:09<00:16, 399.59it/s][1,14]<stderr>:#015 35%|███▌      | 3706/10570 [00:09<00:17, 398.95it/s][1,10]<stderr>:#015 36%|███▌      | 3817/10570 [00:09<00:17, 393.93it/s][1,3]<stderr>:#015 34%|███▎      | 3561/10570 [00:09<00:18, 386.74it/s][1,11]<stderr>:#015 34%|███▎      | 3559/10570 [00:09<00:18, 386.02it/s][1,2]<stderr>:#015 37%|███▋      | 3881/10570 [00:09<00:16, 403.67it/s][1,5]<stderr>:#015 35%|███▍      | 3672/10570 [00:09<00:17, 395.81it/s][1,9]<stderr>:#015 35%|███▌      | 3722/10570 [00:09<00:17, 396.36it/s][1,13]<stderr>:#015 32%|███▏      | 3388/10570 [00:08<00:18, 380.32it/s][1,12]<stderr>:#015 37%|███▋      | 3930/10570 [00:09<00:16, 400.08it/s][1,7]<stderr>:#015 38%|███▊      | 4003/10570 [00:09<00:16, 404.14it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 34%|███▍      | 3640/10570 [00:09<00:17, 391.02it/s]#033[A[1,4]<stderr>:#015 36%|███▌      | 3823/10570 [00:09<00:17, 396.56it/s][1,15]<stderr>:#015 36%|███▌      | 3787/10570 [00:09<00:17, 395.01it/s][1,6]<stderr>:#015 33%|███▎      | 3491/10570 [00:08<00:18, 378.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|███       | 3176/10570 [00:08<00:20, 366.91it/s]#033[A[1,1]<stderr>:#015 36%|███▋      | 3845/10570 [00:09<00:16, 399.62it/s][1,14]<stderr>:#015 35%|███▌      | 3746/10570 [00:09<00:17, 396.77it/s][1,10]<stderr>:#015 36%|███▋      | 3858/10570 [00:09<00:16, 397.18it/s][1,3]<stderr>:#015 34%|███▍      | 3600/10570 [00:09<00:18, 379.57it/s][1,2]<stderr>:#015 37%|███▋      | 3922/10570 [00:09<00:16, 399.74it/s][1,11]<stderr>:#015 34%|███▍      | 3598/10570 [00:09<00:18, 375.22it/s][1,5]<stderr>:#015 35%|███▌      | 3713/10570 [00:09<00:17, 397.96it/s][1,9]<stderr>:#015 36%|███▌      | 3762/10570 [00:09<00:17, 396.68it/s][1,13]<stderr>:#015 32%|███▏      | 3427/10570 [00:09<00:18, 377.98it/s][1,12]<stderr>:#015 38%|███▊      | 3971/10570 [00:09<00:16, 401.45it/s][1,7]<stderr>:#015 38%|███▊      | 4044/10570 [00:09<00:16, 405.23it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 35%|███▍      | 3680/10570 [00:09<00:17, 393.45it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3864/10570 [00:09<00:16, 399.27it/s][1,15]<stderr>:#015 36%|███▌      | 3827/10570 [00:09<00:17, 391.96it/s][1,6]<stderr>:#015 33%|███▎      | 3531/10570 [00:09<00:18, 384.27it/s][1,1]<stderr>:#015 37%|███▋      | 3886/10570 [00:09<00:16, 401.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 30%|███       | 3213/10570 [00:08<00:20, 364.12it/s]#033[A[1,14]<stderr>:#015 36%|███▌      | 3789/10570 [00:09<00:16, 405.38it/s][1,10]<stderr>:#015 37%|███▋      | 3899/10570 [00:09<00:16, 397.83it/s][1,3]<stderr>:#015 34%|███▍      | 3640/10570 [00:09<00:18, 384.28it/s][1,2]<stderr>:#015 37%|███▋      | 3963/10570 [00:09<00:16, 400.08it/s][1,11]<stderr>:#015 34%|███▍      | 3638/10570 [00:09<00:18, 380.62it/s][1,5]<stderr>:#015 36%|███▌      | 3753/10570 [00:09<00:17, 396.42it/s][1,9]<stderr>:#015 36%|███▌      | 3802/10570 [00:09<00:17, 395.85it/s][1,13]<stderr>:#015 33%|███▎      | 3466/10570 [00:09<00:18, 380.85it/s][1,7]<stderr>:#015 39%|███▊      | 4088/10570 [00:09<00:15, 414.31it/s][1,12]<stderr>:#015 38%|███▊      | 4012/10570 [00:09<00:16, 399.99it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 35%|███▌      | 3720/10570 [00:09<00:17, 393.36it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3904/10570 [00:09<00:16, 397.64it/s][1,15]<stderr>:#015 37%|███▋      | 3867/10570 [00:09<00:17, 380.88it/s][1,6]<stderr>:#015 34%|███▍      | 3570/10570 [00:09<00:18, 381.02it/s][1,1]<stderr>:#015 37%|███▋      | 3927/10570 [00:09<00:16, 397.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███       | 3250/10570 [00:08<00:20, 357.93it/s][1,8]<stderr>:#033[A[1,14]<stderr>:#015 36%|███▌      | 3830/10570 [00:09<00:16, 401.07it/s][1,10]<stderr>:#015 37%|███▋      | 3939/10570 [00:09<00:16, 396.26it/s][1,3]<stderr>:#015 35%|███▍      | 3680/10570 [00:09<00:17, 387.28it/s][1,11]<stderr>:#015 35%|███▍      | 3678/10570 [00:09<00:17, 383.77it/s][1,5]<stderr>:#015 36%|███▌      | 3794/10570 [00:09<00:16, 400.38it/s][1,2]<stderr>:#015 38%|███▊      | 4004/10570 [00:09<00:16, 393.42it/s][1,9]<stderr>:#015 36%|███▋      | 3842/10570 [00:09<00:16, 395.98it/s][1,12]<stderr>:#015 38%|███▊      | 4053/10570 [00:09<00:16, 402.31it/s][1,13]<stderr>:#015 33%|███▎      | 3505/10570 [00:09<00:18, 373.80it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▌      | 3760/10570 [00:09<00:17, 393.54it/s]#033[A[1,4]<stderr>:#015 37%|███▋      | 3944/10570 [00:09<00:16, 395.66it/s][1,15]<stderr>:#015 37%|███▋      | 3906/10570 [00:09<00:17, 382.56it/s][1,7]<stderr>:#015 39%|███▉      | 4130/10570 [00:10<00:16, 388.98it/s][1,6]<stderr>:#015 34%|███▍      | 3609/10570 [00:09<00:18, 377.54it/s][1,1]<stderr>:#015 38%|███▊      | 3968/10570 [00:09<00:16, 398.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███       | 3288/10570 [00:08<00:20, 362.03it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3871/10570 [00:09<00:16, 401.56it/s][1,10]<stderr>:#015 38%|███▊      | 3980/10570 [00:09<00:16, 399.90it/s][1,3]<stderr>:#015 35%|███▌      | 3719/10570 [00:09<00:17, 387.63it/s][1,11]<stderr>:#015 35%|███▌      | 3718/10570 [00:09<00:17, 385.99it/s][1,2]<stderr>:#015 38%|███▊      | 4045/10570 [00:09<00:16, 396.28it/s][1,5]<stderr>:#015 36%|███▋      | 3835/10570 [00:09<00:16, 398.47it/s][1,9]<stderr>:#015 37%|███▋      | 3883/10570 [00:09<00:16, 398.22it/s][1,12]<stderr>:#015 39%|███▊      | 4095/10570 [00:10<00:15, 406.47it/s][1,13]<stderr>:#015 34%|███▎      | 3544/10570 [00:09<00:18, 378.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▌      | 3800/10570 [00:09<00:17, 393.00it/s]#033[A[1,4]<stderr>:#015 38%|███▊      | 3984/10570 [00:09<00:16, 395.67it/s][1,15]<stderr>:#015 37%|███▋      | 3945/10570 [00:09<00:17, 383.75it/s][1,6]<stderr>:#015 35%|███▍      | 3649/10570 [00:09<00:18, 382.41it/s][1,1]<stderr>:#015 38%|███▊      | 4008/10570 [00:09<00:16, 396.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 31%|███▏      | 3326/10570 [00:08<00:19, 365.86it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3912/10570 [00:09<00:16, 398.81it/s][1,10]<stderr>:#015 38%|███▊      | 4021/10570 [00:09<00:16, 398.10it/s][1,3]<stderr>:#015 36%|███▌      | 3758/10570 [00:09<00:17, 387.36it/s][1,11]<stderr>:#015 36%|███▌      | 3757/10570 [00:09<00:17, 386.04it/s][1,5]<stderr>:#015 37%|███▋      | 3875/10570 [00:09<00:16, 398.35it/s][1,2]<stderr>:#015 39%|███▊      | 4089/10570 [00:10<00:15, 406.19it/s][1,9]<stderr>:#015 37%|███▋      | 3923/10570 [00:09<00:16, 393.86it/s][1,7]<stderr>:#015 39%|███▉      | 4170/10570 [00:10<00:19, 322.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 36%|███▋      | 3840/10570 [00:09<00:17, 393.61it/s]#033[A[1,13]<stderr>:#015 34%|███▍      | 3582/10570 [00:09<00:18, 371.59it/s][1,4]<stderr>:#015 38%|███▊      | 4024/10570 [00:09<00:16, 392.41it/s][1,12]<stderr>:#015 39%|███▉      | 4136/10570 [00:10<00:16, 383.06it/s][1,15]<stderr>:#015 38%|███▊      | 3986/10570 [00:09<00:16, 389.62it/s][1,6]<stderr>:#015 35%|███▍      | 3689/10570 [00:09<00:17, 384.89it/s][1,1]<stderr>:#015 38%|███▊      | 4049/10570 [00:10<00:16, 397.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 32%|███▏      | 3365/10570 [00:08<00:19, 370.17it/s]#033[A[1,14]<stderr>:#015 37%|███▋      | 3952/10570 [00:09<00:16, 398.09it/s][1,10]<stderr>:#015 38%|███▊      | 4063/10570 [00:10<00:16, 402.45it/s][1,3]<stderr>:#015 36%|███▌      | 3797/10570 [00:09<00:17, 386.61it/s][1,11]<stderr>:#015 36%|███▌      | 3796/10570 [00:09<00:17, 379.95it/s][1,5]<stderr>:#015 37%|███▋      | 3915/10570 [00:09<00:16, 395.50it/s][1,9]<stderr>:#015 37%|███▋      | 3963/10570 [00:09<00:16, 394.08it/s][1,2]<stderr>:#015 39%|███▉      | 4130/10570 [00:10<00:16, 381.02it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3880/10570 [00:09<00:16, 395.36it/s][1,0]<stderr>:#033[A[1,4]<stderr>:#015 38%|███▊      | 4066/10570 [00:10<00:16, 398.42it/s][1,13]<stderr>:#015 34%|███▍      | 3620/10570 [00:09<00:19, 365.61it/s][1,15]<stderr>:#015 38%|███▊      | 4026/10570 [00:10<00:16, 386.09it/s][1,6]<stderr>:#015 35%|███▌      | 3728/10570 [00:09<00:17, 383.07it/s][1,1]<stderr>:#015 39%|███▊      | 4092/10570 [00:10<00:16, 404.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 32%|███▏      | 3403/10570 [00:09<00:19, 372.26it/s]#033[A[1,14]<stderr>:#015 38%|███▊      | 3992/10570 [00:09<00:16, 396.87it/s][1,7]<stderr>:#015 40%|███▉      | 4205/10570 [00:10<00:22, 283.52it/s][1,3]<stderr>:#015 36%|███▋      | 3837/10570 [00:09<00:17, 389.34it/s][1,10]<stderr>:#015 39%|███▉      | 4104/10570 [00:10<00:16, 394.74it/s][1,5]<stderr>:#015 37%|███▋      | 3955/10570 [00:09<00:16, 395.22it/s][1,11]<stderr>:#015 36%|███▋      | 3836/10570 [00:09<00:17, 383.40it/s][1,9]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:16, 388.19it/s][1,12]<stderr>:#015 39%|███▉      | 4175/10570 [00:10<00:20, 316.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3920/10570 [00:09<00:16, 391.82it/s]#033[A[1,13]<stderr>:#015 35%|███▍      | 3659/10570 [00:09<00:18, 369.74it/s][1,4]<stderr>:#015 39%|███▉      | 4106/10570 [00:10<00:16, 388.87it/s][1,15]<stderr>:#015 38%|███▊      | 4067/10570 [00:10<00:16, 390.88it/s][1,6]<stderr>:#015 36%|███▌      | 3768/10570 [00:09<00:17, 386.86it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3441/10570 [00:09<00:19, 371.02it/s]#033[A[1,7]<stderr>:#015 40%|████      | 4239/10570 [00:10<00:21, 298.08it/s][1,14]<stderr>:#015 38%|███▊      | 4032/10570 [00:09<00:16, 393.88it/s][1,3]<stderr>:#015 37%|███▋      | 3876/10570 [00:09<00:17, 389.19it/s][1,1]<stderr>:#015 39%|███▉      | 4133/10570 [00:10<00:16, 379.94it/s][1,2]<stderr>:#015 39%|███▉      | 4169/10570 [00:10<00:20, 315.54it/s][1,11]<stderr>:#015 37%|███▋      | 3875/10570 [00:09<00:17, 385.22it/s][1,5]<stderr>:#015 38%|███▊      | 3995/10570 [00:09<00:16, 394.94it/s][1,10]<stderr>:#015 39%|███▉      | 4144/10570 [00:10<00:17, 366.39it/s][1,9]<stderr>:#015 38%|███▊      | 4043/10570 [00:10<00:16, 390.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 37%|███▋      | 3960/10570 [00:09<00:16, 391.82it/s]#033[A[1,13]<stderr>:#015 35%|███▍      | 3697/10570 [00:09<00:18, 370.88it/s][1,6]<stderr>:#015 36%|███▌      | 3807/10570 [00:09<00:17, 384.68it/s][1,15]<stderr>:#015 39%|███▉      | 4107/10570 [00:10<00:16, 380.66it/s][1,4]<stderr>:#015 39%|███▉      | 4145/10570 [00:10<00:17, 358.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3479/10570 [00:09<00:19, 364.04it/s]#033[A[1,14]<stderr>:#015 39%|███▊      | 4075/10570 [00:10<00:16, 402.40it/s][1,12]<stderr>:#015 40%|███▉      | 4209/10570 [00:10<00:23, 272.72it/s][1,3]<stderr>:#015 37%|███▋      | 3915/10570 [00:09<00:17, 385.60it/s][1,11]<stderr>:#015 37%|███▋      | 3914/10570 [00:09<00:17, 383.56it/s][1,5]<stderr>:#015 38%|███▊      | 4035/10570 [00:10<00:16, 394.17it/s][1,9]<stderr>:#015 39%|███▊      | 4086/10570 [00:10<00:16, 400.57it/s][1,2]<stderr>:#015 40%|███▉      | 4203/10570 [00:10<00:22, 281.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 38%|███▊      | 4000/10570 [00:10<00:16, 391.19it/s]#033[A[1,7]<stderr>:#015 40%|████      | 4271/10570 [00:10<00:24, 256.90it/s][1,13]<stderr>:#015 35%|███▌      | 3735/10570 [00:09<00:18, 373.31it/s][1,1]<stderr>:#015 39%|███▉      | 4172/10570 [00:10<00:20, 312.49it/s][1,6]<stderr>:#015 36%|███▋      | 3846/10570 [00:09<00:17, 384.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 33%|███▎      | 3516/10570 [00:09<00:19, 361.27it/s]#033[A[1,12]<stderr>:#015 40%|████      | 4247/10570 [00:10<00:21, 297.83it/s][1,3]<stderr>:#015 37%|███▋      | 3954/10570 [00:10<00:17, 385.04it/s][1,10]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:21, 302.72it/s][1,14]<stderr>:#015 39%|███▉      | 4116/10570 [00:10<00:16, 387.82it/s][1,11]<stderr>:#015 37%|███▋      | 3953/10570 [00:10<00:17, 384.00it/s][1,5]<stderr>:#015 39%|███▊      | 4078/10570 [00:10<00:16, 401.86it/s][1,15]<stderr>:#015 39%|███▉      | 4146/10570 [00:10<00:18, 345.79it/s][1,9]<stderr>:#015 39%|███▉      | 4127/10570 [00:10<00:16, 379.97it/s][1,4]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:20, 304.54it/s][1,2]<stderr>:#015 40%|████      | 4234/10570 [00:10<00:22, 288.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 38%|███▊      | 4040/10570 [00:10<00:16, 391.51it/s]#033[A[1,13]<stderr>:#015 36%|███▌      | 3775/10570 [00:09<00:17, 379.44it/s][1,6]<stderr>:#015 37%|███▋      | 3886/10570 [00:09<00:17, 386.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▎      | 3554/10570 [00:09<00:19, 366.27it/s][1,8]<stderr>:#033[A[1,3]<stderr>:#015 38%|███▊      | 3993/10570 [00:10<00:17, 381.17it/s][1,7]<stderr>:#015 41%|████      | 4299/10570 [00:10<00:27, 225.82it/s][1,11]<stderr>:#015 38%|███▊      | 3993/10570 [00:10<00:17, 386.26it/s][1,5]<stderr>:#015 39%|███▉      | 4119/10570 [00:10<00:16, 386.74it/s][1,1]<stderr>:#015 40%|███▉      | 4206/10570 [00:10<00:23, 273.86it/s][1,10]<stderr>:#015 40%|███▉      | 4215/10570 [00:10<00:23, 270.30it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▊      | 4082/10570 [00:10<00:16, 399.54it/s]#033[A[1,13]<stderr>:#015 36%|███▌      | 3813/10570 [00:10<00:18, 374.28it/s][1,15]<stderr>:#015 40%|███▉      | 4182/10570 [00:10<00:21, 299.92it/s][1,14]<stderr>:#015 39%|███▉      | 4155/10570 [00:10<00:19, 322.96it/s][1,2]<stderr>:#015 40%|████      | 4265/10570 [00:10<00:23, 270.87it/s][1,6]<stderr>:#015 37%|███▋      | 3925/10570 [00:10<00:17, 382.31it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▍      | 3591/10570 [00:09<00:19, 360.04it/s]#033[A[1,4]<stderr>:#015 40%|███▉      | 4215/10570 [00:10<00:23, 270.49it/s][1,12]<stderr>:#015 40%|████      | 4280/10570 [00:10<00:26, 234.29it/s][1,3]<stderr>:#015 38%|███▊      | 4032/10570 [00:10<00:17, 380.03it/s][1,7]<stderr>:#015 41%|████      | 4325/10570 [00:10<00:26, 234.71it/s][1,11]<stderr>:#015 38%|███▊      | 4032/10570 [00:10<00:17, 383.87it/s][1,9]<stderr>:#015 39%|███▉      | 4166/10570 [00:10<00:20, 313.92it/s][1,1]<stderr>:#015 40%|████      | 4241/10570 [00:10<00:21, 292.37it/s][1,10]<stderr>:#015 40%|████      | 4253/10570 [00:10<00:21, 294.39it/s][1,13]<stderr>:#015 36%|███▋      | 3851/10570 [00:10<00:17, 374.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▉      | 4122/10570 [00:10<00:16, 382.15it/s]#033[A[1,6]<stderr>:#015 38%|███▊      | 3964/10570 [00:10<00:17, 382.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 34%|███▍      | 3628/10570 [00:09<00:19, 362.59it/s][1,8]<stderr>:#033[A[1,5]<stderr>:#015 39%|███▉      | 4158/10570 [00:10<00:20, 318.61it/s][1,4]<stderr>:#015 40%|████      | 4253/10570 [00:10<00:21, 294.28it/s][1,3]<stderr>:#015 39%|███▊      | 4074/10570 [00:10<00:16, 389.02it/s][1,7]<stderr>:#015 41%|████▏     | 4366/10570 [00:10<00:23, 268.70it/s][1,11]<stderr>:#015 39%|███▊      | 4074/10570 [00:10<00:16, 392.84it/s][1,14]<stderr>:#015 40%|███▉      | 4190/10570 [00:10<00:21, 294.82it/s][1,15]<stderr>:#015 40%|███▉      | 4214/10570 [00:10<00:23, 265.23it/s][1,12]<stderr>:#015 41%|████      | 4308/10570 [00:10<00:29, 212.78it/s][1,13]<stderr>:#015 37%|███▋      | 3889/10570 [00:10<00:17, 373.52it/s][1,2]<stderr>:#015 41%|████      | 4294/10570 [00:10<00:28, 222.59it/s][1,9]<stderr>:#015 40%|███▉      | 4200/10570 [00:10<00:22, 278.74it/s][1,6]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:17, 381.70it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▍      | 3665/10570 [00:09<00:18, 363.79it/s]#033[A[1,1]<stderr>:#015 40%|████      | 4273/10570 [00:10<00:25, 247.67it/s][1,7]<stderr>:#015 42%|████▏     | 4407/10570 [00:11<00:20, 298.26it/s][1,3]<stderr>:#015 39%|███▉      | 4113/10570 [00:10<00:17, 372.52it/s][1,15]<stderr>:#015 40%|████      | 4251/10570 [00:10<00:21, 289.45it/s][1,11]<stderr>:#015 39%|███▉      | 4114/10570 [00:10<00:17, 376.32it/s][1,14]<stderr>:#015 40%|███▉      | 4222/10570 [00:10<00:22, 283.94it/s][1,5]<stderr>:#015 40%|███▉      | 4192/10570 [00:10<00:22, 288.30it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 39%|███▉      | 4161/10570 [00:10<00:20, 312.99it/s]#033[A[1,12]<stderr>:#015 41%|████      | 4348/10570 [00:11<00:25, 246.50it/s][1,13]<stderr>:#015 37%|███▋      | 3927/10570 [00:10<00:17, 372.12it/s][1,9]<stderr>:#015 40%|████      | 4231/10570 [00:10<00:22, 284.73it/s][1,2]<stderr>:#015 41%|████      | 4319/10570 [00:11<00:28, 221.31it/s][1,6]<stderr>:#015 38%|███▊      | 4042/10570 [00:10<00:17, 382.42it/s][1,10]<stderr>:#015 41%|████      | 4285/10570 [00:10<00:28, 217.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▌      | 3702/10570 [00:09<00:18, 364.45it/s]#033[A[1,7]<stderr>:#015 42%|████▏     | 4449/10570 [00:11<00:18, 326.06it/s][1,4]<stderr>:#015 41%|████      | 4285/10570 [00:10<00:28, 222.31it/s][1,14]<stderr>:#015 40%|████      | 4257/10570 [00:10<00:21, 300.37it/s][1,5]<stderr>:#015 40%|███▉      | 4223/10570 [00:10<00:22, 279.50it/s][1,12]<stderr>:#015 42%|████▏     | 4388/10570 [00:11<00:22, 277.95it/s][1,13]<stderr>:#015 38%|███▊      | 3965/10570 [00:10<00:17, 368.32it/s][1,1]<stderr>:#015 41%|████      | 4301/10570 [00:11<00:29, 215.70it/s][1,2]<stderr>:#015 41%|████      | 4360/10570 [00:11<00:24, 255.55it/s][1,3]<stderr>:#015 39%|███▉      | 4151/10570 [00:10<00:20, 314.15it/s][1,6]<stderr>:#015 39%|███▊      | 4084/10570 [00:10<00:16, 391.11it/s][1,9]<stderr>:#015 40%|████      | 4262/10570 [00:10<00:22, 281.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|███▉      | 4195/10570 [00:10<00:22, 283.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 35%|███▌      | 3739/10570 [00:09<00:18, 364.73it/s]#033[A[1,11]<stderr>:#015 39%|███▉      | 4152/10570 [00:10<00:20, 311.70it/s][1,7]<stderr>:#015 42%|████▏     | 4492/10570 [00:11<00:17, 350.72it/s][1,10]<stderr>:#015 41%|████      | 4312/10570 [00:11<00:29, 211.72it/s][1,15]<stderr>:#015 41%|████      | 4282/10570 [00:11<00:28, 222.56it/s][1,4]<stderr>:#015 41%|████      | 4312/10570 [00:11<00:29, 215.21it/s][1,5]<stderr>:#015 40%|████      | 4258/10570 [00:10<00:21, 293.61it/s][1,12]<stderr>:#015 42%|████▏     | 4429/10570 [00:11<00:20, 306.41it/s][1,13]<stderr>:#015 38%|███▊      | 4002/10570 [00:10<00:17, 368.45it/s][1,1]<stderr>:#015 41%|████      | 4332/10570 [00:11<00:26, 236.83it/s][1,2]<stderr>:#015 42%|████▏     | 4400/10570 [00:11<00:21, 285.33it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▌      | 3779/10570 [00:10<00:18, 373.00it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|███▉      | 4226/10570 [00:10<00:22, 276.77it/s]#033[A[1,6]<stderr>:#015 39%|███▉      | 4124/10570 [00:10<00:17, 371.47it/s][1,3]<stderr>:#015 40%|███▉      | 4185/10570 [00:10<00:22, 290.16it/s][1,7]<stderr>:#015 43%|████▎     | 4530/10570 [00:11<00:17, 349.41it/s][1,10]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:25, 245.90it/s][1,11]<stderr>:#015 40%|███▉      | 4186/10570 [00:10<00:22, 289.41it/s][1,14]<stderr>:#015 41%|████      | 4289/10570 [00:10<00:27, 228.54it/s][1,4]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:24, 249.04it/s][1,12]<stderr>:#015 42%|████▏     | 4470/10570 [00:11<00:18, 331.18it/s][1,13]<stderr>:#015 38%|███▊      | 4039/10570 [00:10<00:17, 368.77it/s][1,1]<stderr>:#015 41%|████▏     | 4372/10570 [00:11<00:22, 269.82it/s][1,15]<stderr>:#015 41%|████      | 4308/10570 [00:11<00:30, 206.53it/s][1,2]<stderr>:#015 42%|████▏     | 4442/10570 [00:11<00:19, 314.20it/s][1,9]<stderr>:#015 41%|████      | 4292/10570 [00:11<00:28, 221.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▌      | 3817/10570 [00:10<00:18, 364.98it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 40%|████      | 4259/10570 [00:10<00:21, 287.83it/s]#033[A[1,7]<stderr>:#015 43%|████▎     | 4570/10570 [00:11<00:16, 361.72it/s][1,10]<stderr>:#015 42%|████▏     | 4392/10570 [00:11<00:22, 276.68it/s][1,3]<stderr>:#015 40%|███▉      | 4216/10570 [00:10<00:24, 264.13it/s][1,4]<stderr>:#015 42%|████▏     | 4392/10570 [00:11<00:22, 279.40it/s][1,12]<stderr>:#015 43%|████▎     | 4508/10570 [00:11<00:17, 343.39it/s][1,13]<stderr>:#015 39%|███▊      | 4079/10570 [00:10<00:17, 377.50it/s][1,11]<stderr>:#015 40%|███▉      | 4217/10570 [00:10<00:23, 266.35it/s][1,1]<stderr>:#015 42%|████▏     | 4412/10570 [00:11<00:20, 297.69it/s][1,15]<stderr>:#015 41%|████      | 4347/10570 [00:11<00:25, 239.40it/s][1,2]<stderr>:#015 42%|████▏     | 4485/10570 [00:11<00:17, 340.92it/s][1,14]<stderr>:#015 41%|████      | 4316/10570 [00:11<00:28, 220.83it/s][1,6]<stderr>:#015 39%|███▉      | 4162/10570 [00:10<00:21, 302.71it/s][1,5]<stderr>:#015 41%|████      | 4289/10570 [00:11<00:28, 217.61it/s][1,9]<stderr>:#015 41%|████      | 4317/10570 [00:11<00:28, 218.32it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 36%|███▋      | 3854/10570 [00:10<00:18, 365.83it/s]#033[A[1,10]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:20, 305.15it/s][1,7]<stderr>:#015 44%|████▎     | 4608/10570 [00:11<00:16, 352.05it/s][1,3]<stderr>:#015 40%|████      | 4252/10570 [00:11<00:22, 286.97it/s][1,4]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:19, 307.49it/s][1,12]<stderr>:#015 43%|████▎     | 4545/10570 [00:11<00:17, 347.30it/s][1,1]<stderr>:#015 42%|████▏     | 4453/10570 [00:11<00:18, 324.21it/s][1,11]<stderr>:#015 40%|████      | 4253/10570 [00:11<00:21, 287.32it/s][1,15]<stderr>:#015 41%|████▏     | 4386/10570 [00:11<00:22, 270.40it/s][1,13]<stderr>:#015 39%|███▉      | 4117/10570 [00:10<00:17, 362.45it/s][1,14]<stderr>:#015 41%|████      | 4356/10570 [00:11<00:24, 254.78it/s][1,2]<stderr>:#015 43%|████▎     | 4522/10570 [00:11<00:17, 342.19it/s][1,9]<stderr>:#015 41%|████      | 4357/10570 [00:11<00:24, 252.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 37%|███▋      | 3892/10570 [00:10<00:18, 367.34it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4289/10570 [00:11<00:28, 220.21it/s]#033[A[1,5]<stderr>:#015 41%|████      | 4315/10570 [00:11<00:29, 211.74it/s][1,10]<stderr>:#015 42%|████▏     | 4476/10570 [00:11<00:18, 333.22it/s][1,6]<stderr>:#015 40%|███▉      | 4195/10570 [00:10<00:23, 273.93it/s][1,7]<stderr>:#015 44%|████▍     | 4645/10570 [00:11<00:16, 348.62it/s][1,4]<stderr>:#015 42%|████▏     | 4476/10570 [00:11<00:18, 334.81it/s][1,12]<stderr>:#015 43%|████▎     | 4584/10570 [00:11<00:16, 357.35it/s][1,1]<stderr>:#015 43%|████▎     | 4495/10570 [00:11<00:17, 345.78it/s][1,15]<stderr>:#015 42%|████▏     | 4426/10570 [00:11<00:20, 297.87it/s][1,14]<stderr>:#015 42%|████▏     | 4396/10570 [00:11<00:21, 284.81it/s][1,2]<stderr>:#015 43%|████▎     | 4560/10570 [00:11<00:17, 351.94it/s][1,9]<stderr>:#015 42%|████▏     | 4396/10570 [00:11<00:21, 281.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 37%|███▋      | 3929/10570 [00:10<00:18, 365.14it/s]#033[A[1,5]<stderr>:#015 41%|████      | 4355/10570 [00:11<00:25, 245.73it/s][1,10]<stderr>:#015 43%|████▎     | 4513/10570 [00:11<00:18, 331.01it/s][1,7]<stderr>:#015 44%|████▍     | 4682/10570 [00:11<00:16, 352.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4315/10570 [00:11<00:29, 212.65it/s]#033[A[1,6]<stderr>:#015 40%|███▉      | 4225/10570 [00:11<00:23, 266.99it/s][1,13]<stderr>:#015 39%|███▉      | 4154/10570 [00:11<00:21, 299.46it/s][1,4]<stderr>:#015 43%|████▎     | 4513/10570 [00:11<00:17, 340.88it/s][1,12]<stderr>:#015 44%|████▎     | 4621/10570 [00:11<00:16, 350.94it/s][1,15]<stderr>:#015 42%|████▏     | 4468/10570 [00:11<00:18, 325.23it/s][1,3]<stderr>:#015 41%|████      | 4283/10570 [00:11<00:29, 216.58it/s][1,14]<stderr>:#015 42%|████▏     | 4437/10570 [00:11<00:19, 312.83it/s][1,1]<stderr>:#015 43%|████▎     | 4533/10570 [00:11<00:17, 345.10it/s][1,2]<stderr>:#015 43%|████▎     | 4597/10570 [00:11<00:17, 348.97it/s][1,11]<stderr>:#015 41%|████      | 4284/10570 [00:11<00:29, 214.35it/s][1,9]<stderr>:#015 42%|████▏     | 4437/10570 [00:11<00:19, 309.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 3966/10570 [00:10<00:18, 366.00it/s]#033[A[1,5]<stderr>:#015 42%|████▏     | 4395/10570 [00:11<00:22, 276.39it/s][1,10]<stderr>:#015 43%|████▎     | 4549/10570 [00:11<00:17, 338.50it/s][1,7]<stderr>:#015 45%|████▍     | 4725/10570 [00:11<00:15, 370.65it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 41%|████      | 4354/10570 [00:11<00:25, 246.21it/s]#033[A[1,6]<stderr>:#015 40%|████      | 4258/10570 [00:11<00:22, 281.09it/s][1,4]<stderr>:#015 43%|████▎     | 4550/10570 [00:11<00:17, 345.10it/s][1,15]<stderr>:#015 43%|████▎     | 4505/10570 [00:11<00:18, 336.53it/s][1,1]<stderr>:#015 43%|████▎     | 4572/10570 [00:11<00:16, 355.80it/s][1,13]<stderr>:#015 40%|███▉      | 4186/10570 [00:11<00:22, 280.10it/s][1,14]<stderr>:#015 42%|████▏     | 4480/10570 [00:11<00:17, 339.01it/s][1,12]<stderr>:#015 44%|████▍     | 4658/10570 [00:11<00:17, 343.11it/s][1,2]<stderr>:#015 44%|████▍     | 4634/10570 [00:11<00:16, 354.54it/s][1,9]<stderr>:#015 42%|████▏     | 4479/10570 [00:11<00:18, 335.85it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 4003/10570 [00:10<00:18, 359.35it/s]#033[A[1,5]<stderr>:#015 42%|████▏     | 4436/10570 [00:11<00:20, 304.74it/s][1,3]<stderr>:#015 41%|████      | 4309/10570 [00:11<00:30, 205.05it/s][1,11]<stderr>:#015 41%|████      | 4310/10570 [00:11<00:30, 207.18it/s][1,10]<stderr>:#015 43%|████▎     | 4587/10570 [00:11<00:17, 349.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4393/10570 [00:11<00:22, 276.37it/s]#033[A[1,7]<stderr>:#015 45%|████▌     | 4763/10570 [00:12<00:16, 361.63it/s][1,4]<stderr>:#015 43%|████▎     | 4589/10570 [00:11<00:16, 355.11it/s][1,14]<stderr>:#015 43%|████▎     | 4517/10570 [00:11<00:17, 343.36it/s][1,12]<stderr>:#015 44%|████▍     | 4698/10570 [00:11<00:16, 357.24it/s][1,15]<stderr>:#015 43%|████▎     | 4542/10570 [00:11<00:17, 339.00it/s][1,1]<stderr>:#015 44%|████▎     | 4609/10570 [00:11<00:17, 346.40it/s][1,9]<stderr>:#015 43%|████▎     | 4516/10570 [00:11<00:17, 340.20it/s][1,2]<stderr>:#015 44%|████▍     | 4671/10570 [00:11<00:17, 339.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 38%|███▊      | 4040/10570 [00:10<00:18, 362.02it/s]#033[A[1,13]<stderr>:#015 40%|███▉      | 4216/10570 [00:11<00:24, 257.78it/s][1,3]<stderr>:#015 41%|████      | 4347/10570 [00:11<00:26, 237.65it/s][1,5]<stderr>:#015 42%|████▏     | 4479/10570 [00:11<00:18, 332.03it/s][1,11]<stderr>:#015 41%|████      | 4348/10570 [00:11<00:25, 239.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4433/10570 [00:11<00:20, 304.12it/s]#033[A[1,10]<stderr>:#015 44%|████▎     | 4624/10570 [00:11<00:17, 343.16it/s][1,7]<stderr>:#015 45%|████▌     | 4800/10570 [00:12<00:15, 363.71it/s][1,6]<stderr>:#015 41%|████      | 4288/10570 [00:11<00:29, 213.89it/s][1,4]<stderr>:#015 44%|████▍     | 4626/10570 [00:11<00:17, 348.04it/s][1,14]<stderr>:#015 43%|████▎     | 4554/10570 [00:11<00:17, 349.89it/s][1,15]<stderr>:#015 43%|████▎     | 4579/10570 [00:11<00:17, 347.32it/s][1,12]<stderr>:#015 45%|████▍     | 4739/10570 [00:12<00:15, 369.55it/s][1,1]<stderr>:#015 44%|████▍     | 4645/10570 [00:11<00:17, 342.73it/s][1,9]<stderr>:#015 43%|████▎     | 4553/10570 [00:11<00:17, 345.97it/s][1,2]<stderr>:#015 45%|████▍     | 4714/10570 [00:12<00:16, 360.26it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▊      | 4080/10570 [00:10<00:17, 371.32it/s]#033[A[1,13]<stderr>:#015 40%|████      | 4251/10570 [00:11<00:22, 279.79it/s][1,3]<stderr>:#015 41%|████▏     | 4386/10570 [00:11<00:23, 268.02it/s][1,5]<stderr>:#015 43%|████▎     | 4516/10570 [00:11<00:17, 337.87it/s][1,11]<stderr>:#015 42%|████▏     | 4387/10570 [00:11<00:22, 269.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 42%|████▏     | 4475/10570 [00:11<00:18, 330.98it/s]#033[A[1,7]<stderr>:#015 46%|████▌     | 4843/10570 [00:12<00:15, 380.62it/s][1,10]<stderr>:#015 44%|████▍     | 4660/10570 [00:12<00:17, 335.87it/s][1,14]<stderr>:#015 43%|████▎     | 4593/10570 [00:11<00:16, 359.09it/s][1,4]<stderr>:#015 44%|████▍     | 4662/10570 [00:12<00:17, 338.73it/s][1,15]<stderr>:#015 44%|████▎     | 4615/10570 [00:11<00:17, 341.35it/s][1,6]<stderr>:#015 41%|████      | 4313/10570 [00:11<00:30, 203.49it/s][1,1]<stderr>:#015 44%|████▍     | 4681/10570 [00:12<00:16, 346.84it/s][1,12]<stderr>:#015 45%|████▌     | 4777/10570 [00:12<00:16, 353.10it/s][1,9]<stderr>:#015 43%|████▎     | 4591/10570 [00:11<00:16, 354.96it/s][1,2]<stderr>:#015 45%|████▍     | 4751/10570 [00:12<00:16, 361.92it/s][1,3]<stderr>:#015 42%|████▏     | 4425/10570 [00:11<00:20, 294.77it/s][1,5]<stderr>:#015 43%|████▎     | 4553/10570 [00:11<00:17, 344.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▉      | 4118/10570 [00:11<00:18, 353.87it/s]#033[A[1,11]<stderr>:#015 42%|████▏     | 4426/10570 [00:11<00:20, 296.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4512/10570 [00:11<00:17, 337.22it/s]#033[A[1,10]<stderr>:#015 44%|████▍     | 4700/10570 [00:12<00:16, 351.10it/s][1,7]<stderr>:#015 46%|████▌     | 4882/10570 [00:12<00:15, 360.83it/s][1,4]<stderr>:#015 44%|████▍     | 4702/10570 [00:12<00:16, 354.11it/s][1,14]<stderr>:#015 44%|████▍     | 4631/10570 [00:11<00:16, 351.88it/s][1,6]<stderr>:#015 41%|████      | 4352/10570 [00:11<00:26, 236.62it/s][1,1]<stderr>:#015 45%|████▍     | 4723/10570 [00:12<00:16, 363.96it/s][1,15]<stderr>:#015 44%|████▍     | 4651/10570 [00:12<00:17, 335.03it/s][1,12]<stderr>:#015 46%|████▌     | 4818/10570 [00:12<00:15, 366.92it/s][1,9]<stderr>:#015 44%|████▍     | 4628/10570 [00:12<00:17, 346.84it/s][1,3]<stderr>:#015 42%|████▏     | 4466/10570 [00:11<00:19, 321.25it/s][1,5]<stderr>:#015 43%|████▎     | 4591/10570 [00:11<00:16, 353.84it/s][1,13]<stderr>:#015 41%|████      | 4281/10570 [00:11<00:29, 215.81it/s][1,2]<stderr>:#015 45%|████▌     | 4788/10570 [00:12<00:16, 343.94it/s][1,11]<stderr>:#015 42%|████▏     | 4467/10570 [00:11<00:18, 322.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4548/10570 [00:11<00:17, 340.45it/s]#033[A[1,10]<stderr>:#015 45%|████▍     | 4737/10570 [00:12<00:16, 354.71it/s][1,7]<stderr>:#015 47%|████▋     | 4923/10570 [00:12<00:15, 373.20it/s][1,4]<stderr>:#015 45%|████▍     | 4742/10570 [00:12<00:15, 366.73it/s][1,6]<stderr>:#015 42%|████▏     | 4390/10570 [00:11<00:23, 266.55it/s][1,15]<stderr>:#015 44%|████▍     | 4688/10570 [00:12<00:17, 344.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 39%|███▉      | 4154/10570 [00:11<00:22, 287.05it/s]#033[A[1,1]<stderr>:#015 45%|████▌     | 4760/10570 [00:12<00:16, 353.61it/s][1,14]<stderr>:#015 44%|████▍     | 4668/10570 [00:11<00:17, 340.24it/s][1,3]<stderr>:#015 43%|████▎     | 4504/10570 [00:11<00:18, 334.80it/s][1,12]<stderr>:#015 46%|████▌     | 4856/10570 [00:12<00:16, 350.21it/s][1,2]<stderr>:#015 46%|████▌     | 4831/10570 [00:12<00:15, 364.56it/s][1,9]<stderr>:#015 44%|████▍     | 4664/10570 [00:12<00:17, 337.11it/s][1,11]<stderr>:#015 43%|████▎     | 4504/10570 [00:11<00:18, 335.52it/s][1,5]<stderr>:#015 44%|████▍     | 4628/10570 [00:11<00:17, 346.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 43%|████▎     | 4586/10570 [00:11<00:17, 350.30it/s]#033[A[1,10]<stderr>:#015 45%|████▌     | 4773/10570 [00:12<00:16, 342.31it/s][1,7]<stderr>:#015 47%|████▋     | 4964/10570 [00:12<00:14, 380.98it/s][1,13]<stderr>:#015 41%|████      | 4306/10570 [00:11<00:31, 195.97it/s][1,6]<stderr>:#015 42%|████▏     | 4429/10570 [00:11<00:20, 293.61it/s][1,15]<stderr>:#015 45%|████▍     | 4729/10570 [00:12<00:16, 361.50it/s][1,4]<stderr>:#015 45%|████▌     | 4780/10570 [00:12<00:16, 348.08it/s][1,1]<stderr>:#015 45%|████▌     | 4797/10570 [00:12<00:16, 356.65it/s][1,14]<stderr>:#015 45%|████▍     | 4710/10570 [00:12<00:16, 358.90it/s][1,12]<stderr>:#015 46%|████▋     | 4898/10570 [00:12<00:15, 367.75it/s][1,3]<stderr>:#015 43%|████▎     | 4540/10570 [00:11<00:17, 336.51it/s][1,9]<stderr>:#015 45%|████▍     | 4704/10570 [00:12<00:16, 352.51it/s][1,11]<stderr>:#015 43%|████▎     | 4540/10570 [00:12<00:17, 336.06it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|███▉      | 4185/10570 [00:11<00:23, 269.08it/s]#033[A[1,5]<stderr>:#015 44%|████▍     | 4664/10570 [00:12<00:17, 337.66it/s][1,2]<stderr>:#015 46%|████▌     | 4869/10570 [00:12<00:16, 349.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▎     | 4623/10570 [00:12<00:17, 343.20it/s]#033[A[1,10]<stderr>:#015 46%|████▌     | 4812/10570 [00:12<00:16, 354.00it/s][1,7]<stderr>:#015 47%|████▋     | 5003/10570 [00:12<00:14, 376.84it/s][1,13]<stderr>:#015 41%|████      | 4342/10570 [00:11<00:27, 226.54it/s][1,6]<stderr>:#015 42%|████▏     | 4470/10570 [00:11<00:19, 320.56it/s][1,4]<stderr>:#015 46%|████▌     | 4821/10570 [00:12<00:15, 364.47it/s][1,1]<stderr>:#015 46%|████▌     | 4840/10570 [00:12<00:15, 373.86it/s][1,14]<stderr>:#015 45%|████▍     | 4750/10570 [00:12<00:15, 369.00it/s][1,15]<stderr>:#015 45%|████▌     | 4766/10570 [00:12<00:16, 350.51it/s][1,12]<stderr>:#015 47%|████▋     | 4939/10570 [00:12<00:14, 377.51it/s][1,3]<stderr>:#015 43%|████▎     | 4577/10570 [00:12<00:17, 344.36it/s][1,9]<stderr>:#015 45%|████▍     | 4744/10570 [00:12<00:15, 364.72it/s][1,11]<stderr>:#015 43%|████▎     | 4577/10570 [00:12<00:17, 344.28it/s][1,5]<stderr>:#015 45%|████▍     | 4704/10570 [00:12<00:16, 353.47it/s][1,2]<stderr>:#015 46%|████▋     | 4910/10570 [00:12<00:15, 364.02it/s][1,10]<stderr>:#015 46%|████▌     | 4851/10570 [00:12<00:15, 361.66it/s][1,7]<stderr>:#015 48%|████▊     | 5042/10570 [00:12<00:14, 379.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▍     | 4659/10570 [00:12<00:18, 328.07it/s]#033[A[1,13]<stderr>:#015 41%|████▏     | 4380/10570 [00:11<00:24, 257.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|███▉      | 4214/10570 [00:11<00:26, 242.87it/s][1,8]<stderr>:#033[A[1,6]<stderr>:#015 43%|████▎     | 4506/10570 [00:12<00:18, 331.08it/s][1,15]<stderr>:#015 45%|████▌     | 4803/10570 [00:12<00:16, 354.98it/s][1,12]<stderr>:#015 47%|████▋     | 4978/10570 [00:12<00:14, 379.96it/s][1,4]<stderr>:#015 46%|████▌     | 4859/10570 [00:12<00:16, 347.06it/s][1,1]<stderr>:#015 46%|████▌     | 4878/10570 [00:12<00:16, 352.55it/s][1,14]<stderr>:#015 45%|████▌     | 4788/10570 [00:12<00:16, 349.08it/s][1,3]<stderr>:#015 44%|████▎     | 4613/10570 [00:12<00:17, 336.76it/s][1,5]<stderr>:#015 45%|████▍     | 4744/10570 [00:12<00:15, 365.02it/s][1,11]<stderr>:#015 44%|████▎     | 4613/10570 [00:12<00:17, 336.63it/s][1,2]<stderr>:#015 47%|████▋     | 4948/10570 [00:12<00:15, 363.96it/s][1,9]<stderr>:#015 45%|████▌     | 4782/10570 [00:12<00:16, 345.27it/s][1,7]<stderr>:#015 48%|████▊     | 5081/10570 [00:12<00:14, 380.95it/s][1,13]<stderr>:#015 42%|████▏     | 4417/10570 [00:12<00:21, 283.24it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 44%|████▍     | 4697/10570 [00:12<00:17, 340.53it/s]#033[A[1,10]<stderr>:#015 46%|████▌     | 4888/10570 [00:12<00:16, 353.17it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|████      | 4249/10570 [00:11<00:23, 266.50it/s]#033[A[1,6]<stderr>:#015 43%|████▎     | 4542/10570 [00:12<00:18, 333.10it/s][1,15]<stderr>:#015 46%|████▌     | 4845/10570 [00:12<00:15, 370.16it/s][1,12]<stderr>:#015 47%|████▋     | 5017/10570 [00:12<00:14, 374.90it/s][1,4]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 364.34it/s][1,1]<stderr>:#015 47%|████▋     | 4919/10570 [00:12<00:15, 366.40it/s][1,14]<stderr>:#015 46%|████▌     | 4831/10570 [00:12<00:15, 368.06it/s][1,3]<stderr>:#015 44%|████▍     | 4648/10570 [00:12<00:17, 331.23it/s][1,2]<stderr>:#015 47%|████▋     | 4987/10570 [00:12<00:15, 370.07it/s][1,9]<stderr>:#015 46%|████▌     | 4823/10570 [00:12<00:15, 362.04it/s][1,11]<stderr>:#015 44%|████▍     | 4648/10570 [00:12<00:17, 331.53it/s][1,5]<stderr>:#015 45%|████▌     | 4782/10570 [00:12<00:16, 345.30it/s][1,7]<stderr>:#015 48%|████▊     | 5122/10570 [00:12<00:14, 388.59it/s][1,13]<stderr>:#015 42%|████▏     | 4456/10570 [00:12<00:19, 308.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 45%|████▍     | 4737/10570 [00:12<00:16, 355.94it/s]#033[A[1,10]<stderr>:#015 47%|████▋     | 4929/10570 [00:12<00:15, 366.31it/s][1,6]<stderr>:#015 43%|████▎     | 4578/10570 [00:12<00:17, 340.68it/s][1,12]<stderr>:#015 48%|████▊     | 5057/10570 [00:12<00:14, 381.56it/s][1,4]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 373.38it/s][1,1]<stderr>:#015 47%|████▋     | 4959/10570 [00:12<00:15, 373.83it/s][1,15]<stderr>:#015 46%|████▌     | 4883/10570 [00:12<00:16, 351.43it/s][1,3]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 336.99it/s][1,14]<stderr>:#015 46%|████▌     | 4869/10570 [00:12<00:16, 352.34it/s][1,11]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 337.53it/s][1,2]<stderr>:#015 48%|████▊     | 5025/10570 [00:12<00:15, 365.43it/s][1,5]<stderr>:#015 46%|████▌     | 4823/10570 [00:12<00:15, 362.24it/s][1,9]<stderr>:#015 46%|████▌     | 4860/10570 [00:12<00:16, 344.56it/s][1,7]<stderr>:#015 49%|████▉     | 5163/10570 [00:13<00:13, 393.26it/s][1,13]<stderr>:#015 43%|████▎     | 4494/10570 [00:12<00:18, 324.56it/s][1,10]<stderr>:#015 47%|████▋     | 4968/10570 [00:12<00:15, 372.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 40%|████      | 4278/10570 [00:11<00:29, 211.24it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 45%|████▌     | 4774/10570 [00:12<00:16, 341.86it/s]#033[A[1,6]<stderr>:#015 44%|████▎     | 4614/10570 [00:12<00:17, 334.13it/s][1,12]<stderr>:#015 48%|████▊     | 5096/10570 [00:13<00:14, 381.98it/s][1,4]<stderr>:#015 47%|████▋     | 4979/10570 [00:12<00:14, 374.87it/s][1,15]<stderr>:#015 47%|████▋     | 4920/10570 [00:12<00:15, 356.64it/s][1,1]<stderr>:#015 47%|████▋     | 4997/10570 [00:12<00:15, 371.33it/s][1,3]<stderr>:#015 45%|████▍     | 4725/10570 [00:12<00:16, 353.66it/s][1,14]<stderr>:#015 46%|████▋     | 4910/10570 [00:12<00:15, 365.82it/s][1,11]<stderr>:#015 45%|████▍     | 4725/10570 [00:12<00:16, 354.57it/s][1,2]<stderr>:#015 48%|████▊     | 5066/10570 [00:13<00:14, 375.86it/s][1,9]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 361.83it/s][1,7]<stderr>:#015 49%|████▉     | 5204/10570 [00:13<00:13, 396.56it/s][1,10]<stderr>:#015 47%|████▋     | 5006/10570 [00:12<00:15, 368.49it/s][1,5]<stderr>:#015 46%|████▌     | 4860/10570 [00:12<00:16, 344.54it/s][1,13]<stderr>:#015 43%|████▎     | 4529/10570 [00:12<00:18, 323.29it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4813/10570 [00:12<00:16, 353.06it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 44%|████▍     | 4649/10570 [00:12<00:18, 327.92it/s][1,12]<stderr>:#015 49%|████▊     | 5139/10570 [00:13<00:13, 392.21it/s][1,4]<stderr>:#015 47%|████▋     | 5017/10570 [00:12<00:15, 369.39it/s][1,15]<stderr>:#015 47%|████▋     | 4960/10570 [00:12<00:15, 366.09it/s][1,1]<stderr>:#015 48%|████▊     | 5035/10570 [00:13<00:14, 372.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████      | 4303/10570 [00:11<00:33, 187.06it/s]#033[A[1,14]<stderr>:#015 47%|████▋     | 4948/10570 [00:12<00:15, 364.37it/s][1,3]<stderr>:#015 45%|████▌     | 4761/10570 [00:12<00:16, 342.85it/s][1,2]<stderr>:#015 48%|████▊     | 5104/10570 [00:13<00:14, 370.51it/s][1,9]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 371.04it/s][1,11]<stderr>:#015 45%|████▌     | 4762/10570 [00:12<00:16, 344.74it/s][1,7]<stderr>:#015 50%|████▉     | 5244/10570 [00:13<00:13, 393.11it/s][1,5]<stderr>:#015 46%|████▋     | 4901/10570 [00:12<00:15, 361.79it/s][1,10]<stderr>:#015 48%|████▊     | 5045/10570 [00:13<00:14, 372.49it/s][1,13]<stderr>:#015 43%|████▎     | 4566/10570 [00:12<00:17, 334.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4851/10570 [00:12<00:15, 359.06it/s]#033[A[1,6]<stderr>:#015 44%|████▍     | 4684/10570 [00:12<00:17, 333.87it/s][1,12]<stderr>:#015 49%|████▉     | 5179/10570 [00:13<00:13, 391.92it/s][1,15]<stderr>:#015 47%|████▋     | 4997/10570 [00:13<00:15, 365.52it/s][1,4]<stderr>:#015 48%|████▊     | 5057/10570 [00:13<00:14, 376.04it/s][1,1]<stderr>:#015 48%|████▊     | 5075/10570 [00:13<00:14, 377.92it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████      | 4333/10570 [00:12<00:29, 210.62it/s]#033[A[1,14]<stderr>:#015 47%|████▋     | 4987/10570 [00:12<00:15, 370.62it/s][1,3]<stderr>:#015 45%|████▌     | 4796/10570 [00:12<00:16, 343.52it/s][1,2]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 379.81it/s][1,9]<stderr>:#015 47%|████▋     | 4979/10570 [00:13<00:14, 373.09it/s][1,11]<stderr>:#015 45%|████▌     | 4797/10570 [00:12<00:16, 345.81it/s][1,7]<stderr>:#015 50%|█████     | 5285/10570 [00:13<00:13, 397.61it/s][1,5]<stderr>:#015 47%|████▋     | 4941/10570 [00:12<00:15, 370.34it/s][1,10]<stderr>:#015 48%|████▊     | 5085/10570 [00:13<00:14, 377.28it/s][1,13]<stderr>:#015 44%|████▎     | 4601/10570 [00:12<00:18, 323.45it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 46%|████▌     | 4888/10570 [00:12<00:16, 349.35it/s][1,0]<stderr>:#033[A[1,6]<stderr>:#015 45%|████▍     | 4724/10570 [00:12<00:16, 350.99it/s][1,12]<stderr>:#015 49%|████▉     | 5220/10570 [00:13<00:13, 394.75it/s][1,15]<stderr>:#015 48%|████▊     | 5034/10570 [00:13<00:15, 366.30it/s][1,4]<stderr>:#015 48%|████▊     | 5095/10570 [00:13<00:14, 376.29it/s][1,1]<stderr>:#015 48%|████▊     | 5115/10570 [00:13<00:14, 384.01it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 41%|████▏     | 4370/10570 [00:12<00:25, 241.37it/s]#033[A[1,3]<stderr>:#015 46%|████▌     | 4838/10570 [00:12<00:15, 361.27it/s][1,14]<stderr>:#015 48%|████▊     | 5025/10570 [00:12<00:15, 366.17it/s][1,2]<stderr>:#015 49%|████▉     | 5185/10570 [00:13<00:14, 383.38it/s][1,11]<stderr>:#015 46%|████▌     | 4838/10570 [00:12<00:15, 362.34it/s][1,9]<stderr>:#015 47%|████▋     | 5017/10570 [00:13<00:15, 367.61it/s][1,7]<stderr>:#015 50%|█████     | 5328/10570 [00:13<00:12, 406.47it/s][1,5]<stderr>:#015 47%|████▋     | 4979/10570 [00:12<00:14, 372.74it/s][1,10]<stderr>:#015 48%|████▊     | 5126/10570 [00:13<00:14, 385.39it/s][1,13]<stderr>:#015 44%|████▍     | 4638/10570 [00:12<00:17, 335.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 4928/10570 [00:12<00:15, 361.97it/s]#033[A[1,6]<stderr>:#015 45%|████▌     | 4760/10570 [00:12<00:17, 339.66it/s][1,12]<stderr>:#015 50%|████▉     | 5260/10570 [00:13<00:13, 394.14it/s][1,4]<stderr>:#015 49%|████▊     | 5137/10570 [00:13<00:14, 387.19it/s][1,15]<stderr>:#015 48%|████▊     | 5072/10570 [00:13<00:14, 368.09it/s][1,1]<stderr>:#015 49%|████▉     | 5155/10570 [00:13<00:14, 386.02it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4406/10570 [00:12<00:23, 267.54it/s]#033[A[1,14]<stderr>:#015 48%|████▊     | 5066/10570 [00:13<00:14, 376.35it/s][1,2]<stderr>:#015 49%|████▉     | 5226/10570 [00:13<00:13, 388.99it/s][1,9]<stderr>:#015 48%|████▊     | 5056/10570 [00:13<00:14, 373.91it/s][1,7]<stderr>:#015 51%|█████     | 5369/10570 [00:13<00:12, 403.33it/s][1,3]<stderr>:#015 46%|████▌     | 4875/10570 [00:12<00:16, 341.55it/s][1,10]<stderr>:#015 49%|████▉     | 5166/10570 [00:13<00:13, 388.89it/s][1,11]<stderr>:#015 46%|████▌     | 4875/10570 [00:12<00:16, 342.68it/s][1,5]<stderr>:#015 47%|████▋     | 5017/10570 [00:13<00:15, 368.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 4967/10570 [00:13<00:15, 367.57it/s]#033[A[1,13]<stderr>:#015 44%|████▍     | 4673/10570 [00:12<00:18, 318.68it/s][1,12]<stderr>:#015 50%|█████     | 5303/10570 [00:13<00:13, 403.02it/s][1,6]<stderr>:#015 45%|████▌     | 4795/10570 [00:12<00:16, 341.03it/s][1,4]<stderr>:#015 49%|████▉     | 5176/10570 [00:13<00:13, 386.19it/s][1,15]<stderr>:#015 48%|████▊     | 5112/10570 [00:13<00:14, 374.87it/s][1,1]<stderr>:#015 49%|████▉     | 5195/10570 [00:13<00:13, 389.41it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4445/10570 [00:12<00:20, 293.86it/s]#033[A[1,2]<stderr>:#015 50%|████▉     | 5266/10570 [00:13<00:13, 389.61it/s][1,14]<stderr>:#015 48%|████▊     | 5104/10570 [00:13<00:14, 373.52it/s][1,9]<stderr>:#015 48%|████▊     | 5094/10570 [00:13<00:14, 373.96it/s][1,7]<stderr>:#015 51%|█████     | 5411/10570 [00:13<00:12, 406.37it/s][1,3]<stderr>:#015 46%|████▋     | 4911/10570 [00:13<00:16, 345.75it/s][1,10]<stderr>:#015 49%|████▉     | 5206/10570 [00:13<00:13, 391.33it/s][1,11]<stderr>:#015 46%|████▋     | 4914/10570 [00:13<00:15, 355.10it/s][1,5]<stderr>:#015 48%|████▊     | 5057/10570 [00:13<00:14, 374.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 47%|████▋     | 5005/10570 [00:13<00:15, 363.97it/s]#033[A[1,13]<stderr>:#015 45%|████▍     | 4714/10570 [00:12<00:17, 339.81it/s][1,6]<stderr>:#015 46%|████▌     | 4836/10570 [00:12<00:15, 358.99it/s][1,12]<stderr>:#015 51%|█████     | 5344/10570 [00:13<00:12, 403.69it/s][1,4]<stderr>:#015 49%|████▉     | 5216/10570 [00:13<00:13, 387.83it/s][1,1]<stderr>:#015 50%|████▉     | 5235/10570 [00:13<00:13, 392.36it/s][1,15]<stderr>:#015 49%|████▊     | 5152/10570 [00:13<00:14, 379.17it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 42%|████▏     | 4485/10570 [00:12<00:19, 317.89it/s][1,8]<stderr>:#033[A[1,2]<stderr>:#015 50%|█████     | 5309/10570 [00:13<00:13, 399.57it/s][1,14]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 381.76it/s][1,9]<stderr>:#015 49%|████▊     | 5133/10570 [00:13<00:14, 377.25it/s][1,3]<stderr>:#015 47%|████▋     | 4949/10570 [00:13<00:15, 354.95it/s][1,10]<stderr>:#015 50%|████▉     | 5246/10570 [00:13<00:13, 391.50it/s][1,11]<stderr>:#015 47%|████▋     | 4953/10570 [00:13<00:15, 362.74it/s][1,5]<stderr>:#015 48%|████▊     | 5095/10570 [00:13<00:14, 373.64it/s][1,7]<stderr>:#015 52%|█████▏    | 5452/10570 [00:13<00:13, 382.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5043/10570 [00:13<00:15, 366.97it/s]#033[A[1,13]<stderr>:#015 45%|████▍     | 4751/10570 [00:13<00:16, 347.20it/s][1,12]<stderr>:#015 51%|█████     | 5385/10570 [00:13<00:12, 402.14it/s][1,1]<stderr>:#015 50%|████▉     | 5275/10570 [00:13<00:13, 393.21it/s][1,4]<stderr>:#015 50%|████▉     | 5255/10570 [00:13<00:13, 386.33it/s][1,15]<stderr>:#015 49%|████▉     | 5192/10570 [00:13<00:13, 384.64it/s][1,6]<stderr>:#015 46%|████▌     | 4873/10570 [00:13<00:16, 340.02it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4520/10570 [00:12<00:19, 314.98it/s]#033[A[1,14]<stderr>:#015 49%|████▉     | 5185/10570 [00:13<00:13, 384.64it/s][1,2]<stderr>:#015 51%|█████     | 5350/10570 [00:13<00:13, 397.53it/s][1,9]<stderr>:#015 49%|████▉     | 5173/10570 [00:13<00:14, 381.45it/s][1,3]<stderr>:#015 47%|████▋     | 4987/10570 [00:13<00:15, 359.64it/s][1,10]<stderr>:#015 50%|█████     | 5287/10570 [00:13<00:13, 395.90it/s][1,11]<stderr>:#015 47%|████▋     | 4991/10570 [00:13<00:15, 364.67it/s][1,5]<stderr>:#015 49%|████▊     | 5136/10570 [00:13<00:14, 383.85it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5082/10570 [00:13<00:14, 372.14it/s]#033[A[1,7]<stderr>:#015 52%|█████▏    | 5491/10570 [00:13<00:13, 363.44it/s][1,1]<stderr>:#015 50%|█████     | 5317/10570 [00:13<00:13, 399.61it/s][1,4]<stderr>:#015 50%|█████     | 5297/10570 [00:13<00:13, 394.38it/s][1,12]<stderr>:#015 51%|█████▏    | 5426/10570 [00:13<00:12, 399.15it/s][1,15]<stderr>:#015 50%|████▉     | 5233/10570 [00:13<00:13, 388.85it/s][1,13]<stderr>:#015 45%|████▌     | 4787/10570 [00:13<00:17, 327.38it/s][1,6]<stderr>:#015 46%|████▋     | 4912/10570 [00:13<00:16, 351.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4554/10570 [00:12<00:18, 320.29it/s]#033[A[1,14]<stderr>:#015 49%|████▉     | 5226/10570 [00:13<00:13, 390.09it/s][1,2]<stderr>:#015 51%|█████     | 5392/10570 [00:13<00:12, 400.81it/s][1,9]<stderr>:#015 49%|████▉     | 5213/10570 [00:13<00:13, 384.28it/s][1,10]<stderr>:#015 50%|█████     | 5329/10570 [00:13<00:13, 402.53it/s][1,3]<stderr>:#015 48%|████▊     | 5024/10570 [00:13<00:15, 354.13it/s][1,5]<stderr>:#015 49%|████▉     | 5175/10570 [00:13<00:14, 384.88it/s][1,11]<stderr>:#015 48%|████▊     | 5028/10570 [00:13<00:15, 359.00it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 48%|████▊     | 5122/10570 [00:13<00:14, 378.22it/s]#033[A[1,7]<stderr>:#015 52%|█████▏    | 5528/10570 [00:14<00:13, 364.28it/s][1,4]<stderr>:#015 51%|█████     | 5339/10570 [00:13<00:13, 399.94it/s][1,1]<stderr>:#015 51%|█████     | 5358/10570 [00:13<00:13, 399.45it/s][1,15]<stderr>:#015 50%|████▉     | 5273/10570 [00:13<00:13, 389.44it/s][1,13]<stderr>:#015 46%|████▌     | 4827/10570 [00:13<00:16, 345.54it/s][1,6]<stderr>:#015 47%|████▋     | 4950/10570 [00:13<00:15, 359.43it/s][1,12]<stderr>:#015 52%|█████▏    | 5466/10570 [00:13<00:13, 374.00it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 43%|████▎     | 4588/10570 [00:12<00:18, 322.16it/s]#033[A[1,14]<stderr>:#015 50%|████▉     | 5266/10570 [00:13<00:13, 390.84it/s][1,2]<stderr>:#015 51%|█████▏    | 5433/10570 [00:13<00:12, 398.07it/s][1,9]<stderr>:#015 50%|████▉     | 5252/10570 [00:13<00:13, 383.20it/s][1,3]<stderr>:#015 48%|████▊     | 5063/10570 [00:13<00:15, 363.37it/s][1,10]<stderr>:#015 51%|█████     | 5370/10570 [00:13<00:13, 397.37it/s][1,5]<stderr>:#015 49%|████▉     | 5215/10570 [00:13<00:13, 386.40it/s][1,11]<stderr>:#015 48%|████▊     | 5067/10570 [00:13<00:14, 367.75it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 49%|████▉     | 5161/10570 [00:13<00:14, 381.66it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5566/10570 [00:14<00:13, 368.83it/s][1,1]<stderr>:#015 51%|█████     | 5399/10570 [00:13<00:12, 400.90it/s][1,15]<stderr>:#015 50%|█████     | 5315/10570 [00:13<00:13, 397.28it/s][1,4]<stderr>:#015 51%|█████     | 5380/10570 [00:13<00:13, 396.98it/s][1,6]<stderr>:#015 47%|████▋     | 4988/10570 [00:13<00:15, 362.84it/s][1,13]<stderr>:#015 46%|████▌     | 4863/10570 [00:13<00:17, 328.10it/s][1,14]<stderr>:#015 50%|█████     | 5309/10570 [00:13<00:13, 400.61it/s][1,12]<stderr>:#015 52%|█████▏    | 5504/10570 [00:14<00:13, 362.10it/s][1,9]<stderr>:#015 50%|█████     | 5294/10570 [00:13<00:13, 392.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▎     | 4622/10570 [00:12<00:18, 315.07it/s][1,8]<stderr>:#033[A[1,3]<stderr>:#015 48%|████▊     | 5100/10570 [00:13<00:15, 364.09it/s][1,10]<stderr>:#015 51%|█████     | 5411/10570 [00:13<00:12, 400.61it/s][1,5]<stderr>:#015 50%|████▉     | 5254/10570 [00:13<00:13, 385.98it/s][1,11]<stderr>:#015 48%|████▊     | 5105/10570 [00:13<00:14, 368.88it/s][1,2]<stderr>:#015 52%|█████▏    | 5473/10570 [00:14<00:13, 370.70it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 49%|████▉     | 5201/10570 [00:13<00:13, 384.58it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5605/10570 [00:14<00:13, 373.90it/s][1,15]<stderr>:#015 51%|█████     | 5355/10570 [00:13<00:13, 396.44it/s][1,4]<stderr>:#015 51%|█████▏    | 5420/10570 [00:13<00:12, 396.83it/s][1,1]<stderr>:#015 51%|█████▏    | 5440/10570 [00:14<00:12, 398.16it/s][1,6]<stderr>:#015 48%|████▊     | 5025/10570 [00:13<00:15, 355.26it/s][1,13]<stderr>:#015 46%|████▋     | 4902/10570 [00:13<00:16, 343.34it/s][1,12]<stderr>:#015 52%|█████▏    | 5541/10570 [00:14<00:13, 363.13it/s][1,14]<stderr>:#015 51%|█████     | 5350/10570 [00:13<00:13, 398.21it/s][1,9]<stderr>:#015 50%|█████     | 5336/10570 [00:13<00:13, 398.57it/s][1,3]<stderr>:#015 49%|████▊     | 5140/10570 [00:13<00:14, 373.14it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▍     | 4655/10570 [00:13<00:19, 308.00it/s]#033[A[1,5]<stderr>:#015 50%|█████     | 5296/10570 [00:13<00:13, 394.36it/s][1,11]<stderr>:#015 49%|████▊     | 5145/10570 [00:13<00:14, 375.22it/s][1,10]<stderr>:#015 52%|█████▏    | 5452/10570 [00:14<00:13, 376.80it/s][1,2]<stderr>:#015 52%|█████▏    | 5511/10570 [00:14<00:14, 355.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|████▉     | 5241/10570 [00:13<00:13, 386.85it/s]#033[A[1,7]<stderr>:#015 53%|█████▎    | 5643/10570 [00:14<00:13, 371.66it/s][1,15]<stderr>:#015 51%|█████     | 5396/10570 [00:14<00:13, 397.81it/s][1,4]<stderr>:#015 52%|█████▏    | 5460/10570 [00:14<00:13, 372.32it/s][1,6]<stderr>:#015 48%|████▊     | 5064/10570 [00:13<00:15, 363.89it/s][1,13]<stderr>:#015 47%|████▋     | 4940/10570 [00:13<00:16, 351.78it/s][1,1]<stderr>:#015 52%|█████▏    | 5480/10570 [00:14<00:13, 365.20it/s][1,14]<stderr>:#015 51%|█████     | 5391/10570 [00:13<00:12, 401.66it/s][1,12]<stderr>:#015 53%|█████▎    | 5579/10570 [00:14<00:13, 365.20it/s][1,9]<stderr>:#015 51%|█████     | 5376/10570 [00:14<00:13, 395.70it/s][1,3]<stderr>:#015 49%|████▉     | 5178/10570 [00:13<00:14, 373.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 44%|████▍     | 4690/10570 [00:13<00:18, 318.79it/s]#033[A[1,5]<stderr>:#015 51%|█████     | 5338/10570 [00:13<00:13, 399.37it/s][1,11]<stderr>:#015 49%|████▉     | 5183/10570 [00:13<00:14, 376.57it/s][1,2]<stderr>:#015 52%|█████▏    | 5548/10570 [00:14<00:14, 358.18it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|████▉     | 5281/10570 [00:13<00:13, 388.49it/s]#033[A[1,7]<stderr>:#015 54%|█████▎    | 5681/10570 [00:14<00:13, 361.31it/s][1,10]<stderr>:#015 52%|█████▏    | 5491/10570 [00:14<00:14, 358.55it/s][1,15]<stderr>:#015 51%|█████▏    | 5436/10570 [00:14<00:12, 396.05it/s][1,6]<stderr>:#015 48%|████▊     | 5101/10570 [00:13<00:15, 363.38it/s][1,13]<stderr>:#015 47%|████▋     | 4976/10570 [00:13<00:15, 352.79it/s][1,12]<stderr>:#015 53%|█████▎    | 5617/10570 [00:14<00:13, 367.40it/s][1,14]<stderr>:#015 51%|█████▏    | 5432/10570 [00:13<00:12, 397.55it/s][1,4]<stderr>:#015 52%|█████▏    | 5498/10570 [00:14<00:14, 356.13it/s][1,9]<stderr>:#015 51%|█████     | 5416/10570 [00:14<00:13, 395.10it/s][1,1]<stderr>:#015 52%|█████▏    | 5518/10570 [00:14<00:14, 353.87it/s][1,3]<stderr>:#015 49%|████▉     | 5217/10570 [00:13<00:14, 376.04it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▍     | 4728/10570 [00:13<00:17, 333.74it/s]#033[A[1,11]<stderr>:#015 49%|████▉     | 5222/10570 [00:13<00:14, 379.23it/s][1,5]<stderr>:#015 51%|█████     | 5379/10570 [00:13<00:13, 395.74it/s][1,2]<stderr>:#015 53%|█████▎    | 5586/10570 [00:14<00:13, 362.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|█████     | 5323/10570 [00:13<00:13, 395.07it/s]#033[A[1,7]<stderr>:#015 54%|█████▍    | 5723/10570 [00:14<00:12, 375.78it/s][1,10]<stderr>:#015 52%|█████▏    | 5528/10570 [00:14<00:14, 349.58it/s][1,6]<stderr>:#015 49%|████▊     | 5141/10570 [00:13<00:14, 371.29it/s][1,15]<stderr>:#015 52%|█████▏    | 5476/10570 [00:14<00:13, 367.67it/s][1,13]<stderr>:#015 47%|████▋     | 5012/10570 [00:13<00:16, 347.09it/s][1,12]<stderr>:#015 53%|█████▎    | 5654/10570 [00:14<00:13, 362.72it/s][1,4]<stderr>:#015 52%|█████▏    | 5534/10570 [00:14<00:14, 355.96it/s][1,1]<stderr>:#015 53%|█████▎    | 5555/10570 [00:14<00:13, 358.47it/s][1,3]<stderr>:#015 50%|████▉     | 5255/10570 [00:13<00:14, 377.03it/s][1,11]<stderr>:#015 50%|████▉     | 5260/10570 [00:13<00:14, 377.24it/s][1,5]<stderr>:#015 51%|█████▏    | 5419/10570 [00:14<00:13, 395.78it/s][1,14]<stderr>:#015 52%|█████▏    | 5472/10570 [00:14<00:13, 371.29it/s][1,9]<stderr>:#015 52%|█████▏    | 5456/10570 [00:14<00:13, 371.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▌     | 4762/10570 [00:13<00:18, 322.48it/s]#033[A[1,2]<stderr>:#015 53%|█████▎    | 5623/10570 [00:14<00:13, 357.72it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████     | 5363/10570 [00:14<00:13, 393.62it/s]#033[A[1,7]<stderr>:#015 55%|█████▍    | 5762/10570 [00:14<00:12, 378.67it/s][1,10]<stderr>:#015 53%|█████▎    | 5566/10570 [00:14<00:14, 356.17it/s][1,6]<stderr>:#015 49%|████▉     | 5179/10570 [00:13<00:14, 370.83it/s][1,13]<stderr>:#015 48%|████▊     | 5049/10570 [00:13<00:15, 353.49it/s][1,12]<stderr>:#015 54%|█████▍    | 5692/10570 [00:14<00:13, 365.39it/s][1,4]<stderr>:#015 53%|█████▎    | 5572/10570 [00:14<00:13, 360.90it/s][1,1]<stderr>:#015 53%|█████▎    | 5593/10570 [00:14<00:13, 363.95it/s][1,15]<stderr>:#015 52%|█████▏    | 5514/10570 [00:14<00:14, 352.32it/s][1,3]<stderr>:#015 50%|█████     | 5296/10570 [00:14<00:13, 385.27it/s][1,11]<stderr>:#015 50%|█████     | 5301/10570 [00:14<00:13, 386.40it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 45%|████▌     | 4795/10570 [00:13<00:17, 322.69it/s]#033[A[1,14]<stderr>:#015 52%|█████▏    | 5510/10570 [00:14<00:14, 356.56it/s][1,9]<stderr>:#015 52%|█████▏    | 5494/10570 [00:14<00:14, 355.68it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████     | 5403/10570 [00:14<00:13, 394.99it/s]#033[A[1,5]<stderr>:#015 52%|█████▏    | 5459/10570 [00:14<00:13, 371.39it/s][1,2]<stderr>:#015 54%|█████▎    | 5659/10570 [00:14<00:14, 346.96it/s][1,7]<stderr>:#015 55%|█████▍    | 5801/10570 [00:14<00:12, 373.65it/s][1,10]<stderr>:#015 53%|█████▎    | 5603/10570 [00:14<00:13, 358.44it/s][1,6]<stderr>:#015 49%|████▉     | 5217/10570 [00:14<00:14, 370.39it/s][1,13]<stderr>:#015 48%|████▊     | 5086/10570 [00:14<00:15, 355.17it/s][1,12]<stderr>:#015 54%|█████▍    | 5732/10570 [00:14<00:12, 373.56it/s][1,4]<stderr>:#015 53%|█████▎    | 5610/10570 [00:14<00:13, 364.81it/s][1,1]<stderr>:#015 53%|█████▎    | 5630/10570 [00:14<00:13, 362.51it/s][1,15]<stderr>:#015 53%|█████▎    | 5551/10570 [00:14<00:14, 355.62it/s][1,3]<stderr>:#015 50%|█████     | 5337/10570 [00:14<00:13, 389.70it/s][1,11]<stderr>:#015 51%|█████     | 5341/10570 [00:14<00:13, 389.34it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▌     | 4835/10570 [00:13<00:16, 340.32it/s]#033[A[1,14]<stderr>:#015 52%|█████▏    | 5547/10570 [00:14<00:13, 358.95it/s][1,9]<stderr>:#015 52%|█████▏    | 5530/10570 [00:14<00:14, 354.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 51%|█████▏    | 5443/10570 [00:14<00:13, 389.67it/s]#033[A[1,2]<stderr>:#015 54%|█████▍    | 5699/10570 [00:14<00:13, 356.23it/s][1,5]<stderr>:#015 52%|█████▏    | 5497/10570 [00:14<00:14, 355.08it/s][1,7]<stderr>:#015 55%|█████▌    | 5842/10570 [00:14<00:12, 380.04it/s][1,10]<stderr>:#015 53%|█████▎    | 5640/10570 [00:14<00:13, 357.01it/s][1,6]<stderr>:#015 50%|████▉     | 5255/10570 [00:14<00:14, 371.25it/s][1,13]<stderr>:#015 48%|████▊     | 5125/10570 [00:14<00:14, 363.64it/s][1,12]<stderr>:#015 55%|█████▍    | 5770/10570 [00:14<00:12, 373.23it/s][1,4]<stderr>:#015 53%|█████▎    | 5647/10570 [00:14<00:13, 360.59it/s][1,15]<stderr>:#015 53%|█████▎    | 5589/10570 [00:14<00:13, 360.27it/s][1,3]<stderr>:#015 51%|█████     | 5377/10570 [00:14<00:13, 385.24it/s][1,1]<stderr>:#015 54%|█████▎    | 5667/10570 [00:14<00:13, 351.38it/s][1,11]<stderr>:#015 51%|█████     | 5381/10570 [00:14<00:13, 385.88it/s][1,14]<stderr>:#015 53%|█████▎    | 5585/10570 [00:14<00:13, 362.49it/s][1,9]<stderr>:#015 53%|█████▎    | 5567/10570 [00:14<00:13, 358.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▌     | 4870/10570 [00:13<00:17, 321.20it/s]#033[A[1,2]<stderr>:#015 54%|█████▍    | 5735/10570 [00:14<00:13, 352.29it/s][1,5]<stderr>:#015 52%|█████▏    | 5533/10570 [00:14<00:14, 353.99it/s][1,7]<stderr>:#015 56%|█████▌    | 5881/10570 [00:14<00:12, 381.30it/s][1,10]<stderr>:#015 54%|█████▎    | 5676/10570 [00:14<00:13, 351.32it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 52%|█████▏    | 5483/10570 [00:14<00:14, 350.47it/s]#033[A[1,6]<stderr>:#015 50%|█████     | 5296/10570 [00:14<00:13, 379.77it/s][1,13]<stderr>:#015 49%|████▉     | 5163/10570 [00:14<00:14, 366.61it/s][1,12]<stderr>:#015 55%|█████▍    | 5808/10570 [00:14<00:12, 372.53it/s][1,15]<stderr>:#015 53%|█████▎    | 5626/10570 [00:14<00:13, 359.99it/s][1,4]<stderr>:#015 54%|█████▍    | 5684/10570 [00:14<00:13, 353.44it/s][1,3]<stderr>:#015 51%|█████     | 5416/10570 [00:14<00:13, 385.05it/s][1,1]<stderr>:#015 54%|█████▍    | 5708/10570 [00:14<00:13, 365.30it/s][1,11]<stderr>:#015 51%|█████▏    | 5420/10570 [00:14<00:13, 386.18it/s][1,14]<stderr>:#015 53%|█████▎    | 5622/10570 [00:14<00:13, 362.93it/s][1,9]<stderr>:#015 53%|█████▎    | 5605/10570 [00:14<00:13, 363.84it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 46%|████▋     | 4907/10570 [00:13<00:16, 334.38it/s]#033[A[1,2]<stderr>:#015 55%|█████▍    | 5773/10570 [00:14<00:13, 358.15it/s][1,5]<stderr>:#015 53%|█████▎    | 5571/10570 [00:14<00:13, 359.77it/s][1,7]<stderr>:#015 56%|█████▌    | 5920/10570 [00:15<00:12, 382.60it/s][1,10]<stderr>:#015 54%|█████▍    | 5718/10570 [00:14<00:13, 367.07it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 52%|█████▏    | 5519/10570 [00:14<00:14, 349.93it/s]#033[A[1,6]<stderr>:#015 50%|█████     | 5337/10570 [00:14<00:13, 386.21it/s][1,13]<stderr>:#015 49%|████▉     | 5201/10570 [00:14<00:14, 368.77it/s][1,12]<stderr>:#015 55%|█████▌    | 5847/10570 [00:15<00:12, 376.57it/s][1,4]<stderr>:#015 54%|█████▍    | 5720/10570 [00:14<00:13, 353.50it/s][1,1]<stderr>:#015 54%|█████▍    | 5747/10570 [00:14<00:13, 370.40it/s][1,15]<stderr>:#015 54%|█████▎    | 5663/10570 [00:14<00:14, 349.84it/s][1,3]<stderr>:#015 52%|█████▏    | 5455/10570 [00:14<00:14, 361.26it/s][1,9]<stderr>:#015 53%|█████▎    | 5642/10570 [00:14<00:13, 360.72it/s][1,11]<stderr>:#015 52%|█████▏    | 5459/10570 [00:14<00:14, 362.46it/s][1,14]<stderr>:#015 54%|█████▎    | 5659/10570 [00:14<00:13, 351.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 4944/10570 [00:13<00:16, 342.78it/s]#033[A[1,5]<stderr>:#015 53%|█████▎    | 5609/10570 [00:14<00:13, 364.24it/s][1,7]<stderr>:#015 56%|█████▋    | 5961/10570 [00:15<00:11, 387.57it/s][1,2]<stderr>:#015 55%|█████▍    | 5809/10570 [00:15<00:13, 354.07it/s][1,10]<stderr>:#015 54%|█████▍    | 5756/10570 [00:14<00:12, 370.48it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5556/10570 [00:14<00:14, 354.07it/s]#033[A[1,6]<stderr>:#015 51%|█████     | 5376/10570 [00:14<00:13, 383.19it/s][1,13]<stderr>:#015 50%|████▉     | 5239/10570 [00:14<00:14, 370.62it/s][1,12]<stderr>:#015 56%|█████▌    | 5887/10570 [00:15<00:12, 382.10it/s][1,4]<stderr>:#015 54%|█████▍    | 5758/10570 [00:14<00:13, 360.04it/s][1,1]<stderr>:#015 55%|█████▍    | 5785/10570 [00:14<00:12, 371.14it/s][1,15]<stderr>:#015 54%|█████▍    | 5704/10570 [00:14<00:13, 363.94it/s][1,14]<stderr>:#015 54%|█████▍    | 5700/10570 [00:14<00:13, 365.90it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 4979/10570 [00:13<00:16, 343.12it/s]#033[A[1,9]<stderr>:#015 54%|█████▎    | 5679/10570 [00:14<00:13, 350.81it/s][1,3]<stderr>:#015 52%|█████▏    | 5492/10570 [00:14<00:14, 344.68it/s][1,11]<stderr>:#015 52%|█████▏    | 5496/10570 [00:14<00:14, 346.37it/s][1,7]<stderr>:#015 57%|█████▋    | 6001/10570 [00:15<00:11, 390.66it/s][1,5]<stderr>:#015 53%|█████▎    | 5646/10570 [00:14<00:13, 361.92it/s][1,2]<stderr>:#015 55%|█████▌    | 5848/10570 [00:15<00:13, 360.98it/s][1,10]<stderr>:#015 55%|█████▍    | 5794/10570 [00:15<00:13, 366.62it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5594/10570 [00:14<00:13, 360.25it/s]#033[A[1,6]<stderr>:#015 51%|█████     | 5415/10570 [00:14<00:13, 383.16it/s][1,13]<stderr>:#015 50%|████▉     | 5277/10570 [00:14<00:14, 371.59it/s][1,12]<stderr>:#015 56%|█████▌    | 5926/10570 [00:15<00:12, 381.08it/s][1,4]<stderr>:#015 55%|█████▍    | 5795/10570 [00:15<00:13, 358.37it/s][1,15]<stderr>:#015 54%|█████▍    | 5743/10570 [00:15<00:13, 369.12it/s][1,1]<stderr>:#015 55%|█████▌    | 5823/10570 [00:15<00:13, 361.64it/s][1,14]<stderr>:#015 54%|█████▍    | 5739/10570 [00:14<00:13, 370.20it/s][1,3]<stderr>:#015 52%|█████▏    | 5527/10570 [00:14<00:14, 345.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 47%|████▋     | 5014/10570 [00:14<00:16, 336.69it/s]#033[A[1,9]<stderr>:#015 54%|█████▍    | 5715/10570 [00:15<00:13, 347.63it/s][1,2]<stderr>:#015 56%|█████▌    | 5888/10570 [00:15<00:12, 371.07it/s][1,11]<stderr>:#015 52%|█████▏    | 5532/10570 [00:14<00:14, 345.07it/s][1,7]<stderr>:#015 57%|█████▋    | 6041/10570 [00:15<00:11, 386.31it/s][1,5]<stderr>:#015 54%|█████▍    | 5683/10570 [00:14<00:13, 353.24it/s][1,10]<stderr>:#015 55%|█████▌    | 5833/10570 [00:15<00:12, 373.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 53%|█████▎    | 5631/10570 [00:14<00:13, 358.23it/s]#033[A[1,13]<stderr>:#015 50%|█████     | 5317/10570 [00:14<00:13, 377.30it/s][1,12]<stderr>:#015 56%|█████▋    | 5966/10570 [00:15<00:11, 386.31it/s][1,4]<stderr>:#015 55%|█████▌    | 5834/10570 [00:15<00:12, 366.34it/s][1,6]<stderr>:#015 52%|█████▏    | 5454/10570 [00:14<00:14, 358.88it/s][1,15]<stderr>:#015 55%|█████▍    | 5781/10570 [00:15<00:12, 370.25it/s][1,1]<stderr>:#015 55%|█████▌    | 5861/10570 [00:15<00:12, 365.06it/s][1,14]<stderr>:#015 55%|█████▍    | 5777/10570 [00:14<00:12, 371.87it/s][1,3]<stderr>:#015 53%|█████▎    | 5563/10570 [00:14<00:14, 349.35it/s][1,9]<stderr>:#015 54%|█████▍    | 5752/10570 [00:15<00:13, 353.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5050/10570 [00:14<00:16, 341.65it/s]#033[A[1,2]<stderr>:#015 56%|█████▌    | 5926/10570 [00:15<00:12, 372.52it/s][1,11]<stderr>:#015 53%|█████▎    | 5569/10570 [00:14<00:14, 350.11it/s][1,5]<stderr>:#015 54%|█████▍    | 5719/10570 [00:14<00:13, 354.38it/s][1,7]<stderr>:#015 58%|█████▊    | 6080/10570 [00:15<00:11, 379.58it/s][1,10]<stderr>:#015 56%|█████▌    | 5871/10570 [00:15<00:12, 372.61it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▎    | 5668/10570 [00:14<00:14, 347.48it/s]#033[A[1,12]<stderr>:#015 57%|█████▋    | 6005/10570 [00:15<00:11, 385.27it/s][1,13]<stderr>:#015 51%|█████     | 5355/10570 [00:14<00:13, 374.87it/s][1,4]<stderr>:#015 56%|█████▌    | 5871/10570 [00:15<00:12, 367.18it/s][1,15]<stderr>:#015 55%|█████▌    | 5819/10570 [00:15<00:12, 368.76it/s][1,1]<stderr>:#015 56%|█████▌    | 5901/10570 [00:15<00:12, 373.26it/s][1,6]<stderr>:#015 52%|█████▏    | 5491/10570 [00:14<00:14, 341.29it/s][1,3]<stderr>:#015 53%|█████▎    | 5599/10570 [00:14<00:14, 352.06it/s][1,9]<stderr>:#015 55%|█████▍    | 5789/10570 [00:15<00:13, 357.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5085/10570 [00:14<00:15, 343.57it/s]#033[A[1,14]<stderr>:#015 55%|█████▌    | 5815/10570 [00:15<00:13, 363.41it/s][1,2]<stderr>:#015 56%|█████▋    | 5966/10570 [00:15<00:12, 379.31it/s][1,11]<stderr>:#015 53%|█████▎    | 5606/10570 [00:14<00:13, 355.29it/s][1,5]<stderr>:#015 54%|█████▍    | 5757/10570 [00:15<00:13, 359.80it/s][1,10]<stderr>:#015 56%|█████▌    | 5911/10570 [00:15<00:12, 377.70it/s][1,7]<stderr>:#015 58%|█████▊    | 6119/10570 [00:15<00:12, 369.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▍    | 5708/10570 [00:15<00:13, 361.37it/s]#033[A[1,12]<stderr>:#015 57%|█████▋    | 6044/10570 [00:15<00:11, 384.98it/s][1,13]<stderr>:#015 51%|█████     | 5394/10570 [00:14<00:13, 377.00it/s][1,4]<stderr>:#015 56%|█████▌    | 5910/10570 [00:15<00:12, 373.31it/s][1,15]<stderr>:#015 55%|█████▌    | 5857/10570 [00:15<00:12, 370.73it/s][1,1]<stderr>:#015 56%|█████▌    | 5940/10570 [00:15<00:12, 375.42it/s][1,6]<stderr>:#015 52%|█████▏    | 5526/10570 [00:14<00:14, 341.68it/s][1,9]<stderr>:#015 55%|█████▌    | 5825/10570 [00:15<00:13, 355.99it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 48%|████▊     | 5123/10570 [00:14<00:15, 351.49it/s]#033[A[1,14]<stderr>:#015 55%|█████▌    | 5854/10570 [00:15<00:12, 369.46it/s][1,3]<stderr>:#015 53%|█████▎    | 5635/10570 [00:15<00:14, 348.32it/s][1,2]<stderr>:#015 57%|█████▋    | 6005/10570 [00:15<00:11, 380.89it/s][1,11]<stderr>:#015 53%|█████▎    | 5642/10570 [00:15<00:13, 352.12it/s][1,5]<stderr>:#015 55%|█████▍    | 5794/10570 [00:15<00:13, 358.21it/s][1,10]<stderr>:#015 56%|█████▋    | 5950/10570 [00:15<00:12, 380.83it/s][1,7]<stderr>:#015 58%|█████▊    | 6160/10570 [00:15<00:11, 379.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 54%|█████▍    | 5746/10570 [00:15<00:13, 366.76it/s]#033[A[1,13]<stderr>:#015 51%|█████▏    | 5432/10570 [00:14<00:13, 374.63it/s][1,12]<stderr>:#015 58%|█████▊    | 6083/10570 [00:15<00:11, 377.52it/s][1,4]<stderr>:#015 56%|█████▋    | 5949/10570 [00:15<00:12, 376.79it/s][1,15]<stderr>:#015 56%|█████▌    | 5897/10570 [00:15<00:12, 377.29it/s][1,1]<stderr>:#015 57%|█████▋    | 5980/10570 [00:15<00:12, 382.00it/s][1,6]<stderr>:#015 53%|█████▎    | 5562/10570 [00:14<00:14, 345.23it/s][1,9]<stderr>:#015 55%|█████▌    | 5862/10570 [00:15<00:13, 357.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 49%|████▉     | 5160/10570 [00:14<00:15, 355.06it/s]#033[A[1,14]<stderr>:#015 56%|█████▌    | 5894/10570 [00:15<00:12, 376.92it/s][1,2]<stderr>:#015 57%|█████▋    | 6044/10570 [00:15<00:11, 381.42it/s][1,3]<stderr>:#015 54%|█████▎    | 5670/10570 [00:15<00:14, 337.93it/s][1,5]<stderr>:#015 55%|█████▌    | 5833/10570 [00:15<00:12, 365.67it/s][1,11]<stderr>:#015 54%|█████▎    | 5678/10570 [00:15<00:14, 343.84it/s][1,10]<stderr>:#015 57%|█████▋    | 5990/10570 [00:15<00:11, 385.92it/s][1,7]<stderr>:#015 59%|█████▊    | 6200/10570 [00:15<00:11, 383.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▍    | 5783/10570 [00:15<00:13, 367.45it/s]#033[A[1,12]<stderr>:#015 58%|█████▊    | 6122/10570 [00:15<00:11, 380.30it/s][1,4]<stderr>:#015 57%|█████▋    | 5989/10570 [00:15<00:11, 381.93it/s][1,15]<stderr>:#015 56%|█████▌    | 5935/10570 [00:15<00:12, 375.79it/s][1,1]<stderr>:#015 57%|█████▋    | 6019/10570 [00:15<00:11, 381.35it/s][1,13]<stderr>:#015 52%|█████▏    | 5470/10570 [00:15<00:14, 345.47it/s][1,6]<stderr>:#015 53%|█████▎    | 5600/10570 [00:15<00:14, 353.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 49%|████▉     | 5197/10570 [00:14<00:15, 357.50it/s][1,8]<stderr>:#033[A[1,9]<stderr>:#015 56%|█████▌    | 5902/10570 [00:15<00:12, 367.00it/s][1,14]<stderr>:#015 56%|█████▌    | 5932/10570 [00:15<00:12, 375.38it/s][1,3]<stderr>:#015 54%|█████▍    | 5704/10570 [00:15<00:14, 332.08it/s][1,11]<stderr>:#015 54%|█████▍    | 5718/10570 [00:15<00:13, 358.39it/s][1,5]<stderr>:#015 56%|█████▌    | 5870/10570 [00:15<00:12, 363.48it/s][1,10]<stderr>:#015 57%|█████▋    | 6029/10570 [00:15<00:11, 385.24it/s][1,2]<stderr>:#015 58%|█████▊    | 6083/10570 [00:15<00:12, 349.82it/s][1,7]<stderr>:#015 59%|█████▉    | 6239/10570 [00:15<00:11, 367.69it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▌    | 5820/10570 [00:15<00:12, 365.87it/s]#033[A[1,12]<stderr>:#015 58%|█████▊    | 6162/10570 [00:15<00:11, 385.96it/s][1,4]<stderr>:#015 57%|█████▋    | 6028/10570 [00:15<00:11, 381.82it/s][1,15]<stderr>:#015 57%|█████▋    | 5975/10570 [00:15<00:12, 381.40it/s][1,1]<stderr>:#015 57%|█████▋    | 6058/10570 [00:15<00:12, 373.45it/s][1,6]<stderr>:#015 53%|█████▎    | 5636/10570 [00:15<00:14, 347.90it/s][1,13]<stderr>:#015 52%|█████▏    | 5506/10570 [00:15<00:15, 333.74it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|████▉     | 5234/10570 [00:14<00:14, 360.84it/s]#033[A[1,9]<stderr>:#015 56%|█████▌    | 5940/10570 [00:15<00:12, 370.08it/s][1,14]<stderr>:#015 56%|█████▋    | 5972/10570 [00:15<00:12, 381.78it/s][1,3]<stderr>:#015 54%|█████▍    | 5741/10570 [00:15<00:14, 342.22it/s][1,11]<stderr>:#015 54%|█████▍    | 5755/10570 [00:15<00:13, 360.14it/s][1,5]<stderr>:#015 56%|█████▌    | 5909/10570 [00:15<00:12, 369.72it/s][1,10]<stderr>:#015 57%|█████▋    | 6068/10570 [00:15<00:12, 373.68it/s][1,2]<stderr>:#015 58%|█████▊    | 6122/10570 [00:15<00:12, 359.52it/s][1,7]<stderr>:#015 59%|█████▉    | 6278/10570 [00:16<00:11, 373.79it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 55%|█████▌    | 5858/10570 [00:15<00:12, 367.36it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 59%|█████▊    | 6201/10570 [00:15<00:11, 386.09it/s][1,4]<stderr>:#015 57%|█████▋    | 6067/10570 [00:15<00:12, 370.56it/s][1,15]<stderr>:#015 57%|█████▋    | 6014/10570 [00:15<00:11, 380.78it/s][1,1]<stderr>:#015 58%|█████▊    | 6096/10570 [00:15<00:12, 365.13it/s][1,13]<stderr>:#015 52%|█████▏    | 5541/10570 [00:15<00:14, 336.53it/s][1,6]<stderr>:#015 54%|█████▎    | 5671/10570 [00:15<00:14, 337.54it/s][1,9]<stderr>:#015 57%|█████▋    | 5980/10570 [00:15<00:12, 377.10it/s][1,14]<stderr>:#015 57%|█████▋    | 6011/10570 [00:15<00:11, 382.53it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|████▉     | 5271/10570 [00:14<00:14, 360.45it/s]#033[A[1,3]<stderr>:#015 55%|█████▍    | 5778/10570 [00:15<00:13, 348.32it/s][1,5]<stderr>:#015 56%|█████▋    | 5948/10570 [00:15<00:12, 373.51it/s][1,11]<stderr>:#015 55%|█████▍    | 5792/10570 [00:15<00:13, 357.02it/s][1,10]<stderr>:#015 58%|█████▊    | 6106/10570 [00:15<00:11, 374.83it/s][1,2]<stderr>:#015 58%|█████▊    | 6162/10570 [00:15<00:11, 370.29it/s][1,7]<stderr>:#015 60%|█████▉    | 6320/10570 [00:16<00:11, 383.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 56%|█████▌    | 5897/10570 [00:15<00:12, 373.69it/s]#033[A[1,12]<stderr>:#015 59%|█████▉    | 6240/10570 [00:16<00:11, 370.23it/s][1,4]<stderr>:#015 58%|█████▊    | 6105/10570 [00:15<00:12, 371.42it/s][1,15]<stderr>:#015 57%|█████▋    | 6053/10570 [00:15<00:11, 377.57it/s][1,1]<stderr>:#015 58%|█████▊    | 6136/10570 [00:15<00:11, 374.71it/s][1,13]<stderr>:#015 53%|█████▎    | 5576/10570 [00:15<00:14, 340.08it/s][1,6]<stderr>:#015 54%|█████▍    | 5705/10570 [00:15<00:14, 337.64it/s][1,9]<stderr>:#015 57%|█████▋    | 6018/10570 [00:15<00:12, 377.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 50%|█████     | 5310/10570 [00:14<00:14, 368.39it/s]#033[A[1,14]<stderr>:#015 57%|█████▋    | 6050/10570 [00:15<00:11, 380.73it/s][1,5]<stderr>:#015 57%|█████▋    | 5988/10570 [00:15<00:12, 379.70it/s][1,3]<stderr>:#015 55%|█████▍    | 5813/10570 [00:15<00:13, 342.69it/s][1,11]<stderr>:#015 55%|█████▌    | 5830/10570 [00:15<00:13, 361.60it/s][1,10]<stderr>:#015 58%|█████▊    | 6147/10570 [00:15<00:11, 383.62it/s][1,2]<stderr>:#015 59%|█████▊    | 6201/10570 [00:16<00:11, 374.25it/s][1,7]<stderr>:#015 60%|██████    | 6360/10570 [00:16<00:10, 387.07it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 56%|█████▌    | 5935/10570 [00:15<00:12, 371.85it/s]#033[A[1,12]<stderr>:#015 59%|█████▉    | 6279/10570 [00:16<00:11, 374.75it/s][1,4]<stderr>:#015 58%|█████▊    | 6146/10570 [00:15<00:11, 381.26it/s][1,15]<stderr>:#015 58%|█████▊    | 6091/10570 [00:15<00:12, 370.59it/s][1,1]<stderr>:#015 58%|█████▊    | 6176/10570 [00:16<00:11, 379.43it/s][1,13]<stderr>:#015 53%|█████▎    | 5612/10570 [00:15<00:14, 344.14it/s][1,6]<stderr>:#015 54%|█████▍    | 5742/10570 [00:15<00:13, 345.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████     | 5347/10570 [00:14<00:14, 365.14it/s]#033[A[1,9]<stderr>:#015 57%|█████▋    | 6056/10570 [00:15<00:12, 368.49it/s][1,14]<stderr>:#015 58%|█████▊    | 6089/10570 [00:15<00:11, 374.56it/s][1,3]<stderr>:#015 55%|█████▌    | 5850/10570 [00:15<00:13, 349.32it/s][1,5]<stderr>:#015 57%|█████▋    | 6027/10570 [00:15<00:11, 379.05it/s][1,11]<stderr>:#015 56%|█████▌    | 5867/10570 [00:15<00:12, 361.79it/s][1,10]<stderr>:#015 59%|█████▊    | 6186/10570 [00:16<00:11, 384.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 5974/10570 [00:15<00:12, 376.71it/s]#033[A[1,2]<stderr>:#015 59%|█████▉    | 6239/10570 [00:16<00:11, 361.27it/s][1,7]<stderr>:#015 61%|██████    | 6399/10570 [00:16<00:11, 374.06it/s][1,12]<stderr>:#015 60%|█████▉    | 6319/10570 [00:16<00:11, 380.74it/s][1,4]<stderr>:#015 59%|█████▊    | 6185/10570 [00:16<00:11, 381.49it/s][1,15]<stderr>:#015 58%|█████▊    | 6130/10570 [00:16<00:11, 375.92it/s][1,1]<stderr>:#015 59%|█████▉    | 6215/10570 [00:16<00:11, 379.50it/s][1,13]<stderr>:#015 53%|█████▎    | 5647/10570 [00:15<00:14, 342.64it/s][1,6]<stderr>:#015 55%|█████▍    | 5778/10570 [00:15<00:13, 349.49it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████     | 5384/10570 [00:15<00:14, 365.24it/s]#033[A[1,9]<stderr>:#015 58%|█████▊    | 6093/10570 [00:16<00:12, 365.57it/s][1,14]<stderr>:#015 58%|█████▊    | 6129/10570 [00:15<00:11, 379.13it/s][1,3]<stderr>:#015 56%|█████▌    | 5889/10570 [00:15<00:13, 358.26it/s][1,11]<stderr>:#015 56%|█████▌    | 5906/10570 [00:15<00:12, 367.32it/s][1,5]<stderr>:#015 57%|█████▋    | 6065/10570 [00:15<00:12, 366.16it/s][1,10]<stderr>:#015 59%|█████▉    | 6225/10570 [00:16<00:11, 376.98it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 6012/10570 [00:15<00:12, 376.01it/s]#033[A[1,2]<stderr>:#015 59%|█████▉    | 6278/10570 [00:16<00:11, 367.55it/s][1,7]<stderr>:#015 61%|██████    | 6442/10570 [00:16<00:10, 387.10it/s][1,12]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:10, 383.24it/s][1,4]<stderr>:#015 59%|█████▉    | 6224/10570 [00:16<00:11, 378.37it/s][1,15]<stderr>:#015 58%|█████▊    | 6170/10570 [00:16<00:11, 381.14it/s][1,6]<stderr>:#015 55%|█████▌    | 5814/10570 [00:15<00:13, 347.97it/s][1,13]<stderr>:#015 54%|█████▍    | 5682/10570 [00:15<00:14, 334.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 51%|█████▏    | 5421/10570 [00:15<00:14, 364.06it/s]#033[A[1,9]<stderr>:#015 58%|█████▊    | 6133/10570 [00:16<00:11, 373.85it/s][1,1]<stderr>:#015 59%|█████▉    | 6254/10570 [00:16<00:11, 362.62it/s][1,14]<stderr>:#015 58%|█████▊    | 6169/10570 [00:15<00:11, 383.79it/s][1,3]<stderr>:#015 56%|█████▌    | 5925/10570 [00:15<00:12, 358.50it/s][1,11]<stderr>:#015 56%|█████▌    | 5944/10570 [00:15<00:12, 369.15it/s][1,5]<stderr>:#015 58%|█████▊    | 6103/10570 [00:15<00:12, 367.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 57%|█████▋    | 6050/10570 [00:15<00:12, 373.89it/s]#033[A[1,10]<stderr>:#015 59%|█████▉    | 6263/10570 [00:16<00:11, 366.07it/s][1,2]<stderr>:#015 60%|█████▉    | 6319/10570 [00:16<00:11, 378.49it/s][1,7]<stderr>:#015 61%|██████▏   | 6483/10570 [00:16<00:10, 393.29it/s][1,12]<stderr>:#015 61%|██████    | 6397/10570 [00:16<00:11, 370.16it/s][1,15]<stderr>:#015 59%|█████▊    | 6209/10570 [00:16<00:11, 380.16it/s][1,4]<stderr>:#015 59%|█████▉    | 6262/10570 [00:16<00:11, 363.11it/s][1,6]<stderr>:#015 55%|█████▌    | 5851/10570 [00:15<00:13, 353.82it/s][1,13]<stderr>:#015 54%|█████▍    | 5721/10570 [00:15<00:13, 348.23it/s][1,9]<stderr>:#015 58%|█████▊    | 6173/10570 [00:16<00:11, 379.32it/s][1,1]<stderr>:#015 60%|█████▉    | 6294/10570 [00:16<00:11, 370.98it/s][1,14]<stderr>:#015 59%|█████▊    | 6208/10570 [00:16<00:11, 383.13it/s][1,3]<stderr>:#015 56%|█████▋    | 5964/10570 [00:15<00:12, 365.67it/s][1,11]<stderr>:#015 57%|█████▋    | 5983/10570 [00:15<00:12, 374.16it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5458/10570 [00:15<00:15, 340.33it/s]#033[A[1,5]<stderr>:#015 58%|█████▊    | 6144/10570 [00:16<00:11, 378.33it/s][1,10]<stderr>:#015 60%|█████▉    | 6304/10570 [00:16<00:11, 375.94it/s][1,2]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:11, 380.38it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6088/10570 [00:16<00:12, 367.79it/s][1,0]<stderr>:#033[A[1,7]<stderr>:#015 62%|██████▏   | 6523/10570 [00:16<00:10, 392.21it/s][1,12]<stderr>:#015 61%|██████    | 6440/10570 [00:16<00:10, 384.96it/s][1,4]<stderr>:#015 60%|█████▉    | 6303/10570 [00:16<00:11, 374.07it/s][1,15]<stderr>:#015 59%|█████▉    | 6248/10570 [00:16<00:11, 364.81it/s][1,6]<stderr>:#015 56%|█████▌    | 5890/10570 [00:15<00:12, 361.78it/s][1,13]<stderr>:#015 54%|█████▍    | 5757/10570 [00:15<00:13, 349.68it/s][1,1]<stderr>:#015 60%|█████▉    | 6334/10570 [00:16<00:11, 376.96it/s][1,9]<stderr>:#015 59%|█████▉    | 6212/10570 [00:16<00:11, 378.47it/s][1,3]<stderr>:#015 57%|█████▋    | 6002/10570 [00:16<00:12, 368.06it/s][1,14]<stderr>:#015 59%|█████▉    | 6247/10570 [00:16<00:11, 367.95it/s][1,11]<stderr>:#015 57%|█████▋    | 6021/10570 [00:16<00:12, 372.61it/s][1,5]<stderr>:#015 58%|█████▊    | 6183/10570 [00:16<00:11, 380.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5493/10570 [00:15<00:15, 323.96it/s][1,8]<stderr>:#033[A[1,10]<stderr>:#015 60%|██████    | 6344/10570 [00:16<00:11, 380.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6127/10570 [00:16<00:11, 372.18it/s]#033[A[1,7]<stderr>:#015 62%|██████▏   | 6564/10570 [00:16<00:10, 396.03it/s][1,2]<stderr>:#015 61%|██████    | 6397/10570 [00:16<00:11, 367.52it/s][1,12]<stderr>:#015 61%|██████▏   | 6481/10570 [00:16<00:10, 390.92it/s][1,4]<stderr>:#015 60%|██████    | 6343/10570 [00:16<00:11, 379.35it/s][1,15]<stderr>:#015 59%|█████▉    | 6287/10570 [00:16<00:11, 371.25it/s][1,6]<stderr>:#015 56%|█████▌    | 5927/10570 [00:16<00:12, 360.35it/s][1,13]<stderr>:#015 55%|█████▍    | 5793/10570 [00:16<00:13, 341.58it/s][1,1]<stderr>:#015 60%|██████    | 6374/10570 [00:16<00:11, 377.92it/s][1,3]<stderr>:#015 57%|█████▋    | 6039/10570 [00:16<00:12, 366.86it/s][1,14]<stderr>:#015 59%|█████▉    | 6286/10570 [00:16<00:11, 374.03it/s][1,9]<stderr>:#015 59%|█████▉    | 6250/10570 [00:16<00:11, 362.67it/s][1,5]<stderr>:#015 59%|█████▉    | 6222/10570 [00:16<00:11, 380.34it/s][1,11]<stderr>:#015 57%|█████▋    | 6059/10570 [00:16<00:12, 362.82it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 52%|█████▏    | 5526/10570 [00:15<00:15, 323.33it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 58%|█████▊    | 6166/10570 [00:16<00:11, 376.93it/s]#033[A[1,7]<stderr>:#015 62%|██████▏   | 6604/10570 [00:16<00:10, 390.49it/s][1,10]<stderr>:#015 60%|██████    | 6383/10570 [00:16<00:11, 366.71it/s][1,2]<stderr>:#015 61%|██████    | 6440/10570 [00:16<00:10, 382.70it/s][1,12]<stderr>:#015 62%|██████▏   | 6521/10570 [00:16<00:10, 389.41it/s][1,15]<stderr>:#015 60%|█████▉    | 6326/10570 [00:16<00:11, 375.31it/s][1,6]<stderr>:#015 56%|█████▋    | 5966/10570 [00:16<00:12, 366.17it/s][1,4]<stderr>:#015 60%|██████    | 6382/10570 [00:16<00:11, 364.71it/s][1,13]<stderr>:#015 55%|█████▌    | 5830/10570 [00:16<00:13, 348.09it/s][1,1]<stderr>:#015 61%|██████    | 6412/10570 [00:16<00:11, 370.29it/s][1,14]<stderr>:#015 60%|█████▉    | 6325/10570 [00:16<00:11, 378.68it/s][1,9]<stderr>:#015 59%|█████▉    | 6289/10570 [00:16<00:11, 369.23it/s][1,3]<stderr>:#015 57%|█████▋    | 6076/10570 [00:16<00:12, 358.91it/s][1,11]<stderr>:#015 58%|█████▊    | 6096/10570 [00:16<00:12, 361.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5560/10570 [00:15<00:15, 327.84it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▊    | 6204/10570 [00:16<00:11, 376.07it/s]#033[A[1,5]<stderr>:#015 59%|█████▉    | 6261/10570 [00:16<00:12, 358.61it/s][1,7]<stderr>:#015 63%|██████▎   | 6646/10570 [00:16<00:09, 397.85it/s][1,2]<stderr>:#015 61%|██████▏   | 6481/10570 [00:16<00:10, 388.92it/s][1,10]<stderr>:#015 61%|██████    | 6425/10570 [00:16<00:10, 379.43it/s][1,12]<stderr>:#015 62%|██████▏   | 6562/10570 [00:16<00:10, 393.37it/s][1,6]<stderr>:#015 57%|█████▋    | 6003/10570 [00:16<00:12, 367.24it/s][1,15]<stderr>:#015 60%|██████    | 6364/10570 [00:16<00:11, 368.65it/s][1,4]<stderr>:#015 61%|██████    | 6423/10570 [00:16<00:10, 377.16it/s][1,13]<stderr>:#015 55%|█████▌    | 5866/10570 [00:16<00:13, 349.89it/s][1,1]<stderr>:#015 61%|██████    | 6454/10570 [00:16<00:10, 382.76it/s][1,14]<stderr>:#015 60%|██████    | 6365/10570 [00:16<00:10, 382.64it/s][1,9]<stderr>:#015 60%|█████▉    | 6328/10570 [00:16<00:11, 373.51it/s][1,3]<stderr>:#015 58%|█████▊    | 6113/10570 [00:16<00:12, 361.14it/s][1,11]<stderr>:#015 58%|█████▊    | 6135/10570 [00:16<00:12, 368.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5596/10570 [00:15<00:14, 335.76it/s]#033[A[1,7]<stderr>:#015 63%|██████▎   | 6686/10570 [00:17<00:09, 397.92it/s][1,5]<stderr>:#015 60%|█████▉    | 6302/10570 [00:16<00:11, 370.29it/s][1,10]<stderr>:#015 61%|██████    | 6466/10570 [00:16<00:10, 388.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▉    | 6242/10570 [00:16<00:12, 360.32it/s]#033[A[1,2]<stderr>:#015 62%|██████▏   | 6521/10570 [00:16<00:10, 386.74it/s][1,12]<stderr>:#015 62%|██████▏   | 6602/10570 [00:16<00:10, 387.01it/s][1,6]<stderr>:#015 57%|█████▋    | 6040/10570 [00:16<00:12, 365.85it/s][1,4]<stderr>:#015 61%|██████    | 6466/10570 [00:16<00:10, 389.52it/s][1,15]<stderr>:#015 61%|██████    | 6401/10570 [00:16<00:11, 358.97it/s][1,13]<stderr>:#015 56%|█████▌    | 5904/10570 [00:16<00:13, 356.26it/s][1,1]<stderr>:#015 61%|██████▏   | 6494/10570 [00:16<00:10, 386.38it/s][1,9]<stderr>:#015 60%|██████    | 6368/10570 [00:16<00:11, 379.41it/s][1,3]<stderr>:#015 58%|█████▊    | 6152/10570 [00:16<00:11, 368.73it/s][1,14]<stderr>:#015 61%|██████    | 6404/10570 [00:16<00:11, 370.46it/s][1,11]<stderr>:#015 58%|█████▊    | 6174/10570 [00:16<00:11, 372.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 53%|█████▎    | 5630/10570 [00:15<00:14, 331.86it/s]#033[A[1,5]<stderr>:#015 60%|█████▉    | 6341/10570 [00:16<00:11, 375.67it/s][1,7]<stderr>:#015 64%|██████▎   | 6726/10570 [00:17<00:09, 388.16it/s][1,10]<stderr>:#015 62%|██████▏   | 6506/10570 [00:16<00:10, 387.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 59%|█████▉    | 6280/10570 [00:16<00:11, 365.41it/s]#033[A[1,2]<stderr>:#015 62%|██████▏   | 6561/10570 [00:17<00:10, 390.58it/s][1,12]<stderr>:#015 63%|██████▎   | 6644/10570 [00:17<00:09, 395.52it/s][1,4]<stderr>:#015 62%|██████▏   | 6506/10570 [00:16<00:10, 388.21it/s][1,6]<stderr>:#015 57%|█████▋    | 6077/10570 [00:16<00:12, 358.73it/s][1,15]<stderr>:#015 61%|██████    | 6443/10570 [00:16<00:11, 374.69it/s][1,13]<stderr>:#015 56%|█████▌    | 5941/10570 [00:16<00:12, 358.03it/s][1,1]<stderr>:#015 62%|██████▏   | 6533/10570 [00:16<00:10, 386.67it/s][1,3]<stderr>:#015 59%|█████▊    | 6190/10570 [00:16<00:11, 370.83it/s][1,14]<stderr>:#015 61%|██████    | 6446/10570 [00:16<00:10, 383.49it/s][1,9]<stderr>:#015 61%|██████    | 6407/10570 [00:16<00:11, 367.18it/s][1,11]<stderr>:#015 59%|█████▉    | 6212/10570 [00:16<00:11, 370.92it/s][1,7]<stderr>:#015 64%|██████▍   | 6768/10570 [00:17<00:09, 396.19it/s][1,10]<stderr>:#015 62%|██████▏   | 6546/10570 [00:17<00:10, 390.93it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▎    | 5664/10570 [00:15<00:15, 319.56it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 60%|█████▉    | 6320/10570 [00:16<00:11, 372.95it/s]#033[A[1,5]<stderr>:#015 60%|██████    | 6379/10570 [00:16<00:11, 361.01it/s][1,2]<stderr>:#015 62%|██████▏   | 6601/10570 [00:17<00:10, 384.53it/s][1,12]<stderr>:#015 63%|██████▎   | 6684/10570 [00:17<00:09, 396.64it/s][1,4]<stderr>:#015 62%|██████▏   | 6546/10570 [00:17<00:10, 390.30it/s][1,6]<stderr>:#015 58%|█████▊    | 6114/10570 [00:16<00:12, 360.41it/s][1,15]<stderr>:#015 61%|██████▏   | 6483/10570 [00:16<00:10, 381.31it/s][1,13]<stderr>:#015 57%|█████▋    | 5979/10570 [00:16<00:12, 363.34it/s][1,1]<stderr>:#015 62%|██████▏   | 6573/10570 [00:17<00:10, 387.35it/s][1,14]<stderr>:#015 61%|██████▏   | 6486/10570 [00:16<00:10, 388.15it/s][1,9]<stderr>:#015 61%|██████    | 6449/10570 [00:16<00:10, 379.48it/s][1,3]<stderr>:#015 59%|█████▉    | 6228/10570 [00:16<00:12, 352.64it/s][1,11]<stderr>:#015 59%|█████▉    | 6250/10570 [00:16<00:12, 354.44it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▍    | 5702/10570 [00:16<00:14, 334.87it/s][1,8]<stderr>:#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 60%|██████    | 6358/10570 [00:16<00:11, 374.42it/s]#033[A[1,7]<stderr>:#015 64%|██████▍   | 6808/10570 [00:17<00:09, 391.28it/s][1,10]<stderr>:#015 62%|██████▏   | 6586/10570 [00:17<00:10, 387.74it/s][1,5]<stderr>:#015 61%|██████    | 6420/10570 [00:16<00:11, 374.13it/s][1,2]<stderr>:#015 63%|██████▎   | 6643/10570 [00:17<00:09, 393.02it/s][1,12]<stderr>:#015 64%|██████▎   | 6724/10570 [00:17<00:09, 384.71it/s][1,6]<stderr>:#015 58%|█████▊    | 6153/10570 [00:16<00:12, 366.85it/s][1,4]<stderr>:#015 62%|██████▏   | 6586/10570 [00:17<00:10, 386.63it/s][1,15]<stderr>:#015 62%|██████▏   | 6522/10570 [00:17<00:10, 374.99it/s][1,13]<stderr>:#015 57%|█████▋    | 6016/10570 [00:16<00:12, 361.91it/s][1,1]<stderr>:#015 63%|██████▎   | 6612/10570 [00:17<00:10, 382.29it/s][1,14]<stderr>:#015 62%|██████▏   | 6525/10570 [00:16<00:10, 385.82it/s][1,9]<stderr>:#015 61%|██████▏   | 6489/10570 [00:17<00:10, 384.44it/s][1,3]<stderr>:#015 59%|█████▉    | 6265/10570 [00:16<00:12, 356.55it/s][1,11]<stderr>:#015 59%|█████▉    | 6289/10570 [00:16<00:11, 361.46it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 54%|█████▍    | 5738/10570 [00:16<00:14, 339.35it/s]#033[A[1,10]<stderr>:#015 63%|██████▎   | 6625/10570 [00:17<00:10, 387.99it/s][1,5]<stderr>:#015 61%|██████    | 6462/10570 [00:16<00:10, 385.56it/s][1,2]<stderr>:#015 63%|██████▎   | 6683/10570 [00:17<00:09, 394.99it/s][1,7]<stderr>:#015 65%|██████▍   | 6848/10570 [00:17<00:09, 382.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████    | 6396/10570 [00:16<00:11, 360.93it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 64%|██████▍   | 6766/10570 [00:17<00:09, 392.92it/s][1,6]<stderr>:#015 59%|█████▊    | 6191/10570 [00:16<00:11, 369.24it/s][1,4]<stderr>:#015 63%|██████▎   | 6625/10570 [00:17<00:10, 386.44it/s][1,15]<stderr>:#015 62%|██████▏   | 6562/10570 [00:17<00:10, 380.94it/s][1,13]<stderr>:#015 57%|█████▋    | 6053/10570 [00:16<00:12, 358.37it/s][1,1]<stderr>:#015 63%|██████▎   | 6651/10570 [00:17<00:10, 384.17it/s][1,14]<stderr>:#015 62%|██████▏   | 6566/10570 [00:17<00:10, 390.28it/s][1,9]<stderr>:#015 62%|██████▏   | 6528/10570 [00:17<00:10, 383.02it/s][1,3]<stderr>:#015 60%|█████▉    | 6305/10570 [00:16<00:11, 366.35it/s][1,11]<stderr>:#015 60%|█████▉    | 6327/10570 [00:16<00:11, 365.45it/s][1,10]<stderr>:#015 63%|██████▎   | 6665/10570 [00:17<00:09, 390.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▍    | 5773/10570 [00:16<00:14, 339.53it/s]#033[A[1,5]<stderr>:#015 62%|██████▏   | 6501/10570 [00:16<00:10, 385.37it/s][1,7]<stderr>:#015 65%|██████▌   | 6887/10570 [00:17<00:09, 381.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████    | 6436/10570 [00:16<00:11, 369.96it/s]#033[A[1,2]<stderr>:#015 64%|██████▎   | 6723/10570 [00:17<00:10, 381.64it/s][1,12]<stderr>:#015 64%|██████▍   | 6806/10570 [00:17<00:09, 388.54it/s][1,4]<stderr>:#015 63%|██████▎   | 6666/10570 [00:17<00:09, 390.79it/s][1,15]<stderr>:#015 62%|██████▏   | 6601/10570 [00:17<00:10, 376.40it/s][1,13]<stderr>:#015 58%|█████▊    | 6089/10570 [00:16<00:12, 352.17it/s][1,6]<stderr>:#015 59%|█████▉    | 6228/10570 [00:16<00:12, 349.95it/s][1,1]<stderr>:#015 63%|██████▎   | 6690/10570 [00:17<00:10, 382.88it/s][1,9]<stderr>:#015 62%|██████▏   | 6568/10570 [00:17<00:10, 386.19it/s][1,14]<stderr>:#015 62%|██████▏   | 6606/10570 [00:17<00:10, 383.46it/s][1,3]<stderr>:#015 60%|██████    | 6343/10570 [00:17<00:11, 369.40it/s][1,11]<stderr>:#015 60%|██████    | 6366/10570 [00:17<00:11, 371.20it/s][1,5]<stderr>:#015 62%|██████▏   | 6541/10570 [00:17<00:10, 387.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▍    | 5808/10570 [00:16<00:14, 338.53it/s]#033[A[1,10]<stderr>:#015 63%|██████▎   | 6705/10570 [00:17<00:10, 383.95it/s][1,7]<stderr>:#015 66%|██████▌   | 6927/10570 [00:17<00:09, 385.64it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 61%|██████▏   | 6476/10570 [00:17<00:10, 378.37it/s]#033[A[1,2]<stderr>:#015 64%|██████▍   | 6764/10570 [00:17<00:09, 389.65it/s][1,12]<stderr>:#015 65%|██████▍   | 6845/10570 [00:17<00:09, 380.58it/s][1,4]<stderr>:#015 63%|██████▎   | 6706/10570 [00:17<00:10, 382.80it/s][1,15]<stderr>:#015 63%|██████▎   | 6642/10570 [00:17<00:10, 385.55it/s][1,13]<stderr>:#015 58%|█████▊    | 6126/10570 [00:16<00:12, 356.78it/s][1,6]<stderr>:#015 59%|█████▉    | 6265/10570 [00:16<00:12, 353.57it/s][1,1]<stderr>:#015 64%|██████▎   | 6729/10570 [00:17<00:10, 378.06it/s][1,14]<stderr>:#015 63%|██████▎   | 6646/10570 [00:17<00:10, 387.47it/s][1,9]<stderr>:#015 63%|██████▎   | 6607/10570 [00:17<00:10, 377.74it/s][1,3]<stderr>:#015 60%|██████    | 6381/10570 [00:17<00:11, 353.85it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 55%|█████▌    | 5844/10570 [00:16<00:13, 344.69it/s]#033[A[1,5]<stderr>:#015 62%|██████▏   | 6580/10570 [00:17<00:10, 386.34it/s][1,11]<stderr>:#015 61%|██████    | 6404/10570 [00:17<00:11, 358.21it/s][1,10]<stderr>:#015 64%|██████▍   | 6744/10570 [00:17<00:09, 382.94it/s][1,7]<stderr>:#015 66%|██████▌   | 6969/10570 [00:17<00:09, 394.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6515/10570 [00:17<00:10, 380.00it/s]#033[A[1,2]<stderr>:#015 64%|██████▍   | 6804/10570 [00:17<00:09, 386.46it/s][1,12]<stderr>:#015 65%|██████▌   | 6884/10570 [00:17<00:09, 380.28it/s][1,15]<stderr>:#015 63%|██████▎   | 6682/10570 [00:17<00:10, 388.49it/s][1,4]<stderr>:#015 64%|██████▍   | 6745/10570 [00:17<00:09, 382.55it/s][1,13]<stderr>:#015 58%|█████▊    | 6164/10570 [00:17<00:12, 361.22it/s][1,6]<stderr>:#015 60%|█████▉    | 6304/10570 [00:17<00:11, 363.34it/s][1,1]<stderr>:#015 64%|██████▍   | 6770/10570 [00:17<00:09, 386.62it/s][1,14]<stderr>:#015 63%|██████▎   | 6686/10570 [00:17<00:10, 388.13it/s][1,9]<stderr>:#015 63%|██████▎   | 6649/10570 [00:17<00:10, 386.80it/s][1,3]<stderr>:#015 61%|██████    | 6421/10570 [00:17<00:11, 364.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▌    | 5879/10570 [00:16<00:13, 345.70it/s]#033[A[1,5]<stderr>:#015 63%|██████▎   | 6619/10570 [00:17<00:10, 381.44it/s][1,11]<stderr>:#015 61%|██████    | 6445/10570 [00:17<00:11, 370.87it/s][1,10]<stderr>:#015 64%|██████▍   | 6784/10570 [00:17<00:09, 386.25it/s][1,7]<stderr>:#015 66%|██████▋   | 7010/10570 [00:17<00:08, 398.88it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6554/10570 [00:17<00:10, 382.82it/s]#033[A[1,2]<stderr>:#015 65%|██████▍   | 6843/10570 [00:17<00:09, 375.65it/s][1,12]<stderr>:#015 65%|██████▌   | 6923/10570 [00:17<00:09, 382.07it/s][1,4]<stderr>:#015 64%|██████▍   | 6784/10570 [00:17<00:09, 384.09it/s][1,6]<stderr>:#015 60%|██████    | 6342/10570 [00:17<00:11, 367.04it/s][1,13]<stderr>:#015 59%|█████▊    | 6201/10570 [00:17<00:12, 360.72it/s][1,15]<stderr>:#015 64%|██████▎   | 6721/10570 [00:17<00:10, 376.61it/s][1,1]<stderr>:#015 64%|██████▍   | 6809/10570 [00:17<00:09, 381.48it/s][1,9]<stderr>:#015 63%|██████▎   | 6688/10570 [00:17<00:10, 385.04it/s][1,14]<stderr>:#015 64%|██████▎   | 6725/10570 [00:17<00:10, 379.46it/s][1,3]<stderr>:#015 61%|██████    | 6462/10570 [00:17<00:10, 377.02it/s][1,11]<stderr>:#015 61%|██████▏   | 6484/10570 [00:17<00:10, 376.00it/s][1,5]<stderr>:#015 63%|██████▎   | 6658/10570 [00:17<00:10, 382.55it/s][1,7]<stderr>:#015 67%|██████▋   | 7052/10570 [00:17<00:08, 403.83it/s][1,10]<stderr>:#015 65%|██████▍   | 6823/10570 [00:17<00:09, 382.68it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▌    | 5914/10570 [00:16<00:14, 332.51it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 62%|██████▏   | 6593/10570 [00:17<00:10, 377.54it/s]#033[A[1,2]<stderr>:#015 65%|██████▌   | 6881/10570 [00:17<00:09, 375.75it/s][1,12]<stderr>:#015 66%|██████▌   | 6965/10570 [00:17<00:09, 391.81it/s][1,4]<stderr>:#015 65%|██████▍   | 6823/10570 [00:17<00:09, 380.13it/s][1,15]<stderr>:#015 64%|██████▍   | 6762/10570 [00:17<00:09, 384.29it/s][1,1]<stderr>:#015 65%|██████▍   | 6848/10570 [00:17<00:09, 375.16it/s][1,6]<stderr>:#015 60%|██████    | 6379/10570 [00:17<00:11, 350.07it/s][1,13]<stderr>:#015 59%|█████▉    | 6238/10570 [00:17<00:12, 345.32it/s][1,14]<stderr>:#015 64%|██████▍   | 6766/10570 [00:17<00:09, 387.30it/s][1,9]<stderr>:#015 64%|██████▎   | 6727/10570 [00:17<00:10, 377.53it/s][1,3]<stderr>:#015 61%|██████▏   | 6500/10570 [00:17<00:10, 376.50it/s][1,11]<stderr>:#015 62%|██████▏   | 6522/10570 [00:17<00:10, 373.72it/s][1,7]<stderr>:#015 67%|██████▋   | 7093/10570 [00:18<00:08, 403.98it/s][1,5]<stderr>:#015 63%|██████▎   | 6697/10570 [00:17<00:10, 378.28it/s][1,10]<stderr>:#015 65%|██████▍   | 6862/10570 [00:17<00:09, 379.69it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 63%|██████▎   | 6633/10570 [00:17<00:10, 383.37it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 56%|█████▋    | 5950/10570 [00:16<00:13, 338.53it/s]#033[A[1,2]<stderr>:#015 65%|██████▌   | 6919/10570 [00:17<00:09, 366.37it/s][1,12]<stderr>:#015 66%|██████▋   | 7007/10570 [00:18<00:08, 397.64it/s][1,4]<stderr>:#015 65%|██████▍   | 6862/10570 [00:17<00:09, 377.05it/s][1,15]<stderr>:#015 64%|██████▍   | 6801/10570 [00:17<00:09, 378.88it/s][1,6]<stderr>:#015 61%|██████    | 6419/10570 [00:17<00:11, 362.56it/s][1,1]<stderr>:#015 65%|██████▌   | 6886/10570 [00:17<00:09, 374.04it/s][1,13]<stderr>:#015 59%|█████▉    | 6274/10570 [00:17<00:12, 349.15it/s][1,14]<stderr>:#015 64%|██████▍   | 6805/10570 [00:17<00:09, 383.87it/s][1,9]<stderr>:#015 64%|██████▍   | 6768/10570 [00:17<00:09, 384.77it/s][1,3]<stderr>:#015 62%|██████▏   | 6539/10570 [00:17<00:10, 378.44it/s][1,11]<stderr>:#015 62%|██████▏   | 6561/10570 [00:17<00:10, 377.35it/s][1,7]<stderr>:#015 68%|██████▊   | 7135/10570 [00:18<00:08, 407.25it/s][1,5]<stderr>:#015 64%|██████▎   | 6735/10570 [00:17<00:10, 377.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 5987/10570 [00:16<00:13, 347.19it/s]#033[A[1,10]<stderr>:#015 65%|██████▌   | 6900/10570 [00:17<00:09, 376.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 63%|██████▎   | 6673/10570 [00:17<00:10, 385.64it/s]#033[A[1,2]<stderr>:#015 66%|██████▌   | 6958/10570 [00:18<00:09, 372.72it/s][1,12]<stderr>:#015 67%|██████▋   | 7049/10570 [00:18<00:08, 402.15it/s][1,4]<stderr>:#015 65%|██████▌   | 6900/10570 [00:17<00:10, 366.75it/s][1,6]<stderr>:#015 61%|██████    | 6460/10570 [00:17<00:10, 375.34it/s][1,15]<stderr>:#015 65%|██████▍   | 6840/10570 [00:17<00:10, 371.68it/s][1,1]<stderr>:#015 66%|██████▌   | 6925/10570 [00:18<00:09, 376.47it/s][1,13]<stderr>:#015 60%|█████▉    | 6313/10570 [00:17<00:11, 358.47it/s][1,14]<stderr>:#015 65%|██████▍   | 6844/10570 [00:17<00:09, 376.59it/s][1,9]<stderr>:#015 64%|██████▍   | 6807/10570 [00:17<00:09, 379.67it/s][1,3]<stderr>:#015 62%|██████▏   | 6577/10570 [00:17<00:10, 377.58it/s][1,7]<stderr>:#015 68%|██████▊   | 7176/10570 [00:18<00:08, 407.72it/s][1,5]<stderr>:#015 64%|██████▍   | 6776/10570 [00:17<00:09, 384.09it/s][1,11]<stderr>:#015 62%|██████▏   | 6599/10570 [00:17<00:10, 370.89it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 6022/10570 [00:16<00:13, 346.81it/s]#033[A[1,10]<stderr>:#015 66%|██████▌   | 6941/10570 [00:18<00:09, 384.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▎   | 6712/10570 [00:17<00:10, 373.29it/s][1,0]<stderr>:#033[A[1,2]<stderr>:#015 66%|██████▌   | 7000/10570 [00:18<00:09, 384.74it/s][1,12]<stderr>:#015 67%|██████▋   | 7090/10570 [00:18<00:08, 400.48it/s][1,4]<stderr>:#015 66%|██████▌   | 6939/10570 [00:18<00:09, 371.84it/s][1,6]<stderr>:#015 61%|██████▏   | 6498/10570 [00:17<00:10, 374.89it/s][1,1]<stderr>:#015 66%|██████▌   | 6966/10570 [00:18<00:09, 385.56it/s][1,15]<stderr>:#015 65%|██████▌   | 6878/10570 [00:18<00:09, 371.83it/s][1,13]<stderr>:#015 60%|██████    | 6350/10570 [00:17<00:11, 359.99it/s][1,14]<stderr>:#015 65%|██████▌   | 6882/10570 [00:17<00:09, 376.64it/s][1,3]<stderr>:#015 63%|██████▎   | 6615/10570 [00:17<00:10, 372.55it/s][1,9]<stderr>:#015 65%|██████▍   | 6846/10570 [00:18<00:09, 373.04it/s][1,11]<stderr>:#015 63%|██████▎   | 6639/10570 [00:17<00:10, 378.81it/s][1,10]<stderr>:#015 66%|██████▌   | 6982/10570 [00:18<00:09, 390.49it/s][1,7]<stderr>:#015 68%|██████▊   | 7217/10570 [00:18<00:08, 395.10it/s][1,5]<stderr>:#015 64%|██████▍   | 6815/10570 [00:17<00:09, 378.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 57%|█████▋    | 6057/10570 [00:17<00:13, 339.25it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▍   | 6752/10570 [00:17<00:10, 379.49it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7041/10570 [00:18<00:09, 390.97it/s][1,12]<stderr>:#015 67%|██████▋   | 7131/10570 [00:18<00:08, 393.69it/s][1,4]<stderr>:#015 66%|██████▌   | 6980/10570 [00:18<00:09, 380.51it/s][1,6]<stderr>:#015 62%|██████▏   | 6536/10570 [00:17<00:10, 375.66it/s][1,1]<stderr>:#015 66%|██████▋   | 7007/10570 [00:18<00:09, 391.66it/s][1,15]<stderr>:#015 65%|██████▌   | 6916/10570 [00:18<00:09, 372.46it/s][1,13]<stderr>:#015 60%|██████    | 6387/10570 [00:17<00:12, 347.46it/s][1,14]<stderr>:#015 65%|██████▌   | 6920/10570 [00:17<00:09, 374.05it/s][1,3]<stderr>:#015 63%|██████▎   | 6655/10570 [00:17<00:10, 378.71it/s][1,9]<stderr>:#015 65%|██████▌   | 6884/10570 [00:18<00:09, 372.48it/s][1,11]<stderr>:#015 63%|██████▎   | 6678/10570 [00:17<00:10, 380.26it/s][1,10]<stderr>:#015 66%|██████▋   | 7024/10570 [00:18<00:08, 398.42it/s][1,7]<stderr>:#015 69%|██████▊   | 7257/10570 [00:18<00:08, 393.16it/s][1,5]<stderr>:#015 65%|██████▍   | 6853/10570 [00:17<00:09, 374.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6092/10570 [00:17<00:13, 337.67it/s][1,8]<stderr>:#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 64%|██████▍   | 6792/10570 [00:17<00:09, 383.07it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7081/10570 [00:18<00:08, 391.43it/s][1,12]<stderr>:#015 68%|██████▊   | 7172/10570 [00:18<00:08, 397.89it/s][1,4]<stderr>:#015 66%|██████▋   | 7022/10570 [00:18<00:09, 390.21it/s][1,1]<stderr>:#015 67%|██████▋   | 7048/10570 [00:18<00:08, 396.73it/s][1,6]<stderr>:#015 62%|██████▏   | 6574/10570 [00:17<00:10, 374.55it/s][1,15]<stderr>:#015 66%|██████▌   | 6956/10570 [00:18<00:09, 379.99it/s][1,13]<stderr>:#015 61%|██████    | 6426/10570 [00:17<00:11, 358.13it/s][1,14]<stderr>:#015 66%|██████▌   | 6960/10570 [00:18<00:09, 380.83it/s][1,3]<stderr>:#015 63%|██████▎   | 6693/10570 [00:17<00:10, 373.51it/s][1,9]<stderr>:#015 65%|██████▌   | 6922/10570 [00:18<00:10, 364.41it/s][1,10]<stderr>:#015 67%|██████▋   | 7065/10570 [00:18<00:08, 399.30it/s][1,7]<stderr>:#015 69%|██████▉   | 7297/10570 [00:18<00:08, 393.70it/s][1,5]<stderr>:#015 65%|██████▌   | 6891/10570 [00:18<00:09, 371.97it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6128/10570 [00:17<00:12, 344.03it/s]#033[A[1,11]<stderr>:#015 64%|██████▎   | 6717/10570 [00:17<00:10, 366.11it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▍   | 6831/10570 [00:18<00:10, 373.61it/s]#033[A[1,2]<stderr>:#015 67%|██████▋   | 7124/10570 [00:18<00:08, 399.38it/s][1,12]<stderr>:#015 68%|██████▊   | 7212/10570 [00:18<00:08, 392.81it/s][1,4]<stderr>:#015 67%|██████▋   | 7062/10570 [00:18<00:08, 392.94it/s][1,1]<stderr>:#015 67%|██████▋   | 7088/10570 [00:18<00:08, 395.14it/s][1,15]<stderr>:#015 66%|██████▌   | 6997/10570 [00:18<00:09, 388.50it/s][1,6]<stderr>:#015 63%|██████▎   | 6612/10570 [00:17<00:10, 369.20it/s][1,13]<stderr>:#015 61%|██████    | 6466/10570 [00:17<00:11, 369.23it/s][1,14]<stderr>:#015 66%|██████▌   | 7002/10570 [00:18<00:09, 389.33it/s][1,3]<stderr>:#015 64%|██████▎   | 6731/10570 [00:18<00:10, 369.86it/s][1,9]<stderr>:#015 66%|██████▌   | 6961/10570 [00:18<00:09, 371.45it/s][1,10]<stderr>:#015 67%|██████▋   | 7105/10570 [00:18<00:08, 398.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 58%|█████▊    | 6165/10570 [00:17<00:12, 349.81it/s]#033[A[1,5]<stderr>:#015 66%|██████▌   | 6929/10570 [00:18<00:09, 369.46it/s][1,11]<stderr>:#015 64%|██████▍   | 6757/10570 [00:18<00:10, 373.93it/s][1,2]<stderr>:#015 68%|██████▊   | 7165/10570 [00:18<00:08, 401.11it/s][1,7]<stderr>:#015 69%|██████▉   | 7337/10570 [00:18<00:08, 371.92it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▍   | 6869/10570 [00:18<00:10, 366.52it/s]#033[A[1,12]<stderr>:#015 69%|██████▊   | 7252/10570 [00:18<00:08, 390.80it/s][1,4]<stderr>:#015 67%|██████▋   | 7102/10570 [00:18<00:08, 394.36it/s][1,15]<stderr>:#015 67%|██████▋   | 7038/10570 [00:18<00:08, 394.48it/s][1,1]<stderr>:#015 67%|██████▋   | 7129/10570 [00:18<00:08, 398.73it/s][1,6]<stderr>:#015 63%|██████▎   | 6652/10570 [00:17<00:10, 376.44it/s][1,13]<stderr>:#015 62%|██████▏   | 6504/10570 [00:17<00:11, 367.80it/s][1,14]<stderr>:#015 67%|██████▋   | 7043/10570 [00:18<00:08, 394.69it/s][1,3]<stderr>:#015 64%|██████▍   | 6771/10570 [00:18<00:10, 377.36it/s][1,9]<stderr>:#015 66%|██████▌   | 7002/10570 [00:18<00:09, 380.52it/s][1,10]<stderr>:#015 68%|██████▊   | 7147/10570 [00:18<00:08, 403.96it/s][1,5]<stderr>:#015 66%|██████▌   | 6969/10570 [00:18<00:09, 376.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▊    | 6201/10570 [00:17<00:12, 348.82it/s]#033[A[1,11]<stderr>:#015 64%|██████▍   | 6796/10570 [00:18<00:10, 375.50it/s][1,7]<stderr>:#015 70%|██████▉   | 7375/10570 [00:18<00:08, 370.40it/s][1,2]<stderr>:#015 68%|██████▊   | 7206/10570 [00:18<00:08, 394.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 65%|██████▌   | 6906/10570 [00:18<00:10, 363.83it/s]#033[A[1,12]<stderr>:#015 69%|██████▉   | 7292/10570 [00:18<00:08, 390.51it/s][1,4]<stderr>:#015 68%|██████▊   | 7144/10570 [00:18<00:08, 399.85it/s][1,1]<stderr>:#015 68%|██████▊   | 7170/10570 [00:18<00:08, 401.13it/s][1,15]<stderr>:#015 67%|██████▋   | 7078/10570 [00:18<00:08, 391.82it/s][1,6]<stderr>:#015 63%|██████▎   | 6690/10570 [00:18<00:10, 373.00it/s][1,13]<stderr>:#015 62%|██████▏   | 6542/10570 [00:18<00:10, 369.99it/s][1,14]<stderr>:#015 67%|██████▋   | 7083/10570 [00:18<00:08, 393.06it/s][1,9]<stderr>:#015 67%|██████▋   | 7043/10570 [00:18<00:09, 387.40it/s][1,3]<stderr>:#015 64%|██████▍   | 6809/10570 [00:18<00:10, 371.19it/s][1,10]<stderr>:#015 68%|██████▊   | 7188/10570 [00:18<00:08, 401.41it/s][1,5]<stderr>:#015 66%|██████▋   | 7010/10570 [00:18<00:09, 384.45it/s][1,11]<stderr>:#015 65%|██████▍   | 6834/10570 [00:18<00:10, 366.63it/s][1,7]<stderr>:#015 70%|███████   | 7414/10570 [00:18<00:08, 375.76it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▉    | 6236/10570 [00:17<00:13, 332.36it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▌   | 6947/10570 [00:18<00:09, 375.86it/s]#033[A[1,2]<stderr>:#015 69%|██████▊   | 7246/10570 [00:18<00:08, 388.51it/s][1,12]<stderr>:#015 69%|██████▉   | 7332/10570 [00:18<00:08, 386.63it/s][1,4]<stderr>:#015 68%|██████▊   | 7185/10570 [00:18<00:08, 395.46it/s][1,15]<stderr>:#015 67%|██████▋   | 7120/10570 [00:18<00:08, 397.46it/s][1,1]<stderr>:#015 68%|██████▊   | 7211/10570 [00:18<00:08, 393.35it/s][1,6]<stderr>:#015 64%|██████▎   | 6728/10570 [00:18<00:10, 366.24it/s][1,14]<stderr>:#015 67%|██████▋   | 7125/10570 [00:18<00:08, 399.81it/s][1,13]<stderr>:#015 62%|██████▏   | 6580/10570 [00:18<00:10, 367.54it/s][1,9]<stderr>:#015 67%|██████▋   | 7082/10570 [00:18<00:09, 387.06it/s][1,3]<stderr>:#015 65%|██████▍   | 6847/10570 [00:18<00:10, 364.25it/s][1,10]<stderr>:#015 68%|██████▊   | 7229/10570 [00:18<00:08, 392.46it/s][1,5]<stderr>:#015 67%|██████▋   | 7051/10570 [00:18<00:09, 390.26it/s][1,11]<stderr>:#015 65%|██████▌   | 6871/10570 [00:18<00:10, 364.37it/s][1,7]<stderr>:#015 71%|███████   | 7455/10570 [00:19<00:08, 382.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 59%|█████▉    | 6271/10570 [00:17<00:12, 336.11it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▌   | 6987/10570 [00:18<00:09, 382.64it/s]#033[A[1,2]<stderr>:#015 69%|██████▉   | 7285/10570 [00:18<00:08, 387.52it/s][1,4]<stderr>:#015 68%|██████▊   | 7225/10570 [00:18<00:08, 390.74it/s][1,15]<stderr>:#015 68%|██████▊   | 7161/10570 [00:18<00:08, 399.59it/s][1,12]<stderr>:#015 70%|██████▉   | 7371/10570 [00:18<00:08, 363.14it/s][1,1]<stderr>:#015 69%|██████▊   | 7251/10570 [00:18<00:08, 388.89it/s][1,6]<stderr>:#015 64%|██████▍   | 6768/10570 [00:18<00:10, 373.97it/s][1,14]<stderr>:#015 68%|██████▊   | 7166/10570 [00:18<00:08, 401.67it/s][1,13]<stderr>:#015 63%|██████▎   | 6617/10570 [00:18<00:10, 363.29it/s][1,9]<stderr>:#015 67%|██████▋   | 7124/10570 [00:18<00:08, 395.01it/s][1,3]<stderr>:#015 65%|██████▌   | 6884/10570 [00:18<00:10, 362.77it/s][1,5]<stderr>:#015 67%|██████▋   | 7091/10570 [00:18<00:08, 391.01it/s][1,10]<stderr>:#015 69%|██████▉   | 7269/10570 [00:18<00:08, 388.76it/s][1,7]<stderr>:#015 71%|███████   | 7495/10570 [00:19<00:07, 386.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|█████▉    | 6309/10570 [00:17<00:12, 347.28it/s]#033[A[1,11]<stderr>:#015 65%|██████▌   | 6908/10570 [00:18<00:10, 360.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 66%|██████▋   | 7027/10570 [00:18<00:09, 386.27it/s]#033[A[1,2]<stderr>:#015 69%|██████▉   | 7325/10570 [00:18<00:08, 389.72it/s][1,12]<stderr>:#015 70%|███████   | 7410/10570 [00:19<00:08, 368.90it/s][1,4]<stderr>:#015 69%|██████▊   | 7265/10570 [00:18<00:08, 384.63it/s][1,15]<stderr>:#015 68%|██████▊   | 7202/10570 [00:18<00:08, 392.81it/s][1,1]<stderr>:#015 69%|██████▉   | 7290/10570 [00:18<00:08, 387.96it/s][1,6]<stderr>:#015 64%|██████▍   | 6806/10570 [00:18<00:10, 368.71it/s][1,13]<stderr>:#015 63%|██████▎   | 6656/10570 [00:18<00:10, 369.58it/s][1,14]<stderr>:#015 68%|██████▊   | 7207/10570 [00:18<00:08, 394.69it/s][1,9]<stderr>:#015 68%|██████▊   | 7165/10570 [00:18<00:08, 396.99it/s][1,5]<stderr>:#015 67%|██████▋   | 7132/10570 [00:18<00:08, 394.40it/s][1,10]<stderr>:#015 69%|██████▉   | 7308/10570 [00:18<00:08, 382.07it/s][1,3]<stderr>:#015 65%|██████▌   | 6921/10570 [00:18<00:10, 350.08it/s][1,11]<stderr>:#015 66%|██████▌   | 6949/10570 [00:18<00:09, 373.56it/s][1,7]<stderr>:#015 71%|███████▏  | 7537/10570 [00:19<00:07, 393.89it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|██████    | 6345/10570 [00:17<00:12, 349.68it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 67%|██████▋   | 7067/10570 [00:18<00:09, 388.22it/s]#033[A[1,2]<stderr>:#015 70%|██████▉   | 7365/10570 [00:19<00:08, 361.12it/s][1,12]<stderr>:#015 70%|███████   | 7451/10570 [00:19<00:08, 378.09it/s][1,4]<stderr>:#015 69%|██████▉   | 7305/10570 [00:18<00:08, 387.63it/s][1,1]<stderr>:#015 69%|██████▉   | 7329/10570 [00:19<00:08, 387.65it/s][1,15]<stderr>:#015 69%|██████▊   | 7242/10570 [00:18<00:08, 386.97it/s][1,6]<stderr>:#015 65%|██████▍   | 6843/10570 [00:18<00:10, 359.77it/s][1,13]<stderr>:#015 63%|██████▎   | 6694/10570 [00:18<00:10, 364.88it/s][1,14]<stderr>:#015 69%|██████▊   | 7247/10570 [00:18<00:08, 388.97it/s][1,9]<stderr>:#015 68%|██████▊   | 7205/10570 [00:18<00:08, 391.06it/s][1,5]<stderr>:#015 68%|██████▊   | 7173/10570 [00:18<00:08, 396.46it/s][1,3]<stderr>:#015 66%|██████▌   | 6959/10570 [00:18<00:10, 357.91it/s][1,11]<stderr>:#015 66%|██████▌   | 6989/10570 [00:18<00:09, 379.97it/s][1,7]<stderr>:#015 72%|███████▏  | 7579/10570 [00:19<00:07, 399.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 67%|██████▋   | 7107/10570 [00:18<00:08, 389.33it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 60%|██████    | 6381/10570 [00:18<00:12, 333.21it/s]#033[A[1,10]<stderr>:#015 70%|██████▉   | 7347/10570 [00:19<00:09, 349.57it/s][1,2]<stderr>:#015 70%|███████   | 7403/10570 [00:19<00:08, 364.07it/s][1,12]<stderr>:#015 71%|███████   | 7491/10570 [00:19<00:08, 382.00it/s][1,15]<stderr>:#015 69%|██████▉   | 7281/10570 [00:19<00:08, 385.40it/s][1,6]<stderr>:#015 65%|██████▌   | 6880/10570 [00:18<00:10, 359.58it/s][1,14]<stderr>:#015 69%|██████▉   | 7286/10570 [00:18<00:08, 387.77it/s][1,1]<stderr>:#015 70%|██████▉   | 7368/10570 [00:19<00:08, 358.61it/s][1,13]<stderr>:#015 64%|██████▎   | 6731/10570 [00:18<00:10, 359.78it/s][1,4]<stderr>:#015 69%|██████▉   | 7344/10570 [00:19<00:09, 351.96it/s][1,9]<stderr>:#015 69%|██████▊   | 7245/10570 [00:19<00:08, 383.96it/s][1,3]<stderr>:#015 66%|██████▌   | 6999/10570 [00:18<00:09, 369.56it/s][1,5]<stderr>:#015 68%|██████▊   | 7213/10570 [00:18<00:08, 390.49it/s][1,7]<stderr>:#015 72%|███████▏  | 7620/10570 [00:19<00:07, 401.32it/s][1,11]<stderr>:#015 67%|██████▋   | 7030/10570 [00:18<00:09, 386.23it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7148/10570 [00:18<00:08, 394.35it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████    | 6419/10570 [00:18<00:12, 344.96it/s]#033[A[1,10]<stderr>:#015 70%|██████▉   | 7387/10570 [00:19<00:08, 361.68it/s][1,2]<stderr>:#015 70%|███████   | 7444/10570 [00:19<00:08, 373.98it/s][1,12]<stderr>:#015 71%|███████▏  | 7533/10570 [00:19<00:07, 390.44it/s][1,15]<stderr>:#015 69%|██████▉   | 7321/10570 [00:19<00:08, 387.94it/s][1,6]<stderr>:#015 65%|██████▌   | 6917/10570 [00:18<00:10, 360.55it/s][1,14]<stderr>:#015 69%|██████▉   | 7325/10570 [00:18<00:08, 384.89it/s][1,13]<stderr>:#015 64%|██████▍   | 6768/10570 [00:18<00:10, 362.30it/s][1,1]<stderr>:#015 70%|███████   | 7405/10570 [00:19<00:08, 361.16it/s][1,4]<stderr>:#015 70%|██████▉   | 7384/10570 [00:19<00:08, 364.80it/s][1,9]<stderr>:#015 69%|██████▉   | 7284/10570 [00:19<00:08, 383.56it/s][1,3]<stderr>:#015 67%|██████▋   | 7039/10570 [00:18<00:09, 377.43it/s][1,5]<stderr>:#015 69%|██████▊   | 7253/10570 [00:18<00:08, 385.51it/s][1,11]<stderr>:#015 67%|██████▋   | 7069/10570 [00:18<00:09, 386.12it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7188/10570 [00:18<00:08, 392.79it/s]#033[A[1,7]<stderr>:#015 72%|███████▏  | 7661/10570 [00:19<00:07, 386.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████    | 6459/10570 [00:18<00:11, 357.55it/s]#033[A[1,10]<stderr>:#015 70%|███████   | 7428/10570 [00:19<00:08, 374.91it/s][1,2]<stderr>:#015 71%|███████   | 7485/10570 [00:19<00:08, 381.46it/s][1,12]<stderr>:#015 72%|███████▏  | 7573/10570 [00:19<00:07, 392.65it/s][1,6]<stderr>:#015 66%|██████▌   | 6957/10570 [00:18<00:09, 370.48it/s][1,1]<stderr>:#015 70%|███████   | 7445/10570 [00:19<00:08, 370.65it/s][1,4]<stderr>:#015 70%|███████   | 7424/10570 [00:19<00:08, 374.17it/s][1,15]<stderr>:#015 70%|██████▉   | 7360/10570 [00:19<00:09, 354.51it/s][1,13]<stderr>:#015 64%|██████▍   | 6805/10570 [00:18<00:10, 351.17it/s][1,9]<stderr>:#015 69%|██████▉   | 7323/10570 [00:19<00:08, 378.71it/s][1,3]<stderr>:#015 67%|██████▋   | 7077/10570 [00:18<00:09, 377.13it/s][1,14]<stderr>:#015 70%|██████▉   | 7364/10570 [00:19<00:08, 357.10it/s][1,5]<stderr>:#015 69%|██████▉   | 7292/10570 [00:19<00:08, 384.52it/s][1,11]<stderr>:#015 67%|██████▋   | 7108/10570 [00:18<00:08, 385.39it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 68%|██████▊   | 7228/10570 [00:19<00:08, 384.97it/s]#033[A[1,7]<stderr>:#015 73%|███████▎  | 7703/10570 [00:19<00:07, 394.87it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 61%|██████▏   | 6496/10570 [00:18<00:11, 357.55it/s]#033[A[1,10]<stderr>:#015 71%|███████   | 7466/10570 [00:19<00:08, 374.88it/s][1,2]<stderr>:#015 71%|███████   | 7526/10570 [00:19<00:07, 388.58it/s][1,12]<stderr>:#015 72%|███████▏  | 7615/10570 [00:19<00:07, 398.35it/s][1,6]<stderr>:#015 66%|██████▌   | 6998/10570 [00:18<00:09, 379.12it/s][1,1]<stderr>:#015 71%|███████   | 7485/10570 [00:19<00:08, 378.00it/s][1,4]<stderr>:#015 71%|███████   | 7462/10570 [00:19<00:08, 374.36it/s][1,15]<stderr>:#015 70%|██████▉   | 7398/10570 [00:19<00:08, 360.36it/s][1,13]<stderr>:#015 65%|██████▍   | 6841/10570 [00:18<00:10, 346.14it/s][1,3]<stderr>:#015 67%|██████▋   | 7118/10570 [00:19<00:08, 384.36it/s][1,14]<stderr>:#015 70%|███████   | 7402/10570 [00:19<00:08, 362.11it/s][1,5]<stderr>:#015 69%|██████▉   | 7331/10570 [00:19<00:08, 383.34it/s][1,11]<stderr>:#015 68%|██████▊   | 7149/10570 [00:19<00:08, 390.41it/s][1,9]<stderr>:#015 70%|██████▉   | 7361/10570 [00:19<00:09, 350.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7267/10570 [00:19<00:08, 380.42it/s]#033[A[1,7]<stderr>:#015 73%|███████▎  | 7744/10570 [00:19<00:07, 394.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6532/10570 [00:18<00:11, 357.06it/s]#033[A[1,10]<stderr>:#015 71%|███████   | 7507/10570 [00:19<00:07, 383.15it/s][1,2]<stderr>:#015 72%|███████▏  | 7566/10570 [00:19<00:07, 391.54it/s][1,12]<stderr>:#015 72%|███████▏  | 7655/10570 [00:19<00:07, 392.91it/s][1,6]<stderr>:#015 67%|██████▋   | 7038/10570 [00:19<00:09, 383.63it/s][1,1]<stderr>:#015 71%|███████   | 7526/10570 [00:19<00:07, 385.19it/s][1,4]<stderr>:#015 71%|███████   | 7502/10570 [00:19<00:08, 381.53it/s][1,15]<stderr>:#015 70%|███████   | 7438/10570 [00:19<00:08, 370.56it/s][1,13]<stderr>:#015 65%|██████▌   | 6877/10570 [00:19<00:10, 348.69it/s][1,3]<stderr>:#015 68%|██████▊   | 7158/10570 [00:19<00:08, 387.10it/s][1,14]<stderr>:#015 70%|███████   | 7442/10570 [00:19<00:08, 371.99it/s][1,11]<stderr>:#015 68%|██████▊   | 7189/10570 [00:19<00:08, 387.49it/s][1,9]<stderr>:#015 70%|███████   | 7399/10570 [00:19<00:08, 356.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7307/10570 [00:19<00:08, 383.93it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6569/10570 [00:18<00:11, 358.13it/s]#033[A[1,10]<stderr>:#015 71%|███████▏  | 7548/10570 [00:19<00:07, 389.13it/s][1,5]<stderr>:#015 70%|██████▉   | 7370/10570 [00:19<00:09, 354.89it/s][1,7]<stderr>:#015 74%|███████▎  | 7784/10570 [00:19<00:07, 372.69it/s][1,2]<stderr>:#015 72%|███████▏  | 7606/10570 [00:19<00:07, 393.81it/s][1,12]<stderr>:#015 73%|███████▎  | 7695/10570 [00:19<00:07, 389.11it/s][1,6]<stderr>:#015 67%|██████▋   | 7077/10570 [00:19<00:09, 380.48it/s][1,1]<stderr>:#015 72%|███████▏  | 7566/10570 [00:19<00:07, 387.65it/s][1,4]<stderr>:#015 71%|███████▏  | 7542/10570 [00:19<00:07, 386.57it/s][1,15]<stderr>:#015 71%|███████   | 7478/10570 [00:19<00:08, 376.53it/s][1,13]<stderr>:#015 65%|██████▌   | 6912/10570 [00:19<00:10, 348.21it/s][1,14]<stderr>:#015 71%|███████   | 7482/10570 [00:19<00:08, 379.66it/s][1,3]<stderr>:#015 68%|██████▊   | 7197/10570 [00:19<00:08, 380.93it/s][1,11]<stderr>:#015 68%|██████▊   | 7228/10570 [00:19<00:08, 378.77it/s][1,9]<stderr>:#015 70%|███████   | 7438/10570 [00:19<00:08, 366.16it/s][1,10]<stderr>:#015 72%|███████▏  | 7589/10570 [00:19<00:07, 394.74it/s][1,5]<stderr>:#015 70%|███████   | 7408/10570 [00:19<00:08, 359.78it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 62%|██████▏   | 6605/10570 [00:18<00:11, 349.85it/s]#033[A[1,7]<stderr>:#015 74%|███████▍  | 7823/10570 [00:19<00:07, 376.34it/s][1,2]<stderr>:#015 72%|███████▏  | 7646/10570 [00:19<00:07, 381.09it/s][1,12]<stderr>:#015 73%|███████▎  | 7736/10570 [00:19<00:07, 392.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 69%|██████▉   | 7346/10570 [00:19<00:09, 347.18it/s]#033[A[1,6]<stderr>:#015 67%|██████▋   | 7117/10570 [00:19<00:08, 385.73it/s][1,1]<stderr>:#015 72%|███████▏  | 7607/10570 [00:19<00:07, 393.35it/s][1,4]<stderr>:#015 72%|███████▏  | 7583/10570 [00:19<00:07, 392.45it/s][1,15]<stderr>:#015 71%|███████   | 7518/10570 [00:19<00:07, 382.68it/s][1,13]<stderr>:#015 66%|██████▌   | 6952/10570 [00:19<00:10, 360.90it/s][1,14]<stderr>:#015 71%|███████   | 7522/10570 [00:19<00:07, 384.63it/s][1,3]<stderr>:#015 68%|██████▊   | 7236/10570 [00:19<00:08, 373.60it/s][1,9]<stderr>:#015 71%|███████   | 7477/10570 [00:19<00:08, 371.33it/s][1,11]<stderr>:#015 69%|██████▊   | 7266/10570 [00:19<00:08, 374.12it/s][1,10]<stderr>:#015 72%|███████▏  | 7630/10570 [00:19<00:07, 398.78it/s][1,5]<stderr>:#015 70%|███████   | 7448/10570 [00:19<00:08, 369.90it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 63%|██████▎   | 6644/10570 [00:18<00:10, 358.71it/s]#033[A[1,7]<stderr>:#015 74%|███████▍  | 7864/10570 [00:20<00:07, 383.86it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 70%|██████▉   | 7386/10570 [00:19<00:08, 358.80it/s]#033[A[1,2]<stderr>:#015 73%|███████▎  | 7685/10570 [00:19<00:07, 373.37it/s][1,6]<stderr>:#015 68%|██████▊   | 7157/10570 [00:19<00:08, 387.50it/s][1,12]<stderr>:#015 74%|███████▎  | 7776/10570 [00:20<00:07, 371.07it/s][1,4]<stderr>:#015 72%|███████▏  | 7623/10570 [00:19<00:07, 393.85it/s][1,1]<stderr>:#015 72%|███████▏  | 7647/10570 [00:19<00:07, 385.09it/s][1,15]<stderr>:#015 72%|███████▏  | 7558/10570 [00:19<00:07, 385.54it/s][1,13]<stderr>:#015 66%|██████▌   | 6991/10570 [00:19<00:09, 368.67it/s][1,14]<stderr>:#015 72%|███████▏  | 7561/10570 [00:19<00:07, 384.86it/s][1,3]<stderr>:#015 69%|██████▉   | 7274/10570 [00:19<00:08, 373.28it/s][1,9]<stderr>:#015 71%|███████   | 7517/10570 [00:19<00:08, 378.75it/s][1,11]<stderr>:#015 69%|██████▉   | 7305/10570 [00:19<00:08, 376.68it/s][1,5]<stderr>:#015 71%|███████   | 7487/10570 [00:19<00:08, 375.58it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 63%|██████▎   | 6681/10570 [00:18<00:10, 360.09it/s]#033[A[1,7]<stderr>:#015 75%|███████▍  | 7904/10570 [00:20<00:06, 386.81it/s][1,10]<stderr>:#015 73%|███████▎  | 7671/10570 [00:19<00:07, 383.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 70%|███████   | 7427/10570 [00:19<00:08, 370.78it/s]#033[A[1,2]<stderr>:#015 73%|███████▎  | 7725/10570 [00:20<00:07, 379.32it/s][1,12]<stderr>:#015 74%|███████▍  | 7814/10570 [00:20<00:07, 373.38it/s][1,6]<stderr>:#015 68%|██████▊   | 7196/10570 [00:19<00:08, 381.94it/s][1,15]<stderr>:#015 72%|███████▏  | 7599/10570 [00:19<00:07, 391.64it/s][1,13]<stderr>:#015 67%|██████▋   | 7030/10570 [00:19<00:09, 374.51it/s][1,1]<stderr>:#015 73%|███████▎  | 7686/10570 [00:19<00:07, 369.75it/s][1,4]<stderr>:#015 72%|███████▏  | 7663/10570 [00:19<00:07, 370.01it/s][1,14]<stderr>:#015 72%|███████▏  | 7603/10570 [00:19<00:07, 392.25it/s][1,3]<stderr>:#015 69%|██████▉   | 7312/10570 [00:19<00:08, 369.99it/s][1,9]<stderr>:#015 71%|███████▏  | 7556/10570 [00:19<00:07, 377.95it/s][1,5]<stderr>:#015 71%|███████   | 7528/10570 [00:19<00:07, 383.31it/s][1,7]<stderr>:#015 75%|███████▌  | 7946/10570 [00:20<00:06, 394.63it/s][1,10]<stderr>:#015 73%|███████▎  | 7712/10570 [00:20<00:07, 388.22it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▎   | 6718/10570 [00:18<00:11, 346.46it/s]#033[A[1,11]<stderr>:#015 69%|██████▉   | 7343/10570 [00:19<00:09, 341.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████   | 7465/10570 [00:19<00:08, 370.43it/s]#033[A[1,12]<stderr>:#015 74%|███████▍  | 7854/10570 [00:20<00:07, 378.59it/s][1,2]<stderr>:#015 73%|███████▎  | 7764/10570 [00:20<00:07, 365.40it/s][1,6]<stderr>:#015 68%|██████▊   | 7235/10570 [00:19<00:08, 373.67it/s][1,15]<stderr>:#015 72%|███████▏  | 7639/10570 [00:20<00:07, 392.82it/s][1,13]<stderr>:#015 67%|██████▋   | 7068/10570 [00:19<00:09, 374.90it/s][1,1]<stderr>:#015 73%|███████▎  | 7726/10570 [00:20<00:07, 376.22it/s][1,4]<stderr>:#015 73%|███████▎  | 7704/10570 [00:20<00:07, 380.08it/s][1,14]<stderr>:#015 72%|███████▏  | 7643/10570 [00:19<00:07, 390.07it/s][1,9]<stderr>:#015 72%|███████▏  | 7597/10570 [00:19<00:07, 385.83it/s][1,5]<stderr>:#015 72%|███████▏  | 7567/10570 [00:19<00:07, 382.51it/s][1,7]<stderr>:#015 76%|███████▌  | 7986/10570 [00:20<00:06, 392.17it/s][1,3]<stderr>:#015 70%|██████▉   | 7350/10570 [00:19<00:09, 337.07it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▍   | 6756/10570 [00:19<00:10, 353.87it/s]#033[A[1,10]<stderr>:#015 73%|███████▎  | 7751/10570 [00:20<00:07, 376.48it/s][1,11]<stderr>:#015 70%|██████▉   | 7382/10570 [00:19<00:09, 354.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████   | 7505/10570 [00:19<00:08, 377.68it/s]#033[A[1,12]<stderr>:#015 75%|███████▍  | 7894/10570 [00:20<00:06, 384.58it/s][1,2]<stderr>:#015 74%|███████▍  | 7802/10570 [00:20<00:07, 368.27it/s][1,6]<stderr>:#015 69%|██████▉   | 7273/10570 [00:19<00:08, 373.11it/s][1,13]<stderr>:#015 67%|██████▋   | 7106/10570 [00:19<00:09, 374.84it/s][1,15]<stderr>:#015 73%|███████▎  | 7679/10570 [00:20<00:07, 380.08it/s][1,4]<stderr>:#015 73%|███████▎  | 7744/10570 [00:20<00:07, 382.13it/s][1,1]<stderr>:#015 73%|███████▎  | 7764/10570 [00:20<00:07, 361.40it/s][1,14]<stderr>:#015 73%|███████▎  | 7683/10570 [00:19<00:07, 380.11it/s][1,9]<stderr>:#015 72%|███████▏  | 7637/10570 [00:20<00:07, 388.18it/s][1,5]<stderr>:#015 72%|███████▏  | 7608/10570 [00:19<00:07, 388.38it/s][1,7]<stderr>:#015 76%|███████▌  | 8026/10570 [00:20<00:06, 391.54it/s][1,3]<stderr>:#015 70%|██████▉   | 7388/10570 [00:19<00:09, 347.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 64%|██████▍   | 6792/10570 [00:19<00:10, 352.04it/s]#033[A[1,11]<stderr>:#015 70%|███████   | 7422/10570 [00:19<00:08, 364.57it/s][1,10]<stderr>:#015 74%|███████▎  | 7789/10570 [00:20<00:07, 373.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 71%|███████▏  | 7545/10570 [00:19<00:07, 383.26it/s]#033[A[1,12]<stderr>:#015 75%|███████▌  | 7935/10570 [00:20<00:06, 391.72it/s][1,2]<stderr>:#015 74%|███████▍  | 7840/10570 [00:20<00:07, 370.19it/s][1,6]<stderr>:#015 69%|██████▉   | 7311/10570 [00:19<00:08, 374.67it/s][1,13]<stderr>:#015 68%|██████▊   | 7146/10570 [00:19<00:09, 380.20it/s][1,15]<stderr>:#015 73%|███████▎  | 7718/10570 [00:20<00:07, 382.03it/s][1,1]<stderr>:#015 74%|███████▍  | 7802/10570 [00:20<00:07, 365.39it/s][1,14]<stderr>:#015 73%|███████▎  | 7723/10570 [00:20<00:07, 383.65it/s][1,4]<stderr>:#015 74%|███████▎  | 7783/10570 [00:20<00:07, 363.24it/s][1,5]<stderr>:#015 72%|███████▏  | 7647/10570 [00:20<00:07, 377.95it/s][1,9]<stderr>:#015 73%|███████▎  | 7676/10570 [00:20<00:07, 363.00it/s][1,3]<stderr>:#015 70%|███████   | 7428/10570 [00:19<00:08, 361.83it/s][1,7]<stderr>:#015 76%|███████▋  | 8066/10570 [00:20<00:06, 386.23it/s][1,10]<stderr>:#015 74%|███████▍  | 7827/10570 [00:20<00:07, 374.66it/s][1,11]<stderr>:#015 71%|███████   | 7459/10570 [00:19<00:08, 364.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 72%|███████▏  | 7586/10570 [00:19<00:07, 388.24it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▍   | 6828/10570 [00:19<00:10, 342.00it/s]#033[A[1,12]<stderr>:#015 75%|███████▌  | 7975/10570 [00:20<00:06, 389.47it/s][1,2]<stderr>:#015 75%|███████▍  | 7880/10570 [00:20<00:07, 377.99it/s][1,13]<stderr>:#015 68%|██████▊   | 7185/10570 [00:19<00:09, 374.60it/s][1,15]<stderr>:#015 73%|███████▎  | 7757/10570 [00:20<00:07, 369.58it/s][1,1]<stderr>:#015 74%|███████▍  | 7840/10570 [00:20<00:07, 368.17it/s][1,4]<stderr>:#015 74%|███████▍  | 7822/10570 [00:20<00:07, 367.98it/s][1,6]<stderr>:#015 70%|██████▉   | 7349/10570 [00:19<00:09, 338.24it/s][1,14]<stderr>:#015 73%|███████▎  | 7762/10570 [00:20<00:07, 368.77it/s][1,9]<stderr>:#015 73%|███████▎  | 7716/10570 [00:20<00:07, 370.99it/s][1,3]<stderr>:#015 71%|███████   | 7465/10570 [00:20<00:08, 360.95it/s][1,5]<stderr>:#015 73%|███████▎  | 7685/10570 [00:20<00:07, 369.44it/s][1,7]<stderr>:#015 77%|███████▋  | 8109/10570 [00:20<00:06, 395.96it/s][1,11]<stderr>:#015 71%|███████   | 7497/10570 [00:20<00:08, 368.85it/s][1,10]<stderr>:#015 74%|███████▍  | 7867/10570 [00:20<00:07, 380.83it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 72%|███████▏  | 7627/10570 [00:20<00:07, 391.41it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▍   | 6863/10570 [00:19<00:10, 339.99it/s]#033[A[1,2]<stderr>:#015 75%|███████▍  | 7921/10570 [00:20<00:06, 386.66it/s][1,12]<stderr>:#015 76%|███████▌  | 8015/10570 [00:20<00:06, 390.05it/s][1,13]<stderr>:#015 68%|██████▊   | 7223/10570 [00:19<00:09, 370.18it/s][1,1]<stderr>:#015 75%|███████▍  | 7880/10570 [00:20<00:07, 376.12it/s][1,15]<stderr>:#015 74%|███████▎  | 7795/10570 [00:20<00:07, 369.34it/s][1,4]<stderr>:#015 74%|███████▍  | 7862/10570 [00:20<00:07, 375.35it/s][1,6]<stderr>:#015 70%|██████▉   | 7387/10570 [00:19<00:09, 348.46it/s][1,14]<stderr>:#015 74%|███████▍  | 7800/10570 [00:20<00:07, 370.09it/s][1,5]<stderr>:#015 73%|███████▎  | 7724/10570 [00:20<00:07, 375.20it/s][1,3]<stderr>:#015 71%|███████   | 7505/10570 [00:20<00:08, 369.72it/s][1,9]<stderr>:#015 73%|███████▎  | 7754/10570 [00:20<00:07, 363.45it/s][1,10]<stderr>:#015 75%|███████▍  | 7906/10570 [00:20<00:06, 382.90it/s][1,11]<stderr>:#015 71%|███████▏  | 7537/10570 [00:20<00:08, 376.14it/s][1,7]<stderr>:#015 77%|███████▋  | 8149/10570 [00:20<00:06, 388.45it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 65%|██████▌   | 6898/10570 [00:19<00:10, 339.62it/s][1,8]<stderr>:#033[A[1,2]<stderr>:#015 75%|███████▌  | 7962/10570 [00:20<00:06, 391.42it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7667/10570 [00:20<00:07, 375.89it/s]#033[A[1,12]<stderr>:#015 76%|███████▌  | 8055/10570 [00:20<00:06, 382.46it/s][1,13]<stderr>:#015 69%|██████▊   | 7261/10570 [00:20<00:09, 364.83it/s][1,1]<stderr>:#015 75%|███████▍  | 7921/10570 [00:20<00:06, 384.41it/s][1,15]<stderr>:#015 74%|███████▍  | 7833/10570 [00:20<00:07, 368.29it/s][1,4]<stderr>:#015 75%|███████▍  | 7901/10570 [00:20<00:07, 378.75it/s][1,6]<stderr>:#015 70%|███████   | 7427/10570 [00:20<00:08, 360.82it/s][1,14]<stderr>:#015 74%|███████▍  | 7838/10570 [00:20<00:07, 371.30it/s][1,3]<stderr>:#015 71%|███████▏  | 7545/10570 [00:20<00:08, 375.92it/s][1,10]<stderr>:#015 75%|███████▌  | 7947/10570 [00:20<00:06, 390.47it/s][1,9]<stderr>:#015 74%|███████▎  | 7791/10570 [00:20<00:07, 362.55it/s][1,11]<stderr>:#015 72%|███████▏  | 7576/10570 [00:20<00:07, 380.19it/s][1,5]<stderr>:#015 73%|███████▎  | 7762/10570 [00:20<00:07, 362.06it/s][1,7]<stderr>:#015 77%|███████▋  | 8188/10570 [00:20<00:06, 380.43it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▌   | 6935/10570 [00:19<00:10, 347.60it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7707/10570 [00:20<00:07, 382.47it/s]#033[A[1,12]<stderr>:#015 77%|███████▋  | 8097/10570 [00:20<00:06, 392.10it/s][1,2]<stderr>:#015 76%|███████▌  | 8002/10570 [00:20<00:06, 380.90it/s][1,13]<stderr>:#015 69%|██████▉   | 7298/10570 [00:20<00:08, 366.11it/s][1,1]<stderr>:#015 75%|███████▌  | 7961/10570 [00:20<00:06, 388.01it/s][1,15]<stderr>:#015 74%|███████▍  | 7874/10570 [00:20<00:07, 377.78it/s][1,6]<stderr>:#015 71%|███████   | 7464/10570 [00:20<00:08, 360.74it/s][1,4]<stderr>:#015 75%|███████▌  | 7941/10570 [00:20<00:06, 378.81it/s][1,14]<stderr>:#015 75%|███████▍  | 7878/10570 [00:20<00:07, 378.50it/s][1,3]<stderr>:#015 72%|███████▏  | 7584/10570 [00:20<00:07, 379.15it/s][1,9]<stderr>:#015 74%|███████▍  | 7829/10570 [00:20<00:07, 365.91it/s][1,11]<stderr>:#015 72%|███████▏  | 7616/10570 [00:20<00:07, 384.24it/s][1,10]<stderr>:#015 76%|███████▌  | 7987/10570 [00:20<00:06, 388.79it/s][1,5]<stderr>:#015 74%|███████▍  | 7799/10570 [00:20<00:07, 363.48it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▌   | 6973/10570 [00:19<00:10, 354.85it/s]#033[A[1,7]<stderr>:#015 78%|███████▊  | 8227/10570 [00:21<00:06, 365.15it/s][1,12]<stderr>:#015 77%|███████▋  | 8137/10570 [00:20<00:06, 390.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 73%|███████▎  | 7746/10570 [00:20<00:07, 374.75it/s]#033[A[1,2]<stderr>:#015 76%|███████▌  | 8042/10570 [00:20<00:06, 384.19it/s][1,15]<stderr>:#015 75%|███████▍  | 7913/10570 [00:20<00:06, 380.99it/s][1,1]<stderr>:#015 76%|███████▌  | 8000/10570 [00:20<00:06, 381.81it/s][1,13]<stderr>:#015 69%|██████▉   | 7335/10570 [00:20<00:09, 351.44it/s][1,6]<stderr>:#015 71%|███████   | 7503/10570 [00:20<00:08, 367.96it/s][1,4]<stderr>:#015 75%|███████▌  | 7980/10570 [00:20<00:06, 380.21it/s][1,14]<stderr>:#015 75%|███████▍  | 7919/10570 [00:20<00:06, 385.52it/s][1,3]<stderr>:#015 72%|███████▏  | 7624/10570 [00:20<00:07, 383.02it/s][1,9]<stderr>:#015 74%|███████▍  | 7869/10570 [00:20<00:07, 373.63it/s][1,10]<stderr>:#015 76%|███████▌  | 8026/10570 [00:20<00:06, 387.69it/s][1,11]<stderr>:#015 72%|███████▏  | 7655/10570 [00:20<00:07, 378.55it/s][1,5]<stderr>:#015 74%|███████▍  | 7837/10570 [00:20<00:07, 365.66it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 66%|██████▋   | 7012/10570 [00:19<00:09, 362.54it/s]#033[A[1,7]<stderr>:#015 78%|███████▊  | 8264/10570 [00:21<00:06, 361.70it/s][1,2]<stderr>:#015 76%|███████▋  | 8081/10570 [00:20<00:06, 383.02it/s][1,12]<stderr>:#015 77%|███████▋  | 8177/10570 [00:21<00:06, 381.87it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▎  | 7784/10570 [00:20<00:07, 362.79it/s]#033[A[1,1]<stderr>:#015 76%|███████▌  | 8039/10570 [00:20<00:06, 383.56it/s][1,15]<stderr>:#015 75%|███████▌  | 7954/10570 [00:20<00:06, 387.48it/s][1,4]<stderr>:#015 76%|███████▌  | 8019/10570 [00:20<00:06, 382.39it/s][1,6]<stderr>:#015 71%|███████▏  | 7543/10570 [00:20<00:08, 374.45it/s][1,14]<stderr>:#015 75%|███████▌  | 7958/10570 [00:20<00:06, 384.93it/s][1,13]<stderr>:#015 70%|██████▉   | 7371/10570 [00:20<00:09, 341.85it/s][1,9]<stderr>:#015 75%|███████▍  | 7908/10570 [00:20<00:07, 376.44it/s][1,5]<stderr>:#015 75%|███████▍  | 7877/10570 [00:20<00:07, 374.14it/s][1,10]<stderr>:#015 76%|███████▋  | 8065/10570 [00:20<00:06, 381.13it/s][1,11]<stderr>:#015 73%|███████▎  | 7693/10570 [00:20<00:07, 373.47it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7050/10570 [00:19<00:09, 366.05it/s]#033[A[1,3]<stderr>:#015 72%|███████▏  | 7663/10570 [00:20<00:08, 358.15it/s][1,2]<stderr>:#015 77%|███████▋  | 8123/10570 [00:21<00:06, 392.21it/s][1,7]<stderr>:#015 79%|███████▊  | 8301/10570 [00:21<00:06, 353.82it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▍  | 7822/10570 [00:20<00:07, 366.25it/s]#033[A[1,12]<stderr>:#015 78%|███████▊  | 8216/10570 [00:21<00:06, 373.49it/s][1,1]<stderr>:#015 76%|███████▋  | 8078/10570 [00:21<00:06, 381.37it/s][1,15]<stderr>:#015 76%|███████▌  | 7993/10570 [00:20<00:06, 383.40it/s][1,6]<stderr>:#015 72%|███████▏  | 7583/10570 [00:20<00:07, 379.65it/s][1,14]<stderr>:#015 76%|███████▌  | 7997/10570 [00:20<00:06, 381.44it/s][1,13]<stderr>:#015 70%|███████   | 7407/10570 [00:20<00:09, 346.04it/s][1,4]<stderr>:#015 76%|███████▌  | 8058/10570 [00:21<00:06, 374.91it/s][1,9]<stderr>:#015 75%|███████▌  | 7946/10570 [00:20<00:06, 377.33it/s][1,5]<stderr>:#015 75%|███████▍  | 7917/10570 [00:20<00:06, 380.32it/s][1,10]<stderr>:#015 77%|███████▋  | 8105/10570 [00:21<00:06, 384.99it/s][1,11]<stderr>:#015 73%|███████▎  | 7732/10570 [00:20<00:07, 377.49it/s][1,3]<stderr>:#015 73%|███████▎  | 7703/10570 [00:20<00:07, 368.33it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7087/10570 [00:20<00:09, 363.09it/s][1,8]<stderr>:#033[A[1,7]<stderr>:#015 79%|███████▉  | 8340/10570 [00:21<00:06, 363.35it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 74%|███████▍  | 7859/10570 [00:20<00:07, 366.80it/s]#033[A[1,2]<stderr>:#015 77%|███████▋  | 8163/10570 [00:21<00:06, 378.01it/s][1,12]<stderr>:#015 78%|███████▊  | 8254/10570 [00:21<00:06, 370.47it/s][1,1]<stderr>:#015 77%|███████▋  | 8120/10570 [00:21<00:06, 390.12it/s][1,15]<stderr>:#015 76%|███████▌  | 8032/10570 [00:21<00:06, 382.98it/s][1,6]<stderr>:#015 72%|███████▏  | 7622/10570 [00:20<00:07, 382.43it/s][1,4]<stderr>:#015 77%|███████▋  | 8099/10570 [00:21<00:06, 384.67it/s][1,13]<stderr>:#015 70%|███████   | 7444/10570 [00:20<00:08, 352.32it/s][1,14]<stderr>:#015 76%|███████▌  | 8036/10570 [00:20<00:06, 381.90it/s][1,9]<stderr>:#015 76%|███████▌  | 7984/10570 [00:21<00:06, 376.97it/s][1,5]<stderr>:#015 75%|███████▌  | 7957/10570 [00:20<00:06, 385.68it/s][1,10]<stderr>:#015 77%|███████▋  | 8144/10570 [00:21<00:06, 382.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 67%|██████▋   | 7125/10570 [00:20<00:09, 367.47it/s]#033[A[1,3]<stderr>:#015 73%|███████▎  | 7742/10570 [00:20<00:07, 373.24it/s][1,11]<stderr>:#015 74%|███████▎  | 7770/10570 [00:20<00:07, 355.79it/s][1,7]<stderr>:#015 79%|███████▉  | 8381/10570 [00:21<00:05, 373.76it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▍  | 7897/10570 [00:20<00:07, 370.50it/s]#033[A[1,2]<stderr>:#015 78%|███████▊  | 8201/10570 [00:21<00:06, 371.08it/s][1,15]<stderr>:#015 76%|███████▋  | 8071/10570 [00:21<00:06, 379.41it/s][1,12]<stderr>:#015 78%|███████▊  | 8292/10570 [00:21<00:06, 353.65it/s][1,1]<stderr>:#015 77%|███████▋  | 8160/10570 [00:21<00:06, 376.54it/s][1,13]<stderr>:#015 71%|███████   | 7482/10570 [00:20<00:08, 358.64it/s][1,4]<stderr>:#015 77%|███████▋  | 8138/10570 [00:21<00:06, 384.08it/s][1,14]<stderr>:#015 76%|███████▋  | 8075/10570 [00:20<00:06, 379.83it/s][1,6]<stderr>:#015 72%|███████▏  | 7661/10570 [00:20<00:07, 364.96it/s][1,9]<stderr>:#015 76%|███████▌  | 8023/10570 [00:21<00:06, 378.85it/s][1,5]<stderr>:#015 76%|███████▌  | 7996/10570 [00:20<00:06, 381.49it/s][1,10]<stderr>:#015 77%|███████▋  | 8183/10570 [00:21<00:06, 375.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7163/10570 [00:20<00:09, 369.47it/s]#033[A[1,11]<stderr>:#015 74%|███████▍  | 7808/10570 [00:20<00:07, 361.11it/s][1,7]<stderr>:#015 80%|███████▉  | 8423/10570 [00:21<00:05, 385.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▌  | 7938/10570 [00:20<00:06, 379.38it/s]#033[A[1,3]<stderr>:#015 74%|███████▎  | 7780/10570 [00:20<00:07, 350.85it/s][1,2]<stderr>:#015 78%|███████▊  | 8239/10570 [00:21<00:06, 367.90it/s][1,15]<stderr>:#015 77%|███████▋  | 8112/10570 [00:21<00:06, 387.01it/s][1,12]<stderr>:#015 79%|███████▉  | 8331/10570 [00:21<00:06, 361.13it/s][1,13]<stderr>:#015 71%|███████   | 7519/10570 [00:20<00:08, 361.94it/s][1,1]<stderr>:#015 78%|███████▊  | 8198/10570 [00:21<00:06, 370.83it/s][1,14]<stderr>:#015 77%|███████▋  | 8117/10570 [00:21<00:06, 388.67it/s][1,4]<stderr>:#015 77%|███████▋  | 8177/10570 [00:21<00:06, 375.72it/s][1,6]<stderr>:#015 73%|███████▎  | 7701/10570 [00:20<00:07, 373.70it/s][1,9]<stderr>:#015 76%|███████▋  | 8061/10570 [00:21<00:06, 371.25it/s][1,5]<stderr>:#015 76%|███████▌  | 8035/10570 [00:21<00:06, 381.34it/s][1,10]<stderr>:#015 78%|███████▊  | 8221/10570 [00:21<00:06, 367.08it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7200/10570 [00:20<00:09, 363.23it/s]#033[A[1,11]<stderr>:#015 74%|███████▍  | 7845/10570 [00:21<00:07, 359.85it/s][1,7]<stderr>:#015 80%|████████  | 8466/10570 [00:21<00:05, 396.82it/s][1,3]<stderr>:#015 74%|███████▍  | 7817/10570 [00:21<00:07, 355.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 75%|███████▌  | 7977/10570 [00:21<00:06, 378.06it/s]#033[A[1,2]<stderr>:#015 78%|███████▊  | 8276/10570 [00:21<00:06, 354.56it/s][1,12]<stderr>:#015 79%|███████▉  | 8372/10570 [00:21<00:05, 374.48it/s][1,15]<stderr>:#015 77%|███████▋  | 8151/10570 [00:21<00:06, 377.23it/s][1,13]<stderr>:#015 71%|███████▏  | 7557/10570 [00:20<00:08, 365.44it/s][1,1]<stderr>:#015 78%|███████▊  | 8236/10570 [00:21<00:06, 366.85it/s][1,6]<stderr>:#015 73%|███████▎  | 7740/10570 [00:20<00:07, 376.22it/s][1,14]<stderr>:#015 77%|███████▋  | 8156/10570 [00:21<00:06, 376.87it/s][1,4]<stderr>:#015 78%|███████▊  | 8215/10570 [00:21<00:06, 367.20it/s][1,9]<stderr>:#015 77%|███████▋  | 8103/10570 [00:21<00:06, 382.12it/s][1,5]<stderr>:#015 76%|███████▋  | 8074/10570 [00:21<00:06, 378.76it/s][1,10]<stderr>:#015 78%|███████▊  | 8258/10570 [00:21<00:06, 363.93it/s][1,11]<stderr>:#015 75%|███████▍  | 7884/10570 [00:21<00:07, 366.70it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 68%|██████▊   | 7237/10570 [00:20<00:09, 356.09it/s]#033[A[1,7]<stderr>:#015 80%|████████  | 8506/10570 [00:21<00:05, 396.23it/s][1,3]<stderr>:#015 74%|███████▍  | 7855/10570 [00:21<00:07, 360.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 76%|███████▌  | 8015/10570 [00:21<00:06, 375.47it/s]#033[A[1,2]<stderr>:#015 79%|███████▊  | 8313/10570 [00:21<00:06, 358.11it/s][1,12]<stderr>:#015 80%|███████▉  | 8413/10570 [00:21<00:05, 383.67it/s][1,13]<stderr>:#015 72%|███████▏  | 7596/10570 [00:20<00:08, 371.40it/s][1,15]<stderr>:#015 77%|███████▋  | 8189/10570 [00:21<00:06, 372.36it/s][1,1]<stderr>:#015 78%|███████▊  | 8273/10570 [00:21<00:06, 363.98it/s][1,4]<stderr>:#015 78%|███████▊  | 8252/10570 [00:21<00:06, 365.40it/s][1,14]<stderr>:#015 78%|███████▊  | 8194/10570 [00:21<00:06, 367.88it/s][1,6]<stderr>:#015 74%|███████▎  | 7778/10570 [00:21<00:07, 352.82it/s][1,9]<stderr>:#015 77%|███████▋  | 8142/10570 [00:21<00:06, 379.64it/s][1,5]<stderr>:#015 77%|███████▋  | 8116/10570 [00:21<00:06, 388.47it/s][1,11]<stderr>:#015 75%|███████▍  | 7924/10570 [00:21<00:07, 374.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7273/10570 [00:20<00:09, 355.90it/s]#033[A[1,10]<stderr>:#015 78%|███████▊  | 8295/10570 [00:21<00:06, 351.73it/s][1,7]<stderr>:#015 81%|████████  | 8547/10570 [00:21<00:05, 397.65it/s][1,3]<stderr>:#015 75%|███████▍  | 7894/10570 [00:21<00:07, 367.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 76%|███████▌  | 8053/10570 [00:21<00:06, 369.21it/s]#033[A[1,2]<stderr>:#015 79%|███████▉  | 8352/10570 [00:21<00:06, 366.50it/s][1,12]<stderr>:#015 80%|████████  | 8456/10570 [00:21<00:05, 395.41it/s][1,13]<stderr>:#015 72%|███████▏  | 7635/10570 [00:21<00:07, 374.46it/s][1,15]<stderr>:#015 78%|███████▊  | 8227/10570 [00:21<00:06, 367.08it/s][1,1]<stderr>:#015 79%|███████▊  | 8310/10570 [00:21<00:06, 354.54it/s][1,14]<stderr>:#015 78%|███████▊  | 8231/10570 [00:21<00:06, 365.83it/s][1,6]<stderr>:#015 74%|███████▍  | 7814/10570 [00:21<00:07, 354.37it/s][1,4]<stderr>:#015 78%|███████▊  | 8289/10570 [00:21<00:06, 347.55it/s][1,9]<stderr>:#015 77%|███████▋  | 8181/10570 [00:21<00:06, 372.04it/s][1,5]<stderr>:#015 77%|███████▋  | 8155/10570 [00:21<00:06, 375.13it/s][1,11]<stderr>:#015 75%|███████▌  | 7963/10570 [00:21<00:06, 377.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7310/10570 [00:20<00:09, 358.31it/s]#033[A[1,10]<stderr>:#015 79%|███████▉  | 8333/10570 [00:21<00:06, 358.39it/s][1,3]<stderr>:#015 75%|███████▌  | 7934/10570 [00:21<00:07, 374.23it/s][1,7]<stderr>:#015 81%|████████  | 8587/10570 [00:21<00:05, 390.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8093/10570 [00:21<00:06, 376.98it/s]#033[A[1,12]<stderr>:#015 80%|████████  | 8496/10570 [00:21<00:05, 396.37it/s][1,2]<stderr>:#015 79%|███████▉  | 8392/10570 [00:21<00:05, 373.66it/s][1,15]<stderr>:#015 78%|███████▊  | 8264/10570 [00:21<00:06, 360.27it/s][1,13]<stderr>:#015 73%|███████▎  | 7673/10570 [00:21<00:08, 358.96it/s][1,1]<stderr>:#015 79%|███████▉  | 8349/10570 [00:21<00:06, 362.34it/s][1,14]<stderr>:#015 78%|███████▊  | 8268/10570 [00:21<00:06, 362.06it/s][1,6]<stderr>:#015 74%|███████▍  | 7851/10570 [00:21<00:07, 358.89it/s][1,4]<stderr>:#015 79%|███████▉  | 8327/10570 [00:21<00:06, 355.74it/s][1,9]<stderr>:#015 78%|███████▊  | 8219/10570 [00:21<00:06, 364.72it/s][1,5]<stderr>:#015 78%|███████▊  | 8193/10570 [00:21<00:06, 369.48it/s][1,11]<stderr>:#015 76%|███████▌  | 8001/10570 [00:21<00:06, 370.99it/s][1,10]<stderr>:#015 79%|███████▉  | 8374/10570 [00:21<00:05, 372.25it/s][1,7]<stderr>:#015 82%|████████▏ | 8629/10570 [00:22<00:04, 397.60it/s][1,3]<stderr>:#015 75%|███████▌  | 7972/10570 [00:21<00:06, 372.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8133/10570 [00:21<00:06, 380.28it/s]#033[A[1,12]<stderr>:#015 81%|████████  | 8537/10570 [00:21<00:05, 398.68it/s][1,2]<stderr>:#015 80%|███████▉  | 8434/10570 [00:21<00:05, 384.68it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 69%|██████▉   | 7346/10570 [00:20<00:10, 320.28it/s]#033[A[1,13]<stderr>:#015 73%|███████▎  | 7711/10570 [00:21<00:07, 364.53it/s][1,1]<stderr>:#015 79%|███████▉  | 8389/10570 [00:21<00:05, 371.62it/s][1,15]<stderr>:#015 79%|███████▊  | 8301/10570 [00:21<00:06, 350.29it/s][1,6]<stderr>:#015 75%|███████▍  | 7890/10570 [00:21<00:07, 365.42it/s][1,4]<stderr>:#015 79%|███████▉  | 8367/10570 [00:21<00:06, 366.54it/s][1,14]<stderr>:#015 79%|███████▊  | 8305/10570 [00:21<00:06, 352.20it/s][1,9]<stderr>:#015 78%|███████▊  | 8256/10570 [00:21<00:06, 361.87it/s][1,5]<stderr>:#015 78%|███████▊  | 8231/10570 [00:21<00:06, 366.34it/s][1,11]<stderr>:#015 76%|███████▌  | 8039/10570 [00:21<00:06, 372.90it/s][1,10]<stderr>:#015 80%|███████▉  | 8415/10570 [00:21<00:05, 381.17it/s][1,7]<stderr>:#015 82%|████████▏ | 8669/10570 [00:22<00:04, 396.15it/s][1,3]<stderr>:#015 76%|███████▌  | 8010/10570 [00:21<00:06, 372.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 77%|███████▋  | 8172/10570 [00:21<00:06, 369.31it/s]#033[A[1,2]<stderr>:#015 80%|████████  | 8475/10570 [00:22<00:05, 391.27it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 70%|██████▉   | 7383/10570 [00:20<00:09, 333.26it/s]#033[A[1,12]<stderr>:#015 81%|████████  | 8577/10570 [00:22<00:05, 393.75it/s][1,1]<stderr>:#015 80%|███████▉  | 8431/10570 [00:21<00:05, 382.54it/s][1,15]<stderr>:#015 79%|███████▉  | 8339/10570 [00:21<00:06, 358.24it/s][1,13]<stderr>:#015 73%|███████▎  | 7748/10570 [00:21<00:08, 352.22it/s][1,6]<stderr>:#015 75%|███████▌  | 7928/10570 [00:21<00:07, 367.48it/s][1,4]<stderr>:#015 80%|███████▉  | 8407/10570 [00:21<00:05, 375.32it/s][1,14]<stderr>:#015 79%|███████▉  | 8344/10570 [00:21<00:06, 360.86it/s][1,5]<stderr>:#015 78%|███████▊  | 8268/10570 [00:21<00:06, 361.73it/s][1,10]<stderr>:#015 80%|████████  | 8458/10570 [00:22<00:05, 392.54it/s][1,11]<stderr>:#015 76%|███████▋  | 8077/10570 [00:21<00:06, 370.16it/s][1,9]<stderr>:#015 78%|███████▊  | 8293/10570 [00:21<00:06, 346.84it/s][1,7]<stderr>:#015 82%|████████▏ | 8710/10570 [00:22<00:04, 398.31it/s][1,3]<stderr>:#015 76%|███████▌  | 8048/10570 [00:21<00:06, 367.53it/s][1,2]<stderr>:#015 81%|████████  | 8515/10570 [00:22<00:05, 392.35it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 70%|███████   | 7421/10570 [00:20<00:09, 343.70it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8210/10570 [00:21<00:06, 362.65it/s]#033[A[1,12]<stderr>:#015 82%|████████▏ | 8618/10570 [00:22<00:04, 395.61it/s][1,1]<stderr>:#015 80%|████████  | 8472/10570 [00:22<00:05, 388.98it/s][1,15]<stderr>:#015 79%|███████▉  | 8379/10570 [00:22<00:05, 369.60it/s][1,13]<stderr>:#015 74%|███████▎  | 7784/10570 [00:21<00:08, 348.21it/s][1,6]<stderr>:#015 75%|███████▌  | 7966/10570 [00:21<00:07, 371.04it/s][1,4]<stderr>:#015 80%|███████▉  | 8449/10570 [00:22<00:05, 387.56it/s][1,14]<stderr>:#015 79%|███████▉  | 8384/10570 [00:21<00:05, 371.47it/s][1,10]<stderr>:#015 80%|████████  | 8498/10570 [00:22<00:05, 394.55it/s][1,11]<stderr>:#015 77%|███████▋  | 8118/10570 [00:21<00:06, 379.14it/s][1,9]<stderr>:#015 79%|███████▉  | 8331/10570 [00:21<00:06, 354.42it/s][1,5]<stderr>:#015 79%|███████▊  | 8305/10570 [00:21<00:06, 351.34it/s][1,7]<stderr>:#015 83%|████████▎ | 8750/10570 [00:22<00:04, 392.47it/s][1,3]<stderr>:#015 77%|███████▋  | 8087/10570 [00:21<00:06, 373.82it/s][1,2]<stderr>:#015 81%|████████  | 8556/10570 [00:22<00:05, 395.91it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8247/10570 [00:21<00:06, 361.84it/s]#033[A[1,12]<stderr>:#015 82%|████████▏ | 8658/10570 [00:22<00:04, 393.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7456/10570 [00:21<00:09, 339.90it/s]#033[A[1,15]<stderr>:#015 80%|███████▉  | 8420/10570 [00:22<00:05, 379.90it/s][1,1]<stderr>:#015 81%|████████  | 8512/10570 [00:22<00:05, 390.15it/s][1,4]<stderr>:#015 80%|████████  | 8489/10570 [00:22<00:05, 390.89it/s][1,13]<stderr>:#015 74%|███████▍  | 7821/10570 [00:21<00:07, 351.70it/s][1,14]<stderr>:#015 80%|███████▉  | 8425/10570 [00:21<00:05, 381.75it/s][1,6]<stderr>:#015 76%|███████▌  | 8004/10570 [00:21<00:07, 365.34it/s][1,10]<stderr>:#015 81%|████████  | 8538/10570 [00:22<00:05, 396.11it/s][1,9]<stderr>:#015 79%|███████▉  | 8372/10570 [00:22<00:05, 367.20it/s][1,5]<stderr>:#015 79%|███████▉  | 8343/10570 [00:21<00:06, 359.07it/s][1,3]<stderr>:#015 77%|███████▋  | 8127/10570 [00:21<00:06, 379.30it/s][1,11]<stderr>:#015 77%|███████▋  | 8157/10570 [00:21<00:06, 365.85it/s][1,7]<stderr>:#015 83%|████████▎ | 8790/10570 [00:22<00:04, 384.81it/s][1,2]<stderr>:#015 81%|████████▏ | 8596/10570 [00:22<00:05, 391.68it/s][1,12]<stderr>:#015 82%|████████▏ | 8700/10570 [00:22<00:04, 399.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7492/10570 [00:21<00:08, 344.39it/s]#033[A[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 78%|███████▊  | 8284/10570 [00:21<00:06, 340.84it/s]#033[A[1,1]<stderr>:#015 81%|████████  | 8553/10570 [00:22<00:05, 394.48it/s][1,15]<stderr>:#015 80%|████████  | 8462/10570 [00:22<00:05, 389.00it/s][1,13]<stderr>:#015 74%|███████▍  | 7858/10570 [00:21<00:07, 356.74it/s][1,4]<stderr>:#015 81%|████████  | 8530/10570 [00:22<00:05, 394.24it/s][1,14]<stderr>:#015 80%|████████  | 8468/10570 [00:21<00:05, 392.45it/s][1,6]<stderr>:#015 76%|███████▌  | 8042/10570 [00:21<00:06, 368.05it/s][1,10]<stderr>:#015 81%|████████  | 8578/10570 [00:22<00:05, 392.08it/s][1,9]<stderr>:#015 80%|███████▉  | 8412/10570 [00:22<00:05, 376.09it/s][1,5]<stderr>:#015 79%|███████▉  | 8383/10570 [00:21<00:05, 369.13it/s][1,7]<stderr>:#015 84%|████████▎ | 8829/10570 [00:22<00:04, 385.60it/s][1,11]<stderr>:#015 78%|███████▊  | 8194/10570 [00:21<00:06, 358.54it/s][1,3]<stderr>:#015 77%|███████▋  | 8166/10570 [00:21<00:06, 367.09it/s][1,2]<stderr>:#015 82%|████████▏ | 8638/10570 [00:22<00:04, 398.24it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 71%|███████   | 7530/10570 [00:21<00:08, 353.59it/s]#033[A[1,12]<stderr>:#015 83%|████████▎ | 8741/10570 [00:22<00:04, 390.67it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▊  | 8323/10570 [00:22<00:06, 352.54it/s][1,0]<stderr>:#033[A[1,15]<stderr>:#015 80%|████████  | 8502/10570 [00:22<00:05, 388.51it/s][1,1]<stderr>:#015 81%|████████▏ | 8593/10570 [00:22<00:05, 389.30it/s][1,13]<stderr>:#015 75%|███████▍  | 7895/10570 [00:21<00:07, 358.86it/s][1,4]<stderr>:#015 81%|████████  | 8570/10570 [00:22<00:05, 391.76it/s][1,6]<stderr>:#015 76%|███████▋  | 8079/10570 [00:21<00:06, 366.31it/s][1,14]<stderr>:#015 80%|████████  | 8508/10570 [00:22<00:05, 390.90it/s][1,10]<stderr>:#015 82%|████████▏ | 8618/10570 [00:22<00:04, 393.25it/s][1,9]<stderr>:#015 80%|███████▉  | 8454/10570 [00:22<00:05, 386.52it/s][1,5]<stderr>:#015 80%|███████▉  | 8424/10570 [00:22<00:05, 379.17it/s][1,7]<stderr>:#015 84%|████████▍ | 8869/10570 [00:22<00:04, 389.18it/s][1,11]<stderr>:#015 78%|███████▊  | 8231/10570 [00:22<00:06, 354.93it/s][1,3]<stderr>:#015 78%|███████▊  | 8203/10570 [00:22<00:06, 359.45it/s][1,2]<stderr>:#015 82%|████████▏ | 8678/10570 [00:22<00:04, 392.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7567/10570 [00:21<00:08, 355.36it/s]#033[A[1,12]<stderr>:#015 83%|████████▎ | 8781/10570 [00:22<00:04, 382.40it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▉  | 8362/10570 [00:22<00:06, 362.26it/s]#033[A[1,15]<stderr>:#015 81%|████████  | 8543/10570 [00:22<00:05, 392.46it/s][1,1]<stderr>:#015 82%|████████▏ | 8634/10570 [00:22<00:04, 395.22it/s][1,13]<stderr>:#015 75%|███████▌  | 7934/10570 [00:21<00:07, 366.13it/s][1,4]<stderr>:#015 81%|████████▏ | 8610/10570 [00:22<00:04, 392.84it/s][1,6]<stderr>:#015 77%|███████▋  | 8119/10570 [00:21<00:06, 375.72it/s][1,14]<stderr>:#015 81%|████████  | 8550/10570 [00:22<00:05, 396.29it/s][1,10]<stderr>:#015 82%|████████▏ | 8658/10570 [00:22<00:04, 390.99it/s][1,9]<stderr>:#015 80%|████████  | 8494/10570 [00:22<00:05, 387.91it/s][1,5]<stderr>:#015 80%|████████  | 8467/10570 [00:22<00:05, 390.50it/s][1,7]<stderr>:#015 84%|████████▍ | 8908/10570 [00:22<00:04, 387.62it/s][1,11]<stderr>:#015 78%|███████▊  | 8267/10570 [00:22<00:06, 350.43it/s][1,3]<stderr>:#015 78%|███████▊  | 8240/10570 [00:22<00:06, 355.55it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7605/10570 [00:21<00:08, 361.93it/s]#033[A[1,2]<stderr>:#015 82%|████████▏ | 8718/10570 [00:22<00:04, 382.48it/s][1,12]<stderr>:#015 83%|████████▎ | 8820/10570 [00:22<00:04, 382.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 79%|███████▉  | 8402/10570 [00:22<00:05, 370.04it/s]#033[A[1,1]<stderr>:#015 82%|████████▏ | 8674/10570 [00:22<00:04, 390.73it/s][1,15]<stderr>:#015 81%|████████  | 8583/10570 [00:22<00:05, 387.27it/s][1,13]<stderr>:#015 75%|███████▌  | 7971/10570 [00:22<00:07, 364.42it/s][1,4]<stderr>:#015 82%|████████▏ | 8650/10570 [00:22<00:04, 390.29it/s][1,14]<stderr>:#015 81%|████████▏ | 8590/10570 [00:22<00:05, 389.29it/s][1,6]<stderr>:#015 77%|███████▋  | 8157/10570 [00:22<00:06, 361.74it/s][1,10]<stderr>:#015 82%|████████▏ | 8700/10570 [00:22<00:04, 397.48it/s][1,9]<stderr>:#015 81%|████████  | 8534/10570 [00:22<00:05, 390.57it/s][1,5]<stderr>:#015 80%|████████  | 8507/10570 [00:22<00:05, 388.72it/s][1,7]<stderr>:#015 85%|████████▍ | 8948/10570 [00:22<00:04, 390.37it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 72%|███████▏  | 7642/10570 [00:21<00:08, 362.22it/s]#033[A[1,11]<stderr>:#015 79%|███████▊  | 8303/10570 [00:22<00:06, 339.98it/s][1,2]<stderr>:#015 83%|████████▎ | 8757/10570 [00:22<00:04, 378.31it/s][1,3]<stderr>:#015 78%|███████▊  | 8276/10570 [00:22<00:06, 341.97it/s][1,12]<stderr>:#015 84%|████████▍ | 8860/10570 [00:22<00:04, 385.64it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 80%|███████▉  | 8443/10570 [00:22<00:05, 379.46it/s]#033[A[1,1]<stderr>:#015 82%|████████▏ | 8714/10570 [00:22<00:04, 391.18it/s][1,15]<stderr>:#015 82%|████████▏ | 8623/10570 [00:22<00:05, 389.18it/s][1,13]<stderr>:#015 76%|███████▌  | 8008/10570 [00:22<00:07, 362.63it/s][1,4]<stderr>:#015 82%|████████▏ | 8690/10570 [00:22<00:04, 391.24it/s][1,14]<stderr>:#015 82%|████████▏ | 8632/10570 [00:22<00:04, 395.57it/s][1,6]<stderr>:#015 78%|███████▊  | 8194/10570 [00:22<00:06, 356.21it/s][1,10]<stderr>:#015 83%|████████▎ | 8740/10570 [00:22<00:04, 388.50it/s][1,9]<stderr>:#015 81%|████████  | 8574/10570 [00:22<00:05, 386.91it/s][1,5]<stderr>:#015 81%|████████  | 8548/10570 [00:22<00:05, 393.40it/s][1,7]<stderr>:#015 85%|████████▌ | 8989/10570 [00:22<00:04, 393.21it/s][1,11]<stderr>:#015 79%|███████▉  | 8338/10570 [00:22<00:06, 341.50it/s][1,3]<stderr>:#015 79%|███████▊  | 8311/10570 [00:22<00:06, 342.96it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7679/10570 [00:21<00:08, 349.12it/s]#033[A[1,12]<stderr>:#015 84%|████████▍ | 8899/10570 [00:22<00:04, 385.23it/s][1,2]<stderr>:#015 83%|████████▎ | 8795/10570 [00:22<00:04, 372.58it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 80%|████████  | 8483/10570 [00:22<00:05, 385.08it/s]#033[A[1,15]<stderr>:#015 82%|████████▏ | 8662/10570 [00:22<00:04, 387.89it/s][1,13]<stderr>:#015 76%|███████▌  | 8045/10570 [00:22<00:07, 359.83it/s][1,1]<stderr>:#015 83%|████████▎ | 8754/10570 [00:22<00:04, 384.06it/s][1,4]<stderr>:#015 83%|████████▎ | 8730/10570 [00:22<00:04, 384.98it/s][1,14]<stderr>:#015 82%|████████▏ | 8672/10570 [00:22<00:04, 390.40it/s][1,6]<stderr>:#015 78%|███████▊  | 8230/10570 [00:22<00:06, 352.81it/s][1,9]<stderr>:#015 81%|████████▏ | 8614/10570 [00:22<00:05, 389.24it/s][1,10]<stderr>:#015 83%|████████▎ | 8779/10570 [00:22<00:04, 380.76it/s][1,7]<stderr>:#015 85%|████████▌ | 9029/10570 [00:23<00:03, 394.87it/s][1,5]<stderr>:#015 81%|████████  | 8588/10570 [00:22<00:05, 385.80it/s][1,11]<stderr>:#015 79%|███████▉  | 8378/10570 [00:22<00:06, 355.21it/s][1,3]<stderr>:#015 79%|███████▉  | 8348/10570 [00:22<00:06, 350.61it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7715/10570 [00:21<00:08, 351.49it/s]#033[A[1,2]<stderr>:#015 84%|████████▎ | 8833/10570 [00:22<00:04, 373.92it/s][1,12]<stderr>:#015 85%|████████▍ | 8939/10570 [00:23<00:04, 387.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████  | 8522/10570 [00:22<00:05, 385.84it/s]#033[A[1,15]<stderr>:#015 82%|████████▏ | 8703/10570 [00:22<00:04, 393.00it/s][1,13]<stderr>:#015 76%|███████▋  | 8082/10570 [00:22<00:06, 362.13it/s][1,1]<stderr>:#015 83%|████████▎ | 8793/10570 [00:22<00:04, 378.12it/s][1,4]<stderr>:#015 83%|████████▎ | 8769/10570 [00:22<00:04, 382.32it/s][1,14]<stderr>:#015 82%|████████▏ | 8712/10570 [00:22<00:04, 391.66it/s][1,6]<stderr>:#015 78%|███████▊  | 8266/10570 [00:22<00:06, 347.83it/s][1,9]<stderr>:#015 82%|████████▏ | 8653/10570 [00:22<00:04, 385.42it/s][1,10]<stderr>:#015 83%|████████▎ | 8818/10570 [00:22<00:04, 380.58it/s][1,7]<stderr>:#015 86%|████████▌ | 9069/10570 [00:23<00:03, 393.98it/s][1,5]<stderr>:#015 82%|████████▏ | 8629/10570 [00:22<00:04, 392.08it/s][1,11]<stderr>:#015 80%|███████▉  | 8414/10570 [00:22<00:06, 355.05it/s][1,3]<stderr>:#015 79%|███████▉  | 8387/10570 [00:22<00:06, 359.12it/s][1,2]<stderr>:#015 84%|████████▍ | 8873/10570 [00:23<00:04, 380.01it/s][1,12]<stderr>:#015 85%|████████▍ | 8979/10570 [00:23<00:04, 389.60it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████  | 8562/10570 [00:22<00:05, 388.61it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 73%|███████▎  | 7751/10570 [00:21<00:08, 340.76it/s]#033[A[1,13]<stderr>:#015 77%|███████▋  | 8121/10570 [00:22<00:06, 369.97it/s][1,15]<stderr>:#015 83%|████████▎ | 8743/10570 [00:22<00:04, 384.21it/s][1,1]<stderr>:#015 84%|████████▎ | 8831/10570 [00:23<00:04, 377.66it/s][1,4]<stderr>:#015 83%|████████▎ | 8808/10570 [00:22<00:04, 378.37it/s][1,14]<stderr>:#015 83%|████████▎ | 8752/10570 [00:22<00:04, 385.38it/s][1,6]<stderr>:#015 79%|███████▊  | 8301/10570 [00:22<00:06, 336.74it/s][1,9]<stderr>:#015 82%|████████▏ | 8694/10570 [00:22<00:04, 390.27it/s][1,10]<stderr>:#015 84%|████████▍ | 8857/10570 [00:23<00:04, 382.55it/s][1,7]<stderr>:#015 86%|████████▌ | 9109/10570 [00:23<00:03, 391.04it/s][1,5]<stderr>:#015 82%|████████▏ | 8669/10570 [00:22<00:04, 388.84it/s][1,11]<stderr>:#015 80%|███████▉  | 8453/10570 [00:22<00:05, 360.70it/s][1,3]<stderr>:#015 80%|███████▉  | 8427/10570 [00:22<00:05, 369.29it/s][1,12]<stderr>:#015 85%|████████▌ | 9020/10570 [00:23<00:03, 393.29it/s][1,2]<stderr>:#015 84%|████████▍ | 8912/10570 [00:23<00:04, 379.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 81%|████████▏ | 8601/10570 [00:22<00:05, 384.63it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▎  | 7786/10570 [00:22<00:08, 327.93it/s]#033[A[1,1]<stderr>:#015 84%|████████▍ | 8871/10570 [00:23<00:04, 382.20it/s][1,15]<stderr>:#015 83%|████████▎ | 8782/10570 [00:23<00:04, 376.43it/s][1,4]<stderr>:#015 84%|████████▎ | 8846/10570 [00:23<00:04, 377.72it/s][1,13]<stderr>:#015 77%|███████▋  | 8159/10570 [00:22<00:06, 356.93it/s][1,14]<stderr>:#015 83%|████████▎ | 8791/10570 [00:22<00:04, 378.96it/s][1,6]<stderr>:#015 79%|███████▉  | 8338/10570 [00:22<00:06, 343.84it/s][1,10]<stderr>:#015 84%|████████▍ | 8896/10570 [00:23<00:04, 382.73it/s][1,7]<stderr>:#015 87%|████████▋ | 9149/10570 [00:23<00:03, 391.62it/s][1,5]<stderr>:#015 82%|████████▏ | 8709/10570 [00:22<00:04, 384.09it/s][1,9]<stderr>:#015 83%|████████▎ | 8734/10570 [00:23<00:04, 374.34it/s][1,11]<stderr>:#015 80%|████████  | 8492/10570 [00:22<00:05, 366.58it/s][1,3]<stderr>:#015 80%|████████  | 8468/10570 [00:22<00:05, 379.38it/s][1,2]<stderr>:#015 85%|████████▍ | 8952/10570 [00:23<00:04, 383.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 82%|████████▏ | 8642/10570 [00:22<00:04, 389.99it/s]#033[A[1,12]<stderr>:#015 86%|████████▌ | 9060/10570 [00:23<00:03, 383.56it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▍  | 7821/10570 [00:22<00:08, 333.52it/s]#033[A[1,1]<stderr>:#015 84%|████████▍ | 8910/10570 [00:23<00:04, 380.94it/s][1,15]<stderr>:#015 83%|████████▎ | 8820/10570 [00:23<00:04, 377.28it/s][1,4]<stderr>:#015 84%|████████▍ | 8886/10570 [00:23<00:04, 382.51it/s][1,13]<stderr>:#015 78%|███████▊  | 8195/10570 [00:22<00:06, 351.07it/s][1,14]<stderr>:#015 84%|████████▎ | 8829/10570 [00:22<00:04, 378.92it/s][1,6]<stderr>:#015 79%|███████▉  | 8377/10570 [00:22<00:06, 356.34it/s][1,10]<stderr>:#015 85%|████████▍ | 8936/10570 [00:23<00:04, 385.96it/s][1,7]<stderr>:#015 87%|████████▋ | 9190/10570 [00:23<00:03, 395.65it/s][1,9]<stderr>:#015 83%|████████▎ | 8772/10570 [00:23<00:04, 373.13it/s][1,5]<stderr>:#015 83%|████████▎ | 8748/10570 [00:22<00:04, 379.21it/s][1,11]<stderr>:#015 81%|████████  | 8531/10570 [00:22<00:05, 373.30it/s][1,3]<stderr>:#015 80%|████████  | 8507/10570 [00:22<00:05, 378.05it/s][1,2]<stderr>:#015 85%|████████▌ | 8992/10570 [00:23<00:04, 387.42it/s][1,12]<stderr>:#015 86%|████████▌ | 9100/10570 [00:23<00:03, 385.44it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 82%|████████▏ | 8682/10570 [00:22<00:04, 383.76it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 74%|███████▍  | 7857/10570 [00:22<00:07, 340.05it/s]#033[A[1,1]<stderr>:#015 85%|████████▍ | 8949/10570 [00:23<00:04, 383.29it/s][1,15]<stderr>:#015 84%|████████▍ | 8859/10570 [00:23<00:04, 379.94it/s][1,4]<stderr>:#015 84%|████████▍ | 8925/10570 [00:23<00:04, 381.74it/s][1,13]<stderr>:#015 78%|███████▊  | 8231/10570 [00:22<00:06, 347.96it/s][1,14]<stderr>:#015 84%|████████▍ | 8869/10570 [00:23<00:04, 383.44it/s][1,6]<stderr>:#015 80%|███████▉  | 8416/10570 [00:22<00:05, 365.68it/s][1,10]<stderr>:#015 85%|████████▍ | 8975/10570 [00:23<00:04, 385.46it/s][1,7]<stderr>:#015 87%|████████▋ | 9231/10570 [00:23<00:03, 397.21it/s][1,9]<stderr>:#015 83%|████████▎ | 8810/10570 [00:23<00:04, 371.56it/s][1,5]<stderr>:#015 83%|████████▎ | 8786/10570 [00:23<00:04, 372.91it/s][1,11]<stderr>:#015 81%|████████  | 8569/10570 [00:22<00:05, 369.70it/s][1,3]<stderr>:#015 81%|████████  | 8547/10570 [00:22<00:05, 383.64it/s][1,2]<stderr>:#015 85%|████████▌ | 9032/10570 [00:23<00:03, 388.77it/s][1,12]<stderr>:#015 86%|████████▋ | 9140/10570 [00:23<00:03, 387.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8721/10570 [00:23<00:04, 382.16it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▍  | 7893/10570 [00:22<00:07, 344.94it/s]#033[A[1,1]<stderr>:#015 85%|████████▌ | 8989/10570 [00:23<00:04, 386.53it/s][1,15]<stderr>:#015 84%|████████▍ | 8898/10570 [00:23<00:04, 378.83it/s][1,4]<stderr>:#015 85%|████████▍ | 8964/10570 [00:23<00:04, 382.59it/s][1,13]<stderr>:#015 78%|███████▊  | 8266/10570 [00:22<00:06, 341.56it/s][1,14]<stderr>:#015 84%|████████▍ | 8908/10570 [00:23<00:04, 379.63it/s][1,6]<stderr>:#015 80%|████████  | 8457/10570 [00:22<00:05, 376.42it/s][1,10]<stderr>:#015 85%|████████▌ | 9016/10570 [00:23<00:03, 389.95it/s][1,7]<stderr>:#015 88%|████████▊ | 9273/10570 [00:23<00:03, 401.71it/s][1,9]<stderr>:#015 84%|████████▎ | 8848/10570 [00:23<00:04, 372.18it/s][1,5]<stderr>:#015 83%|████████▎ | 8824/10570 [00:23<00:04, 374.37it/s][1,11]<stderr>:#015 81%|████████▏ | 8607/10570 [00:23<00:05, 372.36it/s][1,2]<stderr>:#015 86%|████████▌ | 9071/10570 [00:23<00:03, 388.37it/s][1,3]<stderr>:#015 81%|████████  | 8586/10570 [00:23<00:05, 376.25it/s][1,12]<stderr>:#015 87%|████████▋ | 9179/10570 [00:23<00:03, 387.53it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8760/10570 [00:23<00:04, 377.20it/s][1,0]<stderr>:#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▌  | 7929/10570 [00:22<00:07, 349.19it/s]#033[A[1,1]<stderr>:#015 85%|████████▌ | 9029/10570 [00:23<00:03, 388.58it/s][1,15]<stderr>:#015 85%|████████▍ | 8937/10570 [00:23<00:04, 381.85it/s][1,4]<stderr>:#015 85%|████████▌ | 9005/10570 [00:23<00:04, 390.11it/s][1,14]<stderr>:#015 85%|████████▍ | 8948/10570 [00:23<00:04, 383.05it/s][1,6]<stderr>:#015 80%|████████  | 8495/10570 [00:23<00:05, 377.11it/s][1,13]<stderr>:#015 79%|███████▊  | 8301/10570 [00:22<00:06, 330.43it/s][1,10]<stderr>:#015 86%|████████▌ | 9056/10570 [00:23<00:03, 387.89it/s][1,7]<stderr>:#015 88%|████████▊ | 9314/10570 [00:23<00:03, 398.52it/s][1,9]<stderr>:#015 84%|████████▍ | 8888/10570 [00:23<00:04, 377.80it/s][1,5]<stderr>:#015 84%|████████▍ | 8863/10570 [00:23<00:04, 378.26it/s][1,11]<stderr>:#015 82%|████████▏ | 8646/10570 [00:23<00:05, 376.52it/s][1,2]<stderr>:#015 86%|████████▌ | 9110/10570 [00:23<00:03, 386.90it/s][1,3]<stderr>:#015 82%|████████▏ | 8625/10570 [00:23<00:05, 380.23it/s][1,12]<stderr>:#015 87%|████████▋ | 9219/10570 [00:23<00:03, 390.73it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 83%|████████▎ | 8798/10570 [00:23<00:04, 373.54it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 75%|███████▌  | 7966/10570 [00:22<00:07, 353.15it/s]#033[A[1,1]<stderr>:#015 86%|████████▌ | 9068/10570 [00:23<00:03, 386.20it/s][1,15]<stderr>:#015 85%|████████▍ | 8976/10570 [00:23<00:04, 383.17it/s][1,4]<stderr>:#015 86%|████████▌ | 9045/10570 [00:23<00:03, 381.68it/s][1,14]<stderr>:#015 85%|████████▌ | 8988/10570 [00:23<00:04, 386.53it/s][1,6]<stderr>:#015 81%|████████  | 8534/10570 [00:23<00:05, 379.44it/s][1,13]<stderr>:#015 79%|███████▉  | 8337/10570 [00:23<00:06, 337.37it/s][1,10]<stderr>:#015 86%|████████▌ | 9096/10570 [00:23<00:03, 389.06it/s][1,7]<stderr>:#015 88%|████████▊ | 9354/10570 [00:23<00:03, 394.71it/s][1,9]<stderr>:#015 84%|████████▍ | 8926/10570 [00:23<00:04, 376.42it/s][1,5]<stderr>:#015 84%|████████▍ | 8902/10570 [00:23<00:04, 379.11it/s][1,11]<stderr>:#015 82%|████████▏ | 8684/10570 [00:23<00:05, 375.05it/s][1,2]<stderr>:#015 87%|████████▋ | 9149/10570 [00:23<00:03, 387.61it/s][1,3]<stderr>:#015 82%|████████▏ | 8664/10570 [00:23<00:05, 378.46it/s][1,12]<stderr>:#015 88%|████████▊ | 9261/10570 [00:23<00:03, 397.95it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 84%|████████▎ | 8836/10570 [00:23<00:04, 371.97it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▌  | 8002/10570 [00:22<00:07, 346.98it/s]#033[A[1,15]<stderr>:#015 85%|████████▌ | 9016/10570 [00:23<00:04, 387.39it/s][1,1]<stderr>:#015 86%|████████▌ | 9107/10570 [00:23<00:03, 383.95it/s][1,4]<stderr>:#015 86%|████████▌ | 9085/10570 [00:23<00:03, 384.74it/s][1,14]<stderr>:#015 85%|████████▌ | 9028/10570 [00:23<00:03, 388.96it/s][1,6]<stderr>:#015 81%|████████  | 8573/10570 [00:23<00:05, 375.83it/s][1,13]<stderr>:#015 79%|███████▉  | 8376/10570 [00:23<00:06, 349.12it/s][1,10]<stderr>:#015 86%|████████▋ | 9136/10570 [00:23<00:03, 389.72it/s][1,7]<stderr>:#015 89%|████████▉ | 9394/10570 [00:23<00:02, 393.49it/s][1,9]<stderr>:#015 85%|████████▍ | 8965/10570 [00:23<00:04, 377.88it/s][1,5]<stderr>:#015 85%|████████▍ | 8941/10570 [00:23<00:04, 380.19it/s][1,11]<stderr>:#015 83%|████████▎ | 8722/10570 [00:23<00:04, 373.40it/s][1,2]<stderr>:#015 87%|████████▋ | 9188/10570 [00:23<00:03, 388.14it/s][1,3]<stderr>:#015 82%|████████▏ | 8703/10570 [00:23<00:04, 381.79it/s][1,12]<stderr>:#015 88%|████████▊ | 9301/10570 [00:23<00:03, 395.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▌  | 8038/10570 [00:22<00:07, 349.98it/s]#033[A[1,0]<stderr>:#015 84%|████████▍ | 8874/10570 [00:23<00:04, 368.21it/s]#033[A[1,1]<stderr>:#015 87%|████████▋ | 9147/10570 [00:23<00:03, 385.93it/s][1,15]<stderr>:#015 86%|████████▌ | 9055/10570 [00:23<00:03, 380.37it/s][1,4]<stderr>:#015 86%|████████▋ | 9124/10570 [00:23<00:03, 384.66it/s][1,14]<stderr>:#015 86%|████████▌ | 9067/10570 [00:23<00:03, 384.16it/s][1,6]<stderr>:#015 81%|████████▏ | 8612/10570 [00:23<00:05, 378.68it/s][1,13]<stderr>:#015 80%|███████▉  | 8414/10570 [00:23<00:06, 357.24it/s][1,10]<stderr>:#015 87%|████████▋ | 9175/10570 [00:23<00:03, 387.91it/s][1,7]<stderr>:#015 89%|████████▉ | 9434/10570 [00:24<00:02, 387.18it/s][1,9]<stderr>:#015 85%|████████▌ | 9006/10570 [00:23<00:04, 385.74it/s][1,5]<stderr>:#015 85%|████████▍ | 8980/10570 [00:23<00:04, 383.03it/s][1,2]<stderr>:#015 87%|████████▋ | 9228/10570 [00:23<00:03, 390.35it/s][1,11]<stderr>:#015 83%|████████▎ | 8760/10570 [00:23<00:04, 369.61it/s][1,12]<stderr>:#015 88%|████████▊ | 9341/10570 [00:24<00:03, 392.35it/s][1,3]<stderr>:#015 83%|████████▎ | 8742/10570 [00:23<00:05, 361.77it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 84%|████████▍ | 8912/10570 [00:23<00:04, 369.87it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 76%|███████▋  | 8074/10570 [00:22<00:07, 347.43it/s]#033[A[1,1]<stderr>:#015 87%|████████▋ | 9187/10570 [00:23<00:03, 388.29it/s][1,15]<stderr>:#015 86%|████████▌ | 9094/10570 [00:23<00:03, 383.19it/s][1,4]<stderr>:#015 87%|████████▋ | 9163/10570 [00:23<00:03, 383.39it/s][1,14]<stderr>:#015 86%|████████▌ | 9106/10570 [00:23<00:03, 383.22it/s][1,13]<stderr>:#015 80%|███████▉  | 8454/10570 [00:23<00:05, 368.09it/s][1,6]<stderr>:#015 82%|████████▏ | 8650/10570 [00:23<00:05, 376.23it/s][1,10]<stderr>:#015 87%|████████▋ | 9215/10570 [00:23<00:03, 391.17it/s][1,7]<stderr>:#015 90%|████████▉ | 9475/10570 [00:24<00:02, 393.65it/s][1,9]<stderr>:#015 86%|████████▌ | 9045/10570 [00:23<00:03, 381.37it/s][1,5]<stderr>:#015 85%|████████▌ | 9019/10570 [00:23<00:04, 380.96it/s][1,2]<stderr>:#015 88%|████████▊ | 9269/10570 [00:24<00:03, 392.79it/s][1,11]<stderr>:#015 83%|████████▎ | 8797/10570 [00:23<00:04, 366.05it/s][1,12]<stderr>:#015 89%|████████▉ | 9381/10570 [00:24<00:03, 392.31it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▍ | 8950/10570 [00:23<00:04, 370.98it/s]#033[A[1,3]<stderr>:#015 83%|████████▎ | 8779/10570 [00:23<00:05, 355.98it/s][1,1]<stderr>:#015 87%|████████▋ | 9226/10570 [00:24<00:03, 388.39it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 86%|████████▋ | 9133/10570 [00:23<00:03, 384.68it/s][1,8]<stderr>:#015 77%|███████▋  | 8113/10570 [00:22<00:06, 357.16it/s]#033[A[1,4]<stderr>:#015 87%|████████▋ | 9203/10570 [00:24<00:03, 385.80it/s][1,14]<stderr>:#015 87%|████████▋ | 9146/10570 [00:23<00:03, 385.95it/s][1,13]<stderr>:#015 80%|████████  | 8492/10570 [00:23<00:05, 369.49it/s][1,6]<stderr>:#015 82%|████████▏ | 8688/10570 [00:23<00:05, 376.01it/s][1,10]<stderr>:#015 88%|████████▊ | 9257/10570 [00:24<00:03, 396.78it/s][1,7]<stderr>:#015 90%|█████████ | 9515/10570 [00:24<00:02, 395.15it/s][1,9]<stderr>:#015 86%|████████▌ | 9084/10570 [00:23<00:03, 383.83it/s][1,5]<stderr>:#015 86%|████████▌ | 9058/10570 [00:23<00:03, 378.12it/s][1,2]<stderr>:#015 88%|████████▊ | 9309/10570 [00:24<00:03, 391.68it/s][1,11]<stderr>:#015 84%|████████▎ | 8834/10570 [00:23<00:04, 364.51it/s][1,12]<stderr>:#015 89%|████████▉ | 9421/10570 [00:24<00:02, 391.15it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▌ | 8989/10570 [00:23<00:04, 375.64it/s]#033[A[1,3]<stderr>:#015 83%|████████▎ | 8816/10570 [00:23<00:04, 359.00it/s][1,1]<stderr>:#015 88%|████████▊ | 9267/10570 [00:24<00:03, 392.97it/s][1,15]<stderr>:#015 87%|████████▋ | 9172/10570 [00:24<00:03, 381.57it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 77%|███████▋  | 8149/10570 [00:23<00:06, 347.83it/s]#033[A[1,4]<stderr>:#015 87%|████████▋ | 9243/10570 [00:24<00:03, 389.77it/s][1,14]<stderr>:#015 87%|████████▋ | 9185/10570 [00:23<00:03, 385.24it/s][1,6]<stderr>:#015 83%|████████▎ | 8726/10570 [00:23<00:04, 370.33it/s][1,13]<stderr>:#015 81%|████████  | 8530/10570 [00:23<00:05, 365.56it/s][1,10]<stderr>:#015 88%|████████▊ | 9297/10570 [00:24<00:03, 392.02it/s][1,7]<stderr>:#015 90%|█████████ | 9557/10570 [00:24<00:02, 402.27it/s][1,9]<stderr>:#015 86%|████████▋ | 9123/10570 [00:24<00:03, 381.95it/s][1,5]<stderr>:#015 86%|████████▌ | 9097/10570 [00:23<00:03, 380.71it/s][1,11]<stderr>:#015 84%|████████▍ | 8872/10570 [00:23<00:04, 367.73it/s][1,2]<stderr>:#015 88%|████████▊ | 9349/10570 [00:24<00:03, 386.97it/s][1,12]<stderr>:#015 90%|████████▉ | 9463/10570 [00:24<00:02, 397.04it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 85%|████████▌ | 9028/10570 [00:23<00:04, 379.28it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8854/10570 [00:23<00:04, 362.42it/s][1,1]<stderr>:#015 88%|████████▊ | 9307/10570 [00:24<00:03, 391.59it/s][1,15]<stderr>:#015 87%|████████▋ | 9212/10570 [00:24<00:03, 384.13it/s][1,4]<stderr>:#015 88%|████████▊ | 9283/10570 [00:24<00:03, 392.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 77%|███████▋  | 8184/10570 [00:23<00:06, 341.76it/s]#033[A[1,14]<stderr>:#015 87%|████████▋ | 9224/10570 [00:23<00:03, 377.31it/s][1,13]<stderr>:#015 81%|████████  | 8567/10570 [00:23<00:05, 366.08it/s][1,6]<stderr>:#015 83%|████████▎ | 8764/10570 [00:23<00:04, 367.17it/s][1,10]<stderr>:#015 88%|████████▊ | 9337/10570 [00:24<00:03, 386.95it/s][1,7]<stderr>:#015 91%|█████████ | 9598/10570 [00:24<00:02, 399.69it/s][1,5]<stderr>:#015 86%|████████▋ | 9136/10570 [00:23<00:03, 382.78it/s][1,9]<stderr>:#015 87%|████████▋ | 9162/10570 [00:24<00:03, 380.65it/s][1,11]<stderr>:#015 84%|████████▍ | 8909/10570 [00:23<00:04, 365.96it/s][1,2]<stderr>:#015 89%|████████▉ | 9388/10570 [00:24<00:03, 387.61it/s][1,12]<stderr>:#015 90%|████████▉ | 9503/10570 [00:24<00:02, 395.89it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▌ | 9066/10570 [00:23<00:03, 378.59it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8892/10570 [00:23<00:04, 365.11it/s][1,15]<stderr>:#015 88%|████████▊ | 9253/10570 [00:24<00:03, 389.38it/s][1,1]<stderr>:#015 88%|████████▊ | 9347/10570 [00:24<00:03, 386.24it/s][1,4]<stderr>:#015 88%|████████▊ | 9323/10570 [00:24<00:03, 385.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8219/10570 [00:23<00:07, 333.32it/s]#033[A[1,14]<stderr>:#015 88%|████████▊ | 9265/10570 [00:24<00:03, 385.94it/s][1,13]<stderr>:#015 81%|████████▏ | 8604/10570 [00:23<00:05, 365.56it/s][1,6]<stderr>:#015 83%|████████▎ | 8801/10570 [00:23<00:04, 363.38it/s][1,10]<stderr>:#015 89%|████████▊ | 9377/10570 [00:24<00:03, 388.20it/s][1,7]<stderr>:#015 91%|█████████ | 9640/10570 [00:24<00:02, 404.67it/s][1,9]<stderr>:#015 87%|████████▋ | 9202/10570 [00:24<00:03, 385.43it/s][1,5]<stderr>:#015 87%|████████▋ | 9175/10570 [00:24<00:03, 381.73it/s][1,11]<stderr>:#015 85%|████████▍ | 8946/10570 [00:24<00:04, 366.94it/s][1,2]<stderr>:#015 89%|████████▉ | 9428/10570 [00:24<00:02, 390.72it/s][1,12]<stderr>:#015 90%|█████████ | 9545/10570 [00:24<00:02, 401.96it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▌ | 9104/10570 [00:24<00:03, 376.97it/s]#033[A[1,3]<stderr>:#015 84%|████████▍ | 8930/10570 [00:24<00:04, 368.24it/s][1,15]<stderr>:#015 88%|████████▊ | 9292/10570 [00:24<00:03, 388.30it/s][1,1]<stderr>:#015 89%|████████▉ | 9386/10570 [00:24<00:03, 387.08it/s][1,4]<stderr>:#015 89%|████████▊ | 9363/10570 [00:24<00:03, 387.14it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8253/10570 [00:23<00:06, 331.76it/s]#033[A[1,14]<stderr>:#015 88%|████████▊ | 9305/10570 [00:24<00:03, 387.29it/s][1,13]<stderr>:#015 82%|████████▏ | 8641/10570 [00:23<00:05, 364.32it/s][1,6]<stderr>:#015 84%|████████▎ | 8838/10570 [00:23<00:04, 361.37it/s][1,10]<stderr>:#015 89%|████████▉ | 9416/10570 [00:24<00:02, 388.01it/s][1,7]<stderr>:#015 92%|█████████▏| 9682/10570 [00:24<00:02, 406.75it/s][1,9]<stderr>:#015 87%|████████▋ | 9242/10570 [00:24<00:03, 389.12it/s][1,5]<stderr>:#015 87%|████████▋ | 9215/10570 [00:24<00:03, 385.28it/s][1,2]<stderr>:#015 90%|████████▉ | 9469/10570 [00:24<00:02, 396.04it/s][1,11]<stderr>:#015 85%|████████▍ | 8983/10570 [00:24<00:04, 362.95it/s][1,12]<stderr>:#015 91%|█████████ | 9586/10570 [00:24<00:02, 400.98it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 86%|████████▋ | 9143/10570 [00:24<00:03, 380.39it/s][1,0]<stderr>:#033[A[1,3]<stderr>:#015 85%|████████▍ | 8968/10570 [00:24<00:04, 369.79it/s][1,1]<stderr>:#015 89%|████████▉ | 9426/10570 [00:24<00:02, 390.02it/s][1,15]<stderr>:#015 88%|████████▊ | 9331/10570 [00:24<00:03, 385.10it/s][1,4]<stderr>:#015 89%|████████▉ | 9402/10570 [00:24<00:03, 386.18it/s][1,14]<stderr>:#015 88%|████████▊ | 9344/10570 [00:24<00:03, 383.57it/s][1,13]<stderr>:#015 82%|████████▏ | 8678/10570 [00:24<00:05, 360.67it/s][1,6]<stderr>:#015 84%|████████▍ | 8877/10570 [00:24<00:04, 366.83it/s][1,10]<stderr>:#015 89%|████████▉ | 9457/10570 [00:24<00:02, 393.79it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 78%|███████▊  | 8287/10570 [00:23<00:07, 313.43it/s]#033[A[1,7]<stderr>:#015 92%|█████████▏| 9726/10570 [00:24<00:02, 414.09it/s][1,9]<stderr>:#015 88%|████████▊ | 9282/10570 [00:24<00:03, 390.60it/s][1,5]<stderr>:#015 88%|████████▊ | 9256/10570 [00:24<00:03, 391.79it/s][1,2]<stderr>:#015 90%|████████▉ | 9509/10570 [00:24<00:02, 394.53it/s][1,11]<stderr>:#015 85%|████████▌ | 9022/10570 [00:24<00:04, 369.17it/s][1,12]<stderr>:#015 91%|█████████ | 9627/10570 [00:24<00:02, 403.36it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 87%|████████▋ | 9182/10570 [00:24<00:03, 380.32it/s]#033[A[1,3]<stderr>:#015 85%|████████▌ | 9008/10570 [00:24<00:04, 377.25it/s][1,15]<stderr>:#015 89%|████████▊ | 9370/10570 [00:24<00:03, 386.10it/s][1,1]<stderr>:#015 90%|████████▉ | 9468/10570 [00:24<00:02, 395.63it/s][1,4]<stderr>:#015 89%|████████▉ | 9442/10570 [00:24<00:02, 389.48it/s][1,14]<stderr>:#015 89%|████████▉ | 9383/10570 [00:24<00:03, 384.72it/s][1,13]<stderr>:#015 82%|████████▏ | 8715/10570 [00:24<00:05, 363.12it/s][1,10]<stderr>:#015 90%|████████▉ | 9497/10570 [00:24<00:02, 394.77it/s][1,6]<stderr>:#015 84%|████████▍ | 8914/10570 [00:24<00:04, 366.54it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▊  | 8323/10570 [00:23<00:06, 324.78it/s]#033[A[1,7]<stderr>:#015 92%|█████████▏| 9768/10570 [00:24<00:01, 413.08it/s][1,5]<stderr>:#015 88%|████████▊ | 9296/10570 [00:24<00:03, 389.60it/s][1,9]<stderr>:#015 88%|████████▊ | 9322/10570 [00:24<00:03, 384.20it/s][1,2]<stderr>:#015 90%|█████████ | 9552/10570 [00:24<00:02, 402.74it/s][1,12]<stderr>:#015 91%|█████████▏| 9668/10570 [00:24<00:02, 405.14it/s][1,11]<stderr>:#015 86%|████████▌ | 9059/10570 [00:24<00:04, 358.35it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 87%|████████▋ | 9221/10570 [00:24<00:03, 381.78it/s]#033[A[1,3]<stderr>:#015 86%|████████▌ | 9046/10570 [00:24<00:04, 372.90it/s][1,15]<stderr>:#015 89%|████████▉ | 9409/10570 [00:24<00:03, 384.14it/s][1,1]<stderr>:#015 90%|████████▉ | 9508/10570 [00:24<00:02, 393.74it/s][1,4]<stderr>:#015 90%|████████▉ | 9482/10570 [00:24<00:02, 391.96it/s][1,14]<stderr>:#015 89%|████████▉ | 9422/10570 [00:24<00:02, 385.82it/s][1,10]<stderr>:#015 90%|█████████ | 9538/10570 [00:24<00:02, 398.67it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▉  | 8359/10570 [00:23<00:06, 334.42it/s]#033[A[1,6]<stderr>:#015 85%|████████▍ | 8952/10570 [00:24<00:04, 368.41it/s][1,13]<stderr>:#015 83%|████████▎ | 8752/10570 [00:24<00:05, 351.96it/s][1,7]<stderr>:#015 93%|█████████▎| 9810/10570 [00:25<00:01, 406.54it/s][1,9]<stderr>:#015 89%|████████▊ | 9361/10570 [00:24<00:03, 384.79it/s][1,5]<stderr>:#015 88%|████████▊ | 9335/10570 [00:24<00:03, 382.97it/s][1,2]<stderr>:#015 91%|█████████ | 9593/10570 [00:24<00:02, 397.39it/s][1,12]<stderr>:#015 92%|█████████▏| 9711/10570 [00:24<00:02, 409.73it/s][1,11]<stderr>:#015 86%|████████▌ | 9097/10570 [00:24<00:04, 362.61it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9262/10570 [00:24<00:03, 387.03it/s]#033[A[1,3]<stderr>:#015 86%|████████▌ | 9084/10570 [00:24<00:03, 374.70it/s][1,15]<stderr>:#015 89%|████████▉ | 9448/10570 [00:24<00:02, 384.76it/s][1,1]<stderr>:#015 90%|█████████ | 9551/10570 [00:24<00:02, 401.85it/s][1,4]<stderr>:#015 90%|█████████ | 9523/10570 [00:24<00:02, 396.64it/s][1,14]<stderr>:#015 90%|████████▉ | 9464/10570 [00:24<00:02, 393.26it/s][1,10]<stderr>:#015 91%|█████████ | 9579/10570 [00:24<00:02, 400.55it/s][1,6]<stderr>:#015 85%|████████▌ | 8990/10570 [00:24<00:04, 371.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 79%|███████▉  | 8396/10570 [00:23<00:06, 342.75it/s]#033[A[1,13]<stderr>:#015 83%|████████▎ | 8788/10570 [00:24<00:05, 347.38it/s][1,9]<stderr>:#015 89%|████████▉ | 9400/10570 [00:24<00:03, 383.35it/s][1,7]<stderr>:#015 93%|█████████▎| 9851/10570 [00:25<00:01, 396.87it/s][1,5]<stderr>:#015 89%|████████▊ | 9375/10570 [00:24<00:03, 384.83it/s][1,2]<stderr>:#015 91%|█████████ | 9633/10570 [00:25<00:02, 394.99it/s][1,12]<stderr>:#015 92%|█████████▏| 9752/10570 [00:25<00:01, 409.45it/s][1,11]<stderr>:#015 86%|████████▋ | 9135/10570 [00:24<00:03, 367.12it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9301/10570 [00:24<00:03, 385.56it/s]#033[A[1,3]<stderr>:#015 86%|████████▋ | 9122/10570 [00:24<00:03, 374.18it/s][1,15]<stderr>:#015 90%|████████▉ | 9488/10570 [00:24<00:02, 387.73it/s][1,1]<stderr>:#015 91%|█████████ | 9592/10570 [00:24<00:02, 397.52it/s][1,4]<stderr>:#015 90%|█████████ | 9565/10570 [00:24<00:02, 402.34it/s][1,14]<stderr>:#015 90%|████████▉ | 9504/10570 [00:24<00:02, 391.92it/s][1,6]<stderr>:#015 85%|████████▌ | 9028/10570 [00:24<00:04, 373.64it/s][1,10]<stderr>:#015 91%|█████████ | 9620/10570 [00:24<00:02, 400.12it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 80%|███████▉  | 8434/10570 [00:23<00:06, 352.24it/s]#033[A[1,13]<stderr>:#015 83%|████████▎ | 8824/10570 [00:24<00:04, 349.61it/s][1,9]<stderr>:#015 89%|████████▉ | 9440/10570 [00:24<00:02, 386.27it/s][1,7]<stderr>:#015 94%|█████████▎| 9893/10570 [00:25<00:01, 401.75it/s][1,5]<stderr>:#015 89%|████████▉ | 9414/10570 [00:24<00:03, 384.50it/s][1,2]<stderr>:#015 92%|█████████▏| 9674/10570 [00:25<00:02, 398.72it/s][1,11]<stderr>:#015 87%|████████▋ | 9172/10570 [00:24<00:03, 366.67it/s][1,12]<stderr>:#015 93%|█████████▎| 9793/10570 [00:25<00:01, 405.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 9340/10570 [00:24<00:03, 381.05it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9160/10570 [00:24<00:03, 373.19it/s][1,15]<stderr>:#015 90%|█████████ | 9530/10570 [00:24<00:02, 394.44it/s][1,1]<stderr>:#015 91%|█████████ | 9633/10570 [00:25<00:02, 400.84it/s][1,4]<stderr>:#015 91%|█████████ | 9606/10570 [00:25<00:02, 397.71it/s][1,14]<stderr>:#015 90%|█████████ | 9546/10570 [00:24<00:02, 398.47it/s][1,6]<stderr>:#015 86%|████████▌ | 9066/10570 [00:24<00:04, 372.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 80%|████████  | 8472/10570 [00:24<00:05, 358.89it/s]#033[A[1,10]<stderr>:#015 91%|█████████▏| 9661/10570 [00:25<00:02, 401.07it/s][1,13]<stderr>:#015 84%|████████▍ | 8861/10570 [00:24<00:04, 353.58it/s][1,9]<stderr>:#015 90%|████████▉ | 9480/10570 [00:24<00:02, 389.37it/s][1,7]<stderr>:#015 94%|█████████▍| 9935/10570 [00:25<00:01, 404.85it/s][1,5]<stderr>:#015 89%|████████▉ | 9455/10570 [00:24<00:02, 390.09it/s][1,2]<stderr>:#015 92%|█████████▏| 9717/10570 [00:25<00:02, 405.87it/s][1,11]<stderr>:#015 87%|████████▋ | 9211/10570 [00:24<00:03, 372.11it/s][1,12]<stderr>:#015 93%|█████████▎| 9834/10570 [00:25<00:01, 402.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▊ | 9379/10570 [00:24<00:03, 381.77it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9199/10570 [00:24<00:03, 377.82it/s][1,15]<stderr>:#015 91%|█████████ | 9571/10570 [00:25<00:02, 395.91it/s][1,1]<stderr>:#015 92%|█████████▏| 9674/10570 [00:25<00:02, 402.43it/s][1,4]<stderr>:#015 91%|█████████▏| 9647/10570 [00:25<00:02, 399.33it/s][1,14]<stderr>:#015 91%|█████████ | 9586/10570 [00:24<00:02, 397.34it/s][1,10]<stderr>:#015 92%|█████████▏| 9704/10570 [00:25<00:02, 406.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8509/10570 [00:24<00:05, 358.28it/s]#033[A[1,6]<stderr>:#015 86%|████████▌ | 9104/10570 [00:24<00:03, 369.42it/s][1,13]<stderr>:#015 84%|████████▍ | 8897/10570 [00:24<00:04, 354.09it/s][1,9]<stderr>:#015 90%|█████████ | 9521/10570 [00:25<00:02, 392.92it/s][1,5]<stderr>:#015 90%|████████▉ | 9495/10570 [00:24<00:02, 391.61it/s][1,7]<stderr>:#015 94%|█████████▍| 9976/10570 [00:25<00:01, 396.62it/s][1,2]<stderr>:#015 92%|█████████▏| 9759/10570 [00:25<00:01, 408.22it/s][1,11]<stderr>:#015 88%|████████▊ | 9251/10570 [00:24<00:03, 377.53it/s][1,12]<stderr>:#015 93%|█████████▎| 9875/10570 [00:25<00:01, 400.80it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▉ | 9418/10570 [00:24<00:03, 381.49it/s]#033[A[1,3]<stderr>:#015 87%|████████▋ | 9238/10570 [00:24<00:03, 380.72it/s][1,15]<stderr>:#015 91%|█████████ | 9611/10570 [00:25<00:02, 396.27it/s][1,1]<stderr>:#015 92%|█████████▏| 9717/10570 [00:25<00:02, 407.50it/s][1,4]<stderr>:#015 92%|█████████▏| 9688/10570 [00:25<00:02, 401.56it/s][1,14]<stderr>:#015 91%|█████████ | 9627/10570 [00:24<00:02, 400.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8547/10570 [00:24<00:05, 364.21it/s]#033[A[1,10]<stderr>:#015 92%|█████████▏| 9746/10570 [00:25<00:02, 407.65it/s][1,6]<stderr>:#015 86%|████████▋ | 9143/10570 [00:24<00:03, 372.57it/s][1,13]<stderr>:#015 85%|████████▍ | 8934/10570 [00:24<00:04, 357.22it/s][1,9]<stderr>:#015 90%|█████████ | 9563/10570 [00:25<00:02, 399.07it/s][1,5]<stderr>:#015 90%|█████████ | 9536/10570 [00:24<00:02, 395.40it/s][1,7]<stderr>:#015 95%|█████████▍| 10017/10570 [00:25<00:01, 398.78it/s][1,2]<stderr>:#015 93%|█████████▎| 9800/10570 [00:25<00:01, 402.37it/s][1,11]<stderr>:#015 88%|████████▊ | 9290/10570 [00:24<00:03, 379.12it/s][1,12]<stderr>:#015 94%|█████████▍| 9916/10570 [00:25<00:01, 398.16it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 89%|████████▉ | 9459/10570 [00:24<00:02, 388.18it/s]#033[A[1,3]<stderr>:#015 88%|████████▊ | 9278/10570 [00:24<00:03, 383.78it/s][1,15]<stderr>:#015 91%|█████████▏| 9651/10570 [00:25<00:02, 396.62it/s][1,1]<stderr>:#015 92%|█████████▏| 9758/10570 [00:25<00:01, 408.11it/s][1,4]<stderr>:#015 92%|█████████▏| 9731/10570 [00:25<00:02, 407.13it/s][1,14]<stderr>:#015 91%|█████████▏| 9668/10570 [00:25<00:02, 401.68it/s][1,10]<stderr>:#015 93%|█████████▎| 9787/10570 [00:25<00:01, 404.56it/s][1,6]<stderr>:#015 87%|████████▋ | 9181/10570 [00:24<00:03, 372.29it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 81%|████████  | 8584/10570 [00:24<00:05, 355.77it/s]#033[A[1,13]<stderr>:#015 85%|████████▍ | 8970/10570 [00:24<00:04, 355.28it/s][1,5]<stderr>:#015 91%|█████████ | 9576/10570 [00:25<00:02, 396.54it/s][1,9]<stderr>:#015 91%|█████████ | 9603/10570 [00:25<00:02, 393.88it/s][1,7]<stderr>:#015 95%|█████████▌| 10059/10570 [00:25<00:01, 404.35it/s][1,11]<stderr>:#015 88%|████████▊ | 9328/10570 [00:25<00:03, 375.06it/s][1,2]<stderr>:#015 93%|█████████▎| 9841/10570 [00:25<00:01, 395.74it/s][1,12]<stderr>:#015 94%|█████████▍| 9958/10570 [00:25<00:01, 401.93it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|████████▉ | 9498/10570 [00:25<00:02, 388.26it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9692/10570 [00:25<00:02, 399.32it/s][1,3]<stderr>:#015 88%|████████▊ | 9317/10570 [00:25<00:03, 375.97it/s][1,1]<stderr>:#015 93%|█████████▎| 9799/10570 [00:25<00:01, 402.49it/s][1,4]<stderr>:#015 92%|█████████▏| 9772/10570 [00:25<00:01, 406.91it/s][1,14]<stderr>:#015 92%|█████████▏| 9710/10570 [00:25<00:02, 406.65it/s][1,6]<stderr>:#015 87%|████████▋ | 9219/10570 [00:24<00:03, 373.91it/s][1,10]<stderr>:#015 93%|█████████▎| 9828/10570 [00:25<00:01, 403.83it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8621/10570 [00:24<00:05, 358.08it/s]#033[A[1,13]<stderr>:#015 85%|████████▌ | 9009/10570 [00:24<00:04, 363.06it/s][1,5]<stderr>:#015 91%|█████████ | 9616/10570 [00:25<00:02, 397.29it/s][1,9]<stderr>:#015 91%|█████████ | 9643/10570 [00:25<00:02, 395.61it/s][1,7]<stderr>:#015 96%|█████████▌| 10100/10570 [00:25<00:01, 402.44it/s][1,11]<stderr>:#015 89%|████████▊ | 9366/10570 [00:25<00:03, 376.04it/s][1,2]<stderr>:#015 93%|█████████▎| 9882/10570 [00:25<00:01, 398.21it/s][1,12]<stderr>:#015 95%|█████████▍| 9999/10570 [00:25<00:01, 393.46it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 90%|█████████ | 9539/10570 [00:25<00:02, 392.02it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9734/10570 [00:25<00:02, 404.13it/s][1,3]<stderr>:#015 89%|████████▊ | 9355/10570 [00:25<00:03, 376.27it/s][1,1]<stderr>:#015 93%|█████████▎| 9840/10570 [00:25<00:01, 396.02it/s][1,4]<stderr>:#015 93%|█████████▎| 9813/10570 [00:25<00:01, 402.60it/s][1,14]<stderr>:#015 92%|█████████▏| 9751/10570 [00:25<00:02, 405.89it/s][1,6]<stderr>:#015 88%|████████▊ | 9260/10570 [00:25<00:03, 381.15it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8657/10570 [00:24<00:05, 355.92it/s]#033[A[1,10]<stderr>:#015 93%|█████████▎| 9869/10570 [00:25<00:01, 395.78it/s][1,13]<stderr>:#015 86%|████████▌ | 9046/10570 [00:25<00:04, 358.57it/s][1,5]<stderr>:#015 91%|█████████▏| 9656/10570 [00:25<00:02, 397.01it/s][1,9]<stderr>:#015 92%|█████████▏| 9683/10570 [00:25<00:02, 396.86it/s][1,7]<stderr>:#015 96%|█████████▌| 10142/10570 [00:25<00:01, 406.62it/s][1,11]<stderr>:#015 89%|████████▉ | 9404/10570 [00:25<00:03, 374.82it/s][1,2]<stderr>:#015 94%|█████████▍| 9922/10570 [00:25<00:01, 398.03it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015 95%|█████████▍| 10041/10570 [00:25<00:01, 400.29it/s][1,0]<stderr>:#015 91%|█████████ | 9579/10570 [00:25<00:02, 394.09it/s]#033[A[1,15]<stderr>:#015 92%|█████████▏| 9775/10570 [00:25<00:01, 404.57it/s][1,3]<stderr>:#015 89%|████████▉ | 9393/10570 [00:25<00:03, 374.47it/s][1,1]<stderr>:#015 93%|█████████▎| 9881/10570 [00:25<00:01, 397.21it/s][1,4]<stderr>:#015 93%|█████████▎| 9854/10570 [00:25<00:01, 392.78it/s][1,14]<stderr>:#015 93%|█████████▎| 9792/10570 [00:25<00:01, 402.90it/s][1,6]<stderr>:#015 88%|████████▊ | 9299/10570 [00:25<00:03, 378.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 82%|████████▏ | 8695/10570 [00:24<00:05, 362.13it/s]#033[A[1,10]<stderr>:#015 94%|█████████▎| 9909/10570 [00:25<00:01, 394.20it/s][1,5]<stderr>:#015 92%|█████████▏| 9698/10570 [00:25<00:02, 402.16it/s][1,13]<stderr>:#015 86%|████████▌ | 9084/10570 [00:25<00:04, 362.24it/s][1,9]<stderr>:#015 92%|█████████▏| 9726/10570 [00:25<00:02, 403.45it/s][1,7]<stderr>:#015 96%|█████████▋| 10183/10570 [00:25<00:00, 394.48it/s][1,2]<stderr>:#015 94%|█████████▍| 9963/10570 [00:25<00:01, 400.33it/s][1,11]<stderr>:#015 89%|████████▉ | 9444/10570 [00:25<00:02, 379.47it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 91%|█████████ | 9619/10570 [00:25<00:02, 393.60it/s]#033[A[1,12]<stderr>:#015 95%|█████████▌| 10082/10570 [00:25<00:01, 398.81it/s][1,3]<stderr>:#015 89%|████████▉ | 9432/10570 [00:25<00:03, 376.69it/s][1,15]<stderr>:#015 93%|█████████▎| 9816/10570 [00:25<00:01, 395.03it/s][1,1]<stderr>:#015 94%|█████████▍| 9921/10570 [00:25<00:01, 396.09it/s][1,4]<stderr>:#015 94%|█████████▎| 9894/10570 [00:25<00:01, 393.38it/s][1,14]<stderr>:#015 93%|█████████▎| 9833/10570 [00:25<00:01, 400.10it/s][1,6]<stderr>:#015 88%|████████▊ | 9337/10570 [00:25<00:03, 373.81it/s][1,10]<stderr>:#015 94%|█████████▍| 9951/10570 [00:25<00:01, 399.24it/s][1,5]<stderr>:#015 92%|█████████▏| 9739/10570 [00:25<00:02, 404.24it/s][1,13]<stderr>:#015 86%|████████▋ | 9121/10570 [00:25<00:04, 362.08it/s][1,9]<stderr>:#015 92%|█████████▏| 9767/10570 [00:25<00:01, 404.13it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8732/10570 [00:24<00:05, 353.51it/s]#033[A[1,7]<stderr>:#015 97%|█████████▋| 10224/10570 [00:26<00:00, 398.17it/s][1,11]<stderr>:#015 90%|████████▉ | 9483/10570 [00:25<00:02, 381.71it/s][1,2]<stderr>:#015 95%|█████████▍| 10004/10570 [00:25<00:01, 391.52it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 91%|█████████▏| 9659/10570 [00:25<00:02, 393.77it/s]#033[A[1,12]<stderr>:#015 96%|█████████▌| 10124/10570 [00:26<00:01, 402.84it/s][1,3]<stderr>:#015 90%|████████▉ | 9472/10570 [00:25<00:02, 381.52it/s][1,15]<stderr>:#015 93%|█████████▎| 9856/10570 [00:25<00:01, 388.23it/s][1,1]<stderr>:#015 94%|█████████▍| 9962/10570 [00:25<00:01, 398.67it/s][1,4]<stderr>:#015 94%|█████████▍| 9935/10570 [00:25<00:01, 396.52it/s][1,14]<stderr>:#015 93%|█████████▎| 9874/10570 [00:25<00:01, 398.33it/s][1,6]<stderr>:#015 89%|████████▊ | 9375/10570 [00:25<00:03, 374.76it/s][1,5]<stderr>:#015 93%|█████████▎| 9780/10570 [00:25<00:01, 402.99it/s][1,10]<stderr>:#015 95%|█████████▍| 9991/10570 [00:25<00:01, 390.91it/s][1,13]<stderr>:#015 87%|████████▋ | 9158/10570 [00:25<00:03, 361.65it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8768/10570 [00:24<00:05, 350.96it/s]#033[A[1,9]<stderr>:#015 93%|█████████▎| 9808/10570 [00:25<00:01, 399.87it/s][1,7]<stderr>:#015 97%|█████████▋| 10264/10570 [00:26<00:00, 394.75it/s][1,11]<stderr>:#015 90%|█████████ | 9523/10570 [00:25<00:02, 386.42it/s][1,2]<stderr>:#015 95%|█████████▌| 10046/10570 [00:26<00:01, 397.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 92%|█████████▏| 9700/10570 [00:25<00:02, 398.46it/s]#033[A[1,3]<stderr>:#015 90%|████████▉ | 9511/10570 [00:25<00:02, 380.81it/s][1,12]<stderr>:#015 96%|█████████▌| 10165/10570 [00:26<00:01, 390.39it/s][1,15]<stderr>:#015 94%|█████████▎| 9896/10570 [00:25<00:01, 390.58it/s][1,1]<stderr>:#015 95%|█████████▍| 10002/10570 [00:25<00:01, 382.76it/s][1,4]<stderr>:#015 94%|█████████▍| 9975/10570 [00:25<00:01, 389.20it/s][1,14]<stderr>:#015 94%|█████████▍| 9914/10570 [00:25<00:01, 395.25it/s][1,6]<stderr>:#015 89%|████████▉ | 9413/10570 [00:25<00:03, 373.77it/s][1,10]<stderr>:#015 95%|█████████▍| 10033/10570 [00:26<00:01, 398.61it/s][1,13]<stderr>:#015 87%|████████▋ | 9196/10570 [00:25<00:03, 365.64it/s][1,5]<stderr>:#015 93%|█████████▎| 9821/10570 [00:25<00:01, 399.20it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 83%|████████▎ | 8804/10570 [00:24<00:05, 346.63it/s]#033[A[1,9]<stderr>:#015 93%|█████████▎| 9849/10570 [00:25<00:01, 388.99it/s][1,7]<stderr>:#015 97%|█████████▋| 10305/10570 [00:26<00:00, 397.37it/s][1,11]<stderr>:#015 90%|█████████ | 9564/10570 [00:25<00:02, 391.70it/s][1,2]<stderr>:#015 95%|█████████▌| 10086/10570 [00:26<00:01, 396.09it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 92%|█████████▏| 9741/10570 [00:25<00:02, 401.84it/s]#033[A[1,3]<stderr>:#015 90%|█████████ | 9552/10570 [00:25<00:02, 388.76it/s][1,12]<stderr>:#015 97%|█████████▋| 10206/10570 [00:26<00:00, 395.57it/s][1,15]<stderr>:#015 94%|█████████▍| 9937/10570 [00:26<00:01, 394.36it/s][1,1]<stderr>:#015 95%|█████████▌| 10043/10570 [00:26<00:01, 390.31it/s][1,4]<stderr>:#015 95%|█████████▍| 10015/10570 [00:26<00:01, 391.76it/s][1,14]<stderr>:#015 94%|█████████▍| 9956/10570 [00:25<00:01, 400.86it/s][1,6]<stderr>:#015 89%|████████▉ | 9453/10570 [00:25<00:02, 378.60it/s][1,10]<stderr>:#015 95%|█████████▌| 10073/10570 [00:26<00:01, 398.60it/s][1,13]<stderr>:#015 87%|████████▋ | 9234/10570 [00:25<00:03, 368.25it/s][1,5]<stderr>:#015 93%|█████████▎| 9861/10570 [00:25<00:01, 391.42it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▎ | 8839/10570 [00:25<00:05, 345.86it/s]#033[A[1,9]<stderr>:#015 94%|█████████▎| 9890/10570 [00:26<00:01, 393.51it/s][1,7]<stderr>:#015 98%|█████████▊| 10347/10570 [00:26<00:00, 401.81it/s][1,11]<stderr>:#015 91%|█████████ | 9604/10570 [00:25<00:02, 387.19it/s][1,2]<stderr>:#015 96%|█████████▌| 10128/10570 [00:26<00:01, 400.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9782/10570 [00:25<00:01, 399.11it/s][1,0]<stderr>:#033[A[1,12]<stderr>:#015 97%|█████████▋| 10247/10570 [00:26<00:00, 398.78it/s][1,3]<stderr>:#015 91%|█████████ | 9591/10570 [00:25<00:02, 384.61it/s][1,15]<stderr>:#015 94%|█████████▍| 9977/10570 [00:26<00:01, 387.72it/s][1,1]<stderr>:#015 95%|█████████▌| 10083/10570 [00:26<00:01, 391.24it/s][1,4]<stderr>:#015 95%|█████████▌| 10056/10570 [00:26<00:01, 396.82it/s][1,14]<stderr>:#015 95%|█████████▍| 9997/10570 [00:25<00:01, 390.58it/s][1,6]<stderr>:#015 90%|████████▉ | 9492/10570 [00:25<00:02, 380.93it/s][1,13]<stderr>:#015 88%|████████▊ | 9272/10570 [00:25<00:03, 371.37it/s][1,10]<stderr>:#015 96%|█████████▌| 10113/10570 [00:26<00:01, 390.01it/s][1,5]<stderr>:#015 94%|█████████▎| 9901/10570 [00:25<00:01, 392.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▍ | 8876/10570 [00:25<00:04, 350.87it/s]#033[A[1,9]<stderr>:#015 94%|█████████▍| 9930/10570 [00:26<00:01, 394.05it/s][1,7]<stderr>:#015 98%|█████████▊| 10388/10570 [00:26<00:00, 395.91it/s][1,11]<stderr>:#015 91%|█████████ | 9643/10570 [00:25<00:02, 387.55it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9822/10570 [00:25<00:01, 397.69it/s]#033[A[1,2]<stderr>:#015 96%|█████████▌| 10169/10570 [00:26<00:01, 389.10it/s][1,3]<stderr>:#015 91%|█████████ | 9630/10570 [00:25<00:02, 385.73it/s][1,12]<stderr>:#015 97%|█████████▋| 10287/10570 [00:26<00:00, 394.77it/s][1,15]<stderr>:#015 95%|█████████▍| 10016/10570 [00:26<00:01, 385.05it/s][1,1]<stderr>:#015 96%|█████████▌| 10124/10570 [00:26<00:01, 396.05it/s][1,4]<stderr>:#015 96%|█████████▌| 10096/10570 [00:26<00:01, 393.83it/s][1,14]<stderr>:#015 95%|█████████▍| 10038/10570 [00:26<00:01, 394.80it/s][1,6]<stderr>:#015 90%|█████████ | 9532/10570 [00:25<00:02, 385.29it/s][1,10]<stderr>:#015 96%|█████████▌| 10153/10570 [00:26<00:01, 392.48it/s][1,5]<stderr>:#015 94%|█████████▍| 9941/10570 [00:25<00:01, 392.94it/s][1,13]<stderr>:#015 88%|████████▊ | 9310/10570 [00:25<00:03, 361.11it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 84%|████████▍ | 8912/10570 [00:25<00:04, 348.08it/s]#033[A[1,9]<stderr>:#015 94%|█████████▍| 9970/10570 [00:26<00:01, 384.28it/s][1,7]<stderr>:#015 99%|█████████▊| 10428/10570 [00:26<00:00, 392.68it/s][1,11]<stderr>:#015 92%|█████████▏| 9682/10570 [00:25<00:02, 385.59it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 93%|█████████▎| 9862/10570 [00:25<00:01, 389.39it/s]#033[A[1,2]<stderr>:#015 97%|█████████▋| 10210/10570 [00:26<00:00, 393.61it/s][1,3]<stderr>:#015 91%|█████████▏| 9670/10570 [00:25<00:02, 388.77it/s][1,12]<stderr>:#015 98%|█████████▊| 10327/10570 [00:26<00:00, 394.42it/s][1,15]<stderr>:#015 95%|█████████▌| 10057/10570 [00:26<00:01, 389.79it/s][1,4]<stderr>:#015 96%|█████████▌| 10137/10570 [00:26<00:01, 397.77it/s][1,1]<stderr>:#015 96%|█████████▌| 10164/10570 [00:26<00:01, 384.04it/s][1,14]<stderr>:#015 95%|█████████▌| 10078/10570 [00:26<00:01, 389.50it/s][1,6]<stderr>:#015 91%|█████████ | 9571/10570 [00:25<00:02, 386.03it/s][1,10]<stderr>:#015 96%|█████████▋| 10193/10570 [00:26<00:00, 382.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▍ | 8947/10570 [00:25<00:04, 347.76it/s]#033[A[1,5]<stderr>:#015 94%|█████████▍| 9981/10570 [00:26<00:01, 384.84it/s][1,13]<stderr>:#015 88%|████████▊ | 9347/10570 [00:25<00:03, 356.60it/s][1,9]<stderr>:#015 95%|█████████▍| 10009/10570 [00:26<00:01, 385.05it/s][1,11]<stderr>:#015 92%|█████████▏| 9721/10570 [00:26<00:02, 385.56it/s][1,7]<stderr>:#015 99%|█████████▉| 10468/10570 [00:26<00:00, 387.92it/s][1,2]<stderr>:#015 97%|█████████▋| 10250/10570 [00:26<00:00, 395.20it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▎| 9902/10570 [00:26<00:01, 389.40it/s]#033[A[1,3]<stderr>:#015 92%|█████████▏| 9711/10570 [00:26<00:02, 394.19it/s][1,12]<stderr>:#015 98%|█████████▊| 10367/10570 [00:26<00:00, 391.86it/s][1,15]<stderr>:#015 96%|█████████▌| 10097/10570 [00:26<00:01, 378.20it/s][1,1]<stderr>:#015 97%|█████████▋| 10205/10570 [00:26<00:00, 390.01it/s][1,4]<stderr>:#015 96%|█████████▋| 10177/10570 [00:26<00:01, 386.06it/s][1,14]<stderr>:#015 96%|█████████▌| 10118/10570 [00:26<00:01, 392.24it/s][1,6]<stderr>:#015 91%|█████████ | 9610/10570 [00:25<00:02, 384.97it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▍ | 8983/10570 [00:25<00:04, 350.74it/s]#033[A[1,10]<stderr>:#015 97%|█████████▋| 10233/10570 [00:26<00:00, 386.15it/s][1,5]<stderr>:#015 95%|█████████▍| 10021/10570 [00:26<00:01, 389.12it/s][1,13]<stderr>:#015 89%|████████▉ | 9384/10570 [00:25<00:03, 357.80it/s][1,9]<stderr>:#015 95%|█████████▌| 10049/10570 [00:26<00:01, 388.31it/s][1,11]<stderr>:#015 92%|█████████▏| 9761/10570 [00:26<00:02, 387.54it/s][1,7]<stderr>:#015 99%|█████████▉| 10509/10570 [00:26<00:00, 393.13it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▍| 9942/10570 [00:26<00:01, 391.36it/s]#033[A[1,2]<stderr>:#015 97%|█████████▋| 10290/10570 [00:26<00:00, 393.12it/s][1,3]<stderr>:#015 92%|█████████▏| 9751/10570 [00:26<00:02, 393.04it/s][1,12]<stderr>:#015 98%|█████████▊| 10407/10570 [00:26<00:00, 386.56it/s][1,15]<stderr>:#015 96%|█████████▌| 10138/10570 [00:26<00:01, 384.97it/s][1,1]<stderr>:#015 97%|█████████▋| 10246/10570 [00:26<00:00, 392.76it/s][1,4]<stderr>:#015 97%|█████████▋| 10218/10570 [00:26<00:00, 390.56it/s][1,14]<stderr>:#015 96%|█████████▌| 10158/10570 [00:26<00:01, 391.15it/s][1,6]<stderr>:#015 91%|█████████▏| 9649/10570 [00:26<00:02, 384.31it/s][1,10]<stderr>:#015 97%|█████████▋| 10272/10570 [00:26<00:00, 385.38it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 85%|████████▌ | 9020/10570 [00:25<00:04, 353.76it/s]#033[A[1,13]<stderr>:#015 89%|████████▉ | 9421/10570 [00:26<00:03, 359.35it/s][1,5]<stderr>:#015 95%|█████████▌| 10060/10570 [00:26<00:01, 382.85it/s][1,9]<stderr>:#015 95%|█████████▌| 10088/10570 [00:26<00:01, 386.43it/s][1,7]<stderr>:#015100%|█████████▉| 10550/10570 [00:26<00:00, 396.19it/s][1,11]<stderr>:#015 93%|█████████▎| 9800/10570 [00:26<00:02, 381.68it/s][1,2]<stderr>:#015 98%|█████████▊| 10330/10570 [00:26<00:00, 390.19it/s][1,3]<stderr>:#015 93%|█████████▎| 9791/10570 [00:26<00:02, 387.63it/s][1,12]<stderr>:#015 99%|█████████▉| 10446/10570 [00:26<00:00, 386.50it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 94%|█████████▍| 9982/10570 [00:26<00:01, 376.69it/s]#033[A[1,7]<stderr>:#015100%|██████████| 10570/10570 [00:26<00:00, 392.51it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 96%|█████████▋| 10177/10570 [00:26<00:01, 375.73it/s][1,4]<stderr>:#015 97%|█████████▋| 10258/10570 [00:26<00:00, 388.52it/s][1,6]<stderr>:#015 92%|█████████▏| 9688/10570 [00:26<00:02, 383.20it/s][1,14]<stderr>:#015 96%|█████████▋| 10198/10570 [00:26<00:00, 380.98it/s][1,1]<stderr>:#015 97%|█████████▋| 10286/10570 [00:26<00:00, 358.49it/s][1,10]<stderr>:#015 98%|█████████▊| 10312/10570 [00:26<00:00, 387.81it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 86%|████████▌ | 9056/10570 [00:25<00:04, 351.04it/s]#033[A[1,13]<stderr>:#015 89%|████████▉ | 9460/10570 [00:26<00:03, 367.16it/s][1,5]<stderr>:#015 96%|█████████▌| 10099/10570 [00:26<00:01, 383.44it/s][1,9]<stderr>:#015 96%|█████████▌| 10129/10570 [00:26<00:01, 391.03it/s][1,11]<stderr>:#015 93%|█████████▎| 9839/10570 [00:26<00:01, 376.50it/s][1,2]<stderr>:#015 98%|█████████▊| 10370/10570 [00:26<00:00, 390.85it/s][1,3]<stderr>:#015 93%|█████████▎| 9830/10570 [00:26<00:01, 385.84it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 95%|█████████▍| 10022/10570 [00:26<00:01, 383.16it/s]#033[A[1,12]<stderr>:#015 99%|█████████▉| 10485/10570 [00:26<00:00, 382.54it/s][1,4]<stderr>:#015 97%|█████████▋| 10297/10570 [00:26<00:00, 388.92it/s][1,15]<stderr>:#015 97%|█████████▋| 10218/10570 [00:26<00:00, 382.99it/s][1,6]<stderr>:#015 92%|█████████▏| 9729/10570 [00:26<00:02, 389.00it/s][1,14]<stderr>:#015 97%|█████████▋| 10238/10570 [00:26<00:00, 385.33it/s][1,1]<stderr>:#015 98%|█████████▊| 10325/10570 [00:26<00:00, 366.44it/s][1,10]<stderr>:#015 98%|█████████▊| 10352/10570 [00:26<00:00, 389.85it/s][1,13]<stderr>:#015 90%|████████▉ | 9497/10570 [00:26<00:02, 366.60it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015 96%|█████████▌| 10139/10570 [00:26<00:01, 387.69it/s][1,8]<stderr>:#015 86%|████████▌ | 9092/10570 [00:25<00:04, 351.64it/s]#033[A[1,9]<stderr>:#015 96%|█████████▌| 10169/10570 [00:26<00:01, 380.37it/s][1,11]<stderr>:#015 93%|█████████▎| 9878/10570 [00:26<00:01, 378.32it/s][1,2]<stderr>:#015 98%|█████████▊| 10410/10570 [00:26<00:00, 385.29it/s][1,12]<stderr>:#015100%|█████████▉| 10526/10570 [00:27<00:00, 389.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 95%|█████████▌| 10061/10570 [00:26<00:01, 378.94it/s]#033[A[1,3]<stderr>:#015 93%|█████████▎| 9869/10570 [00:26<00:01, 379.17it/s][1,4]<stderr>:#015 98%|█████████▊| 10336/10570 [00:26<00:00, 387.57it/s][1,15]<stderr>:#015 97%|█████████▋| 10257/10570 [00:26<00:00, 383.91it/s][1,6]<stderr>:#015 92%|█████████▏| 9769/10570 [00:26<00:02, 389.42it/s][1,14]<stderr>:#015 97%|█████████▋| 10277/10570 [00:26<00:00, 383.73it/s][1,1]<stderr>:#015 98%|█████████▊| 10364/10570 [00:26<00:00, 371.95it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 86%|████████▋ | 9128/10570 [00:25<00:04, 352.44it/s]#033[A[1,13]<stderr>:#015 90%|█████████ | 9536/10570 [00:26<00:02, 371.02it/s][1,10]<stderr>:#015 98%|█████████▊| 10392/10570 [00:26<00:00, 384.51it/s][1,5]<stderr>:#015 96%|█████████▋| 10178/10570 [00:26<00:01, 378.31it/s][1,9]<stderr>:#015 97%|█████████▋| 10209/10570 [00:26<00:00, 384.92it/s][1,11]<stderr>:#015 94%|█████████▍| 9916/10570 [00:26<00:01, 373.85it/s][1,2]<stderr>:#015 99%|█████████▉| 10449/10570 [00:27<00:00, 383.63it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▌| 10100/10570 [00:26<00:01, 381.52it/s]#033[A[1,12]<stderr>:#015100%|█████████▉| 10568/10570 [00:27<00:00, 395.87it/s][1,3]<stderr>:#015 94%|█████████▎| 9907/10570 [00:26<00:01, 377.76it/s][1,12]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 389.33it/s]\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 98%|█████████▊| 10375/10570 [00:26<00:00, 387.34it/s][1,15]<stderr>:#015 97%|█████████▋| 10296/10570 [00:26<00:00, 385.05it/s][1,6]<stderr>:#015 93%|█████████▎| 9808/10570 [00:26<00:01, 385.52it/s][1,14]<stderr>:#015 98%|█████████▊| 10316/10570 [00:26<00:00, 382.95it/s][1,1]<stderr>:#015 98%|█████████▊| 10402/10570 [00:27<00:00, 370.05it/s][1,13]<stderr>:#015 91%|█████████ | 9574/10570 [00:26<00:02, 372.68it/s][1,10]<stderr>:#015 99%|█████████▊| 10431/10570 [00:27<00:00, 384.23it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9164/10570 [00:25<00:04, 350.39it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10216/10570 [00:26<00:00, 369.49it/s][1,9]<stderr>:#015 97%|█████████▋| 10249/10570 [00:26<00:00, 386.45it/s][1,11]<stderr>:#015 94%|█████████▍| 9956/10570 [00:26<00:01, 379.81it/s][1,2]<stderr>:#015 99%|█████████▉| 10488/10570 [00:27<00:00, 380.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▌| 10140/10570 [00:26<00:01, 386.32it/s]#033[A[1,3]<stderr>:#015 94%|█████████▍| 9947/10570 [00:26<00:01, 382.94it/s][1,15]<stderr>:#015 98%|█████████▊| 10335/10570 [00:27<00:00, 383.90it/s][1,4]<stderr>:#015 99%|█████████▊| 10414/10570 [00:27<00:00, 383.99it/s][1,14]<stderr>:#015 98%|█████████▊| 10355/10570 [00:26<00:00, 384.74it/s][1,6]<stderr>:#015 93%|█████████▎| 9847/10570 [00:26<00:01, 373.93it/s][1,1]<stderr>:#015 99%|█████████▉| 10441/10570 [00:27<00:00, 374.08it/s][1,13]<stderr>:#015 91%|█████████ | 9612/10570 [00:26<00:02, 372.69it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9201/10570 [00:26<00:03, 355.50it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10254/10570 [00:26<00:00, 367.78it/s][1,9]<stderr>:#015 97%|█████████▋| 10288/10570 [00:27<00:00, 385.25it/s][1,10]<stderr>:#015 99%|█████████▉| 10470/10570 [00:27<00:00, 365.79it/s][1,11]<stderr>:#015 95%|█████████▍| 9995/10570 [00:26<00:01, 370.05it/s][1,2]<stderr>:#015100%|█████████▉| 10529/10570 [00:27<00:00, 387.43it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 96%|█████████▋| 10179/10570 [00:26<00:01, 375.88it/s]#033[A[1,3]<stderr>:#015 94%|█████████▍| 9986/10570 [00:26<00:01, 374.93it/s][1,15]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 385.34it/s][1,4]<stderr>:#015 99%|█████████▉| 10453/10570 [00:27<00:00, 381.55it/s][1,14]<stderr>:#015 98%|█████████▊| 10394/10570 [00:26<00:00, 380.07it/s][1,6]<stderr>:#015 94%|█████████▎| 9887/10570 [00:26<00:01, 379.74it/s][1,1]<stderr>:#015 99%|█████████▉| 10479/10570 [00:27<00:00, 371.69it/s][1,13]<stderr>:#015 91%|█████████▏| 9650/10570 [00:26<00:02, 372.40it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 87%|████████▋ | 9238/10570 [00:26<00:03, 358.49it/s]#033[A[1,5]<stderr>:#015 97%|█████████▋| 10293/10570 [00:26<00:00, 372.88it/s][1,10]<stderr>:#015 99%|█████████▉| 10510/10570 [00:27<00:00, 375.14it/s][1,9]<stderr>:#015 98%|█████████▊| 10328/10570 [00:27<00:00, 386.56it/s][1,11]<stderr>:#015 95%|█████████▍| 10036/10570 [00:26<00:01, 378.83it/s][1,2]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 386.09it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10219/10570 [00:26<00:00, 381.68it/s]#033[A[1,3]<stderr>:#015 95%|█████████▍| 10026/10570 [00:26<00:01, 380.58it/s][1,15]<stderr>:#015 99%|█████████▊| 10414/10570 [00:27<00:00, 383.19it/s][1,4]<stderr>:#015 99%|█████████▉| 10492/10570 [00:27<00:00, 377.38it/s][1,14]<stderr>:#015 99%|█████████▊| 10433/10570 [00:27<00:00, 382.18it/s][1,6]<stderr>:#015 94%|█████████▍| 9926/10570 [00:26<00:01, 379.67it/s][1,1]<stderr>:#015100%|█████████▉| 10520/10570 [00:27<00:00, 380.51it/s][1,13]<stderr>:#015 92%|█████████▏| 9689/10570 [00:26<00:02, 375.80it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9275/10570 [00:26<00:03, 360.50it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10332/10570 [00:27<00:00, 375.44it/s][1,9]<stderr>:#015 98%|█████████▊| 10367/10570 [00:27<00:00, 386.20it/s][1,10]<stderr>:#015100%|█████████▉| 10551/10570 [00:27<00:00, 382.76it/s][1,11]<stderr>:#015 95%|█████████▌| 10074/10570 [00:26<00:01, 377.26it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10258/10570 [00:27<00:00, 382.01it/s]#033[A[1,3]<stderr>:#015 95%|█████████▌| 10065/10570 [00:27<00:01, 381.06it/s][1,10]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 385.40it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 99%|█████████▉| 10453/10570 [00:27<00:00, 377.94it/s][1,4]<stderr>:#015100%|█████████▉| 10533/10570 [00:27<00:00, 384.42it/s][1,14]<stderr>:#015 99%|█████████▉| 10472/10570 [00:27<00:00, 374.58it/s][1,6]<stderr>:#015 94%|█████████▍| 9965/10570 [00:26<00:01, 378.97it/s][1,1]<stderr>:#015100%|█████████▉| 10561/10570 [00:27<00:00, 387.55it/s][1,13]<stderr>:#015 92%|█████████▏| 9728/10570 [00:26<00:02, 379.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9312/10570 [00:26<00:03, 357.15it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10371/10570 [00:27<00:00, 378.61it/s][1,1]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 384.73it/s]\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015 98%|█████████▊| 10406/10570 [00:27<00:00, 380.23it/s][1,11]<stderr>:#015 96%|█████████▌| 10112/10570 [00:27<00:01, 373.10it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 97%|█████████▋| 10297/10570 [00:27<00:00, 383.31it/s]#033[A[1,3]<stderr>:#015 96%|█████████▌| 10104/10570 [00:27<00:01, 382.64it/s][1,4]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 384.64it/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015 99%|█████████▉| 10491/10570 [00:27<00:00, 373.90it/s][1,14]<stderr>:#015 99%|█████████▉| 10513/10570 [00:27<00:00, 382.70it/s][1,13]<stderr>:#015 92%|█████████▏| 9766/10570 [00:27<00:02, 379.03it/s][1,6]<stderr>:#015 95%|█████████▍| 10003/10570 [00:27<00:01, 369.44it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 88%|████████▊ | 9348/10570 [00:26<00:03, 350.84it/s]#033[A[1,5]<stderr>:#015 98%|█████████▊| 10409/10570 [00:27<00:00, 376.11it/s][1,9]<stderr>:#015 99%|█████████▉| 10445/10570 [00:27<00:00, 379.44it/s][1,11]<stderr>:#015 96%|█████████▌| 10150/10570 [00:27<00:01, 364.47it/s][1,3]<stderr>:#015 96%|█████████▌| 10144/10570 [00:27<00:01, 385.06it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 98%|█████████▊| 10336/10570 [00:27<00:00, 378.01it/s]#033[A[1,15]<stderr>:#015100%|█████████▉| 10532/10570 [00:27<00:00, 382.12it/s][1,14]<stderr>:#015100%|█████████▉| 10554/10570 [00:27<00:00, 389.69it/s][1,13]<stderr>:#015 93%|█████████▎| 9804/10570 [00:27<00:02, 373.99it/s][1,6]<stderr>:#015 95%|█████████▌| 10043/10570 [00:27<00:01, 376.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9384/10570 [00:26<00:03, 352.95it/s]#033[A[1,5]<stderr>:#015 99%|█████████▉| 10447/10570 [00:27<00:00, 375.52it/s][1,9]<stderr>:#015 99%|█████████▉| 10483/10570 [00:27<00:00, 375.36it/s][1,14]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 385.88it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 96%|█████████▋| 10187/10570 [00:27<00:01, 360.01it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 379.56it/s]#033[A[1,3]<stderr>:#015 96%|█████████▋| 10183/10570 [00:27<00:01, 373.70it/s][1,15]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 382.12it/s]\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:#015 95%|█████████▌| 10081/10570 [00:27<00:01, 376.30it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9420/10570 [00:26<00:03, 353.58it/s]#033[A[1,13]<stderr>:#015 93%|█████████▎| 9842/10570 [00:27<00:01, 364.49it/s][1,5]<stderr>:#015 99%|█████████▉| 10485/10570 [00:27<00:00, 373.12it/s][1,9]<stderr>:#015100%|█████████▉| 10524/10570 [00:27<00:00, 382.70it/s][1,11]<stderr>:#015 97%|█████████▋| 10227/10570 [00:27<00:00, 368.94it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▊| 10413/10570 [00:27<00:00, 374.81it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10222/10570 [00:27<00:00, 377.96it/s][1,6]<stderr>:#015 96%|█████████▌| 10120/10570 [00:27<00:01, 379.94it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 89%|████████▉ | 9458/10570 [00:26<00:03, 360.79it/s]#033[A[1,13]<stderr>:#015 93%|█████████▎| 9880/10570 [00:27<00:01, 368.46it/s][1,5]<stderr>:#015100%|█████████▉| 10525/10570 [00:27<00:00, 379.40it/s][1,9]<stderr>:#015100%|█████████▉| 10565/10570 [00:27<00:00, 389.89it/s][1,9]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 380.61it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 97%|█████████▋| 10265/10570 [00:27<00:00, 368.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▉| 10451/10570 [00:27<00:00, 373.68it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10260/10570 [00:27<00:00, 375.52it/s][1,6]<stderr>:#015 96%|█████████▌| 10159/10570 [00:27<00:01, 382.36it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 90%|████████▉ | 9495/10570 [00:26<00:02, 361.73it/s]#033[A[1,13]<stderr>:#015 94%|█████████▍| 9917/10570 [00:27<00:01, 366.36it/s][1,5]<stderr>:#015100%|█████████▉| 10567/10570 [00:27<00:00, 388.23it/s][1,5]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 382.20it/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015 97%|█████████▋| 10304/10570 [00:27<00:00, 372.71it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 99%|█████████▉| 10489/10570 [00:27<00:00, 371.51it/s]#033[A[1,3]<stderr>:#015 97%|█████████▋| 10299/10570 [00:27<00:00, 377.28it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 90%|█████████ | 9533/10570 [00:27<00:02, 365.73it/s]#033[A[1,6]<stderr>:#015 96%|█████████▋| 10198/10570 [00:27<00:01, 371.58it/s][1,13]<stderr>:#015 94%|█████████▍| 9956/10570 [00:27<00:01, 371.47it/s][1,11]<stderr>:#015 98%|█████████▊| 10343/10570 [00:27<00:00, 376.51it/s][1,3]<stderr>:#015 98%|█████████▊| 10337/10570 [00:27<00:00, 377.19it/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|█████████▉| 10530/10570 [00:27<00:00, 379.41it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9570/10570 [00:27<00:02, 366.80it/s]#033[A[1,6]<stderr>:#015 97%|█████████▋| 10237/10570 [00:27<00:00, 374.88it/s][1,13]<stderr>:#015 95%|█████████▍| 9994/10570 [00:27<00:01, 360.68it/s][1,11]<stderr>:#015 98%|█████████▊| 10381/10570 [00:27<00:00, 373.71it/s][1,0]<stderr>:#015100%|██████████| 10570/10570 [00:27<00:00, 379.53it/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 98%|█████████▊| 10375/10570 [00:27<00:00, 377.19it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9607/10570 [00:27<00:02, 364.20it/s]#033[A[1,6]<stderr>:#015 97%|█████████▋| 10275/10570 [00:27<00:00, 373.22it/s][1,13]<stderr>:#015 95%|█████████▍| 10033/10570 [00:27<00:01, 368.91it/s][1,11]<stderr>:#015 99%|█████████▊| 10419/10570 [00:27<00:00, 371.57it/s][1,3]<stderr>:#015 99%|█████████▊| 10413/10570 [00:27<00:00, 373.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 91%|█████████ | 9645/10570 [00:27<00:02, 366.24it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10313/10570 [00:27<00:00, 374.75it/s][1,13]<stderr>:#015 95%|█████████▌| 10071/10570 [00:27<00:01, 368.15it/s][1,3]<stderr>:#015 99%|█████████▉| 10451/10570 [00:28<00:00, 371.71it/s][1,11]<stderr>:#015 99%|█████████▉| 10457/10570 [00:28<00:00, 360.73it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9682/10570 [00:27<00:02, 366.11it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10352/10570 [00:27<00:00, 375.72it/s][1,13]<stderr>:#015 96%|█████████▌| 10109/10570 [00:27<00:01, 369.02it/s][1,3]<stderr>:#015 99%|█████████▉| 10489/10570 [00:28<00:00, 367.78it/s][1,11]<stderr>:#015 99%|█████████▉| 10494/10570 [00:28<00:00, 358.25it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9721/10570 [00:27<00:02, 372.27it/s]#033[A[1,6]<stderr>:#015 98%|█████████▊| 10390/10570 [00:28<00:00, 371.39it/s][1,13]<stderr>:#015 96%|█████████▌| 10147/10570 [00:28<00:01, 370.76it/s][1,3]<stderr>:#015100%|█████████▉| 10529/10570 [00:28<00:00, 374.64it/s][1,11]<stderr>:#015100%|█████████▉| 10534/10570 [00:28<00:00, 367.98it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 92%|█████████▏| 9759/10570 [00:27<00:02, 373.46it/s]#033[A[1,6]<stderr>:#015 99%|█████████▊| 10428/10570 [00:28<00:00, 367.94it/s][1,13]<stderr>:#015 96%|█████████▋| 10185/10570 [00:28<00:01, 361.85it/s][1,11]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 373.07it/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015100%|█████████▉| 10568/10570 [00:28<00:00, 379.04it/s][1,3]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 372.82it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9797/10570 [00:27<00:02, 366.63it/s]#033[A[1,6]<stderr>:#015 99%|█████████▉| 10465/10570 [00:28<00:00, 363.11it/s][1,13]<stderr>:#015 97%|█████████▋| 10224/10570 [00:28<00:00, 368.18it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9834/10570 [00:27<00:02, 358.27it/s]#033[A[1,6]<stderr>:#015 99%|█████████▉| 10503/10570 [00:28<00:00, 367.40it/s][1,13]<stderr>:#015 97%|█████████▋| 10261/10570 [00:28<00:00, 365.66it/s][1,6]<stderr>:#015100%|█████████▉| 10543/10570 [00:28<00:00, 374.88it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 93%|█████████▎| 9870/10570 [00:27<00:01, 355.74it/s]#033[A[1,13]<stderr>:#015 97%|█████████▋| 10299/10570 [00:28<00:00, 368.74it/s][1,6]<stderr>:#015100%|██████████| 10570/10570 [00:28<00:00, 370.33it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▎| 9906/10570 [00:28<00:01, 354.99it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10336/10570 [00:28<00:00, 368.63it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▍| 9945/10570 [00:28<00:01, 362.73it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10373/10570 [00:28<00:00, 368.91it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 94%|█████████▍| 9982/10570 [00:28<00:01, 353.77it/s]#033[A[1,13]<stderr>:#015 98%|█████████▊| 10410/10570 [00:28<00:00, 356.75it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 95%|█████████▍| 10020/10570 [00:28<00:01, 358.73it/s]#033[A[1,13]<stderr>:#015 99%|█████████▉| 10446/10570 [00:28<00:00, 357.64it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 95%|█████████▌| 10058/10570 [00:28<00:01, 363.35it/s]#033[A[1,13]<stderr>:#015 99%|█████████▉| 10482/10570 [00:28<00:00, 355.03it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10095/10570 [00:28<00:01, 359.48it/s]#033[A[1,13]<stderr>:#015100%|█████████▉| 10521/10570 [00:29<00:00, 363.21it/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10133/10570 [00:28<00:01, 364.23it/s]#033[A[1,13]<stderr>:#015100%|█████████▉| 10560/10570 [00:29<00:00, 368.81it/s][1,13]<stderr>:#015100%|██████████| 10570/10570 [00:29<00:00, 361.93it/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 96%|█████████▌| 10170/10570 [00:28<00:01, 351.04it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10207/10570 [00:28<00:01, 355.40it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10245/10570 [00:28<00:00, 360.32it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 97%|█████████▋| 10282/10570 [00:29<00:00, 357.03it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10319/10570 [00:29<00:00, 358.80it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10356/10570 [00:29<00:00, 359.37it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 98%|█████████▊| 10392/10570 [00:29<00:00, 354.28it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▊| 10428/10570 [00:29<00:00, 351.92it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 10464/10570 [00:29<00:00, 346.91it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015 99%|█████████▉| 10501/10570 [00:29<00:00, 350.89it/s]#033[A[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|█████████▉| 10539/10570 [00:29<00:00, 357.21it/s]#033[A[1,8]<stderr>:#015100%|██████████| 10570/10570 [00:29<00:00, 353.51it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 169/169 [00:59<00:00,  2.86it/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:907] 2021-09-05 08:03:12,263 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   epoch        =   0.07\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   eval_samples =  10784\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   exact_match  = 0.3217\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[INFO|trainer_pt_utils.py:912] 2021-09-05 08:03:12,263 >>   f1           = 4.4442\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 169/169 [01:01<00:00,  2.74it/s]\n",
      "\u001b[0m\n",
      "\u001b[35m2021-09-05 08:03:49,459 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-09-05 08:03:49,459 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "# Please replace the old_training_job_name with your training job\n",
    "old_training_job_name='tensorflow-training-2021-09-03-05-13-29-338'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-09-03 05:25:37 Starting - Preparing the instances for training\n",
      "2021-09-03 05:25:37 Downloading - Downloading input data\n",
      "2021-09-03 05:25:37 Training - Training image download completed. Training in progress.\n",
      "2021-09-03 05:25:37 Uploading - Uploading generated training model\n",
      "2021-09-03 05:25:37 Completed - Training job completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-029438132834/tensorflow-training-2021-09-03-05-13-29-338/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
